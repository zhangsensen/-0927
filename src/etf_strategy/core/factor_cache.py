"""
Factor Cache | å› å­è®¡ç®—ç¼“å­˜å±‚

é¿å… WFO/VEC/BT ä¸‰å¼•æ“é‡å¤è®¡ç®—å› å­ï¼ŒåŸºäº mtime è‡ªåŠ¨å¤±æ•ˆã€‚

ä½œè€…: Sensen
æ—¥æœŸ: 2026-02-05
"""

from __future__ import annotations

import hashlib
import logging
import pickle
import time
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class FactorCache:
    """å› å­è®¡ç®—ç¼“å­˜ â€” é¿å… WFO/VEC/BT é‡å¤è®¡ç®—"""

    def __init__(self, cache_dir: Optional[Path] = None):
        if cache_dir is None:
            cache_dir = Path(__file__).parent.parent / ".cache"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _get_source_mtime(self) -> float:
        """è·å–å› å­åº“å’Œæ ‡å‡†åŒ–ä»£ç çš„æœ€æ–° mtime"""
        source_files = [
            Path(__file__).parent / "precise_factor_library_v2.py",
            Path(__file__).parent / "cross_section_processor.py",
        ]
        mtimes = []
        for f in source_files:
            try:
                mtimes.append(f.stat().st_mtime)
            except OSError:
                # æ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›0ï¼Œå¼ºåˆ¶ç¼“å­˜miss
                mtimes.append(0)
        return max(mtimes) if mtimes else 0

    def _get_data_mtime(self, data_dir: Path) -> float:
        """è·å–OHLCVæ•°æ®ç›®å½•çš„æœ€æ–° mtime"""
        try:
            parquet_files = list(data_dir.glob("*.parquet"))
            if parquet_files:
                return max(f.stat().st_mtime for f in parquet_files)
            return data_dir.stat().st_mtime
        except OSError:
            logger.warning(f"æ— æ³•è·å–æ•°æ®ç›®å½• {data_dir} çš„ä¿®æ”¹æ—¶é—´")
            return 0

    def _generate_cache_key(
        self,
        data_dir: Path,
        etf_codes: List[str],
        start_date: str,
        end_date: str,
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”® (åŒ…å«æ•°æ®mtime + æºç mtime)"""
        codes_str = "-".join(sorted(etf_codes))
        data_mtime = int(self._get_data_mtime(data_dir))
        source_mtime = int(self._get_source_mtime())
        key_str = f"factors_{codes_str}_{start_date}_{end_date}_{data_mtime}_{source_mtime}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def get_or_compute(
        self,
        ohlcv: Dict[str, pd.DataFrame],
        config: dict,
        data_dir: Path,
    ) -> Dict:
        """
        è·å–æˆ–è®¡ç®—å› å­æ•°æ® (å¸¦ç¼“å­˜)

        Returns:
            {
                "std_factors": dict[str, pd.DataFrame],
                "factor_names": list[str],
                "factors_3d": np.ndarray,  # (T, N, F)
                "dates": pd.DatetimeIndex,
                "etf_codes": list[str],
            }
        """
        # ä»å®é™… OHLCV æ•°æ®æ¨å¯¼æ—¥æœŸèŒƒå›´ (é¿å… config ä¸­ training_end_date/end_date æ­§ä¹‰)
        close_df = ohlcv["close"]
        etf_codes = list(close_df.columns)
        actual_start = str(close_df.index[0].date())
        actual_end = str(close_df.index[-1].date())
        n_days = len(close_df)

        cache_key = self._generate_cache_key(data_dir, etf_codes, actual_start, actual_end)
        cache_file = self.cache_dir / f"factor_cache_{cache_key}.pkl"

        # å°è¯•è¯»å–ç¼“å­˜
        if cache_file.exists():
            try:
                t0 = time.time()
                with open(cache_file, "rb") as f:
                    cached = pickle.load(f)
                elapsed = time.time() - t0
                logger.info(
                    f"âœ… Loading cached factors: {cache_file.name} ({elapsed:.1f}s)"
                )
                return cached
            except Exception as e:
                logger.warning(f"å› å­ç¼“å­˜è¯»å–å¤±è´¥ï¼Œå°†é‡æ–°è®¡ç®—: {e}")

        # ç¼“å­˜æœªå‘½ä¸­ â€” è®¡ç®—å› å­
        logger.info("ğŸ”§ å› å­ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹è®¡ç®—...")
        t0 = time.time()

        from .cross_section_processor import CrossSectionProcessor
        from .precise_factor_library_v2 import PreciseFactorLibrary

        factor_lib = PreciseFactorLibrary()
        raw_factors_df = factor_lib.compute_all_factors(prices=ohlcv)

        # æå–å› å­å­—å…¸
        factor_names_list = raw_factors_df.columns.get_level_values(0).unique().tolist()
        raw_factors = {fname: raw_factors_df[fname] for fname in factor_names_list}

        # æ¨ªæˆªé¢æ ‡å‡†åŒ–
        cross_section_cfg = config.get("cross_section", {})
        processor = CrossSectionProcessor(
            lower_percentile=cross_section_cfg.get("winsorize_lower", 0.025) * 100,
            upper_percentile=cross_section_cfg.get("winsorize_upper", 0.975) * 100,
            verbose=False,
        )
        std_factors = processor.process_all_factors(raw_factors)

        # ç»„ç»‡è¾“å‡º
        factor_names = sorted(std_factors.keys())
        first_factor = std_factors[factor_names[0]]
        dates = first_factor.index
        etf_cols = first_factor.columns.tolist()
        factors_3d = np.stack([std_factors[f].values for f in factor_names], axis=-1)

        result = {
            "std_factors": std_factors,
            "factor_names": factor_names,
            "factors_3d": factors_3d,
            "dates": dates,
            "etf_codes": etf_cols,
        }

        # å†™å…¥ç¼“å­˜
        try:
            with open(cache_file, "wb") as f:
                pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)
            size_mb = cache_file.stat().st_size / (1024 * 1024)
            elapsed = time.time() - t0
            logger.info(
                f"âœ… å› å­ç¼“å­˜å·²å†™å…¥: {cache_file.name} ({size_mb:.1f}MB, {elapsed:.1f}s)"
            )
        except Exception as e:
            logger.warning(f"å› å­ç¼“å­˜å†™å…¥å¤±è´¥ (ä¸å½±å“è¿è¡Œ): {e}")

        return result
