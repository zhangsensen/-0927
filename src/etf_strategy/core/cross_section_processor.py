"""
è·¨æˆªé¢æ ‡å‡†åŒ–å¤„ç†æ¨¡å— | Cross-Section Standardization Processor

åŠŸèƒ½:
  1. å¯¹æ— ç•Œå› å­æ‰§è¡Œæ—¥æœŸå†…æ¨ªæˆªé¢ Z-score æ ‡å‡†åŒ–
  2. å¯¹æ— ç•Œå› å­æ‰§è¡Œ Winsorize æå€¼æˆªæ–­ (2.5%, 97.5%)
  3. å¯¹æœ‰ç•Œå› å­ç›´æ¥é€ä¼ ï¼Œä¸åšä»»ä½•å¤„ç†
  4. ä¸¥æ ¼ä¿ç•™ NaNï¼Œä¸åšä»»ä½•å¡«å……

å·¥ä½œæµ:
  åŸå§‹å› å­çŸ©é˜µ (date Ã— symbol Ã— factor)
     â†“
  CrossSectionProcessor.process_all_factors()
     â†“
  æ ‡å‡†åŒ–å› å­çŸ©é˜µ (meanâ‰ˆ0, stdâ‰ˆ1, æ— æç«¯å€¼, NaNä¿ç•™)
     â†“
  Step 4: ICè®¡ç®— & WFOç­›é€‰

ä½œè€…: Step 3 Implementation
æ—¥æœŸ: 2025-10-26
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm


@dataclass
class FactorMetadata:
    """å› å­å…ƒæ•°æ®"""

    name: str
    description: str
    bounded: bool
    bounds: Optional[Tuple[float, float]] = None

    def __repr__(self):
        bound_str = (
            f" [{self.bounds[0]:.1f}, {self.bounds[1]:.1f}]"
            if self.bounded
            else " (æ— ç•Œ)"
        )
        return f"{self.name}: {self.description}{bound_str}"


class CrossSectionProcessor:
    """
    è·¨æˆªé¢æ ‡å‡†åŒ–å¤„ç†å™¨

    è´Ÿè´£å°†åŸå§‹å› å­çŸ©é˜µè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼ŒåŒ…æ‹¬ï¼š
    1. Z-score æ ‡å‡†åŒ– (æ¨ªæˆªé¢)
    2. Winsorize æå€¼æˆªæ–­ (æ— ç•Œå› å­)
    3. æœ‰ç•Œå› å­é€ä¼ ä¿æŠ¤
    4. NaN ä¸¥æ ¼ä¿ç•™

    å±æ€§:
        lower_percentile (float): ä¸‹æˆªæ–­åˆ†ä½æ•° (é»˜è®¤ 2.5%)
        upper_percentile (float): ä¸Šæˆªæ–­åˆ†ä½æ•° (é»˜è®¤ 97.5%)
        verbose (bool): æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        factor_metadata (Dict): å› å­å…ƒæ•°æ®
        processing_report (Dict): å¤„ç†æŠ¥å‘Š
    """

    # æœ‰ç•Œå› å­åå•ï¼ˆå¿…é¡»ä¸ precise_factor_library_v2.py çš„ bounded=True ä¿æŒåŒæ­¥ï¼‰
    BOUNDED_FACTORS = {
        "PRICE_POSITION_20D",
        "PRICE_POSITION_120D",
        "PV_CORR_20D",
        "RSI_14",
    }

    # æœ‰ç•Œå› å­çš„å€¼åŸŸ
    FACTOR_BOUNDS = {
        "PRICE_POSITION_20D": (0.0, 1.0),
        "PRICE_POSITION_120D": (0.0, 1.0),
        "PV_CORR_20D": (-1.0, 1.0),
        "RSI_14": (0.0, 100.0),
        "ADX_14D": (0.0, 100.0),
        "CMF_20D": (-1.0, 1.0),
        "CORRELATION_TO_MARKET_20D": (-1.0, 1.0),
    }

    def __init__(
        self,
        lower_percentile: float = 2.5,
        upper_percentile: float = 97.5,
        verbose: bool = True,
    ):
        """
        åˆå§‹åŒ–è·¨æˆªé¢å¤„ç†å™¨

        å‚æ•°:
            lower_percentile: ä¸‹æˆªæ–­åˆ†ä½æ•° (é»˜è®¤ 2.5%)
            upper_percentile: ä¸Šæˆªæ–­åˆ†ä½æ•° (é»˜è®¤ 97.5%)
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.lower_percentile = lower_percentile
        self.upper_percentile = upper_percentile
        self.verbose = verbose

        # æ„å»ºå› å­å…ƒæ•°æ®
        self.factor_metadata = self._build_metadata()

        # å¤„ç†æŠ¥å‘Š
        self.processing_report = {
            "timestamp": None,
            "factors_processed": [],
            "standardization_stats": {},
            "winsorization_stats": {},
            "bounded_factors_passed": [],
            "nan_stats": {},
            "warnings": [],
        }

    def _build_metadata(self) -> Dict[str, FactorMetadata]:
        """æ„å»ºå› å­å…ƒæ•°æ®"""
        metadata = {
            # æ— ç•Œå› å­
            "MOM_20D": FactorMetadata("MOM_20D", "20æ—¥åŠ¨é‡ç™¾åˆ†æ¯”", bounded=False),
            "SLOPE_20D": FactorMetadata("SLOPE_20D", "20æ—¥çº¿æ€§å›å½’æ–œç‡", bounded=False),
            "RET_VOL_20D": FactorMetadata(
                "RET_VOL_20D", "20æ—¥æ”¶ç›Šæ³¢åŠ¨ç‡", bounded=False
            ),
            "MAX_DD_60D": FactorMetadata("MAX_DD_60D", "60æ—¥æœ€å¤§å›æ’¤", bounded=False),
            "VOL_RATIO_20D": FactorMetadata(
                "VOL_RATIO_20D", "20æ—¥æˆäº¤é‡æ¯”ç‡", bounded=False
            ),
            "VOL_RATIO_60D": FactorMetadata(
                "VOL_RATIO_60D", "60æ—¥æˆäº¤é‡æ¯”ç‡", bounded=False
            ),
            # æœ‰ç•Œå› å­
            "PRICE_POSITION_20D": FactorMetadata(
                "PRICE_POSITION_20D", "20æ—¥ä»·æ ¼ä½ç½®", bounded=True, bounds=(0.0, 1.0)
            ),
            "PRICE_POSITION_120D": FactorMetadata(
                "PRICE_POSITION_120D", "120æ—¥ä»·æ ¼ä½ç½®", bounded=True, bounds=(0.0, 1.0)
            ),
            "PV_CORR_20D": FactorMetadata(
                "PV_CORR_20D", "20æ—¥ä»·é‡ç›¸å…³æ€§", bounded=True, bounds=(-1.0, 1.0)
            ),
            "RSI_14": FactorMetadata(
                "RSI_14", "14æ—¥ç›¸å¯¹å¼ºåº¦", bounded=True, bounds=(0.0, 100.0)
            ),
        }
        return metadata

    def standardize_factor(self, factor: pd.Series) -> Tuple[pd.Series, Dict]:
        """
        å¯¹å•ä¸ªå› å­æ‰§è¡Œæ¨ªæˆªé¢ Z-score æ ‡å‡†åŒ–

        å‚æ•°:
            factor: å› å­åºåˆ— (index: symbols, values: factor values)

        è¿”å›:
            standardized: æ ‡å‡†åŒ–åçš„å› å­åºåˆ—
            stats: æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯

        è¯´æ˜:
            - NaN ä¿æŒ NaN
            - æ— æ•ˆæ•°æ®ï¼ˆinfï¼‰è¢«æ ‡è®°ä¸º NaN
            - æ ‡å‡†åŒ–å…¬å¼: (x - mean) / std
        """
        stats = {
            "count": factor.notna().sum(),
            "original_mean": factor.mean(),
            "original_std": factor.std(),
            "nan_count": factor.isna().sum(),
            "inf_count": np.isinf(pd.to_numeric(factor, errors="coerce")).sum(),
            "standardized_mean": None,
            "standardized_std": None,
        }

        # ç§»é™¤æ— æ•ˆå€¼ (inf)ï¼Œè½¬æ¢ä¸º numeric
        factor_numeric = pd.to_numeric(factor, errors="coerce")
        factor_clean = factor_numeric.replace([np.inf, -np.inf], np.nan)

        # æ›´æ–°æœ‰æ•ˆæ•°æ®countï¼ˆç§»é™¤infåï¼‰
        valid_count = factor_clean.count()

        if valid_count < 2:
            # æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ ‡å‡†åŒ–
            if self.verbose:
                print(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ•°æ®ä¸è¶³ ({valid_count} < 2)ï¼Œæ— æ³•æ ‡å‡†åŒ–")
            self.processing_report["warnings"].append(
                f"å› å­æœ‰æ•ˆæ•°æ®ä¸è¶³ (count={valid_count})"
            )
            stats["count"] = valid_count
            return factor_clean, stats

        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        mean = factor_clean.mean()
        std = factor_clean.std()

        if std == 0 or np.isnan(std):
            # æ–¹å·®ä¸º0ï¼Œæ— æ³•æ ‡å‡†åŒ–ï¼Œè¿”å›åŸå€¼
            if self.verbose:
                print(f"âš ï¸ è­¦å‘Š: æ ‡å‡†å·®ä¸º0ï¼Œæ— æ³•æ ‡å‡†åŒ–")
            self.processing_report["warnings"].append("å› å­æ–¹å·®ä¸º0ï¼Œæ— æ³•æ ‡å‡†åŒ–")
            stats["standardized_mean"] = mean
            stats["standardized_std"] = 0.0
            return factor_clean, stats

        # æ‰§è¡Œ Z-score æ ‡å‡†åŒ–
        standardized = (factor_clean - mean) / std

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats["standardized_mean"] = standardized.mean()
        stats["standardized_std"] = standardized.std()

        return standardized, stats

    def winsorize_factor(self, factor: pd.Series) -> Tuple[pd.Series, Dict]:
        """
        å¯¹å•ä¸ªå› å­æ‰§è¡Œ Winsorize æå€¼æˆªæ–­

        å‚æ•°:
            factor: å› å­åºåˆ—

        è¿”å›:
            winsorized: æˆªæ–­åçš„å› å­åºåˆ—
            stats: æˆªæ–­ç»Ÿè®¡ä¿¡æ¯

        è¯´æ˜:
            - è®¡ç®—ä¸‹ä¸Šåˆ†ä½æ•°ï¼Œå°†æç«¯å€¼æ›¿æ¢ä¸ºåˆ†ä½æ•°å€¼
            - NaN ä¿æŒ NaNï¼Œä¸å‚ä¸æˆªæ–­
            - æˆªæ–­åçš„å€¼è¢«é™åˆ¶åœ¨ [lower, upper] èŒƒå›´å†…
        """
        stats = {
            "count": factor.notna().sum(),
            "lower_percentile": self.lower_percentile,
            "upper_percentile": self.upper_percentile,
            "lower_bound": None,
            "upper_bound": None,
            "clipped_count": 0,
            "nan_count": factor.isna().sum(),
        }

        if stats["count"] < 2:
            # æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æˆªæ–­
            if self.verbose:
                print(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ•°æ®ä¸è¶³ ({stats['count']} < 2)ï¼Œè·³è¿‡ Winsorize")
            return factor, stats

        # è®¡ç®—åˆ†ä½æ•°
        lower_bound = factor.quantile(self.lower_percentile / 100)
        upper_bound = factor.quantile(self.upper_percentile / 100)

        stats["lower_bound"] = lower_bound
        stats["upper_bound"] = upper_bound

        # è®¡ç®—éœ€è¦è¢«æˆªæ–­çš„å€¼çš„ä¸ªæ•°
        lower_clip = (factor < lower_bound).sum()
        upper_clip = (factor > upper_bound).sum()
        stats["clipped_count"] = lower_clip + upper_clip

        # æ‰§è¡Œæˆªæ–­
        winsorized = factor.clip(lower=lower_bound, upper=upper_bound)

        return winsorized, stats

    def process_factor(
        self, factor_name: str, factor: pd.Series
    ) -> Tuple[pd.Series, Dict]:
        """
        å¤„ç†å•ä¸ªå› å­

        å‚æ•°:
            factor_name: å› å­åç§°
            factor: å› å­åºåˆ—

        è¿”å›:
            processed: å¤„ç†åçš„å› å­åºåˆ—
            process_stats: å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        process_stats = {
            "factor_name": factor_name,
            "is_bounded": factor_name in self.BOUNDED_FACTORS,
            "steps": [],
        }

        # 1. æ£€æŸ¥æœ‰ç•Œæ€§
        if factor_name in self.BOUNDED_FACTORS:
            # æœ‰ç•Œå› å­ï¼šç›´æ¥é€ä¼ 
            process_stats["steps"].append("passed_through")
            if self.verbose:
                print(f"  âœ“ {factor_name:20s} [æœ‰ç•Œ] ç›´æ¥é€ä¼ ")
            return factor, process_stats

        # 2. æ— ç•Œå› å­ï¼šæ‰§è¡Œæ ‡å‡†åŒ–
        standardized, std_stats = self.standardize_factor(factor)
        process_stats["standardization_stats"] = std_stats
        process_stats["steps"].append("standardized")

        if self.verbose and std_stats["standardized_mean"] is not None:
            print(
                f"  âœ“ {factor_name:20s} Z-scoreæ ‡å‡†åŒ–: mean={std_stats['standardized_mean']:.4f}, std={std_stats['standardized_std']:.4f}"
            )

        # 3. æ— ç•Œå› å­ï¼šæ‰§è¡Œ Winsorize
        winsorized, win_stats = self.winsorize_factor(standardized)
        process_stats["winsorization_stats"] = win_stats
        process_stats["steps"].append("winsorized")

        if self.verbose:
            clipped = win_stats["clipped_count"]
            total = win_stats["count"]
            pct = 100 * clipped / total if total > 0 else 0
            print(
                f"  âœ“ {factor_name:20s} Winsorize [{self.lower_percentile}%, {self.upper_percentile}%]: æˆªæ–­ {clipped}/{total} ({pct:.2f}%)"
            )

        return winsorized, process_stats

    def process_all_factors(
        self, factors_dict: Dict[str, pd.DataFrame]
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰å› å­ï¼ˆå®Œå…¨å‘é‡åŒ– - æ— é€æ—¥æœŸå¾ªç¯ï¼‰

        å‚æ•°:
            factors_dict: å› å­å­—å…¸
                key: å› å­åç§°
                value: å› å­ DataFrame (index: date, columns: symbols)

        è¿”å›:
            processed_factors: å¤„ç†åçš„å› å­å­—å…¸

        å®ç°ï¼š
            - DataFrameçº§åˆ«çš„axis=1æ“ä½œï¼ˆæ¨ªæˆªé¢ï¼‰
            - ä½¿ç”¨quantileã€clipã€nanmeanã€nanstdç­‰å‘é‡åŒ–æ–¹æ³•
            - O(TÃ—N)å¤æ‚åº¦ï¼Œæ— Pythonå¾ªç¯
        """
        self.processing_report["timestamp"] = datetime.now().isoformat()

        if self.verbose:
            print("\n" + "=" * 70)
            print("è·¨æˆªé¢æ ‡å‡†åŒ–å¤„ç† | Cross-Section Standardization (å‘é‡åŒ–)")
            print("=" * 70)

        processed_factors = {}

        factor_items = list(factors_dict.items())
        for factor_name, factor_data in tqdm(
            factor_items,
            desc="æ¨ªæˆªé¢æ ‡å‡†åŒ–",
            unit="å› å­",
            ncols=80,
            disable=not self.verbose,
        ):
            if self.verbose:
                print(f"\nğŸ“Š å¤„ç†å› å­: {factor_name}")

            # ç»Ÿè®¡NaNæ•°é‡
            original_nan_count = factor_data.isna().sum().sum()

            # æ£€æŸ¥æœ‰ç•Œæ€§
            if factor_name in self.BOUNDED_FACTORS:
                # æœ‰ç•Œå› å­ï¼šç›´æ¥é€ä¼ 
                processed_factors[factor_name] = factor_data
                if self.verbose:
                    print(f"  âœ“ {factor_name:20s} [æœ‰ç•Œ] ç›´æ¥é€ä¼ ")

                self.processing_report["factors_processed"].append(factor_name)
                self.processing_report["nan_stats"][factor_name] = {
                    "original_nan_count": original_nan_count,
                    "final_nan_count": original_nan_count,
                    "nan_preserved": True,
                }
                continue

            # æ— ç•Œå› å­ï¼šæ‰§è¡Œå‘é‡åŒ–æ ‡å‡†åŒ– + Winsorize
            # Step 1: Z-scoreæ ‡å‡†åŒ–ï¼ˆaxis=1 = æ¨ªæˆªé¢ï¼‰
            mean_cs = factor_data.mean(axis=1, skipna=True)
            std_cs = factor_data.std(axis=1, skipna=True)

            # æ£€æµ‹é›¶æ–¹å·®æ—¥æœŸï¼ˆæ‰€æœ‰æ ‡çš„å€¼ç›¸åŒï¼‰
            zero_var_dates = std_cs < 1e-10
            if zero_var_dates.any():
                num_zero_var = zero_var_dates.sum()
                msg = f"{factor_name}: {num_zero_var}ä¸ªæ—¥æœŸæ–¹å·®ä¸º0ï¼ˆæ‰€æœ‰æ ‡çš„å€¼ç›¸åŒï¼‰"
                self.processing_report["warnings"].append(msg)
                if self.verbose:
                    print(f"  âš ï¸ {msg}")

            # å¹¿æ’­å‡å‡å€¼é™¤æ ‡å‡†å·®ï¼ˆé›¶æ–¹å·®æ—¥æœŸä¼šäº§ç”ŸNaNï¼‰
            standardized = factor_data.sub(mean_cs, axis=0).div(std_cs, axis=0)

            # Step 2: Winsorizeï¼ˆæ¨ªæˆªé¢åˆ†ä½æ•°è£å‰ªï¼‰
            # è®¡ç®—æ¯è¡Œï¼ˆæ—¥æœŸï¼‰çš„åˆ†ä½æ•°
            lower_bound = standardized.quantile(self.lower_percentile / 100, axis=1)
            upper_bound = standardized.quantile(self.upper_percentile / 100, axis=1)

            # å¹¿æ’­è£å‰ª
            winsorized = standardized.clip(lower=lower_bound, upper=upper_bound, axis=0)

            processed_factors[factor_name] = winsorized

            # ç»Ÿè®¡ä¿¡æ¯
            final_nan_count = winsorized.isna().sum().sum()
            self.processing_report["factors_processed"].append(factor_name)
            self.processing_report["nan_stats"][factor_name] = {
                "original_nan_count": original_nan_count,
                "final_nan_count": final_nan_count,
                "nan_preserved": (original_nan_count == final_nan_count),
            }

            if self.verbose:
                # è®¡ç®—è¢«è£å‰ªçš„å€¼æ•°é‡
                clipped_lower = (standardized < lower_bound.values[:, None]).sum().sum()
                clipped_upper = (standardized > upper_bound.values[:, None]).sum().sum()
                clipped_total = clipped_lower + clipped_upper
                total_valid = standardized.notna().sum().sum()
                pct = 100 * clipped_total / total_valid if total_valid > 0 else 0

                print(f"  âœ“ {factor_name:20s} Z-scoreæ ‡å‡†åŒ– + Winsorize")
                print(
                    f"    æˆªæ–­ {clipped_total}/{total_valid} ({pct:.2f}%) "
                    f"[{self.lower_percentile}%, {self.upper_percentile}%]"
                )
                print(f"    NaNä¿ç•™: {original_nan_count} â†’ {final_nan_count}")

        if self.verbose:
            print("\n" + "=" * 70)
            print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_factors)} ä¸ªå› å­")
            print("=" * 70 + "\n")

        return processed_factors

    def get_metadata(self, factor_name: str) -> FactorMetadata:
        """è·å–å› å­å…ƒæ•°æ®"""
        return self.factor_metadata.get(factor_name)

    def list_bounded_factors(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰æœ‰ç•Œå› å­"""
        return list(self.BOUNDED_FACTORS)

    def list_unbounded_factors(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰æ— ç•Œå› å­"""
        unbounded = set(self.factor_metadata.keys()) - self.BOUNDED_FACTORS
        return sorted(list(unbounded))

    def get_factor_bounds(self, factor_name: str) -> Optional[Tuple[float, float]]:
        """è·å–æœ‰ç•Œå› å­çš„å€¼åŸŸ"""
        return self.FACTOR_BOUNDS.get(factor_name)

    def process_all_factors_split_pool(
        self,
        factors_dict: Dict[str, pd.DataFrame],
        pool_definitions: Dict[str, list],
    ) -> Dict[str, pd.DataFrame]:
        """Process factors with per-pool normalization.

        Instead of normalizing across all symbols, normalize WITHIN each pool.
        This prevents QDII ETFs from always dominating unbounded factor rankings.

        Parameters
        ----------
        factors_dict : dict
            Same as ``process_all_factors``.
        pool_definitions : dict
            Mapping of pool_name -> list of symbol codes.
            Example: {"qdii": ["513100", ...], "a_share": ["510300", ...]}

        Returns
        -------
        dict : processed factors with per-pool normalization applied.
        """
        self.processing_report["timestamp"] = datetime.now().isoformat()

        if self.verbose:
            print("\n" + "=" * 70)
            print("è·¨æˆªé¢æ ‡å‡†åŒ–å¤„ç† | Split-Pool Cross-Section (å‘é‡åŒ–)")
            print("=" * 70)
            for pool_name, symbols in pool_definitions.items():
                print(f"  Pool '{pool_name}': {len(symbols)} symbols")

        processed_factors = {}

        for factor_name, factor_data in factors_dict.items():
            if self.verbose:
                print(f"\nğŸ“Š å¤„ç†å› å­: {factor_name}")

            if factor_name in self.BOUNDED_FACTORS:
                processed_factors[factor_name] = factor_data
                if self.verbose:
                    print(f"  âœ“ {factor_name:20s} [æœ‰ç•Œ] ç›´æ¥é€ä¼ ")
                continue

            # Per-pool Z-score + Winsorize, then merge back
            result_df = factor_data.copy()
            result_df[:] = np.nan  # Start with NaN

            for pool_name, pool_symbols in pool_definitions.items():
                # Select only columns present in factor_data
                pool_cols = [s for s in pool_symbols if s in factor_data.columns]
                if len(pool_cols) < 2:
                    # Not enough symbols for cross-section normalization
                    if pool_cols:
                        result_df[pool_cols] = factor_data[pool_cols]
                    continue

                pool_data = factor_data[pool_cols]

                # Z-score within pool (axis=1)
                mean_cs = pool_data.mean(axis=1, skipna=True)
                std_cs = pool_data.std(axis=1, skipna=True)
                std_cs = std_cs.replace(0, np.nan)

                standardized = pool_data.sub(mean_cs, axis=0).div(std_cs, axis=0)

                # Winsorize within pool
                lower_bound = standardized.quantile(
                    self.lower_percentile / 100, axis=1
                )
                upper_bound = standardized.quantile(
                    self.upper_percentile / 100, axis=1
                )
                winsorized = standardized.clip(
                    lower=lower_bound, upper=upper_bound, axis=0
                )

                result_df[pool_cols] = winsorized

            processed_factors[factor_name] = result_df

            if self.verbose:
                print(f"  âœ“ {factor_name:20s} Split-pool Z-score + Winsorize")

        if self.verbose:
            print("\n" + "=" * 70)
            print(f"âœ… Split-pool å¤„ç†å®Œæˆ: {len(processed_factors)} ä¸ªå› å­")
            print("=" * 70 + "\n")

        return processed_factors

    def get_report(self) -> Dict:
        """è·å–å¤„ç†æŠ¥å‘Š"""
        return self.processing_report

    def print_summary(self):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        print("\n" + "=" * 70)
        print("å¤„ç†æ€»ç»“ | Processing Summary")
        print("=" * 70)

        print(f"\nğŸ“Š å› å­å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»æ•°: {len(self.processing_report['factors_processed'])}")
        print(f"  æœ‰ç•Œå› å­: {len(self.BOUNDED_FACTORS)} (ç›´æ¥é€ä¼ )")
        print(
            f"  æ— ç•Œå› å­: {len(self.processing_report['factors_processed']) - len(self.BOUNDED_FACTORS)} (æ ‡å‡†åŒ–+æˆªæ–­)"
        )

        if self.processing_report["warnings"]:
            print(f"\nâš ï¸ è­¦å‘Š ({len(self.processing_report['warnings'])}):")
            for warning in self.processing_report["warnings"]:
                print(f"  - {warning}")
        else:
            print(f"\nâœ… æ— è­¦å‘Š")

        print("\n" + "=" * 70 + "\n")


def create_sample_factors() -> Dict[str, pd.DataFrame]:
    """
    åˆ›å»ºç¤ºä¾‹å› å­çŸ©é˜µä¾›æµ‹è¯•ä½¿ç”¨

    è¿”å›:
        factors: å› å­å­—å…¸
            key: å› å­åç§°
            value: DataFrame (index: date, columns: symbols)
    """
    np.random.seed(42)

    dates = pd.date_range("2025-01-01", periods=20)
    symbols = [f"ETF{i:02d}" for i in range(30)]

    factors = {}

    # æ— ç•Œå› å­
    for factor_name in [
        "MOM_20D",
        "SLOPE_20D",
        "RET_VOL_20D",
        "MAX_DD_60D",
        "VOL_RATIO_20D",
        "VOL_RATIO_60D",
    ]:
        data = (
            np.random.randn(len(dates), len(symbols)) * 10
            + np.random.randn(len(symbols)) * 5
        )
        df = pd.DataFrame(data, index=dates, columns=symbols)
        # éšæœºæ’å…¥ NaN
        mask = np.random.random((len(dates), len(symbols))) < 0.05
        df[mask] = np.nan
        factors[factor_name] = df

    # æœ‰ç•Œå› å­
    factors["PRICE_POSITION_20D"] = pd.DataFrame(
        np.random.rand(len(dates), len(symbols)), index=dates, columns=symbols
    )
    factors["PRICE_POSITION_120D"] = pd.DataFrame(
        np.random.rand(len(dates), len(symbols)), index=dates, columns=symbols
    )
    factors["PV_CORR_20D"] = pd.DataFrame(
        np.random.uniform(-1, 1, (len(dates), len(symbols))),
        index=dates,
        columns=symbols,
    )
    factors["RSI_14"] = pd.DataFrame(
        np.random.uniform(0, 100, (len(dates), len(symbols))),
        index=dates,
        columns=symbols,
    )

    return factors


if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("ç¤ºä¾‹: CrossSectionProcessor ä½¿ç”¨")

    # åˆ›å»ºç¤ºä¾‹å› å­
    factors = create_sample_factors()

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = CrossSectionProcessor(verbose=True)

    # å¤„ç†å› å­
    processed = processor.process_all_factors(factors)

    # æ‰“å°æ€»ç»“
    processor.print_summary()

    # éªŒè¯ç»“æœ
    print("\néªŒè¯:")
    for factor_name, factor_df in processed.items():
        metadata = processor.get_metadata(factor_name)
        if metadata.bounded:
            print(
                f"âœ“ {factor_name:20s} [æœ‰ç•Œ] min={factor_df.min().min():.4f}, max={factor_df.max().max():.4f}"
            )
        else:
            print(
                f"âœ“ {factor_name:20s} [æ— ç•Œ] mean={factor_df.mean().mean():.4f}, std={factor_df.std().mean():.4f}"
            )
