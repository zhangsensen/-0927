#!/usr/bin/env python3
"""
ç»„åˆçº§WFOä¼˜åŒ–å¯åŠ¨è„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½OHLCVæ•°æ®
2. è®¡ç®—ç²¾ç¡®å› å­åº“
3. æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤„ç†
4. æ‰§è¡Œç»„åˆçº§Walk-Forwardä¼˜åŒ–
5. ä¿å­˜Topç»„åˆåˆ° results/run_XXXXXX/

è¾“å‡ºï¼š
- results/run_XXXXXX/top100_by_ic.parquet
- results/run_XXXXXX/all_combos.parquet
- results/run_XXXXXX/wfo_summary.json

ç”¨æ³•ï¼š
    python run_combo_wfo.py
"""

import json
import logging
import os
import sys
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

# Ensure src/ is on sys.path when running as a script
ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT / "src"))

from etf_strategy.core.combo_wfo_optimizer import (
    ComboWFOOptimizer,
    warmup_numba_kernels,
)
from etf_strategy.core.cross_section_processor import CrossSectionProcessor
from etf_strategy.core.data_loader import DataLoader
from etf_strategy.core.factor_cache import FactorCache
from etf_strategy.core.cost_model import load_cost_model, build_cost_array
from etf_strategy.core.frozen_params import load_frozen_config, FrozenETFPool
from etf_strategy.core.precise_factor_library_v2 import PreciseFactorLibrary
from etf_strategy.regime_gate import compute_regime_gate_arr, gate_stats

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""

    # ========== 1. åŠ è½½é…ç½® ==========
    logger.info("=" * 100)
    logger.info("ğŸš€ ç»„åˆçº§WFOä¼˜åŒ–å¯åŠ¨")
    logger.info("=" * 100)
    logger.info("")

    config_path = Path(
        os.environ.get("WFO_CONFIG_PATH", "configs/combo_wfo_config.yaml")
    )
    if not config_path.exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)

    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    frozen = load_frozen_config(config, config_path=str(config_path))
    logger.info(f"ğŸ”’ å‚æ•°å†»ç»“æ ¡éªŒé€šè¿‡ (version={frozen.version})")

    logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    logger.info(f"  - ETFæ•°é‡: {len(config['data']['symbols'])}")
    logger.info(
        f"  - æ—¥æœŸèŒƒå›´: {config['data']['start_date']} â†’ {config['data']['end_date']}"
    )
    logger.info(f"  - ç»„åˆè§„æ¨¡: {config['combo_wfo']['combo_sizes']}")
    logger.info(f"  - ISçª—å£: {config['combo_wfo']['is_period']}å¤©")
    logger.info(f"  - OOSçª—å£: {config['combo_wfo']['oos_period']}å¤©")
    logger.info("")

    # ç¯å¢ƒå˜é‡è¦†ç›–ï¼šå¯é€šè¿‡ RB_FREQ_SUBSET æŒ‡å®šé€—å·åˆ†éš”çš„æ¢ä»“é¢‘ç‡åˆ—è¡¨ï¼ˆä¾‹å¦‚ "8" æˆ– "8,16,24"ï¼‰
    # å¯é€šè¿‡ RB_RESULT_TS æŒ‡å®šè¾“å‡ºç›®å½•çš„æ—¶é—´æˆ³ï¼Œä»¥ä¾¿ä¸å¯åŠ¨è„šæœ¬çš„æ—¥å¿—æ—¶é—´æˆ³ä¸€è‡´ã€‚
    env_freq = os.environ.get("RB_FREQ_SUBSET")
    if env_freq:
        try:
            override = [int(x.strip()) for x in env_freq.split(",") if x.strip()]
            if override:
                config["combo_wfo"]["rebalance_frequencies"] = override
                logger.info(f"ğŸ”§ é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é¢‘ç‡: RB_FREQ_SUBSET={override}")
        except Exception as e:
            logger.warning(f"å¿½ç•¥éæ³•çš„ RB_FREQ_SUBSET å€¼: {env_freq} ({e})")
    env_ts = os.environ.get("RB_RESULT_TS", "").strip()
    if env_ts:
        logger.info(f"ğŸ”§ ä½¿ç”¨å¤–éƒ¨æŒ‡å®šæ—¶é—´æˆ³ RB_RESULT_TS={env_ts}")

    # ========== 2. åŠ è½½æ•°æ® ==========
    logger.info("=" * 100)
    logger.info("ğŸ“Š åŠ è½½OHLCVæ•°æ®")
    logger.info("=" * 100)

    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )

    # ä½¿ç”¨ training_end_date å¦‚æœè®¾ç½®äº†ï¼ˆHoldoutéªŒè¯æ¨¡å¼ï¼‰
    data_end_date = (
        config["data"].get("training_end_date") or config["data"]["end_date"]
    )

    if config["data"].get("training_end_date"):
        logger.info("=" * 100)
        logger.info("ğŸ”¬ HOLDOUTéªŒè¯æ¨¡å¼")
        logger.info("=" * 100)
        logger.info(f"è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸ: {data_end_date}")
        logger.info(f"å®Œæ•´æ•°æ®æˆªæ­¢æ—¥æœŸ: {config['data']['end_date']}")
        logger.info(f"HoldoutæœŸ: {data_end_date} è‡³ {config['data']['end_date']}")
        logger.info("âš ï¸  æ³¨æ„: å½“å‰ä»…ä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼ŒHoldoutæœŸæ•°æ®å°†ç”¨äºæœ€ç»ˆéªŒè¯")
        logger.info("")

    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=data_end_date,  # ä½¿ç”¨è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸ
        use_cache=True,
    )

    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"  - äº¤æ˜“æ—¥æ•°: {len(ohlcv['close'])}")
    logger.info(f"  - ETFæ•°é‡: {len(ohlcv['close'].columns)}")
    logger.info("")

    # ========== 3-4. è®¡ç®—å› å­ + æ¨ªæˆªé¢æ ‡å‡†åŒ– (å¸¦ç¼“å­˜) ==========
    logger.info("=" * 100)
    logger.info("ğŸ”§ è®¡ç®—ç²¾ç¡®å› å­åº“ + æ¨ªæˆªé¢æ ‡å‡†åŒ– (å¸¦ç¼“å­˜)")
    logger.info("=" * 100)

    factor_cache = FactorCache(
        cache_dir=Path(config["data"].get("cache_dir") or ".cache")
    )
    cached = factor_cache.get_or_compute(
        ohlcv=ohlcv,
        config=config,
        data_dir=loader.data_dir,
    )
    standardized_factors = cached["std_factors"]
    all_factor_names = list(cached["factor_names"])  # Convert to list for modification
    factors_3d = cached["factors_3d"]
    dates = cached["dates"]
    etf_codes = cached["etf_codes"]

    # â”€â”€ åŠ è½½å¤–éƒ¨å› å­ (ä»parquetæ–‡ä»¶) â”€â”€
    active_factors_cfg = config.get("active_factors")
    if active_factors_cfg:
        external_factors = set(active_factors_cfg) - set(all_factor_names)
        if external_factors:
            logger.info(f"ğŸ”§ æ£€æµ‹åˆ°å¤–éƒ¨å› å­: {sorted(external_factors)}")
            # Resolve external factors directory: env var > config > skip
            _ext_dir = os.environ.get("EXTRA_FACTORS_DIR", "").strip()
            if not _ext_dir:
                _ext_dir = (
                    config.get("combo_wfo", {})
                    .get("extra_factors", {})
                    .get("factors_dir", "")
                )
            if not _ext_dir:
                logger.warning(
                    "âš ï¸ å¤–éƒ¨å› å­ç›®å½•æœªé…ç½® (EXTRA_FACTORS_DIR env æˆ– "
                    "combo_wfo.extra_factors.factors_dir), è·³è¿‡å¤–éƒ¨å› å­åŠ è½½"
                )
                external_factors = set()  # skip loading
            factors_dir = Path(_ext_dir) if _ext_dir else None

            for factor_name in sorted(external_factors):
                factor_path = factors_dir / f"{factor_name}.parquet"
                if factor_path.exists():
                    # åŠ è½½å¤–éƒ¨å› å­
                    factor_df = pd.read_parquet(factor_path)
                    factor_df.index = pd.to_datetime(factor_df.index)

                    # å¯¹é½æ—¥æœŸå’Œsymbol
                    factor_aligned = factor_df.reindex(dates)
                    factor_aligned = factor_aligned[etf_codes]  # æŒ‰é¡ºåºæ’åˆ—

                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    factor_arr = factor_aligned.values  # Shape: (T, N)

                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
                    valid_ratio = np.isfinite(factor_arr).sum() / factor_arr.size
                    if valid_ratio > 0.01:  # è‡³å°‘1%æœ‰æ•ˆæ•°æ®
                        logger.info(
                            f"  âœ“ {factor_name}: {valid_ratio * 100:.1f}% æœ‰æ•ˆæ•°æ®"
                        )

                        # æ ‡å‡†åŒ–å¤„ç†: ä½¿ç”¨ CrossSectionProcessor (ä¸ base å› å­ä¸€è‡´)
                        factor_df_aligned = pd.DataFrame(
                            factor_arr, index=dates, columns=etf_codes
                        )
                        cs_processor = CrossSectionProcessor(
                            lower_percentile=config["cross_section"]["winsorize_lower"] * 100,
                            upper_percentile=config["cross_section"]["winsorize_upper"] * 100,
                            verbose=False,
                        )
                        processed = cs_processor.process_all_factors(
                            {factor_name: factor_df_aligned}
                        )
                        factor_std = processed[factor_name].values
                        factor_std = np.where(
                            np.isfinite(factor_std), factor_std, 0.0
                        )

                        # æ·»åŠ åˆ°factors_3d
                        factor_std_expanded = factor_std[
                            :, :, np.newaxis
                        ]  # Shape: (T, N, 1)
                        factors_3d = np.concatenate(
                            [factors_3d, factor_std_expanded], axis=2
                        )
                        all_factor_names.append(factor_name)

                        # æ·»åŠ åˆ°standardized_factors
                        for j, symbol in enumerate(etf_codes):
                            factor_series = pd.Series(factor_std[:, j], index=dates)
                            standardized_factors[(factor_name, symbol)] = factor_series
                    else:
                        logger.warning(
                            f"  âš ï¸ {factor_name}: æœ‰æ•ˆæ•°æ®ä¸è¶³ ({valid_ratio * 100:.1f}%), è·³è¿‡"
                        )
                else:
                    logger.warning(f"  âš ï¸ {factor_name}: å› å­æ–‡ä»¶ä¸å­˜åœ¨ {factor_path}")

    # â”€â”€ æ­£äº¤å› å­é›†è¿‡æ»¤ â”€â”€
    if active_factors_cfg:
        active_set = set(active_factors_cfg)
        missing = active_set - set(all_factor_names)
        if missing:
            raise ValueError(f"active_factors ä¸­æŒ‡å®šäº†ä¸å­˜åœ¨çš„å› å­: {sorted(missing)}")
        factor_names = sorted(active_set & set(all_factor_names))
        idx_map = {name: i for i, name in enumerate(all_factor_names)}
        selected_idx = [idx_map[f] for f in factor_names]
        factors_data = factors_3d[:, :, selected_idx]
        logger.info(
            f"âœ… æ­£äº¤å› å­é›†: {len(factor_names)}/{len(all_factor_names)} ä¸ªå› å­å·²æ¿€æ´»"
        )
        logger.info(f"  å·²æ’é™¤: {sorted(set(all_factor_names) - active_set)}")
    else:
        factor_names = all_factor_names
        factors_data = factors_3d

    # â”€â”€ åŠ è½½é¢å¤–å› å­çŸ©é˜µ (æ¥è‡ª factor mining prefilter) â”€â”€
    extra_cfg = config.get("combo_wfo", {}).get("extra_factors", {})
    # ç¯å¢ƒå˜é‡è¦†ç›– config ä¸­çš„ extra_factors è®¾ç½®
    env_npz = os.environ.get("EXTRA_FACTORS_NPZ")
    if env_npz:
        extra_cfg = {"enabled": True, "path": env_npz}
        logger.info(f"ç¯å¢ƒå˜é‡è¦†ç›– extra_factors: {env_npz}")
    if extra_cfg.get("enabled", False):
        extra_path = Path(extra_cfg["path"])
        if not extra_path.is_absolute():
            extra_path = ROOT / extra_path

        if not extra_path.exists():
            raise FileNotFoundError(f"Extra factors not found: {extra_path}")

        extra = np.load(extra_path)
        extra_names = list(extra["factor_names"])
        extra_dates = list(extra["dates"])
        extra_symbols = list(extra["symbols"])

        # Date alignment: extra may have more dates (mining uses full data,
        # WFO uses training_end_date). Subset extra to base date range.
        base_dates = [str(d.date()) for d in cached["dates"]]
        if extra_dates == base_dates:
            date_slice = slice(None)  # Perfect match
        elif set(base_dates).issubset(set(extra_dates)):
            # Extra has more dates â€” find contiguous slice
            start_idx = extra_dates.index(base_dates[0])
            end_idx = extra_dates.index(base_dates[-1])
            date_slice = slice(start_idx, end_idx + 1)
            sliced_dates = extra_dates[date_slice]
            if sliced_dates != base_dates:
                raise ValueError(
                    f"Date alignment failed: sliced extra has {len(sliced_dates)} dates "
                    f"but base has {len(base_dates)}"
                )
            logger.info(
                f"  Extra factors date subset: {len(extra_dates)} â†’ {len(base_dates)} dates"
            )
        else:
            raise ValueError(
                f"Date mismatch: base has {len(base_dates)} dates "
                f"({base_dates[0]}~{base_dates[-1]}), "
                f"extra has {len(extra_dates)} ({extra_dates[0]}~{extra_dates[-1]})"
            )

        # Symbol alignment: extra may have more symbols than WFO (mining loads
        # all parquet files, WFO uses config symbol list). Subset to base symbols.
        base_symbols = cached["etf_codes"]
        if extra_symbols == base_symbols:
            symbol_indices = None  # Perfect match
        elif set(base_symbols).issubset(set(extra_symbols)):
            symbol_indices = [extra_symbols.index(s) for s in base_symbols]
            logger.info(
                f"  Extra factors symbol subset: {len(extra_symbols)} â†’ {len(base_symbols)} ETFs"
            )
        else:
            missing = set(base_symbols) - set(extra_symbols)
            raise ValueError(
                f"Symbol mismatch: base needs {sorted(missing)} "
                f"but extra only has {len(extra_symbols)} symbols"
            )

        # Exclude factors already in base pool
        new_mask = [n not in set(factor_names) for n in extra_names]
        new_indices = [i for i, keep in enumerate(new_mask) if keep]
        new_names = [extra_names[i] for i in new_indices]

        if new_names:
            raw_extra = extra["data"][date_slice, :, :][:, :, new_indices]
            if symbol_indices is not None:
                extra_data = raw_extra[:, symbol_indices, :]
            else:
                extra_data = raw_extra
            factors_data = np.concatenate([factors_data, extra_data], axis=-1)
            factor_names = factor_names + new_names

            # Register extra factors into bucket system
            meta_path = extra_path.parent / "survivors_meta.json"
            if meta_path.exists():
                with open(meta_path) as f:
                    extra_meta = json.load(f)
                bucket_map = extra_meta.get("factor_bucket_map", {})
                mapped = {
                    n: b
                    for n, b in bucket_map.items()
                    if n in new_names and b != "UNMAPPED"
                }
                if mapped:
                    from etf_strategy.core.factor_buckets import register_extra_factors

                    register_extra_factors(mapped)
                    logger.info(
                        f"  Registered {len(mapped)} extra factors into buckets"
                    )

            logger.info(
                f"âœ… Extra factors loaded: +{len(new_names)} â†’ "
                f"total {len(factor_names)} factors"
            )
            logger.info(
                f"  New: {', '.join(new_names[:10])}{'...' if len(new_names) > 10 else ''}"
            )
        else:
            logger.info("  Extra factors: all already in base pool, skipped")

    logger.info(f"âœ… å› å­å‡†å¤‡å®Œæˆ: {len(factor_names)} ä¸ªå› å­")
    logger.info(f"  - å› å­åˆ—è¡¨: {', '.join(factor_names[:10])}...")
    logger.info("")

    # ========== 5. å‡†å¤‡æ•°æ® ==========
    logger.info("=" * 100)
    logger.info("ğŸ”„ å‡†å¤‡WFOè¾“å…¥æ•°æ®")
    logger.info("=" * 100)

    # å‡†å¤‡æ”¶ç›Šç‡
    returns_df = ohlcv["close"].pct_change()
    returns = returns_df.values

    # Regime gateï¼ˆä½œä¸ºäº¤æ˜“è§„åˆ™çš„ä¸€éƒ¨åˆ†è¿›å…¥ WFOï¼šç”¨äº OOS æ”¶ç›Šæ¨¡æ‹Ÿï¼‰
    backtest_cfg = config.get("backtest", {})
    gate_arr = compute_regime_gate_arr(
        ohlcv["close"],
        returns_df.index,
        backtest_config=backtest_cfg,
    )
    if bool((backtest_cfg.get("regime_gate") or {}).get("enabled", False)):
        stats = gate_stats(gate_arr)
        logger.info(
            "ğŸ§¯ Regime gate enabled (WFO): mean=%.3f min=%.3f max=%.3f",
            stats["mean"],
            stats["min"],
            stats["max"],
        )

    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    logger.info(
        f"  - æ•°æ®ç»´åº¦: {factors_data.shape[0]}å¤© Ã— {factors_data.shape[1]}åªETF Ã— {factors_data.shape[2]}ä¸ªå› å­"
    )
    logger.info(f"  - å› å­åç§°: {factor_names}")
    logger.info("")

    # ========== 5.5 Numba é¢„çƒ­ ==========
    warmup_numba_kernels()

    # ========== 6. æ‰§è¡ŒWFOä¼˜åŒ– ==========
    logger.info("=" * 100)
    logger.info("âš¡ æ‰§è¡Œç»„åˆçº§WFOä¼˜åŒ–")
    logger.info("=" * 100)
    logger.info("")

    # è·¨æ¡¶çº¦æŸé…ç½® (é»˜è®¤å…³é—­, éœ€åœ¨ combo_wfo.bucket_constraints ä¸­å¯ç”¨)
    bucket_cfg = config["combo_wfo"].get("bucket_constraints", {})

    # Hysteresis é…ç½® (ä» backtest.hysteresis è¯»å–)
    hyst_cfg = config.get("backtest", {}).get("hysteresis", {})

    optimizer = ComboWFOOptimizer(
        combo_sizes=config["combo_wfo"]["combo_sizes"],
        is_period=config["combo_wfo"]["is_period"],
        oos_period=config["combo_wfo"]["oos_period"],
        step_size=config["combo_wfo"]["step_size"],
        n_jobs=config["combo_wfo"]["n_jobs"],
        verbose=1 if config["combo_wfo"]["verbose"] else 0,
        enable_fdr=config["combo_wfo"]["enable_fdr"],
        fdr_alpha=config["combo_wfo"]["fdr_alpha"],
        complexity_penalty_lambda=config["combo_wfo"]["scoring"][
            "complexity_penalty_lambda"
        ],
        rebalance_frequencies=config["combo_wfo"]["rebalance_frequencies"],
        use_t1_open=config.get("backtest", {}).get("execution_model", "COC")
        == "T1_OPEN",
        delta_rank=float(hyst_cfg.get("delta_rank", 0.0)),
        min_hold_days=int(hyst_cfg.get("min_hold_days", 0)),
        use_bucket_constraints=bucket_cfg.get("enabled", False),
        bucket_min_buckets=bucket_cfg.get("min_buckets", 3),
        bucket_max_per_bucket=bucket_cfg.get("max_per_bucket", 2),
        max_parent_occurrence=bucket_cfg.get("max_parent_occurrence", 0),
    )

    # âœ… Exp2: åŠ è½½æˆæœ¬æ¨¡å‹ â†’ æ„å»º per-ETF æˆæœ¬æ•°ç»„
    cost_model = load_cost_model(config)
    etf_codes = list(ohlcv["close"].columns)
    qdii_set = set(FrozenETFPool().qdii_codes)
    cost_arr = build_cost_array(cost_model, etf_codes, qdii_set)
    tier = cost_model.active_tier
    logger.info(
        f"âœ… æˆæœ¬æ¨¡å‹: mode={cost_model.mode}, tier={cost_model.tier}, "
        f"Aè‚¡={tier.a_share * 10000:.0f}bp, QDII={tier.qdii * 10000:.0f}bp"
    )

    top_combos_list, all_combos_df = optimizer.run_combo_search(
        factors_data=factors_data,
        returns=returns,
        factor_names=factor_names,
        top_n=config["combo_wfo"].get("top_n", 100),
        pos_size=config["backtest"].get("pos_size", 2),
        commission_rate=config["backtest"].get("commission_rate", 0.0002),
        cost_arr=cost_arr,
        exposures=gate_arr,
    )

    logger.info("")
    logger.info("âœ… WFOä¼˜åŒ–å®Œæˆ")
    logger.info("")

    # ========== 7. ä¿å­˜ç»“æœ ==========
    logger.info("=" * 100)
    logger.info("ğŸ’¾ ä¿å­˜ç»“æœ")
    logger.info("=" * 100)

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåŸå­å†™å…¥ï¼‰ï¼špending_run_<ts> -> run_<ts>
    timestamp = env_ts if env_ts else datetime.now().strftime("%Y%m%d_%H%M%S")
    results_root = Path("results").resolve()
    final_dir = results_root / f"run_{timestamp}"
    pending_dir = results_root / f"pending_run_{timestamp}"
    if pending_dir.exists():
        import shutil

        logger.warning(f"æ¸…ç†æ®‹ç•™çš„ä¸´æ—¶ç›®å½•: {pending_dir}")
        shutil.rmtree(pending_dir, ignore_errors=True)
    pending_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é…ç½® (åŒ¹é…ç°æœ‰æ ¼å¼)
    run_config = {
        "timestamp": timestamp,
        "config_file": "configs/combo_wfo_config.yaml",
        "quick_mode": False,
        "parameters": {
            "run_id": config.get("run_id", "COMBO_WFO_DEEP_MINING"),
            "data": config["data"],
            "cross_section": config["cross_section"],
            "combo_wfo": config["combo_wfo"],
            "output_root": config.get("output_root", "results_combo_wfo"),
        },
    }

    with open(pending_dir / "run_config.json", "w", encoding="utf-8") as f:
        json.dump(run_config, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… é…ç½®å·²ä¿å­˜: {pending_dir}/run_config.json")

    # ä¿å­˜WFOç»“æœ
    all_combos_df.to_parquet(pending_dir / "all_combos.parquet", index=False)
    logger.info(
        f"âœ… å…¨éƒ¨ç»„åˆå·²ä¿å­˜: {pending_dir}/all_combos.parquet ({len(all_combos_df)} ä¸ªç»„åˆ)"
    )

    # ä¿å­˜Topç»„åˆ (åŒ¹é…ç°æœ‰æ–‡ä»¶å: top_combos.parquet)
    top_n = config["combo_wfo"].get("top_n", 100)
    top_combos = all_combos_df.head(top_n)  # å·²ç»æ’åºè¿‡äº†
    top_combos.to_parquet(pending_dir / "top_combos.parquet", index=False)
    logger.info(f"âœ… Top{top_n}ç»„åˆå·²ä¿å­˜: {pending_dir}/top_combos.parquet")

    # åŒæ—¶ä¿å­˜ä¸º top100_by_ic.parquet (ä¸ºäº†å…¼å®¹å›æµ‹è„šæœ¬)
    top_combos.to_parquet(pending_dir / "top100_by_ic.parquet", index=False)
    logger.info(f"âœ… Top{top_n}ç»„åˆå·²ä¿å­˜(å…¼å®¹): {pending_dir}/top100_by_ic.parquet")

    # ä¿å­˜å› å­æ•°æ®åˆ° factors/ ç›®å½• (ä»…baseå› å­; extraå› å­å·²åœ¨miningè¾“å‡ºä¸­)
    factors_dir = pending_dir / "factors"
    factors_dir.mkdir(exist_ok=True)
    saved_count = 0
    for factor_name in factor_names:
        if factor_name in standardized_factors:
            factor_df = standardized_factors[factor_name]
            factor_df.to_parquet(factors_dir / f"{factor_name}.parquet")
            saved_count += 1
    skipped = len(factor_names) - saved_count
    msg = f"âœ… {saved_count}ä¸ªå› å­å·²ä¿å­˜: {factors_dir}/"
    if skipped > 0:
        msg += f" (è·³è¿‡{skipped}ä¸ªextraå› å­)"
    logger.info(msg)

    # ä¿å­˜å› å­ç­›é€‰æ±‡æ€» (åŒ¹é…ç°æœ‰æ ¼å¼)
    factor_selection_summary = {
        "timestamp": timestamp,
        "n_factors": len(factor_names),
        "factor_names": factor_names,
        "data_shape": {
            "n_days": factors_data.shape[0],
            "n_etfs": factors_data.shape[1],
            "n_factors": factors_data.shape[2],
        },
        "winsorize": {
            "lower": config["cross_section"]["winsorize_lower"],
            "upper": config["cross_section"]["winsorize_upper"],
        },
    }

    with open(
        pending_dir / "factor_selection_summary.json", "w", encoding="utf-8"
    ) as f:
        json.dump(factor_selection_summary, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… å› å­æ±‡æ€»å·²ä¿å­˜: {pending_dir}/factor_selection_summary.json")

    # ä¿å­˜WFOæ±‡æ€»ä¿¡æ¯ (åŒ¹é…ç°æœ‰æ ¼å¼)
    significant_combos = all_combos_df[all_combos_df.get("is_significant", True)]

    summary = {
        "timestamp": timestamp,
        "total_combos": len(all_combos_df),
        "significant_combos": len(significant_combos),
        "mean_ic": float(all_combos_df["mean_oos_ic"].mean()),
        "mean_oos_return": (
            float(all_combos_df["mean_oos_return"].mean())
            if "mean_oos_return" in all_combos_df.columns
            else 0.0
        ),
        "best_combo": {
            "combo": top_combos.iloc[0]["combo"],
            "ic": float(top_combos.iloc[0]["mean_oos_ic"]),
            "score": float(top_combos.iloc[0]["stability_score"]),
            "freq": int(top_combos.iloc[0]["best_rebalance_freq"]),
            "mean_oos_return": (
                float(top_combos.iloc[0]["mean_oos_return"])
                if "mean_oos_return" in top_combos.columns
                else 0.0
            ),
            "cum_oos_return": (
                float(top_combos.iloc[0]["cum_oos_return"])
                if "cum_oos_return" in top_combos.columns
                else 0.0
            ),
        },
        "config": {
            "is_period": config["combo_wfo"]["is_period"],
            "oos_period": config["combo_wfo"]["oos_period"],
            "step_size": config["combo_wfo"]["step_size"],
            "combo_sizes": config["combo_wfo"]["combo_sizes"],
            "pos_size": config["backtest"].get("pos_size", 2),
        },
        "runtime_minutes": 0.0,  # è¿è¡Œæ—¶é—´å°†åœ¨åç»­æ›´æ–°
    }

    with open(pending_dir / "wfo_summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… WFOæ±‡æ€»å·²ä¿å­˜: {pending_dir}/wfo_summary.json")
    # åŸå­åˆ‡æ¢åˆ°æœ€ç»ˆç›®å½•
    try:
        if final_dir.exists():
            import shutil

            logger.warning(f"ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå°†è¢«æ›¿æ¢: {final_dir}")
            shutil.rmtree(final_dir, ignore_errors=True)
        pending_dir.rename(final_dir)
        # å†™å…¥å°±ç»ªæ ‡è®°
        (final_dir / "READY").write_text("ok", encoding="utf-8")
        # ç»´æŠ¤æŒ‡é’ˆ
        latest_ptr = results_root / ".latest_run"
        latest_ptr.write_text(final_dir.name, encoding="utf-8")
        latest_link = results_root / "run_latest"
        try:
            if latest_link.exists() or latest_link.is_symlink():
                latest_link.unlink()
            latest_link.symlink_to(final_dir)
        except Exception as e:
            logger.warning(f"åˆ›å»ºæœ€æ–°è¿è¡Œç¬¦å·é“¾æ¥å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"åˆ‡æ¢åˆ°æœ€ç»ˆç›®å½•å¤±è´¥: {e}")
        raise
    logger.info("")

    # ========== 8. ç»“æœæ±‡æ€» ==========
    logger.info("=" * 100)
    logger.info("ğŸ“Š ç»“æœæ±‡æ€»")
    logger.info("=" * 100)
    logger.info("")
    logger.info(f"è¾“å‡ºç›®å½•: {final_dir}")
    logger.info(f"æ€»ç»„åˆæ•°: {summary['total_combos']}")
    logger.info("")
    logger.info("ğŸ† Top 1 ç»„åˆ:")
    logger.info(f"  - åç§°: {summary['best_combo']['combo']}")
    logger.info(f"  - OOS Sharpe: {summary['best_combo']['ic']:.4f} (åŸICå­—æ®µ)")
    logger.info(f"  - ç¨³å®šæ€§å¾—åˆ†: {summary['best_combo']['score']:.2f}")
    logger.info(f"  - æœ€ä¼˜æ¢ä»“é¢‘ç‡: {summary['best_combo']['freq']}å¤©")
    if "best_trailing_stop" in top_combos.iloc[0]:
        logger.info(
            f"  - æœ€ä¼˜åŠ¨æ€æ­¢æŸ: {top_combos.iloc[0]['best_trailing_stop'] * 100:.1f}%"
        )
    logger.info("")
    logger.info("ğŸ“ˆ æ•´ä½“ç»Ÿè®¡:")
    logger.info(f"  - å¹³å‡OOS Sharpe: {summary['mean_ic']:.4f}")
    logger.info(
        f"  - æ˜¾è‘—ç»„åˆæ•°: {summary['significant_combos']}/{summary['total_combos']}"
    )
    logger.info("")
    logger.info("=" * 100)
    logger.info("âœ… WFOä¼˜åŒ–å®Œæˆï¼")
    logger.info("=" * 100)
    logger.info("")
    logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    logger.info("   è¿è¡ŒçœŸå®å›æµ‹: python test_freq_no_lookahead.py")
    logger.info("")


if __name__ == "__main__":
    main()
