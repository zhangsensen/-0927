#!/usr/bin/env python3
"""
ç»„åˆçº§WFOä¼˜åŒ–å¯åŠ¨è„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½OHLCVæ•°æ®
2. è®¡ç®—ç²¾ç¡®å› å­åº“
3. æ¨ªæˆªé¢æ ‡å‡†åŒ–å¤„ç†
4. æ‰§è¡Œç»„åˆçº§Walk-Forwardä¼˜åŒ–
5. ä¿å­˜Topç»„åˆåˆ° results/run_XXXXXX/

è¾“å‡ºï¼š
- results/run_XXXXXX/top100_by_ic.parquet
- results/run_XXXXXX/all_combos.parquet
- results/run_XXXXXX/wfo_summary.json

ç”¨æ³•ï¼š
    python run_combo_wfo.py
"""

import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

# Ensure src/ is on sys.path when running as a script
ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT / "src"))

from etf_strategy.core.combo_wfo_optimizer import ComboWFOOptimizer, warmup_numba_kernels
from etf_strategy.core.cross_section_processor import CrossSectionProcessor
from etf_strategy.core.data_loader import DataLoader
from etf_strategy.core.factor_cache import FactorCache
from etf_strategy.core.frozen_params import load_frozen_config
from etf_strategy.core.precise_factor_library_v2 import PreciseFactorLibrary
from etf_strategy.regime_gate import compute_regime_gate_arr, gate_stats

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def main():
    """ä¸»å‡½æ•°"""

    # ========== 1. åŠ è½½é…ç½® ==========
    logger.info("=" * 100)
    logger.info("ğŸš€ ç»„åˆçº§WFOä¼˜åŒ–å¯åŠ¨")
    logger.info("=" * 100)
    logger.info("")

    config_path = Path(
        os.environ.get("WFO_CONFIG_PATH", "configs/combo_wfo_config.yaml")
    )
    if not config_path.exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)

    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    frozen = load_frozen_config(config, config_path=str(config_path))
    logger.info(f"ğŸ”’ å‚æ•°å†»ç»“æ ¡éªŒé€šè¿‡ (version={frozen.version})")

    logger.info("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    logger.info(f'  - ETFæ•°é‡: {len(config["data"]["symbols"])}')
    logger.info(
        f'  - æ—¥æœŸèŒƒå›´: {config["data"]["start_date"]} â†’ {config["data"]["end_date"]}'
    )
    logger.info(f'  - ç»„åˆè§„æ¨¡: {config["combo_wfo"]["combo_sizes"]}')
    logger.info(f'  - ISçª—å£: {config["combo_wfo"]["is_period"]}å¤©')
    logger.info(f'  - OOSçª—å£: {config["combo_wfo"]["oos_period"]}å¤©')
    logger.info("")

    # ç¯å¢ƒå˜é‡è¦†ç›–ï¼šå¯é€šè¿‡ RB_FREQ_SUBSET æŒ‡å®šé€—å·åˆ†éš”çš„æ¢ä»“é¢‘ç‡åˆ—è¡¨ï¼ˆä¾‹å¦‚ "8" æˆ– "8,16,24"ï¼‰
    # å¯é€šè¿‡ RB_RESULT_TS æŒ‡å®šè¾“å‡ºç›®å½•çš„æ—¶é—´æˆ³ï¼Œä»¥ä¾¿ä¸å¯åŠ¨è„šæœ¬çš„æ—¥å¿—æ—¶é—´æˆ³ä¸€è‡´ã€‚
    env_freq = os.environ.get("RB_FREQ_SUBSET")
    if env_freq:
        try:
            override = [int(x.strip()) for x in env_freq.split(",") if x.strip()]
            if override:
                config["combo_wfo"]["rebalance_frequencies"] = override
                logger.info(f"ğŸ”§ é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–é¢‘ç‡: RB_FREQ_SUBSET={override}")
        except Exception as e:
            logger.warning(f"å¿½ç•¥éæ³•çš„ RB_FREQ_SUBSET å€¼: {env_freq} ({e})")
    env_ts = os.environ.get("RB_RESULT_TS", "").strip()
    if env_ts:
        logger.info(f"ğŸ”§ ä½¿ç”¨å¤–éƒ¨æŒ‡å®šæ—¶é—´æˆ³ RB_RESULT_TS={env_ts}")

    # ========== 2. åŠ è½½æ•°æ® ==========
    logger.info("=" * 100)
    logger.info("ğŸ“Š åŠ è½½OHLCVæ•°æ®")
    logger.info("=" * 100)

    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )

    # ä½¿ç”¨ training_end_date å¦‚æœè®¾ç½®äº†ï¼ˆHoldoutéªŒè¯æ¨¡å¼ï¼‰
    data_end_date = (
        config["data"].get("training_end_date") or config["data"]["end_date"]
    )

    if config["data"].get("training_end_date"):
        logger.info("=" * 100)
        logger.info("ğŸ”¬ HOLDOUTéªŒè¯æ¨¡å¼")
        logger.info("=" * 100)
        logger.info(f"è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸ: {data_end_date}")
        logger.info(f"å®Œæ•´æ•°æ®æˆªæ­¢æ—¥æœŸ: {config['data']['end_date']}")
        logger.info(f"HoldoutæœŸ: {data_end_date} è‡³ {config['data']['end_date']}")
        logger.info("âš ï¸  æ³¨æ„: å½“å‰ä»…ä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼ŒHoldoutæœŸæ•°æ®å°†ç”¨äºæœ€ç»ˆéªŒè¯")
        logger.info("")

    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=data_end_date,  # ä½¿ç”¨è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸ
        use_cache=True,
    )

    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f'  - äº¤æ˜“æ—¥æ•°: {len(ohlcv["close"])}')
    logger.info(f'  - ETFæ•°é‡: {len(ohlcv["close"].columns)}')
    logger.info("")

    # ========== 3-4. è®¡ç®—å› å­ + æ¨ªæˆªé¢æ ‡å‡†åŒ– (å¸¦ç¼“å­˜) ==========
    logger.info("=" * 100)
    logger.info("ğŸ”§ è®¡ç®—ç²¾ç¡®å› å­åº“ + æ¨ªæˆªé¢æ ‡å‡†åŒ– (å¸¦ç¼“å­˜)")
    logger.info("=" * 100)

    factor_cache = FactorCache(cache_dir=Path(config["data"].get("cache_dir") or ".cache"))
    cached = factor_cache.get_or_compute(
        ohlcv=ohlcv,
        config=config,
        data_dir=loader.data_dir,
    )
    standardized_factors = cached["std_factors"]
    all_factor_names = cached["factor_names"]

    # â”€â”€ æ­£äº¤å› å­é›†è¿‡æ»¤ â”€â”€
    active_factors_cfg = config.get("active_factors")
    if active_factors_cfg:
        active_set = set(active_factors_cfg)
        missing = active_set - set(all_factor_names)
        if missing:
            raise ValueError(f"active_factors ä¸­æŒ‡å®šäº†ä¸å­˜åœ¨çš„å› å­: {sorted(missing)}")
        factor_names = sorted(active_set & set(all_factor_names))
        idx_map = {name: i for i, name in enumerate(all_factor_names)}
        selected_idx = [idx_map[f] for f in factor_names]
        factors_data = cached["factors_3d"][:, :, selected_idx]
        logger.info(
            f"âœ… æ­£äº¤å› å­é›†: {len(factor_names)}/{len(all_factor_names)} ä¸ªå› å­å·²æ¿€æ´»"
        )
        logger.info(f"  å·²æ’é™¤: {sorted(set(all_factor_names) - active_set)}")
    else:
        factor_names = all_factor_names
        factors_data = cached["factors_3d"]

    logger.info(f"âœ… å› å­å‡†å¤‡å®Œæˆ: {len(factor_names)} ä¸ªå› å­")
    logger.info(f'  - å› å­åˆ—è¡¨: {", ".join(factor_names[:10])}...')
    logger.info("")

    # ========== 5. å‡†å¤‡æ•°æ® ==========
    logger.info("=" * 100)
    logger.info("ğŸ”„ å‡†å¤‡WFOè¾“å…¥æ•°æ®")
    logger.info("=" * 100)

    # å‡†å¤‡æ”¶ç›Šç‡
    returns_df = ohlcv["close"].pct_change()
    returns = returns_df.values

    # Regime gateï¼ˆä½œä¸ºäº¤æ˜“è§„åˆ™çš„ä¸€éƒ¨åˆ†è¿›å…¥ WFOï¼šç”¨äº OOS æ”¶ç›Šæ¨¡æ‹Ÿï¼‰
    backtest_cfg = config.get("backtest", {})
    gate_arr = compute_regime_gate_arr(
        ohlcv["close"],
        returns_df.index,
        backtest_config=backtest_cfg,
    )
    if bool((backtest_cfg.get("regime_gate") or {}).get("enabled", False)):
        stats = gate_stats(gate_arr)
        logger.info(
            "ğŸ§¯ Regime gate enabled (WFO): mean=%.3f min=%.3f max=%.3f",
            stats["mean"],
            stats["min"],
            stats["max"],
        )

    logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    logger.info(
        f"  - æ•°æ®ç»´åº¦: {factors_data.shape[0]}å¤© Ã— {factors_data.shape[1]}åªETF Ã— {factors_data.shape[2]}ä¸ªå› å­"
    )
    logger.info(f"  - å› å­åç§°: {factor_names}")
    logger.info("")

    # ========== 5.5 Numba é¢„çƒ­ ==========
    warmup_numba_kernels()

    # ========== 6. æ‰§è¡ŒWFOä¼˜åŒ– ==========
    logger.info("=" * 100)
    logger.info("âš¡ æ‰§è¡Œç»„åˆçº§WFOä¼˜åŒ–")
    logger.info("=" * 100)
    logger.info("")

    optimizer = ComboWFOOptimizer(
        combo_sizes=config["combo_wfo"]["combo_sizes"],
        is_period=config["combo_wfo"]["is_period"],
        oos_period=config["combo_wfo"]["oos_period"],
        step_size=config["combo_wfo"]["step_size"],
        n_jobs=config["combo_wfo"]["n_jobs"],
        verbose=1 if config["combo_wfo"]["verbose"] else 0,
        enable_fdr=config["combo_wfo"]["enable_fdr"],
        fdr_alpha=config["combo_wfo"]["fdr_alpha"],
        complexity_penalty_lambda=config["combo_wfo"]["scoring"][
            "complexity_penalty_lambda"
        ],
        rebalance_frequencies=config["combo_wfo"]["rebalance_frequencies"],
        use_t1_open=config.get("backtest", {}).get("execution_model", "COC") == "T1_OPEN",
    )

    top_combos_list, all_combos_df = optimizer.run_combo_search(
        factors_data=factors_data,
        returns=returns,
        factor_names=factor_names,
        top_n=config["combo_wfo"].get("top_n", 100),
        pos_size=config["backtest"].get("pos_size", 2),
        commission_rate=config["backtest"].get("commission_rate", 0.0002),
        exposures=gate_arr,
    )

    logger.info("")
    logger.info("âœ… WFOä¼˜åŒ–å®Œæˆ")
    logger.info("")

    # ========== 7. ä¿å­˜ç»“æœ ==========
    logger.info("=" * 100)
    logger.info("ğŸ’¾ ä¿å­˜ç»“æœ")
    logger.info("=" * 100)

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåŸå­å†™å…¥ï¼‰ï¼špending_run_<ts> -> run_<ts>
    timestamp = env_ts if env_ts else datetime.now().strftime("%Y%m%d_%H%M%S")
    results_root = Path("results").resolve()
    final_dir = results_root / f"run_{timestamp}"
    pending_dir = results_root / f"pending_run_{timestamp}"
    if pending_dir.exists():
        import shutil

        logger.warning(f"æ¸…ç†æ®‹ç•™çš„ä¸´æ—¶ç›®å½•: {pending_dir}")
        shutil.rmtree(pending_dir, ignore_errors=True)
    pending_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é…ç½® (åŒ¹é…ç°æœ‰æ ¼å¼)
    run_config = {
        "timestamp": timestamp,
        "config_file": "configs/combo_wfo_config.yaml",
        "quick_mode": False,
        "parameters": {
            "run_id": config.get("run_id", "COMBO_WFO_DEEP_MINING"),
            "data": config["data"],
            "cross_section": config["cross_section"],
            "combo_wfo": config["combo_wfo"],
            "output_root": config.get("output_root", "results_combo_wfo"),
        },
    }

    with open(pending_dir / "run_config.json", "w", encoding="utf-8") as f:
        json.dump(run_config, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… é…ç½®å·²ä¿å­˜: {pending_dir}/run_config.json")

    # ä¿å­˜WFOç»“æœ
    all_combos_df.to_parquet(pending_dir / "all_combos.parquet", index=False)
    logger.info(
        f"âœ… å…¨éƒ¨ç»„åˆå·²ä¿å­˜: {pending_dir}/all_combos.parquet ({len(all_combos_df)} ä¸ªç»„åˆ)"
    )

    # ä¿å­˜Topç»„åˆ (åŒ¹é…ç°æœ‰æ–‡ä»¶å: top_combos.parquet)
    top_n = config["combo_wfo"].get("top_n", 100)
    top_combos = all_combos_df.head(top_n)  # å·²ç»æ’åºè¿‡äº†
    top_combos.to_parquet(pending_dir / "top_combos.parquet", index=False)
    logger.info(f"âœ… Top{top_n}ç»„åˆå·²ä¿å­˜: {pending_dir}/top_combos.parquet")

    # åŒæ—¶ä¿å­˜ä¸º top100_by_ic.parquet (ä¸ºäº†å…¼å®¹å›æµ‹è„šæœ¬)
    top_combos.to_parquet(pending_dir / "top100_by_ic.parquet", index=False)
    logger.info(f"âœ… Top{top_n}ç»„åˆå·²ä¿å­˜(å…¼å®¹): {pending_dir}/top100_by_ic.parquet")

    # ä¿å­˜å› å­æ•°æ®åˆ° factors/ ç›®å½•
    factors_dir = pending_dir / "factors"
    factors_dir.mkdir(exist_ok=True)
    for factor_name in factor_names:
        factor_df = standardized_factors[factor_name]
        factor_df.to_parquet(factors_dir / f"{factor_name}.parquet")
    logger.info(f"âœ… {len(factor_names)}ä¸ªå› å­å·²ä¿å­˜: {factors_dir}/")

    # ä¿å­˜å› å­ç­›é€‰æ±‡æ€» (åŒ¹é…ç°æœ‰æ ¼å¼)
    factor_selection_summary = {
        "timestamp": timestamp,
        "n_factors": len(factor_names),
        "factor_names": factor_names,
        "data_shape": {
            "n_days": factors_data.shape[0],
            "n_etfs": factors_data.shape[1],
            "n_factors": factors_data.shape[2],
        },
        "winsorize": {
            "lower": config["cross_section"]["winsorize_lower"],
            "upper": config["cross_section"]["winsorize_upper"],
        },
    }

    with open(
        pending_dir / "factor_selection_summary.json", "w", encoding="utf-8"
    ) as f:
        json.dump(factor_selection_summary, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… å› å­æ±‡æ€»å·²ä¿å­˜: {pending_dir}/factor_selection_summary.json")

    # ä¿å­˜WFOæ±‡æ€»ä¿¡æ¯ (åŒ¹é…ç°æœ‰æ ¼å¼)
    significant_combos = all_combos_df[all_combos_df.get("is_significant", True)]

    summary = {
        "timestamp": timestamp,
        "total_combos": len(all_combos_df),
        "significant_combos": len(significant_combos),
        "mean_ic": float(all_combos_df["mean_oos_ic"].mean()),
        "mean_oos_return": (
            float(all_combos_df["mean_oos_return"].mean())
            if "mean_oos_return" in all_combos_df.columns
            else 0.0
        ),
        "best_combo": {
            "combo": top_combos.iloc[0]["combo"],
            "ic": float(top_combos.iloc[0]["mean_oos_ic"]),
            "score": float(top_combos.iloc[0]["stability_score"]),
            "freq": int(top_combos.iloc[0]["best_rebalance_freq"]),
            "mean_oos_return": (
                float(top_combos.iloc[0]["mean_oos_return"])
                if "mean_oos_return" in top_combos.columns
                else 0.0
            ),
            "cum_oos_return": (
                float(top_combos.iloc[0]["cum_oos_return"])
                if "cum_oos_return" in top_combos.columns
                else 0.0
            ),
        },
        "config": {
            "is_period": config["combo_wfo"]["is_period"],
            "oos_period": config["combo_wfo"]["oos_period"],
            "step_size": config["combo_wfo"]["step_size"],
            "combo_sizes": config["combo_wfo"]["combo_sizes"],
            "pos_size": config["backtest"].get("pos_size", 2),
        },
        "runtime_minutes": 0.0,  # è¿è¡Œæ—¶é—´å°†åœ¨åç»­æ›´æ–°
    }

    with open(pending_dir / "wfo_summary.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    logger.info(f"âœ… WFOæ±‡æ€»å·²ä¿å­˜: {pending_dir}/wfo_summary.json")
    # åŸå­åˆ‡æ¢åˆ°æœ€ç»ˆç›®å½•
    try:
        if final_dir.exists():
            import shutil

            logger.warning(f"ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå°†è¢«æ›¿æ¢: {final_dir}")
            shutil.rmtree(final_dir, ignore_errors=True)
        pending_dir.rename(final_dir)
        # å†™å…¥å°±ç»ªæ ‡è®°
        (final_dir / "READY").write_text("ok", encoding="utf-8")
        # ç»´æŠ¤æŒ‡é’ˆ
        latest_ptr = results_root / ".latest_run"
        latest_ptr.write_text(final_dir.name, encoding="utf-8")
        latest_link = results_root / "run_latest"
        try:
            if latest_link.exists() or latest_link.is_symlink():
                latest_link.unlink()
            latest_link.symlink_to(final_dir)
        except Exception as e:
            logger.warning(f"åˆ›å»ºæœ€æ–°è¿è¡Œç¬¦å·é“¾æ¥å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"åˆ‡æ¢åˆ°æœ€ç»ˆç›®å½•å¤±è´¥: {e}")
        raise
    logger.info("")

    # ========== 8. ç»“æœæ±‡æ€» ==========
    logger.info("=" * 100)
    logger.info("ğŸ“Š ç»“æœæ±‡æ€»")
    logger.info("=" * 100)
    logger.info("")
    logger.info(f"è¾“å‡ºç›®å½•: {final_dir}")
    logger.info(f'æ€»ç»„åˆæ•°: {summary["total_combos"]}')
    logger.info("")
    logger.info("ğŸ† Top 1 ç»„åˆ:")
    logger.info(f'  - åç§°: {summary["best_combo"]["combo"]}')
    logger.info(f'  - OOS Sharpe: {summary["best_combo"]["ic"]:.4f} (åŸICå­—æ®µ)')
    logger.info(f'  - ç¨³å®šæ€§å¾—åˆ†: {summary["best_combo"]["score"]:.2f}')
    logger.info(f'  - æœ€ä¼˜æ¢ä»“é¢‘ç‡: {summary["best_combo"]["freq"]}å¤©')
    if "best_trailing_stop" in top_combos.iloc[0]:
        logger.info(
            f'  - æœ€ä¼˜åŠ¨æ€æ­¢æŸ: {top_combos.iloc[0]["best_trailing_stop"]*100:.1f}%'
        )
    logger.info("")
    logger.info("ğŸ“ˆ æ•´ä½“ç»Ÿè®¡:")
    logger.info(f'  - å¹³å‡OOS Sharpe: {summary["mean_ic"]:.4f}')
    logger.info(
        f'  - æ˜¾è‘—ç»„åˆæ•°: {summary["significant_combos"]}/{summary["total_combos"]}'
    )
    logger.info("")
    logger.info("=" * 100)
    logger.info("âœ… WFOä¼˜åŒ–å®Œæˆï¼")
    logger.info("=" * 100)
    logger.info("")
    logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    logger.info("   è¿è¡ŒçœŸå®å›æµ‹: python test_freq_no_lookahead.py")
    logger.info("")


if __name__ == "__main__":
    main()
