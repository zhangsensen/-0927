#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› å­ç¼“å­˜ç®¡ç†å™¨
=============

è®¾è®¡ç†å¿µ:
- å› å­è®¡ç®—11ç§’ â†’ ç¼“å­˜å0.1ç§’ï¼ˆ100å€æé€Ÿï¼‰
- è‡ªåŠ¨æ£€æµ‹æ•°æ®/ä»£ç å˜åŒ–ï¼Œå¤±æ•ˆè‡ªåŠ¨é‡ç®—
- æ”¯æŒæ ‡å‡†åŒ–å› å­ç¼“å­˜

ä½œè€…: Linus-Approved
"""

import hashlib
import inspect
import pickle
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

import pandas as pd


class FactorCache:
    """å› å­ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: Path = None, use_timestamp: bool = True):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•/cache/factors
            use_timestamp: æ˜¯å¦ä½¿ç”¨æ—¶é—´æˆ³å‘½åï¼ˆæ¨èTrueï¼Œé¿å…æ•°æ®æ··æ·†ï¼‰
        """
        if cache_dir is None:
            project_root = Path(__file__).parent.parent
            cache_dir = project_root / "cache" / "factors"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ä½¿ç”¨æ—¶é—´æˆ³å‘½åï¼Œé¿å…å“ˆå¸Œç¢°æ’
        self.use_timestamp = use_timestamp

        # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        self.ttl_hours = 24 * 7  # 1å‘¨

    def _compute_data_hash(self, ohlcv: Dict[str, pd.DataFrame]) -> str:
        """
        è®¡ç®—OHLCVæ•°æ®çš„å“ˆå¸Œå€¼

        Args:
            ohlcv: OHLCVæ•°æ®å­—å…¸

        Returns:
            MD5å“ˆå¸Œå€¼
        """
        # åªç”¨closeçš„shapeå’Œæœ€åä¸€è¡Œæ¥è®¡ç®—hashï¼ˆå¿«é€Ÿï¼‰
        close = ohlcv["close"]
        hash_input = f"{close.shape}_{close.iloc[-1].sum():.6f}_{close.index[-1]}"
        return hashlib.md5(hash_input.encode()).hexdigest()[:16]

    def _compute_code_hash(self, lib_class) -> str:
        """
        è®¡ç®—å› å­åº“ä»£ç çš„å“ˆå¸Œå€¼

        Args:
            lib_class: å› å­åº“ç±»

        Returns:
            MD5å“ˆå¸Œå€¼
        """
        # è·å–ç±»çš„æºä»£ç 
        source = inspect.getsource(lib_class)
        return hashlib.md5(source.encode()).hexdigest()[:16]

    def _get_cache_key(self, data_hash: str, code_hash: str, stage: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆä½¿ç”¨æ—¶é—´æˆ³é¿å…æ··æ·†ï¼‰"""
        if self.use_timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"{stage}_{timestamp}_{data_hash[:8]}_{code_hash[:8]}"
        else:
            return f"{stage}_{data_hash}_{code_hash}"

    def _find_latest_cache(
        self, stage: str, data_hash: str, code_hash: str
    ) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„æœ‰æ•ˆç¼“å­˜æ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰"""
        if not self.use_timestamp:
            # æ—§æ¨¡å¼ï¼šç›´æ¥å“ˆå¸ŒåŒ¹é…
            cache_key = f"{stage}_{data_hash}_{code_hash}"
            cache_path = self.cache_dir / f"{cache_key}.pkl"
            return cache_path if cache_path.exists() else None

        # æ–°æ¨¡å¼ï¼šæŸ¥æ‰¾æœ€æ–°çš„åŒ¹é…æ–‡ä»¶
        pattern = f"{stage}_*_{data_hash[:8]}_{code_hash[:8]}.pkl"
        matching_files = sorted(self.cache_dir.glob(pattern), reverse=True)

        if matching_files:
            return matching_files[0]  # è¿”å›æœ€æ–°çš„

        return None

    def _get_cache_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.pkl"

    def _is_cache_valid(self, cache_path: Path) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_path.exists():
            return False

        # æ£€æŸ¥æ—¶æ•ˆ
        age_hours = (time.time() - cache_path.stat().st_mtime) / 3600
        if age_hours > self.ttl_hours:
            return False

        return True

    def load_factors(
        self, ohlcv: Dict[str, pd.DataFrame], lib_class, stage: str = "raw"
    ) -> Optional[Dict[str, pd.DataFrame]]:
        """
        åŠ è½½ç¼“å­˜çš„å› å­

        Args:
            ohlcv: OHLCVæ•°æ®
            lib_class: å› å­åº“ç±»
            stage: é˜¶æ®µæ ‡è¯†ï¼ˆraw/standardizedï¼‰

        Returns:
            ç¼“å­˜çš„å› å­å­—å…¸ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        data_hash = self._compute_data_hash(ohlcv)
        code_hash = self._compute_code_hash(lib_class)

        # æŸ¥æ‰¾æœ€æ–°çš„æœ‰æ•ˆç¼“å­˜
        cache_path = self._find_latest_cache(stage, data_hash, code_hash)

        if cache_path and self._is_cache_valid(cache_path):
            print(f"âœ… åŠ è½½å› å­ç¼“å­˜: {cache_path.name}")
            with open(cache_path, "rb") as f:
                return pickle.load(f)

        return None

    def save_factors(
        self,
        factors: Dict[str, pd.DataFrame],
        ohlcv: Dict[str, pd.DataFrame],
        lib_class,
        stage: str = "raw",
    ):
        """
        ä¿å­˜å› å­åˆ°ç¼“å­˜

        Args:
            factors: å› å­å­—å…¸
            ohlcv: OHLCVæ•°æ®
            lib_class: å› å­åº“ç±»
            stage: é˜¶æ®µæ ‡è¯†
        """
        data_hash = self._compute_data_hash(ohlcv)
        code_hash = self._compute_code_hash(lib_class)
        cache_key = self._get_cache_key(data_hash, code_hash, stage)
        cache_path = self._get_cache_path(cache_key)

        with open(cache_path, "wb") as f:
            pickle.dump(factors, f)

        print(f"âœ… ä¿å­˜å› å­ç¼“å­˜: {cache_key}")

    def clear_old_cache(self, max_age_days: int = 30):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜

        Args:
            max_age_days: æœ€å¤§ä¿ç•™å¤©æ•°
        """
        cutoff_time = time.time() - (max_age_days * 24 * 3600)
        removed_count = 0

        for cache_file in self.cache_dir.glob("*.pkl"):
            if cache_file.stat().st_mtime < cutoff_time:
                cache_file.unlink()
                removed_count += 1

        if removed_count > 0:
            print(f"ğŸ—‘ï¸  æ¸…ç†{removed_count}ä¸ªè¿‡æœŸç¼“å­˜")

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        cache_files = list(self.cache_dir.glob("*.pkl"))

        if not cache_files:
            return {"count": 0, "total_size_mb": 0, "oldest_hours": 0}

        total_size = sum(f.stat().st_size for f in cache_files)
        oldest_file = min(cache_files, key=lambda f: f.stat().st_mtime)
        oldest_hours = (time.time() - oldest_file.stat().st_mtime) / 3600

        return {
            "count": len(cache_files),
            "total_size_mb": total_size / 1024 / 1024,
            "oldest_hours": oldest_hours,
        }
