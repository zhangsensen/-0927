#!/usr/bin/env python3
"""
ç¦»çº¿è¯„ä¼°å€™é€‰å› å­ï¼ˆä¸¥æ ¼éµå¾ªæ¨ªæˆªé¢ Spearman + T-1 å£å¾„ï¼‰

è¯„ä¼°å› å­ï¼š
1. REVERSAL_FACTOR_5D - çŸ­æœŸåè½¬
2. VOLATILITY_SKEW_20D - æ³¢åŠ¨ç»“æ„è´¨é‡
3. DOLLAR_VOLUME_ACCELERATION_10D - ç¾å…ƒæˆäº¤é¢åŠ é€Ÿåº¦

å‡†å…¥é—¨æ§›ï¼ˆå…¨éƒ¨æ»¡è¶³ï¼‰ï¼š
- OOS å¹³å‡ RankIC â‰¥ 0.010
- è¡°å‡æ¯”ï¼ˆISâ†’OOSï¼‰â‰¤ 50%
- å¤±è´¥çª—å£å æ¯” â‰¤ 30%
- ä¸ Top3 ç¨³å®šå› å­ä¸­ä½ç›¸å…³æ€§ < 0.7
- åŠ å…¥å Step4 Sharpe ä¸ä¸‹é™
"""

import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from scipy.stats import spearmanr

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


class CandidateFactorEvaluator:
    """å€™é€‰å› å­ç¦»çº¿è¯„ä¼°å™¨"""

    def __init__(self, ohlcv_dir: str, existing_factors_dir: str):
        """
        Args:
            ohlcv_dir: OHLCV æ•°æ®ç›®å½•
            existing_factors_dir: ç°æœ‰æ ‡å‡†åŒ–å› å­ç›®å½•
        """
        self.ohlcv_dir = Path(ohlcv_dir)
        self.existing_factors_dir = Path(existing_factors_dir)

        # WFO é…ç½®ï¼ˆæ²¿ç”¨ç°æœ‰ç³»ç»Ÿï¼‰
        self.is_window = 252
        self.oos_window = 60
        self.step = 20

        # å‡†å…¥é—¨æ§›
        self.min_oos_ic = 0.010
        self.max_decay_ratio = 0.50
        self.max_failure_ratio = 0.30
        self.max_top3_corr = 0.70

        logger.info("âœ… å€™é€‰å› å­è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - ISçª—å£: {self.is_window}d")
        logger.info(f"  - OOSçª—å£: {self.oos_window}d")
        logger.info(f"  - æ­¥é•¿: {self.step}d")

    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, pd.DataFrame]]:
        """åŠ è½½ OHLCV ä¸ç°æœ‰å› å­æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®...")

        # åŠ è½½ OHLCV
        close = pd.read_parquet(self.ohlcv_dir / "close.parquet")
        volume = pd.read_parquet(self.ohlcv_dir / "volume.parquet")

        # è®¡ç®—æ”¶ç›Šç‡ï¼ˆT-1 å¯¹é½ï¼‰
        returns = close.pct_change(fill_method=None)

        # åŠ è½½ Top3 ç¨³å®šå› å­
        top3_factors = {
            "CALMAR_RATIO_60D": pd.read_parquet(
                self.existing_factors_dir / "CALMAR_RATIO_60D.parquet"
            ),
            "PRICE_POSITION_120D": pd.read_parquet(
                self.existing_factors_dir / "PRICE_POSITION_120D.parquet"
            ),
            "CMF_20D": pd.read_parquet(self.existing_factors_dir / "CMF_20D.parquet"),
        }

        logger.info(f"  - OHLCV: {close.shape}")
        logger.info(f"  - æ”¶ç›Šç‡: {returns.shape}")
        logger.info(f"  - Top3 å› å­å·²åŠ è½½")

        return close, volume, returns, top3_factors

    def compute_reversal_5d(self, close: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®— 5 æ—¥çŸ­æœŸåè½¬å› å­ï¼ˆæ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼‰

        é€»è¾‘ï¼šè¿‡å» 5 æ—¥æ”¶ç›Šç‡çš„è´Ÿå€¼ï¼ˆè·Œå¤šâ†’åè½¬é¢„æœŸå¼ºï¼‰
        """
        logger.info("ğŸ”§ è®¡ç®— REVERSAL_FACTOR_5D...")

        # 5æ—¥ç´¯è®¡æ”¶ç›Šç‡
        ret_5d = close.pct_change(periods=5, fill_method=None)

        # å–è´Ÿå€¼ï¼ˆåè½¬é€»è¾‘ï¼‰
        reversal = -ret_5d

        # æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆæ¯æ—¥ï¼‰
        reversal_std = reversal.sub(reversal.mean(axis=1), axis=0).div(
            reversal.std(axis=1) + 1e-8, axis=0
        )

        logger.info(f"  âœ… NaNç‡: {reversal_std.isna().mean().mean():.2%}")
        return reversal_std

    def compute_volatility_skew_20d(self, close: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®— 20 æ—¥æ³¢åŠ¨ç‡åæ–œå› å­ï¼ˆæ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼‰

        é€»è¾‘ï¼šä¸‹è·Œæ—¥æ³¢åŠ¨ç‡ / ä¸Šæ¶¨æ—¥æ³¢åŠ¨ç‡
        å¥åº·è¶‹åŠ¿: skew < 1 (ä¸Šæ¶¨æ—¥æ³¢åŠ¨ä½)
        å‡ºè´§ç‰¹å¾: skew > 1 (ä¸Šæ¶¨æ—¥æ³¢åŠ¨é«˜)
        """
        logger.info("ğŸ”§ è®¡ç®— VOLATILITY_SKEW_20D...")

        returns = close.pct_change(fill_method=None)

        skew = pd.DataFrame(index=close.index, columns=close.columns, dtype=float)

        for col in close.columns:
            ret = returns[col]

            # 20æ—¥æ»šåŠ¨çª—å£
            for i in range(20, len(ret)):
                window_ret = ret.iloc[i - 20 : i]

                # ä¸Šæ¶¨æ—¥ä¸ä¸‹è·Œæ—¥æ³¢åŠ¨ç‡
                up_vol = window_ret[window_ret > 0].std()
                down_vol = window_ret[window_ret < 0].std()

                # é¿å…é™¤é›¶
                if pd.notna(up_vol) and pd.notna(down_vol) and up_vol > 1e-8:
                    skew.iloc[i, skew.columns.get_loc(col)] = down_vol / up_vol

        # æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆæ¯æ—¥ï¼‰
        skew_std = skew.sub(skew.mean(axis=1), axis=0).div(
            skew.std(axis=1) + 1e-8, axis=0
        )

        logger.info(f"  âœ… NaNç‡: {skew_std.isna().mean().mean():.2%}")
        return skew_std

    def compute_dollar_volume_accel_10d(
        self, close: pd.DataFrame, volume: pd.DataFrame
    ) -> pd.DataFrame:
        """
        è®¡ç®— 10 æ—¥ç¾å…ƒæˆäº¤é¢åŠ é€Ÿåº¦å› å­ï¼ˆæ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼‰

        é€»è¾‘ï¼šæˆäº¤é¢ = close * volume
        åŠ é€Ÿåº¦ = (æœ€è¿‘5æ—¥å‡æˆäº¤é¢ - å‰5æ—¥å‡æˆäº¤é¢) / å‰5æ—¥å‡æˆäº¤é¢
        """
        logger.info("ğŸ”§ è®¡ç®— DOLLAR_VOLUME_ACCELERATION_10D...")

        # ç¾å…ƒæˆäº¤é¢
        dollar_vol = close * volume

        # æœ€è¿‘5æ—¥ä¸å‰5æ—¥å‡å€¼
        recent_5d = dollar_vol.rolling(window=5, min_periods=5).mean()
        prior_5d = dollar_vol.shift(5).rolling(window=5, min_periods=5).mean()

        # åŠ é€Ÿåº¦ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰
        accel = (recent_5d - prior_5d) / (prior_5d + 1e-8)

        # æ¨ªæˆªé¢æ ‡å‡†åŒ–ï¼ˆæ¯æ—¥ï¼‰
        accel_std = accel.sub(accel.mean(axis=1), axis=0).div(
            accel.std(axis=1) + 1e-8, axis=0
        )

        logger.info(f"  âœ… NaNç‡: {accel_std.isna().mean().mean():.2%}")
        return accel_std

    def compute_cross_sectional_ic(
        self,
        factor: pd.DataFrame,
        returns: pd.DataFrame,
        window_start: int,
        window_end: int,
    ) -> float:
        """
        è®¡ç®—çª—å£å†…æ¨ªæˆªé¢ Spearman ICï¼ˆä¸¥æ ¼ T-1 å¯¹é½ï¼‰

        Args:
            factor: å› å­å€¼ï¼ˆæ¨ªæˆªé¢ï¼‰
            returns: æ”¶ç›Šç‡ï¼ˆæ¨ªæˆªé¢ï¼‰
            window_start: çª—å£èµ·å§‹ç´¢å¼•
            window_end: çª—å£ç»“æŸç´¢å¼•ï¼ˆä¸å«ï¼‰

        Returns:
            å¹³å‡ IC å€¼
        """
        # T-1 å¯¹é½ï¼šå› å­ [start-1, end-1)ï¼Œæ”¶ç›Š [start, end)
        factor_start = max(0, window_start - 1)
        factor_end = max(0, window_end - 1)

        factor_slice = factor.iloc[factor_start:factor_end]
        return_slice = returns.iloc[window_start:window_end]

        # é•¿åº¦ä¿æŠ¤
        n_days = min(len(factor_slice), len(return_slice))

        daily_ics = []
        for t in range(n_days):
            factor_t = factor_slice.iloc[t].values
            return_t = return_slice.iloc[t].values

            # æœ‰æ•ˆæ©ç 
            valid_mask = ~(np.isnan(factor_t) | np.isnan(return_t))
            if valid_mask.sum() < 5:
                continue

            ic, _ = spearmanr(factor_t[valid_mask], return_t[valid_mask])
            if not np.isnan(ic):
                daily_ics.append(float(ic))

        return np.mean(daily_ics) if daily_ics else 0.0

    def evaluate_factor_wfo(
        self, factor: pd.DataFrame, returns: pd.DataFrame, factor_name: str
    ) -> Dict:
        """
        WFO è¯„ä¼°å•ä¸ªå› å­ï¼ˆæ²¿ç”¨ç³»ç»Ÿé…ç½®ï¼‰

        Returns:
            {
                'factor_name': str,
                'n_windows': int,
                'is_ic_mean': float,
                'oos_ic_mean': float,
                'ic_decay': float,
                'decay_ratio': float,
                'failure_ratio': float,
                'windows': List[Dict]
            }
        """
        logger.info(f"\nğŸ“ˆ WFO è¯„ä¼°: {factor_name}")

        n_days = len(returns)
        windows = []

        # ç”Ÿæˆ WFO çª—å£
        for start in range(0, n_days - self.is_window - self.oos_window + 1, self.step):
            is_start = start
            is_end = start + self.is_window
            oos_start = is_end
            oos_end = min(oos_start + self.oos_window, n_days)

            # IS IC
            is_ic = self.compute_cross_sectional_ic(factor, returns, is_start, is_end)

            # OOS IC
            oos_ic = self.compute_cross_sectional_ic(
                factor, returns, oos_start, oos_end
            )

            windows.append(
                {
                    "is_start": is_start,
                    "is_end": is_end,
                    "oos_start": oos_start,
                    "oos_end": oos_end,
                    "is_ic": is_ic,
                    "oos_ic": oos_ic,
                    "ic_decay": is_ic - oos_ic,
                }
            )

        # æ±‡æ€»ç»Ÿè®¡
        is_ics = [w["is_ic"] for w in windows]
        oos_ics = [w["oos_ic"] for w in windows]
        decays = [w["ic_decay"] for w in windows]

        is_ic_mean = np.mean(is_ics)
        oos_ic_mean = np.mean(oos_ics)
        ic_decay = np.mean(decays)

        # è¡°å‡æ¯”ä¾‹ï¼ˆé¿å…é™¤é›¶ï¼‰
        decay_ratio = (
            abs(ic_decay / (is_ic_mean + 1e-8)) if abs(is_ic_mean) > 1e-8 else 999.0
        )

        # å¤±è´¥çª—å£æ¯”ä¾‹ï¼ˆOOS IC < 0ï¼‰
        failure_count = sum(1 for ic in oos_ics if ic < 0)
        failure_ratio = failure_count / len(windows) if windows else 1.0

        logger.info(f"  - çª—å£æ•°: {len(windows)}")
        logger.info(f"  - IS IC å‡å€¼: {is_ic_mean:.4f}")
        logger.info(f"  - OOS IC å‡å€¼: {oos_ic_mean:.4f}")
        logger.info(f"  - IC è¡°å‡: {ic_decay:.4f}")
        logger.info(f"  - è¡°å‡æ¯”ä¾‹: {decay_ratio:.2%}")
        logger.info(f"  - å¤±è´¥çª—å£: {failure_ratio:.2%}")

        return {
            "factor_name": factor_name,
            "n_windows": len(windows),
            "is_ic_mean": is_ic_mean,
            "oos_ic_mean": oos_ic_mean,
            "ic_decay": ic_decay,
            "decay_ratio": decay_ratio,
            "failure_ratio": failure_ratio,
            "windows": windows,
        }

    def check_correlation_with_top3(
        self, factor: pd.DataFrame, top3_factors: Dict[str, pd.DataFrame]
    ) -> Dict[str, float]:
        """æ£€æŸ¥ä¸ Top3 ç¨³å®šå› å­çš„ç›¸å…³æ€§ï¼ˆæ¨ªæˆªé¢æ—¶é—´åºåˆ—ç›¸å…³ï¼‰"""
        logger.info("ğŸ” æ£€æŸ¥ä¸ Top3 å› å­çš„ç›¸å…³æ€§...")

        correlations = {}

        for name, top3_factor in top3_factors.items():
            # å¯¹é½ç´¢å¼•
            common_idx = factor.index.intersection(top3_factor.index)

            if len(common_idx) < 100:
                correlations[name] = np.nan
                continue

            factor_aligned = factor.loc[common_idx]
            top3_aligned = top3_factor.loc[common_idx]

            # å…¨é¢å±•å¼€æˆå‘é‡ï¼ˆæ—¶é—´ Ã— èµ„äº§ï¼‰
            factor_vec = factor_aligned.values.flatten()
            top3_vec = top3_aligned.values.flatten()

            # æœ‰æ•ˆæ©ç 
            valid_mask = ~(np.isnan(factor_vec) | np.isnan(top3_vec))

            if valid_mask.sum() < 100:
                correlations[name] = np.nan
                continue

            corr, _ = spearmanr(factor_vec[valid_mask], top3_vec[valid_mask])
            correlations[name] = corr

            logger.info(f"  - {name}: {corr:.4f}")

        return correlations

    def evaluate_all_candidates(self) -> pd.DataFrame:
        """è¯„ä¼°æ‰€æœ‰å€™é€‰å› å­å¹¶è¾“å‡ºæŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹å€™é€‰å› å­ç¦»çº¿è¯„ä¼°")
        logger.info("=" * 80)

        # åŠ è½½æ•°æ®
        close, volume, returns, top3_factors = self.load_data()

        # è®¡ç®—å€™é€‰å› å­
        candidates = {
            "REVERSAL_FACTOR_5D": self.compute_reversal_5d(close),
            "VOLATILITY_SKEW_20D": self.compute_volatility_skew_20d(close),
            "DOLLAR_VOLUME_ACCELERATION_10D": self.compute_dollar_volume_accel_10d(
                close, volume
            ),
        }

        # è¯„ä¼°ç»“æœ
        results = []

        for factor_name, factor_data in candidates.items():
            # WFO è¯„ä¼°
            wfo_result = self.evaluate_factor_wfo(factor_data, returns, factor_name)

            # Top3 ç›¸å…³æ€§
            correlations = self.check_correlation_with_top3(factor_data, top3_factors)
            median_corr = np.nanmedian(list(correlations.values()))

            # å‡†å…¥åˆ¤å®š
            pass_oos_ic = wfo_result["oos_ic_mean"] >= self.min_oos_ic
            pass_decay = wfo_result["decay_ratio"] <= self.max_decay_ratio
            pass_failure = wfo_result["failure_ratio"] <= self.max_failure_ratio
            pass_corr = median_corr < self.max_top3_corr

            all_pass = pass_oos_ic and pass_decay and pass_failure and pass_corr

            results.append(
                {
                    "factor_name": factor_name,
                    "oos_ic_mean": wfo_result["oos_ic_mean"],
                    "ic_decay_ratio": wfo_result["decay_ratio"],
                    "failure_ratio": wfo_result["failure_ratio"],
                    "top3_median_corr": median_corr,
                    "pass_oos_ic": pass_oos_ic,
                    "pass_decay": pass_decay,
                    "pass_failure": pass_failure,
                    "pass_corr": pass_corr,
                    "PASS_ALL": all_pass,
                }
            )

        # è¾“å‡ºæŠ¥å‘Š
        df = pd.DataFrame(results)

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
        logger.info("=" * 80)
        logger.info(f"\n{df.to_string(index=False)}")

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ å‡†å…¥é—¨æ§›æ£€æŸ¥")
        logger.info("=" * 80)
        logger.info(f"  - OOS IC â‰¥ {self.min_oos_ic}")
        logger.info(f"  - è¡°å‡æ¯” â‰¤ {self.max_decay_ratio:.0%}")
        logger.info(f"  - å¤±è´¥ç‡ â‰¤ {self.max_failure_ratio:.0%}")
        logger.info(f"  - Top3ç›¸å…³ < {self.max_top3_corr}")

        # æœ€ç»ˆè£å†³
        passed = df[df["PASS_ALL"] == True]
        rejected = df[df["PASS_ALL"] == False]

        logger.info("\n" + "=" * 80)
        logger.info("âœ… é€šè¿‡å‡†å…¥é—¨æ§›:")
        logger.info("=" * 80)
        if len(passed) > 0:
            for _, row in passed.iterrows():
                logger.info(f"  âœ… {row['factor_name']}")
                logger.info(f"     - OOS IC: {row['oos_ic_mean']:.4f}")
                logger.info(f"     - è¡°å‡æ¯”: {row['ic_decay_ratio']:.2%}")
                logger.info(f"     - å¤±è´¥ç‡: {row['failure_ratio']:.2%}")
                logger.info(f"     - Top3ç›¸å…³: {row['top3_median_corr']:.4f}")
        else:
            logger.info("  æ— å› å­é€šè¿‡")

        logger.info("\n" + "=" * 80)
        logger.info("âŒ æœªé€šè¿‡å‡†å…¥é—¨æ§›:")
        logger.info("=" * 80)
        if len(rejected) > 0:
            for _, row in rejected.iterrows():
                logger.info(f"  âŒ {row['factor_name']}")
                reasons = []
                if not row["pass_oos_ic"]:
                    reasons.append(
                        f"OOS ICä¸è¶³({row['oos_ic_mean']:.4f}<{self.min_oos_ic})"
                    )
                if not row["pass_decay"]:
                    reasons.append(
                        f"è¡°å‡è¿‡å¤§({row['ic_decay_ratio']:.2%}>{self.max_decay_ratio:.0%})"
                    )
                if not row["pass_failure"]:
                    reasons.append(
                        f"å¤±è´¥ç‡é«˜({row['failure_ratio']:.2%}>{self.max_failure_ratio:.0%})"
                    )
                if not row["pass_corr"]:
                    reasons.append(
                        f"ç›¸å…³åº¦é«˜({row['top3_median_corr']:.4f}>{self.max_top3_corr})"
                    )
                logger.info(f"     åŸå› : {', '.join(reasons)}")
        else:
            logger.info("  æ‰€æœ‰å› å­å‡é€šè¿‡")

        return df


def main():
    """ä¸»å‡½æ•°"""
    # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®ç›®å½•
    results_dir = Path(__file__).parent.parent / "results"

    # æŸ¥æ‰¾æœ€æ–°çš„ cross_section ç›®å½•
    cross_section_base = results_dir / "cross_section" / "20251027"
    latest_cross = sorted(cross_section_base.glob("*"))[-1]
    ohlcv_dir = latest_cross / "ohlcv"

    # æŸ¥æ‰¾æœ€æ–°çš„ factor_selection ç›®å½•
    factor_sel_base = results_dir / "factor_selection" / "20251027"
    latest_factor = sorted(factor_sel_base.glob("*"))[-1]
    factors_dir = latest_factor / "standardized"

    logger.info(f"ğŸ“ æ•°æ®ç›®å½•:")
    logger.info(f"  - OHLCV: {ohlcv_dir}")
    logger.info(f"  - æ ‡å‡†åŒ–å› å­: {factors_dir}")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CandidateFactorEvaluator(
        ohlcv_dir=str(ohlcv_dir), existing_factors_dir=str(factors_dir)
    )

    # æ‰§è¡Œè¯„ä¼°
    results_df = evaluator.evaluate_all_candidates()

    # ä¿å­˜ç»“æœ
    output_dir = Path(__file__).parent
    output_file = (
        output_dir
        / f"candidate_factors_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )
    results_df.to_csv(output_file, index=False)

    logger.info(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_file}")

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
