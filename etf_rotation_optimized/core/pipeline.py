"""
æµç¨‹ç¼–æ’å™¨ | Pipeline Orchestrator

ç»Ÿä¸€æ‰§è¡Œå…¥å£ï¼Œæ›¿ä»£ scripts/step*.py çš„æ‰‹åŠ¨æµç¨‹

å·¥ä½œæµ:
  é…ç½®æ–‡ä»¶ â†’ Pipeline.from_config()
    â†“
  æ¨ªæˆªé¢åŠ å·¥ (cross_section)
    â†“
  å› å­ç­›é€‰ (factor_selection)
    â†“
  WFOéªŒè¯ (wfo)
    â†“
  VBTå›æµ‹ (backtest)

ä½œè€…: Linus Refactor
æ—¥æœŸ: 2025-10-28
"""

import json
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import yaml

from .cross_section_processor import CrossSectionProcessor
from .precise_factor_library_v2 import PreciseFactorLibrary

logger = logging.getLogger(__name__)


@dataclass
class PipelineConfig:
    """æµç¨‹é…ç½®"""

    run_id: str
    data: Dict
    cross_section: Dict
    factor_selection: Dict
    wfo: Dict
    backtest: Dict
    output_root: Path


class Pipeline:
    """
    ETFè½®åŠ¨ç³»ç»Ÿæµç¨‹ç¼–æ’å™¨

    è´Ÿè´£æŒ‰é¡ºåºæ‰§è¡Œ: æ¨ªæˆªé¢ -> å› å­ç­›é€‰ -> WFO -> å›æµ‹
    æ¯ä¸ªé˜¶æ®µè¾“å‡ºè½ç›˜ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
    """

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_date = self.timestamp[:8]

        # è¾“å‡ºç›®å½•
        self.output_root = config.output_root
        self.cross_section_dir = (
            self.output_root / "cross_section" / self.run_date / self.timestamp
        )
        self.factor_selection_dir = (
            self.output_root / "factor_selection" / self.run_date / self.timestamp
        )
        self.wfo_dir = self.output_root / "wfo" / self.run_date / self.timestamp
        self.backtest_dir = (
            self.output_root / "backtest" / self.run_date / self.timestamp
        )

        # åˆ›å»ºç›®å½•
        for d in [
            self.cross_section_dir,
            self.factor_selection_dir,
            self.wfo_dir,
            self.backtest_dir,
        ]:
            d.mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—
        self._setup_logging()

        # æ•°æ®å®¹å™¨
        self.ohlcv_data: Optional[Dict[str, pd.DataFrame]] = None
        self.factors_dict: Optional[Dict[str, pd.DataFrame]] = None
        self.standardized_factors: Optional[Dict[str, pd.DataFrame]] = None
        self.selected_factors: Optional[List[str]] = None
        self.wfo_results: Optional[pd.DataFrame] = None

    @classmethod
    def from_config(cls, config_path: Path) -> "Pipeline":
        """ä»é…ç½®æ–‡ä»¶åˆ›å»ºPipeline"""
        with open(config_path, encoding="utf-8") as f:
            config_dict = yaml.safe_load(f)

        pipeline_config = PipelineConfig(
            run_id=config_dict.get("run_id", "DEFAULT_RUN"),
            data=config_dict.get("data", {}),
            cross_section=config_dict.get("cross_section", {}),
            factor_selection=config_dict.get("factor_selection", {}),
            wfo=config_dict.get("wfo", {}),
            backtest=config_dict.get("backtest", {}),
            output_root=Path(config_dict.get("output_root", "results")),
        )

        return cls(config=pipeline_config)

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        log_file = self.output_root / "logs" / f"pipeline_{self.timestamp}.log"
        log_file.parent.mkdir(parents=True, exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format="[%(asctime)s] %(levelname)s - %(message)s",
            datefmt="%H:%M:%S",
            handlers=[
                logging.FileHandler(log_file, encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )

        logger.info("=" * 80)
        logger.info("ETFè½®åŠ¨ç³»ç»Ÿ - æµç¨‹å¯åŠ¨")
        logger.info("=" * 80)
        logger.info(f"Run ID: {self.config.run_id}")
        logger.info(f"æ—¶é—´æˆ³: {self.timestamp}")
        logger.info(f"è¾“å‡ºæ ¹ç›®å½•: {self.output_root}")
        logger.info("")

    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹")
        logger.info("")

        self.run_step("cross_section")
        self.run_step("factor_selection")
        self.run_step("wfo")
        self.run_step("backtest")

        logger.info("=" * 80)
        logger.info("âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 80)

    def run_step(self, step: str):
        """è¿è¡Œå•ä¸ªæ­¥éª¤"""
        if step == "cross_section":
            self._run_cross_section()
        elif step == "factor_selection":
            self._run_factor_selection()
        elif step == "wfo":
            self._run_wfo()
        elif step == "backtest":
            self._run_backtest()
        else:
            raise ValueError(f"æœªçŸ¥æ­¥éª¤: {step}")

    def _run_cross_section(self):
        """æ¨ªæˆªé¢åŠ å·¥ - åŠ è½½æ•°æ®å¹¶è®¡ç®—å› å­"""
        logger.info("-" * 80)
        logger.info("Step 1: æ¨ªæˆªé¢åŠ å·¥")
        logger.info("-" * 80)

        # 1. åŠ è½½æ•°æ®
        from .data_loader import DataLoader

        loader = DataLoader()
        symbols = self.config.data.get("symbols", [])
        start_date = self.config.data.get("start_date")
        end_date = self.config.data.get("end_date")

        logger.info(f"åŠ è½½æ•°æ®: {len(symbols)} åªæ ‡çš„")
        logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} -> {end_date}")

        self.ohlcv_data = loader.load_ohlcv(
            etf_codes=symbols, start_date=start_date, end_date=end_date
        )

        # æ•°æ®å¥‘çº¦éªŒè¯
        from .data_contract import DataContract

        DataContract.validate_ohlcv(self.ohlcv_data)

        data_summary = loader.get_summary(self.ohlcv_data)
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {data_summary['total_dates']} å¤©")
        logger.info("")

        # 2. è®¡ç®—å› å­
        logger.info("è®¡ç®—ç²¾ç¡®å› å­...")
        lib = PreciseFactorLibrary()
        factors_df = lib.compute_all_factors(prices=self.ohlcv_data)

        # è½¬æ¢ä¸ºå­—å…¸
        self.factors_dict = {}
        for factor_name in lib.list_factors():
            self.factors_dict[factor_name] = factors_df[factor_name]

        logger.info(f"âœ… å› å­è®¡ç®—å®Œæˆ: {len(self.factors_dict)} ä¸ªå› å­")
        logger.info("")

        # 3. ä¿å­˜OHLCV
        ohlcv_dir = self.cross_section_dir / "ohlcv"
        ohlcv_dir.mkdir(exist_ok=True)

        for col_name, df in self.ohlcv_data.items():
            df.to_parquet(ohlcv_dir / f"{col_name}.parquet")

        logger.info(f"âœ… OHLCVå·²ä¿å­˜: {ohlcv_dir}")

        # 4. ä¿å­˜å› å­
        factors_dir = self.cross_section_dir / "factors"
        factors_dir.mkdir(exist_ok=True)

        for fname, fdata in self.factors_dict.items():
            if isinstance(fdata, pd.Series):
                df_to_save = fdata.to_frame(name=fname)
            else:
                df_to_save = fdata
            df_to_save.to_parquet(factors_dir / f"{fname}.parquet")

        logger.info(f"âœ… å› å­å·²ä¿å­˜: {factors_dir}")

        # 5. ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "timestamp": self.timestamp,
            "step": "cross_section",
            "symbols": symbols,
            "date_range": [start_date, end_date],
            "factor_count": len(self.factors_dict),
            "factor_names": list(self.factors_dict.keys()),
        }

        with open(self.cross_section_dir / "metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… Step 1 å®Œæˆ: {self.cross_section_dir}")
        logger.info("")

    def _run_factor_selection(self):
        """å› å­ç­›é€‰ - æ ‡å‡†åŒ–å¤„ç†"""
        logger.info("-" * 80)
        logger.info("Step 2: å› å­ç­›é€‰ï¼ˆæ ‡å‡†åŒ–ï¼‰")
        logger.info("-" * 80)

        # åŠ è½½æ¨ªæˆªé¢æ•°æ®ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if self.factors_dict is None:
            self._load_cross_section_data()

        logger.info(
            f"ğŸ“Š å¾…å¤„ç†å› å­æ•°é‡: {len(self.factors_dict) if self.factors_dict else 0}"
        )

        # æ ‡å‡†åŒ–
        processor = CrossSectionProcessor()
        self.standardized_factors = processor.process_all_factors(self.factors_dict)

        logger.info(f"âœ… å› å­æ ‡å‡†åŒ–å®Œæˆ: {len(self.standardized_factors)} ä¸ª")
        logger.info("")

        # ä¿å­˜æ ‡å‡†åŒ–å› å­
        standardized_dir = self.factor_selection_dir / "standardized"
        standardized_dir.mkdir(parents=True, exist_ok=True)

        for fname, fdata in self.standardized_factors.items():
            fdata.to_parquet(standardized_dir / f"{fname}.parquet")

        logger.info(f"âœ… æ ‡å‡†åŒ–å› å­å·²ä¿å­˜: {standardized_dir}")

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "timestamp": self.timestamp,
            "step": "factor_selection",
            "standardized_factor_count": len(self.standardized_factors),
            "standardized_factor_names": list(self.standardized_factors.keys()),
        }

        with open(
            self.factor_selection_dir / "metadata.json", "w", encoding="utf-8"
        ) as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        logger.info(f"âœ… Step 2 å®Œæˆ: {self.factor_selection_dir}")
        logger.info("")

    def _run_wfo(self):
        """WFOéªŒè¯ - é›†æˆå‰å‘å›æµ‹"""
        logger.info("-" * 80)
        logger.info("Step 3: WFOéªŒè¯")
        logger.info("-" * 80)

        # åŠ è½½OHLCVæ•°æ®ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if self.ohlcv_data is None:
            self._load_cross_section_data()

        # åŠ è½½æ ‡å‡†åŒ–å› å­ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
        if self.standardized_factors is None:
            self._load_factor_selection_data()

        # å‡†å¤‡WFOæ•°æ®
        factor_names = sorted(self.standardized_factors.keys())
        factor_arrays = []

        for factor_name in factor_names:
            factor_df = self.standardized_factors[factor_name]
            factor_arrays.append(factor_df.values)

        # å †å : (K, T, N) â†’ (T, N, K)
        import numpy as np

        factors_array = np.stack(factor_arrays, axis=0)
        factors_array = np.transpose(factors_array, (1, 2, 0))

        # æå–æ”¶ç›Šç‡
        returns_df = self.ohlcv_data["close"].pct_change(fill_method=None)
        returns_array = returns_df.values

        # è·³è¿‡å› å­é¢„çƒ­æœŸï¼ˆæ ¹æ®å®é™…æµ‹è¯•éœ€è¦371å¤©ï¼‰
        # åŸå› ï¼šVOL_RATIO_60Déœ€è¦119å¤© + ISçª—å£252å¤© = 371å¤©
        warmup_offset = 371
        if factors_array.shape[0] > warmup_offset:
            logger.info(f"âš ï¸  è·³è¿‡å‰{warmup_offset}å¤©å› å­é¢„çƒ­æœŸ")
            factors_array = factors_array[warmup_offset:]
            returns_array = returns_array[warmup_offset:]

        logger.info(f"WFOæ•°æ®: {factors_array.shape}")
        logger.info("")

        # åŠ è½½çº¦æŸé…ç½®
        constraints_path = Path("configs/FACTOR_SELECTION_CONSTRAINTS.yaml")
        if constraints_path.exists():
            with open(constraints_path, encoding="utf-8") as f:
                constraints_config = yaml.safe_load(f)
        else:
            constraints_config = {}

        # è¿è¡ŒDirect Factor WFO (æ–°ç‰ˆæœ¬ - ç›´æ¥å› å­çº§åŠ æƒ)
        wfo_config = self.config.wfo
        from core.direct_factor_wfo_optimizer import DirectFactorWFOOptimizer

        optimizer = DirectFactorWFOOptimizer(
            factor_weighting=wfo_config.get("factor_weighting", "ic_weighted"),
            min_factor_ic=wfo_config.get("min_factor_ic", 0.01),
            ic_floor=wfo_config.get("ic_floor", 0.0),
            verbose=True,
        )

        wfo_results_list, wfo_summary_df = optimizer.run_wfo(
            factors_data=factors_array,
            returns=returns_array,
            factor_names=factor_names,
            is_period=wfo_config.get("is_period", 252),
            oos_period=wfo_config.get("oos_period", 60),
            step_size=wfo_config.get("step_size", 20),
        )

        self.wfo_results = wfo_results_list

        # ä¿å­˜ç»“æœ
        wfo_summary_df.to_csv(self.wfo_dir / "wfo_summary.csv", index=False)
        logger.info(f"   - ç»“æœå·²ä¿å­˜: {self.wfo_dir / 'wfo_summary.csv'}")

        # ğŸ”§ Phase 1: è®¡ç®—çœŸå®æ”¶ç›Šå’ŒKPIï¼ˆäº‹ä»¶é©±åŠ¨ T+1 Top-Nï¼‰
        logger.info("   - Phase 1: è®¡ç®—çœŸå®æ”¶ç›Šå’ŒKPI...")
        from .wfo_performance_evaluator_basic import WfoPerformanceEvaluator

        backtest_cfg = self.config.backtest or {}
        top_n = int(backtest_cfg.get("top_n", 6))

        # dates å¯¹é½åˆ° warmup ä¹‹å
        dates_aligned = returns_df.index[-factors_array.shape[0] :]

        evaluator = WfoPerformanceEvaluator(top_n=top_n)
        evaluator.evaluate_and_save(
            results_list=wfo_results_list,
            factors=factors_array,
            returns=returns_array,
            factor_names=factor_names,
            dates=dates_aligned,
            out_dir=self.wfo_dir,
        )

        # ğŸ”§ Phase 2: å¤šç­–ç•¥æšä¸¾ + Top-5 ç»„åˆé€‰æ‹©ï¼ˆä¸¥æ ¼T+1çœŸå®æ”¶ç›Šï¼‰
        logger.info("\n   - Phase 2: å¤šç­–ç•¥æšä¸¾ + Top-5 ç»„åˆé€‰æ‹©â€¦")
        from .wfo_multi_strategy_selector import WFOMultiStrategySelector

        phase2_cfg = (self.config.wfo or {}).get("phase2", {})
        selector = WFOMultiStrategySelector(
            min_factor_freq=phase2_cfg.get("min_factor_freq", 0.3),
            min_factors=phase2_cfg.get("min_factors", 3),
            max_factors=phase2_cfg.get("max_factors", 5),
            subset_mode=phase2_cfg.get("subset_mode", "enumerate"),
            tau_grid=phase2_cfg.get("tau_grid", [0.7, 1.0, 1.5]),
            topn_grid=phase2_cfg.get("topn_grid", [top_n]),
            signal_z_threshold_grid=phase2_cfg.get("signal_z_threshold_grid", [None]),
            max_strategies=phase2_cfg.get("max_strategies", 200),
            non_overlap_oos=phase2_cfg.get("non_overlap_oos", False),
            turnover_penalty=phase2_cfg.get("turnover_penalty", 0.0),
            coverage_penalty_coef=phase2_cfg.get("coverage_penalty_coef", 1.0),
            coverage_min=phase2_cfg.get("coverage_min", 0.0),
            avg_turnover_max=phase2_cfg.get("avg_turnover_max", None),
            rank_by=phase2_cfg.get("rank_by", "score"),
            stratified_by_k=phase2_cfg.get("stratified_by_k", False),
            k_quota=phase2_cfg.get("k_quota", None),
            subset_shuffle=phase2_cfg.get("subset_shuffle", False),
            random_seed=phase2_cfg.get("random_seed", None),
        )

        top5_df = selector.select_and_save(
            results_list=wfo_results_list,
            factors=factors_array,
            returns=returns_array,
            factor_names=factor_names,
            dates=dates_aligned,
            out_dir=self.wfo_dir,
        )

        # ğŸ”§ å†™å…¥å…ƒæ•°æ®
        logger.info("\n   - å†™å…¥å…ƒæ•°æ®...")
        from .wfo_metadata_writer import WFOMetadataWriter

        WFOMetadataWriter.write_metadata(
            out_dir=self.wfo_dir,
            config_path=Path("configs/default.yaml"),
            wfo_results_count=len(wfo_results_list),
            strategies_count=len(top5_df) if top5_df is not None else 0,
            phase2_params={
                "min_factor_freq": phase2_cfg.get("min_factor_freq", 0.3),
                "min_factors": phase2_cfg.get("min_factors", 3),
                "max_factors": phase2_cfg.get("max_factors", 5),
                "subset_mode": phase2_cfg.get("subset_mode", "enumerate"),
                "tau_grid": phase2_cfg.get("tau_grid", [0.7, 1.0, 1.5]),
                "topn_grid": phase2_cfg.get("topn_grid", [top_n]),
                "signal_z_threshold_grid": phase2_cfg.get(
                    "signal_z_threshold_grid", [None]
                ),
                "max_strategies": phase2_cfg.get("max_strategies", 200),
                "non_overlap_oos": phase2_cfg.get("non_overlap_oos", False),
                "turnover_penalty": phase2_cfg.get("turnover_penalty", 0.0),
                "coverage_penalty_coef": phase2_cfg.get("coverage_penalty_coef", 1.0),
                "coverage_min": phase2_cfg.get("coverage_min", 0.0),
                "avg_turnover_max": phase2_cfg.get("avg_turnover_max", None),
                "rank_by": phase2_cfg.get("rank_by", "score"),
                "stratified_by_k": phase2_cfg.get("stratified_by_k", False),
                "k_quota": phase2_cfg.get("k_quota", None),
                "subset_shuffle": phase2_cfg.get("subset_shuffle", False),
                "random_seed": phase2_cfg.get("random_seed", None),
            },
        )

        logger.info("\nâœ… WFOå®Œæ•´æµç¨‹å®Œæˆ")
        logger.info(f"   - æ€»çª—å£æ•°: {len(self.wfo_results)}")
        logger.info(f"   - å¹³å‡OOS IC: {wfo_summary_df['oos_ensemble_ic'].mean():.4f}")
        logger.info("\nâš ï¸  ä¿¡å·å»¶è¿Ÿè¯´æ˜ (P0-2):")
        logger.info("   - WFOé˜¶æ®µ: æ— å»¶è¿Ÿ (çº¯ä¿¡å·ICéªŒè¯)")
        logger.info("   - å›æµ‹é˜¶æ®µ: T+1å»¶è¿Ÿç”±VectorBTå±‚æˆ–portfolio_constructoråº”ç”¨")
        logger.info("   - é…ç½®ä½ç½®: configs/default.yaml::backtest.signal_delay_days=1")
        logger.info("   - ç”Ÿæ•ˆè¯æ®: å›æµ‹ç»“æœä¸­ä¿¡å·[t]å¯¹åº”æŒä»“[t+1]")
        logger.info(f"âœ… Step 3 å®Œæˆ: {self.wfo_dir}")
        logger.info("")

    def _run_backtest(self):
        """VBTå›æµ‹ - æš´åŠ›æµ‹è¯•"""
        logger.info("-" * 80)
        logger.info("Step 4: VBTå›æµ‹")
        logger.info("-" * 80)

        # å›æµ‹æ¨¡å—ç‹¬ç«‹è¿è¡Œ
        # ä½¿ç”¨: python vectorbt_backtest/run_backtest.py --config configs/backtest_config.yaml
        logger.info("âœ“ å›æµ‹æ¨¡å—ä½äº vectorbt_backtest/")
        logger.info("  è¿è¡Œå‘½ä»¤: python vectorbt_backtest/run_backtest.py")
        logger.info("  é…ç½®æ–‡ä»¶: vectorbt_backtest/configs/backtest_config.yaml")
        logger.info("")
        logger.info("âš ï¸  å›æµ‹æ¨¡å—ç‹¬ç«‹è¿è¡Œï¼Œä¸é›†æˆåˆ°Pipelineä¸­")
        logger.info("  åŸå› : å›æµ‹éœ€è¦å®Œæ•´å†å²æ•°æ®ï¼Œä¸WFOéªŒè¯é€»è¾‘åˆ†ç¦»")
        logger.info("")

    def _load_cross_section_data(self):
        """åŠ è½½æ¨ªæˆªé¢æ•°æ®"""
        logger.info("åŠ è½½æ¨ªæˆªé¢æ•°æ®...")

        # æŸ¥æ‰¾æœ€æ–°çš„cross_sectionç»“æœï¼ˆå¦‚æœå½“å‰ç›®å½•ä¸ºç©ºï¼‰
        ohlcv_dir = self.cross_section_dir / "ohlcv"
        factors_dir = self.cross_section_dir / "factors"

        if not ohlcv_dir.exists() or not list(factors_dir.glob("*.parquet")):
            results_base = self.output_root / "cross_section"
            if results_base.exists():
                date_dirs = sorted(
                    [d for d in results_base.glob("*") if d.is_dir()], reverse=True
                )
                for date_dir in date_dirs:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æ—¶é—´æˆ³ç›®å½•
                    time_dirs = sorted(
                        [d for d in date_dir.glob("*") if d.is_dir()], reverse=True
                    )
                    for time_dir in time_dirs:
                        test_factors_dir = time_dir / "factors"
                        if test_factors_dir.exists() and list(
                            test_factors_dir.glob("*.parquet")
                        ):
                            self.cross_section_dir = time_dir
                            ohlcv_dir = self.cross_section_dir / "ohlcv"
                            factors_dir = self.cross_section_dir / "factors"
                            logger.info(f"ä½¿ç”¨æœ€æ–°æ¨ªæˆªé¢ç»“æœ: {self.cross_section_dir}")
                            break
                    if (
                        self.cross_section_dir
                        != self.output_root
                        / "cross_section"
                        / self.run_date
                        / self.timestamp
                    ):
                        break

        # åŠ è½½OHLCV
        self.ohlcv_data = {}
        for col_name in ["open", "high", "low", "close", "volume"]:
            parquet_path = ohlcv_dir / f"{col_name}.parquet"
            if parquet_path.exists():
                self.ohlcv_data[col_name] = pd.read_parquet(parquet_path)

        # åŠ è½½å› å­
        factors_dir = self.cross_section_dir / "factors"
        self.factors_dict = {}
        for factor_file in factors_dir.glob("*.parquet"):
            factor_name = factor_file.stem
            self.factors_dict[factor_name] = pd.read_parquet(factor_file)

        logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(self.factors_dict)} ä¸ªå› å­")

    def _load_factor_selection_data(self):
        """åŠ è½½å› å­ç­›é€‰æ•°æ®"""
        logger.info("åŠ è½½å› å­ç­›é€‰æ•°æ®...")

        # æŸ¥æ‰¾æœ€æ–°çš„factor_selectionç»“æœï¼ˆå¦‚æœå½“å‰ç›®å½•ä¸ºç©ºï¼‰
        standardized_dir = self.factor_selection_dir / "standardized"

        if not standardized_dir.exists() or not list(
            standardized_dir.glob("*.parquet")
        ):
            results_base = self.output_root / "factor_selection"
            if results_base.exists():
                date_dirs = sorted(
                    [d for d in results_base.glob("*") if d.is_dir()], reverse=True
                )
                for date_dir in date_dirs:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æ—¶é—´æˆ³ç›®å½•
                    time_dirs = sorted(
                        [d for d in date_dir.glob("*") if d.is_dir()], reverse=True
                    )
                    for time_dir in time_dirs:
                        test_standardized_dir = time_dir / "standardized"
                        if test_standardized_dir.exists() and list(
                            test_standardized_dir.glob("*.parquet")
                        ):
                            self.factor_selection_dir = time_dir
                            standardized_dir = test_standardized_dir
                            logger.info(
                                f"ä½¿ç”¨æœ€æ–°å› å­ç­›é€‰ç»“æœ: {self.factor_selection_dir}"
                            )
                            break
                    if standardized_dir.exists() and list(
                        standardized_dir.glob("*.parquet")
                    ):
                        break

        # åŠ è½½æ ‡å‡†åŒ–å› å­
        if not standardized_dir.exists() or not list(
            standardized_dir.glob("*.parquet")
        ):
            raise FileNotFoundError(
                f"æœªæ‰¾åˆ°æ ‡å‡†åŒ–å› å­ç›®å½•: {standardized_dir}\n"
                "è¯·å…ˆè¿è¡Œæ¨ªæˆªé¢å¤„ç†å’Œå› å­ç­›é€‰æ­¥éª¤"
            )

        self.standardized_factors = {}
        for factor_file in standardized_dir.glob("*.parquet"):
            factor_name = factor_file.stem
            self.standardized_factors[factor_name] = pd.read_parquet(factor_file)

        logger.info(f"âœ… åŠ è½½å®Œæˆ: {len(self.standardized_factors)} ä¸ªæ ‡å‡†åŒ–å› å­")
