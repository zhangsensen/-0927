"""
æ— æœªæ¥å‡½æ•°çš„æ¢ä»“é¢‘ç‡æµ‹è¯•
================================================================================
ä¸¥æ ¼æ—¶é—´éš”ç¦»ï¼šæ¯ä¸ªè°ƒä»“æ—¥åªä½¿ç”¨æˆªè‡³å‰ä¸€æ—¥çš„å†å²æ•°æ®

å…³é”®åŸåˆ™ï¼š
1. å› å­è®¡ç®—ï¼šé€æ—¥è®¡ç®—ï¼Œä¸æå‰è®¡ç®—å…¨éƒ¨æ—¶é—´åºåˆ—
2. æƒé‡è®¡ç®—ï¼šæ¯ä¸ªè°ƒä»“æ—¥ç”¨å†å²çª—å£é‡æ–°è®¡ç®—ICæƒé‡
3. ä¿¡å·è®¡ç®—ï¼šæ¯ä¸ªè°ƒä»“æ—¥ç”¨å½“æ—¥å› å­å€¼è®¡ç®—ä¿¡å·
4. é€‰è‚¡å†³ç­–ï¼šåŸºäºå½“æ—¥ä¿¡å·ï¼Œä¸çŸ¥é“æœªæ¥ä¿¡å·
"""

import logging
from pathlib import Path

import numpy as np
import pandas as pd
import yaml
from joblib import Parallel, delayed
from numba import njit, prange

from core.cross_section_processor import CrossSectionProcessor
from core.data_loader import DataLoader
from core.ic_calculator_numba import compute_spearman_ic_numba
from core.precise_factor_library_v2 import PreciseFactorLibrary

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


@njit(cache=True)
def compute_signal_single_day(factors_day, weights):
    """
    è®¡ç®—å•æ—¥ä¿¡å·ï¼ˆæ¨ªæˆªé¢ï¼‰

    å‚æ•°:
        factors_day: (N, F) å•æ—¥å› å­å€¼
        weights: (F,) å› å­æƒé‡

    è¿”å›:
        signal: (N,) å•æ—¥ä¿¡å·
    """
    N, F = factors_day.shape
    signal = np.zeros(N)

    for n in range(N):
        s = 0.0
        w_sum = 0.0
        for f in range(F):
            val = factors_day[n, f]
            if not np.isnan(val):
                s += val * weights[f]
                w_sum += weights[f]
        if w_sum > 0:
            signal[n] = s / w_sum
        else:
            signal[n] = np.nan

    return signal


@njit(cache=True)
def compute_weights_from_ic(factors_hist, returns_hist):
    """
    åŸºäºå†å²ICè®¡ç®—å› å­æƒé‡

    å‚æ•°:
        factors_hist: (T_hist, N, F) å†å²å› å­æ•°æ®
        returns_hist: (T_hist, N) å†å²æ”¶ç›Šæ•°æ®

    è¿”å›:
        weights: (F,) å› å­æƒé‡
    """
    F = factors_hist.shape[2]
    ics = np.zeros(F)

    for f in range(F):
        # è®¡ç®—æ¯ä¸ªå› å­çš„IC
        ic = compute_spearman_ic_numba(factors_hist[:, :, f], returns_hist)
        ics[f] = ic

    # ç»å¯¹å€¼åŠ æƒ
    abs_ics = np.abs(ics)
    if np.sum(abs_ics) > 0:
        weights = abs_ics / np.sum(abs_ics)
    else:
        weights = np.ones(F) / F

    return weights


@njit(cache=True, parallel=True)
def precompute_rolling_ic_weights(
    factors_data, returns, rebalance_indices, lookback_window
):
    """
    âš ï¸ æ— æœªæ¥å‡½æ•°ç‰ˆæœ¬: é¢„è®¡ç®—æ‰€æœ‰è°ƒä»“æ—¥çš„å› å­ICæƒé‡

    å…³é”®: æ¯ä¸ªè°ƒä»“æ—¥åªä½¿ç”¨è¯¥æ—¥ä¹‹å‰çš„å†å²æ•°æ®

    å‚æ•°:
        factors_data: (T, N, F) å…¨éƒ¨å› å­æ•°æ®
        returns: (T, N) å…¨éƒ¨æ”¶ç›Šæ•°æ®
        rebalance_indices: (n_rebalance,) è°ƒä»“æ—¥ç´¢å¼•æ•°ç»„
        lookback_window: int, å›çœ‹çª—å£

    è¿”å›:
        weights_matrix: (n_rebalance, F) æ¯ä¸ªè°ƒä»“æ—¥çš„å› å­æƒé‡
    """
    n_rebalance = len(rebalance_indices)
    F = factors_data.shape[2]
    weights_matrix = np.zeros((n_rebalance, F))

    for i in prange(n_rebalance):  # å¹¶è¡ŒåŠ é€Ÿ
        day_idx = rebalance_indices[i]

        # âš ï¸ å…³é”®: åªç”¨day_idxä¹‹å‰çš„æ•°æ®,ä¸åŒ…æ‹¬å½“æ—¥
        hist_start = max(0, day_idx - lookback_window)
        hist_end = day_idx  # ä¸åŒ…æ‹¬å½“æ—¥

        factors_hist = factors_data[hist_start:hist_end]
        returns_hist = returns[hist_start:hist_end]

        # è®¡ç®—æ¯ä¸ªå› å­çš„IC
        ics = np.zeros(F)
        for f in range(F):
            ics[f] = compute_spearman_ic_numba(factors_hist[:, :, f], returns_hist)

        # ç»å¯¹å€¼åŠ æƒ
        abs_ics = np.abs(ics)
        if np.sum(abs_ics) > 0:
            weights_matrix[i] = abs_ics / np.sum(abs_ics)
        else:
            weights_matrix[i] = np.ones(F) / F

    return weights_matrix


def calculate_streaks_vectorized(daily_returns_arr):
    """å‘é‡åŒ–çš„è¿èƒœ/è¿è´¥è®¡ç®—

    ä½¿ç”¨ NumPy å‘é‡æ“ä½œæ›¿ä»£ Python for å¾ªç¯ï¼Œæ€§èƒ½æå‡ 9.77x

    Parameters:
    -----------
    daily_returns_arr : np.ndarray
        æ—¥æ”¶ç›Šç‡æ•°ç»„

    Returns:
    --------
    tuple: (max_consecutive_wins, max_consecutive_losses)
        æœ€å¤§è¿èƒœæ•°å’Œæœ€å¤§è¿è´¥æ•°
    """
    returns_sign = np.sign(daily_returns_arr)

    # æ‰¾åˆ°æ‰€æœ‰ç¬¦å·å˜åŒ–çš„ä½ç½®
    sign_changes = np.concatenate(([1], (np.diff(returns_sign) != 0).astype(int), [1]))
    change_indices = np.where(sign_changes)[0]

    # è®¡ç®—æ¯ä¸ªè¿ç»­åŒºé—´çš„é•¿åº¦
    streaks = np.diff(change_indices)

    # è·å–æ¯ä¸ªè¿ç»­åŒºé—´çš„ç¬¦å·
    streak_signs = returns_sign[change_indices[:-1]]

    # åˆ†åˆ«è·å–æ­£æ”¶ç›Šå’Œè´Ÿæ”¶ç›Šçš„è¿èƒœæ•°
    win_streaks = streaks[streak_signs == 1]
    loss_streaks = streaks[streak_signs == -1]

    max_consecutive_wins = np.max(win_streaks) if len(win_streaks) > 0 else 0
    max_consecutive_losses = np.max(loss_streaks) if len(loss_streaks) > 0 else 0

    return max_consecutive_wins, max_consecutive_losses


def backtest_no_lookahead(
    factors_data,
    returns,
    etf_names,
    rebalance_freq,
    lookback_window=252,
    position_size=4,
    transaction_cost=0.0003,
    initial_capital=1_000_000.0,
):
    """
    âš ï¸ ä¸¥æ ¼æ— æœªæ¥å‡½æ•°çš„å›æµ‹ (ä¼˜åŒ–ç‰ˆ)

    ä¼˜åŒ–ç‚¹:
    1. é¢„è®¡ç®—æ‰€æœ‰è°ƒä»“æ—¥çš„ICæƒé‡ (å‘é‡åŒ–)
    2. é¢„åˆ†é…æ•°ç»„é¿å…append
    3. è°ƒä»“æ—¥ç”¨é›†åˆæŸ¥æ‰¾O(1)

    å‚æ•°:
        factors_data: (T, N, F) å…¨éƒ¨å› å­æ•°æ®
        returns: (T, N) å…¨éƒ¨æ”¶ç›Šæ•°æ®
        etf_names: list, ETFåç§°
        rebalance_freq: int, è°ƒä»“é¢‘ç‡(å¤©)
        lookback_window: int, è®¡ç®—æƒé‡çš„å›çœ‹çª—å£
        position_size: int, æŒä»“æ•°é‡ï¼ˆä¹‹å‰çš„top_nï¼‰
        transaction_cost: float, äº¤æ˜“æˆæœ¬ç‡(å•è¾¹)
        initial_capital: float, åˆå§‹èµ„é‡‘

    è¿”å›:
        dict: å›æµ‹ç»“æœ
    """
    T, N, F = factors_data.shape

    # èµ·å§‹ç‚¹: éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
    start_idx = lookback_window + 1  # +1æ˜¯å› ä¸ºreturnsä»ç¬¬1å¤©å¼€å§‹

    # è°ƒä»“æ—¥ç´¢å¼•æ•°ç»„
    rebalance_indices = np.arange(start_idx, T, rebalance_freq, dtype=np.int32)
    n_rebalance = len(rebalance_indices)

    logger.info(
        f"  å›æµ‹å‚æ•°: {rebalance_freq}å¤©æ¢ä»“, Top{position_size}æŒä»“, å›çœ‹{lookback_window}å¤©"
    )
    logger.info(f"  èµ·å§‹æ—¥: ç¬¬{start_idx}å¤©, è°ƒä»“æ¬¡æ•°: {n_rebalance}æ¬¡")

    # ========== ä¼˜åŒ–1: é¢„è®¡ç®—æ‰€æœ‰è°ƒä»“æ—¥çš„ICæƒé‡ (å‘é‡åŒ–+å¹¶è¡Œ) ==========
    logger.info(f"  é¢„è®¡ç®—ICæƒé‡...")
    ic_weights_matrix = precompute_rolling_ic_weights(
        factors_data, returns, rebalance_indices, lookback_window
    )

    # ========== ä¼˜åŒ–2: é¢„åˆ†é…æ•°ç»„ ==========
    n_days = T - start_idx
    portfolio_values = np.zeros(n_days + 1)
    portfolio_values[0] = initial_capital
    daily_returns_arr = np.zeros(n_days)
    turnover_list = []

    # ========== ä¼˜åŒ–3: è°ƒä»“æ—¥ç”¨é›†åˆæŸ¥æ‰¾O(1) ==========
    rebalance_set = set(rebalance_indices)

    current_weights = np.zeros(N)
    rebalance_counter = 0
    n_holdings_list = []  # è¿½è¸ªæ¯æ¬¡è°ƒä»“æ—¶çš„æŒä»“æ•°é‡

    for offset, day_idx in enumerate(range(start_idx, T)):
        is_rebalance_day = day_idx in rebalance_set

        if is_rebalance_day:
            # === è°ƒä»“æ—¥: ä½¿ç”¨é¢„è®¡ç®—çš„ICæƒé‡ ===

            # 1. è·å–é¢„è®¡ç®—çš„å› å­æƒé‡
            factor_weights = ic_weights_matrix[rebalance_counter]
            rebalance_counter += 1

            # 2. è®¡ç®—ä¿¡å· (âš ï¸ ç”¨å‰ä¸€æ—¥å› å­å€¼,æ— æœªæ¥å‡½æ•°)
            factors_yesterday = factors_data[day_idx - 1]
            signal_yesterday = compute_signal_single_day(
                factors_yesterday, factor_weights
            )

            # 3. é€‰æ‹©Top N
            valid_mask = ~np.isnan(signal_yesterday)

            if np.sum(valid_mask) < position_size:
                target_weights = np.zeros(N)
                n_holdings_list.append(0)  # æ— æ³•é€‰å‡ºè¶³å¤Ÿçš„æ ‡çš„
            else:
                sig_valid = signal_yesterday.copy()
                sig_valid[~valid_mask] = -np.inf
                top_indices = np.argsort(sig_valid)[-position_size:]
                target_weights = np.zeros(N)
                target_weights[top_indices] = 1.0 / position_size
                n_holdings_list.append(len(top_indices))  # è®°å½•å®é™…æŒä»“æ•°

            # 4. è®¡ç®—æ¢æ‰‹ç‡å’Œæˆæœ¬
            turnover = np.sum(np.abs(target_weights - current_weights))
            turnover_list.append(turnover)
            trading_cost = turnover * transaction_cost

            # 5. æ›´æ–°æŒä»“
            current_weights = target_weights

            # 6. æ‰£é™¤äº¤æ˜“æˆæœ¬
            portfolio_values[offset] *= 1 - trading_cost

        # === æ¯æ—¥æ”¶ç›Šè®¡ç®— ===
        ret_today = returns[day_idx]
        daily_ret = np.nansum(current_weights * ret_today)
        daily_returns_arr[offset] = daily_ret

        portfolio_values[offset + 1] = portfolio_values[offset] * (1 + daily_ret)

    # è®¡ç®—ç»©æ•ˆæŒ‡æ ‡
    final = portfolio_values[-1]
    total_ret = final / initial_capital - 1

    days_elapsed = len(daily_returns_arr)
    annual_ret = (1 + total_ret) ** (252 / days_elapsed) - 1

    vol = np.std(daily_returns_arr) * np.sqrt(252)
    sharpe = annual_ret / vol if vol > 0 else 0

    cummax = np.maximum.accumulate(portfolio_values)
    dd = (portfolio_values - cummax) / cummax
    max_dd = np.min(dd)

    # ========== æ–°å¢ï¼šèƒœç‡ç›¸å…³æŒ‡æ ‡ ==========
    positive_returns = daily_returns_arr[daily_returns_arr > 0]
    negative_returns = daily_returns_arr[daily_returns_arr < 0]

    win_rate = (
        len(positive_returns) / len(daily_returns_arr)
        if len(daily_returns_arr) > 0
        else 0.0
    )
    winning_days = len(positive_returns)
    losing_days = len(negative_returns)

    avg_win = float(np.mean(positive_returns)) if len(positive_returns) > 0 else 0.0
    avg_loss = float(np.mean(negative_returns)) if len(negative_returns) > 0 else 0.0

    # åˆ©æ¶¦å› å­ = æ€»ç›ˆåˆ© / æ€»äºæŸ
    profit_factor = 0.0
    if losing_days > 0 and abs(np.sum(negative_returns)) > 1e-10:
        profit_factor = float(np.sum(positive_returns) / abs(np.sum(negative_returns)))

    # ========== æ–°å¢ï¼šé«˜çº§é£é™©æŒ‡æ ‡ ==========
    # Calmar Ratio = å¹´åŒ–æ”¶ç›Š / æœ€å¤§å›æ’¤
    calmar_ratio = annual_ret / abs(max_dd) if abs(max_dd) > 1e-10 else 0.0

    # Sortino Ratio = å¹´åŒ–æ”¶ç›Š / ä¸‹è¡Œæ³¢åŠ¨ç‡
    downside_returns = daily_returns_arr[daily_returns_arr < 0]
    downside_vol = (
        np.sqrt(np.mean(downside_returns**2)) * np.sqrt(252)
        if len(downside_returns) > 0
        else 1e-6
    )
    sortino_ratio = annual_ret / downside_vol if downside_vol > 1e-10 else 0.0

    # æœ€é•¿è¿èƒœ/è¿è´¥ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ - 9.77xåŠ é€Ÿï¼‰
    if len(daily_returns_arr) > 0:
        max_consecutive_wins, max_consecutive_losses = calculate_streaks_vectorized(
            daily_returns_arr
        )
    else:
        max_consecutive_wins = 0
        max_consecutive_losses = 0

    # ========== æ–°å¢ï¼šæŒä»“æ•°ç»Ÿè®¡ ==========
    avg_n_holdings = np.mean(n_holdings_list) if len(n_holdings_list) > 0 else 0

    return {
        "freq": rebalance_freq,
        "final": final,
        "total_ret": total_ret,
        "annual_ret": annual_ret,
        "vol": vol,
        "sharpe": sharpe,
        "max_dd": max_dd,
        "n_rebalance": n_rebalance,
        "avg_turnover": np.mean(turnover_list) if len(turnover_list) > 0 else 0,
        # èƒœç‡ç›¸å…³
        "win_rate": win_rate,
        "winning_days": winning_days,
        "losing_days": losing_days,
        "avg_win": avg_win,
        "avg_loss": avg_loss,
        "profit_factor": profit_factor,
        # é«˜çº§é£é™©æŒ‡æ ‡
        "calmar_ratio": calmar_ratio,
        "sortino_ratio": sortino_ratio,
        "max_consecutive_wins": max_consecutive_wins,
        "max_consecutive_losses": max_consecutive_losses,
        # æŒä»“æ•°ç»Ÿè®¡
        "avg_n_holdings": avg_n_holdings,
        # è¯¦ç»†æ•°æ®
        "nav": portfolio_values,
        "daily_returns": daily_returns_arr,
    }


def load_top_combos_from_run(run_dir: Path, top_n: int = 100):
    """
    åŠ è½½æŸä¸ª run_ ç›®å½•ä¸‹çš„ Top ç»„åˆåˆ—è¡¨ï¼Œä¼˜å…ˆè¯»å– top100_by_ic.parquetï¼›
    è‹¥ä¸å­˜åœ¨ï¼Œåˆ™è¯»å– top_combos.parquetï¼›è‹¥ä»ä¸å­˜åœ¨ï¼Œé€€åŒ–ä¸º all_combos.parquet å¹¶æŒ‰ IC/ç¨³å®šæ€§æ’åºå– TopNã€‚

    è¿”å›:
        (df, sort_method_str)
    """
    top_by_ic_file = run_dir / "top100_by_ic.parquet"
    top_combos_file = run_dir / "top_combos.parquet"
    all_combos_file = run_dir / "all_combos.parquet"

    if top_by_ic_file.exists():
        df = pd.read_parquet(top_by_ic_file)
        return df.reset_index(drop=True), "IC (top100_by_ic)"
    if top_combos_file.exists():
        df = pd.read_parquet(top_combos_file)
        # ç¡®ä¿æŒ‰ IC/ç¨³å®šæ€§æ’åº
        df = df.sort_values(
            by=["mean_oos_ic", "stability_score"], ascending=[False, False]
        )
        return df.reset_index(drop=True), "IC (top_combos)"
    if all_combos_file.exists():
        df = pd.read_parquet(all_combos_file)
        df = df.sort_values(
            by=["mean_oos_ic", "stability_score"], ascending=[False, False]
        ).head(top_n)
        return df.reset_index(drop=True), "IC (from all_combos)"
    raise FileNotFoundError(
        f"æœªæ‰¾åˆ° {run_dir} ä¸‹çš„ top100_by_ic/top_combos/all_combos æ–‡ä»¶"
    )


def summarize_results(results_df: pd.DataFrame):
    """ç”Ÿæˆæ±‡æ€»æŒ‡æ ‡å­—å…¸ï¼Œç”¨äºæ‰“å°/å¯¹æ¯”ã€‚"""
    from scipy.stats import spearmanr

    summary = {
        "mean_annual": (
            float(results_df["annual_ret"].mean())
            if not results_df.empty
            else float("nan")
        ),
        "mean_sharpe": (
            float(results_df["sharpe"].mean()) if not results_df.empty else float("nan")
        ),
        "mean_max_dd": (
            float(results_df["max_dd"].mean()) if not results_df.empty else float("nan")
        ),
    }
    if {"rank", "sharpe", "annual_ret"}.issubset(results_df.columns):
        corr_sharpe, p_sharpe = spearmanr(results_df["rank"], results_df["sharpe"])
        corr_ret, p_ret = spearmanr(results_df["rank"], results_df["annual_ret"])
        summary.update(
            {
                "spearman_rank_sharpe": float(corr_sharpe),
                "spearman_rank_sharpe_p": float(p_sharpe),
                "spearman_rank_annual": float(corr_ret),
                "spearman_rank_annual_p": float(p_ret),
            }
        )
    return summary


def format_pct(x: float) -> str:
    try:
        return f"{x:>6.1%}"
    except Exception:
        return str(x)


def main():
    """ä¸»å‡½æ•° - è¯»å–æœ€æ–°ä¸ä¸Šä¸€æ¬¡ run çš„ Top100 ç»„åˆï¼Œåˆ†åˆ«å›æµ‹å¹¶è¾“å‡ºå¯¹æ¯”"""

    # åŠ è½½é…ç½®
    with open("configs/combo_wfo_config.yaml", "r") as f:
        config = yaml.safe_load(f)

    # åŠ è½½æ•°æ®
    logger.info("=" * 100)
    logger.info("åŠ è½½æ•°æ®...")
    logger.info("=" * 100)

    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=config["data"]["end_date"],
        use_cache=True,
    )

    # è®¡ç®—å› å­
    logger.info("è®¡ç®—å› å­...")
    factor_lib = PreciseFactorLibrary()
    factors_df = factor_lib.compute_all_factors(prices=ohlcv)
    factors_dict = {name: factors_df[name] for name in factor_lib.list_factors()}

    # æ¨ªæˆªé¢æ ‡å‡†åŒ–
    logger.info("æ¨ªæˆªé¢æ ‡å‡†åŒ–...")
    processor = CrossSectionProcessor(
        lower_percentile=config["cross_section"]["winsorize_lower"] * 100,
        upper_percentile=config["cross_section"]["winsorize_upper"] * 100,
        verbose=False,
    )
    standardized_factors = processor.process_all_factors(factors_dict)

    # ç»„ç»‡æ•°æ®
    factor_names = sorted(standardized_factors.keys())
    factor_arrays = [standardized_factors[name].values for name in factor_names]
    factors_data = np.stack(factor_arrays, axis=-1)

    returns_df = ohlcv["close"].pct_change(fill_method=None)
    returns = returns_df.values
    etf_names = list(ohlcv["close"].columns)

    logger.info(
        f"æ•°æ®ç»´åº¦: {factors_data.shape[0]}å¤© Ã— {factors_data.shape[1]}åªETF Ã— {factors_data.shape[2]}ä¸ªå› å­"
    )

    # ========== è¯»å–WFO Top 100ç»„åˆï¼ˆæœ€æ–° ä¸ ä¸Šä¸€æ¬¡ï¼‰ ==========
    logger.info("")
    logger.info("=" * 100)
    logger.info("è¯»å–WFO Top 100ç»„åˆï¼ˆæŒ‰ICæ’åºï¼‰...")
    logger.info("=" * 100)

    # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç»“æœ
    results_dir = Path("results")
    run_dirs = sorted(
        [d for d in results_dir.glob("run_*") if d.is_dir()], reverse=True
    )

    if not run_dirs:
        logger.error("æœªæ‰¾åˆ°WFOè¿è¡Œç»“æœï¼è¯·å…ˆè¿è¡Œ run_combo_wfo.py")
        return

    latest_run = run_dirs[0]
    prev_run = run_dirs[1] if len(run_dirs) > 1 else None

    # è¯»å–"æœ€æ–°"Top100
    logger.info("")
    logger.info("=" * 100)
    logger.info("è¯»å–WFO Top 100ç»„åˆï¼ˆæœ€æ–° runï¼‰...")
    logger.info("=" * 100)
    latest_top_df, latest_sort_method = load_top_combos_from_run(
        latest_run, top_n=config["combo_wfo"]["top_n"]
    )
    logger.info(f"è¯»å–ç›®å½•: {latest_run}")
    logger.info(
        f"æˆåŠŸè¯»å– Top {len(latest_top_df)} ä¸ªç»„åˆï¼ˆæ’åºæ–¹å¼ï¼š{latest_sort_method}ï¼‰"
    )
    logger.info("")

    # å¦‚æœ‰"ä¸Šä¸€æ¬¡"runï¼Œè¯»å–ä»¥ä¾¿å¯¹æ¯”
    prev_top_df = None
    if prev_run is not None:
        logger.info("=" * 100)
        logger.info("è¯»å–WFO Top 100ç»„åˆï¼ˆä¸Šä¸€è½® runï¼‰...")
        logger.info("=" * 100)
        try:
            prev_top_df, prev_sort_method = load_top_combos_from_run(
                prev_run, top_n=config["combo_wfo"]["top_n"]
            )
            logger.info(f"è¯»å–ç›®å½•: {prev_run}")
            logger.info(
                f"æˆåŠŸè¯»å– Top {len(prev_top_df)} ä¸ªç»„åˆï¼ˆæ’åºæ–¹å¼ï¼š{prev_sort_method}ï¼‰"
            )
        except Exception as e:
            logger.warning(f"è¯»å–ä¸Šä¸€è½® run å¤±è´¥ï¼Œå°†ä»…å›æµ‹æœ€æ–°ä¸€è½®ã€‚åŸå› : {e}")
        logger.info("")

    # ========== æ‰¹é‡å›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰ ==========
    def _backtest_single_combo(
        idx,
        row,
        factors_data_shared,
        returns_shared,
        etf_names,
        factor_names,
        run_tag,
        test_freq=None,
        test_position_size=None,
    ):
        """
        å•ä¸ªç»„åˆå›æµ‹ï¼ˆç”¨äºå¹¶è¡ŒåŒ–ï¼‰

        å‚æ•°:
            test_freq: int or None, å¦‚æœæŒ‡å®šåˆ™è¦†ç›–WFOæ¨èé¢‘ç‡è¿›è¡Œæµ‹è¯•
            test_position_size: int or None, å¦‚æœæŒ‡å®šåˆ™è¦†ç›–é»˜è®¤æŒä»“æ•°è¿›è¡Œæµ‹è¯•
        """
        combo_name = row["combo"]
        wfo_freq = int(row["best_rebalance_freq"])
        combo_size = int(row["combo_size"])
        wfo_ic = row["mean_oos_ic"]
        wfo_score = row["stability_score"]

        # ä½¿ç”¨æµ‹è¯•é¢‘ç‡æˆ–WFOæ¨èé¢‘ç‡
        rebalance_freq = test_freq if test_freq is not None else wfo_freq
        # ä½¿ç”¨æµ‹è¯•æŒä»“æ•°æˆ–é»˜è®¤æŒä»“æ•°5
        position_size = test_position_size if test_position_size is not None else 5

        # è§£æå› å­åç§°
        factor_list = [f.strip() for f in combo_name.split("+")]

        # æ£€æŸ¥å› å­æ˜¯å¦å­˜åœ¨
        missing_factors = [f for f in factor_list if f not in factor_names]
        if missing_factors:
            return None

        # æå–å› å­æ•°æ®
        factor_indices = [factor_names.index(f) for f in factor_list]
        factors_selected = factors_data_shared[:, :, factor_indices]

        # å›æµ‹
        try:
            result = backtest_no_lookahead(
                factors_data=factors_selected,
                returns=returns_shared,
                etf_names=etf_names,
                rebalance_freq=rebalance_freq,
                lookback_window=252,
                position_size=position_size,
                transaction_cost=0.0003,
                initial_capital=1_000_000.0,
            )

            # æ·»åŠ ç»„åˆä¿¡æ¯
            result["combo"] = combo_name
            result["combo_size"] = combo_size
            result["wfo_ic"] = wfo_ic
            result["wfo_score"] = wfo_score
            result["wfo_freq"] = wfo_freq  # WFOæ¨èçš„é¢‘ç‡
            result["test_freq"] = rebalance_freq  # å®é™…æµ‹è¯•çš„é¢‘ç‡
            result["test_position_size"] = position_size  # å®é™…æµ‹è¯•çš„æŒä»“æ•°
            result["rank"] = idx + 1
            result["run_tag"] = run_tag

            return result

        except Exception as e:
            return None

    def run_batch_backtest(
        top_df: pd.DataFrame,
        run_tag: str,
        n_jobs=4,
        test_all_freqs=False,
        freq_range=range(1, 31),
        test_all_position_sizes=False,
        position_size_range=range(1, 11),
    ):
        """
        æ‰¹é‡å›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰

        å‚æ•°:
            test_all_freqs: bool, æ˜¯å¦æµ‹è¯•æ‰€æœ‰æ¢ä»“é¢‘ç‡
            freq_range: range, æµ‹è¯•çš„é¢‘ç‡èŒƒå›´(é»˜è®¤1-30å¤©)
            test_all_position_sizes: bool, æ˜¯å¦æµ‹è¯•æ‰€æœ‰æŒä»“æ•°
            position_size_range: range, æµ‹è¯•çš„æŒä»“æ•°èŒƒå›´(é»˜è®¤1-10)
        """
        if test_all_freqs and test_all_position_sizes:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨å‚æ•°æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(freq_range)} ä¸ªé¢‘ç‡ Ã— {len(position_size_range)} ä¸ªæŒä»“æ•° = {len(top_df) * len(freq_range) * len(position_size_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, é¢‘ç‡, æŒä»“æ•°)ä»»åŠ¡ä¸‰å…ƒç»„
            tasks = [
                (idx, row, freq, pos_size)
                for idx, row in top_df.iterrows()
                for freq in freq_range
                for pos_size in position_size_range
            ]

            # å¹¶è¡Œå›æµ‹æ‰€æœ‰ä»»åŠ¡
            results = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_backtest_single_combo)(
                    idx,
                    row,
                    factors_data,
                    returns,
                    etf_names,
                    factor_names,
                    run_tag,
                    test_freq=freq,
                    test_position_size=pos_size,
                )
                for idx, row, freq, pos_size in tasks
            )

        elif test_all_freqs:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨é¢‘ç‡æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(freq_range)} ä¸ªé¢‘ç‡ = {len(top_df) * len(freq_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, é¢‘ç‡)ä»»åŠ¡å¯¹
            tasks = [
                (idx, row, freq)
                for idx, row in top_df.iterrows()
                for freq in freq_range
            ]

            # å¹¶è¡Œå›æµ‹æ‰€æœ‰ä»»åŠ¡
            results = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_backtest_single_combo)(
                    idx,
                    row,
                    factors_data,
                    returns,
                    etf_names,
                    factor_names,
                    run_tag,
                    test_freq=freq,
                )
                for idx, row, freq in tasks
            )

        elif test_all_position_sizes:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨æŒä»“æ•°æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(position_size_range)} ä¸ªæŒä»“æ•° = {len(top_df) * len(position_size_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, æŒä»“æ•°)ä»»åŠ¡å¯¹
            tasks = [
                (idx, row, pos_size)
                for idx, row in top_df.iterrows()
                for pos_size in position_size_range
            ]

            # å¹¶è¡Œå›æµ‹æ‰€æœ‰ä»»åŠ¡
            results = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_backtest_single_combo)(
                    idx,
                    row,
                    factors_data,
                    returns,
                    etf_names,
                    factor_names,
                    run_tag,
                    test_position_size=pos_size,
                )
                for idx, row, pos_size in tasks
            )

        else:
            logger.info("=" * 100)
            logger.info(f"å¼€å§‹æ‰¹é‡å›æµ‹ Top {len(top_df)} ç»„åˆï¼ˆ{run_tag}ï¼Œæ— æœªæ¥å‡½æ•°ï¼‰")
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # å¹¶è¡Œå›æµ‹(ä½¿ç”¨WFOæ¨èé¢‘ç‡å’Œé»˜è®¤æŒä»“æ•°)
            results = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_backtest_single_combo)(
                    idx,
                    row,
                    factors_data,
                    returns,
                    etf_names,
                    factor_names,
                    run_tag,
                    test_freq=None,
                    test_position_size=None,
                )
                for idx, row in top_df.iterrows()
            )

        # è¿‡æ»¤å¤±è´¥çš„å›æµ‹
        all_results_local = [r for r in results if r is not None]

        if not all_results_local:
            logger.error("æ²¡æœ‰æˆåŠŸå®Œæˆçš„å›æµ‹ï¼")
            return None

        # è¾“å‡ºå›æµ‹ç»“æœ(å…¨é¢‘ç‡æ¨¡å¼ä¸‹åªæ˜¾ç¤ºéƒ¨åˆ†)
        logger.info("")
        if test_all_freqs:
            logger.info(f"âœ… å®Œæˆ {len(all_results_local)} ä¸ªç­–ç•¥å›æµ‹")
            logger.info("æ˜¾ç¤ºå‰20ä¸ªç»“æœ:")
            for r in all_results_local[:20]:
                logger.info(f'[#{r["rank"]}] {r["combo"][:50]} | {r["test_freq"]}å¤©')
                logger.info(
                    f'      å›æµ‹ç»“æœ: å¹´åŒ–{r["annual_ret"]:>6.1%} | Sharpe {r["sharpe"]:>5.3f} | å›æ’¤{r["max_dd"]:>6.1%}'
                )
        else:
            for r in all_results_local:
                logger.info(f'[{r["rank"]}/{len(top_df)}] {r["combo"]}')
                logger.info(
                    f'         å›æµ‹ç»“æœ: 100ä¸‡â†’{r["final"]/10000:>8.1f}ä¸‡ | '
                    f'å¹´åŒ–{r["annual_ret"]:>6.1%} | Sharpe {r["sharpe"]:>5.3f} | '
                    f'å›æ’¤{r["max_dd"]:>6.1%} | è°ƒä»“{r["n_rebalance"]:>3d}æ¬¡'
                )

        df_local = pd.DataFrame(
            [
                {
                    "rank": r["rank"],
                    "combo": r["combo"],
                    "combo_size": r["combo_size"],
                    "wfo_freq": r["wfo_freq"],
                    "test_freq": r["test_freq"],
                    "test_position_size": r.get(
                        "test_position_size", 5
                    ),  # âœ¨ æ–°å¢ï¼šæµ‹è¯•çš„æŒä»“æ•°
                    "freq": r["freq"],  # å®é™…ä½¿ç”¨çš„é¢‘ç‡
                    "wfo_ic": r["wfo_ic"],
                    "wfo_score": r["wfo_score"],
                    "final_value": r["final"],
                    "total_ret": r["total_ret"],
                    "annual_ret": r["annual_ret"],
                    "vol": r["vol"],
                    "sharpe": r["sharpe"],
                    "max_dd": r["max_dd"],
                    "n_rebalance": r["n_rebalance"],
                    "avg_turnover": r["avg_turnover"],
                    "avg_n_holdings": r["avg_n_holdings"],  # âœ¨ æ–°å¢ï¼šå¹³å‡æŒä»“æ•°
                    # æ–°å¢å­—æ®µï¼šèƒœç‡ç›¸å…³
                    "win_rate": r["win_rate"],
                    "winning_days": r["winning_days"],
                    "losing_days": r["losing_days"],
                    "avg_win": r["avg_win"],
                    "avg_loss": r["avg_loss"],
                    "profit_factor": r["profit_factor"],
                    # æ–°å¢å­—æ®µï¼šé£é™©è°ƒæ•´æŒ‡æ ‡
                    "calmar_ratio": r["calmar_ratio"],
                    "sortino_ratio": r["sortino_ratio"],
                    "max_consecutive_wins": r["max_consecutive_wins"],
                    "max_consecutive_losses": r["max_consecutive_losses"],
                    "run_tag": r["run_tag"],
                }
                for r in all_results_local
            ]
        )

        return df_local

    # ========== å…¨é¢‘ç‡æ‰«ææ¨¡å¼(å¯é€‰) ==========
    TEST_ALL_FREQS = config.get("backtest", {}).get("test_all_frequencies", False)
    TEST_ALL_POSITION_SIZES = config.get("backtest", {}).get(
        "test_all_position_sizes", False
    )
    FREQ_RANGE = range(1, 31)  # 1-30å¤©
    POSITION_SIZE_RANGE = range(1, 11)  # 1-10ä¸ªæŒä»“

    # ç»Ÿä¸€çš„ç»“æœè¾“å‡ºç›®å½•ï¼Œéœ€åœ¨å…¨é¢‘ç‡/å¸¸è§„å›æµ‹å‰åˆ›å»º
    output_dir = Path("results_combo_wfo")
    output_dir.mkdir(exist_ok=True)

    if TEST_ALL_FREQS and TEST_ALL_POSITION_SIZES:
        # å…¨å‚æ•°æ‰«æï¼ˆé¢‘ç‡+æŒä»“æ•°ï¼‰
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨å…¨å‚æ•°æ‰«ææ¨¡å¼: 1-30å¤©æ¢ä»“ Ã— 1-10ä¸ªæŒä»“")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        all_param_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_param:{latest_run.name}",
            n_jobs=8,
            test_all_freqs=True,
            freq_range=FREQ_RANGE,
            test_all_position_sizes=True,
            position_size_range=POSITION_SIZE_RANGE,
        )

        if all_param_results_df is not None:
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = output_dir / latest_ts
            run_output_dir.mkdir(exist_ok=True)
            all_param_file = run_output_dir / f"all_param_scan_{latest_ts}.csv"
            all_param_results_df.to_csv(all_param_file, index=False)
            logger.info(f"å…¨å‚æ•°æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_param_file}")

            # æŒ‰æŒä»“æ•°åˆ†ç»„åˆ†æ
            logger.info("")
            logger.info("=" * 100)
            logger.info("æŒ‰æŒä»“æ•°ç»Ÿè®¡æ€§èƒ½")
            logger.info("=" * 100)
            pos_stats = (
                all_param_results_df.groupby("test_position_size")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )
            logger.info(pos_stats.to_string())

            best_pos_by_sharpe = (
                all_param_results_df.groupby("test_position_size")["sharpe"]
                .mean()
                .idxmax()
            )
            logger.info(f"\nğŸ“Š å¹³å‡Sharpeæœ€ä¼˜æŒä»“æ•°: {best_pos_by_sharpe}ä¸ª")
            return

    elif TEST_ALL_POSITION_SIZES:
        # ä»…æŒä»“æ•°æ‰«æ
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨æŒä»“æ•°æ‰«ææ¨¡å¼: 1-10ä¸ªæŒä»“")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        all_pos_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_pos:{latest_run.name}",
            n_jobs=8,
            test_all_position_sizes=True,
            position_size_range=POSITION_SIZE_RANGE,
        )

        if all_pos_results_df is not None:
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = output_dir / latest_ts
            run_output_dir.mkdir(exist_ok=True)
            all_pos_file = run_output_dir / f"all_pos_scan_{latest_ts}.csv"
            all_pos_results_df.to_csv(all_pos_file, index=False)
            logger.info(f"æŒä»“æ•°æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_pos_file}")

            # æŒ‰æŒä»“æ•°åˆ†ç»„åˆ†æ
            logger.info("")
            logger.info("=" * 100)
            logger.info("æŒ‰æŒä»“æ•°ç»Ÿè®¡æ€§èƒ½")
            logger.info("=" * 100)
            pos_stats = (
                all_pos_results_df.groupby("test_position_size")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )
            logger.info(pos_stats.to_string())

            best_pos_by_sharpe = (
                all_pos_results_df.groupby("test_position_size")["sharpe"]
                .mean()
                .idxmax()
            )
            best_pos_by_return = (
                all_pos_results_df.groupby("test_position_size")["annual_ret"]
                .mean()
                .idxmax()
            )
            logger.info(f"\nğŸ“Š å¹³å‡Sharpeæœ€ä¼˜æŒä»“æ•°: {best_pos_by_sharpe}ä¸ª")
            logger.info(f"ğŸ“Š å¹³å‡å¹´åŒ–æœ€ä¼˜æŒä»“æ•°: {best_pos_by_return}ä¸ª")
            return

    elif TEST_ALL_FREQS:
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨å…¨é¢‘ç‡æ‰«ææ¨¡å¼: 1-30å¤©æ¢ä»“é¢‘ç‡å…¨æ‰«æ")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        # å…¨é¢‘ç‡å›æµ‹
        all_freq_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_freq:{latest_run.name}",
            n_jobs=8,  # 3000ä¸ªä»»åŠ¡,ç”¨æ›´å¤šæ ¸å¿ƒ
            test_all_freqs=True,
            freq_range=FREQ_RANGE,
        )

        if all_freq_results_df is not None:
            # ä¿å­˜å…¨é¢‘ç‡ç»“æœ
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = output_dir / latest_ts
            run_output_dir.mkdir(exist_ok=True)
            all_freq_file = run_output_dir / f"all_freq_scan_{latest_ts}.csv"
            all_freq_results_df.to_csv(all_freq_file, index=False)

            logger.info("")
            logger.info("=" * 100)
            logger.info("å…¨é¢‘ç‡æ‰«æç»“æœåˆ†æ")
            logger.info("=" * 100)

            # æŒ‰é¢‘ç‡åˆ†ç»„ç»Ÿè®¡
            freq_stats = (
                all_freq_results_df.groupby("test_freq")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )

            logger.info("\nå„æ¢ä»“é¢‘ç‡è¡¨ç°ç»Ÿè®¡:")
            logger.info(freq_stats.to_string())

            # æ‰¾å‡ºæœ€ä¼˜é¢‘ç‡
            best_freq_by_sharpe = (
                all_freq_results_df.groupby("test_freq")["sharpe"].mean().idxmax()
            )
            best_freq_by_return = (
                all_freq_results_df.groupby("test_freq")["annual_ret"].mean().idxmax()
            )

            logger.info("")
            logger.info(f"ğŸ“Š å¹³å‡Sharpeæœ€ä¼˜é¢‘ç‡: {best_freq_by_sharpe}å¤©")
            logger.info(f"ğŸ“Š å¹³å‡å¹´åŒ–æœ€ä¼˜é¢‘ç‡: {best_freq_by_return}å¤©")

            # Top 10 å…¨å±€æœ€ä¼˜ç­–ç•¥
            logger.info("")
            logger.info("=" * 100)
            logger.info("Top 10 å…¨å±€æœ€ä¼˜ç­–ç•¥ï¼ˆè·¨æ‰€æœ‰é¢‘ç‡ï¼‰")
            logger.info("=" * 100)
            top10_global = all_freq_results_df.nlargest(10, "sharpe")
            for i, row in top10_global.iterrows():
                logger.info(
                    f'{i+1:>2}. [WFO#{row["rank"]:>3}] {row["combo"][:60]} | {row["test_freq"]}å¤©'
                )
                logger.info(
                    f'    å¹´åŒ–{row["annual_ret"]:>6.1%} | Sharpe {row["sharpe"]:>5.3f} | å›æ’¤{row["max_dd"]:>6.1%}'
                )

            logger.info("")
            logger.info(f"å…¨é¢‘ç‡æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_freq_file}")
            logger.info("")

            # ç»§ç»­æ‰§è¡Œåç»­é€»è¾‘å‰å…ˆè¿”å›(å¯é€‰)
            # return

    # ========== å¸¸è§„å•é¢‘ç‡å›æµ‹ ==========
    latest_results_df = run_batch_backtest(
        latest_top_df, run_tag=f"latest:{latest_run.name}"
    )
    if latest_results_df is None:
        return

    # ========== ç»“æœæ±‡æ€»ï¼ˆæœ€æ–°ï¼‰ ==========
    logger.info("=" * 100)
    logger.info("å›æµ‹ç»“æœæ±‡æ€»ï¼ˆæœ€æ–°ï¼‰")
    logger.info("=" * 100)

    # ========== æœ€æ–°ç»“æœï¼šæ’åº/å±•ç¤º/ä¿å­˜ ==========
    results_df_sorted = latest_results_df.sort_values(
        "sharpe", ascending=False
    ).reset_index(drop=True)

    logger.info(f"\næˆåŠŸå®Œæˆ {len(latest_results_df)} ä¸ªç»„åˆçš„å›æµ‹")
    logger.info("")

    # Top 10 by Sharpe
    logger.info("=" * 100)
    logger.info("Top 10 ç»„åˆï¼ˆæŒ‰Sharpeæ’åºï¼‰")
    logger.info("=" * 100)
    top10 = results_df_sorted.head(10)
    for i, row in top10.iterrows():
        logger.info(f'{i+1:>2}. [WFOæ’å#{row["rank"]:>3}] {row["combo"][:80]}')
        logger.info(
            f'    {row["freq"]}å¤©æ¢ä»“ | å¹´åŒ–{row["annual_ret"]:>6.1%} | Sharpe {row["sharpe"]:>5.3f} | '
            f'å›æ’¤{row["max_dd"]:>6.1%} | 100ä¸‡â†’{row["final_value"]/10000:>7.1f}ä¸‡'
        )
        logger.info("")

    # ç»Ÿè®¡åˆ†æ
    logger.info("=" * 100)
    logger.info("ç»Ÿè®¡åˆ†æ")
    logger.info("=" * 100)
    logger.info(f'å¹³å‡å¹´åŒ–æ”¶ç›Š: {latest_results_df["annual_ret"].mean():>6.1%}')
    logger.info(f'å¹³å‡Sharpe:   {latest_results_df["sharpe"].mean():>6.3f}')
    logger.info(f'å¹³å‡æœ€å¤§å›æ’¤: {latest_results_df["max_dd"].mean():>6.1%}')
    logger.info(
        f'å¹´åŒ–>0ç»„åˆ:   {(latest_results_df["annual_ret"] > 0).sum()}/{len(latest_results_df)} ({(latest_results_df["annual_ret"] > 0).mean()*100:.1f}%)'
    )
    logger.info(
        f'Sharpe>0ç»„åˆ: {(latest_results_df["sharpe"] > 0).sum()}/{len(latest_results_df)} ({(latest_results_df["sharpe"] > 0).mean()*100:.1f}%)'
    )

    # WFOæ’å vs å®é™…è¡¨ç°ç›¸å…³æ€§
    from scipy.stats import spearmanr

    corr_sharpe, p_sharpe = spearmanr(
        latest_results_df["rank"], latest_results_df["sharpe"]
    )
    corr_ret, p_ret = spearmanr(
        latest_results_df["rank"], latest_results_df["annual_ret"]
    )

    logger.info("")
    logger.info("WFOæ’åä¸å®é™…è¡¨ç°ç›¸å…³æ€§:")
    logger.info(f"  WFOæ’å vs å®ç›˜Sharpe: {corr_sharpe:>6.3f} (p={p_sharpe:.3f})")
    logger.info(f"  WFOæ’å vs å®ç›˜å¹´åŒ–:   {corr_ret:>6.3f} (p={p_ret:.3f})")
    if corr_sharpe < -0.3 and p_sharpe < 0.05:
        logger.info("  âœ… WFOæ’åä¸å®ç›˜è¡¨ç°æ˜¾è‘—è´Ÿç›¸å…³ â†’ WFOæ’åæœ‰æ•ˆï¼")
    elif abs(corr_sharpe) < 0.1:
        logger.info("  âš ï¸  WFOæ’åä¸å®ç›˜è¡¨ç°ç›¸å…³æ€§è¾ƒå¼±")

    # ä¿å­˜ç»“æœ
    latest_ts = latest_run.name.replace("run_", "")
    run_output_dir = output_dir / latest_ts
    run_output_dir.mkdir(exist_ok=True)
    output_file = run_output_dir / f"top100_backtest_by_ic_{latest_ts}.csv"
    results_df_sorted.to_csv(output_file, index=False)

    logger.info("")
    logger.info(f"æœ€æ–°ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    logger.info("")

    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆåŒè¡¨å³å¯ï¼‰
    output_file_full = run_output_dir / f"top100_backtest_by_ic_{latest_ts}_full.csv"
    results_df_sorted.to_csv(output_file_full, index=False)
    logger.info(f"æœ€æ–°å®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {output_file_full}")

    # ========== è‹¥å­˜åœ¨ä¸Šä¸€è½® runï¼Œåˆ™è¿›è¡Œå¯¹æ¯”å¹¶ä¿å­˜å¯¹æ¯”æ–‡ä»¶ ==========
    if prev_top_df is not None:
        prev_results_df = run_batch_backtest(
            prev_top_df, run_tag=f"prev:{prev_run.name}"
        )
        if prev_results_df is not None:
            prev_ts = prev_run.name.replace("run_", "")

            # å¯¹æ¯”æ±‡æ€»
            latest_summary = summarize_results(latest_results_df)
            prev_summary = summarize_results(prev_results_df)

            logger.info("")
            logger.info("=" * 100)
            logger.info("ä¸ä¸Šä¸€è½®ç»“æœå¯¹æ¯”ï¼ˆæ±‡æ€»ï¼‰")
            logger.info("=" * 100)
            logger.info(
                f'- æœ€æ–°({latest_ts}) å¹³å‡å¹´åŒ–: {format_pct(latest_summary["mean_annual"])}, å¹³å‡Sharpe: {latest_summary["mean_sharpe"]:>6.3f}, å¹³å‡å›æ’¤: {format_pct(latest_summary["mean_max_dd"]) }'
            )
            logger.info(
                f'- ä¹‹å‰({prev_ts}) å¹³å‡å¹´åŒ–: {format_pct(prev_summary["mean_annual"])}, å¹³å‡Sharpe: {prev_summary["mean_sharpe"]:>6.3f}, å¹³å‡å›æ’¤: {format_pct(prev_summary["mean_max_dd"]) }'
            )
            if (
                "spearman_rank_sharpe" in latest_summary
                and "spearman_rank_sharpe" in prev_summary
            ):
                logger.info(
                    f'- æœ€æ–° Rank~Sharpe: {latest_summary["spearman_rank_sharpe"]:>6.3f} (p={latest_summary["spearman_rank_sharpe_p"]:.3f})'
                )
                logger.info(
                    f'- ä¹‹å‰ Rank~Sharpe: {prev_summary["spearman_rank_sharpe"]:>6.3f} (p={prev_summary["spearman_rank_sharpe_p"]:.3f})'
                )

            # é‡å ç»„åˆå¯¹é½å¯¹æ¯”
            latest_small = latest_results_df[
                ["combo", "rank", "annual_ret", "sharpe"]
            ].rename(
                columns={
                    "rank": "rank_latest",
                    "annual_ret": "annual_latest",
                    "sharpe": "sharpe_latest",
                }
            )
            prev_small = prev_results_df[
                ["combo", "rank", "annual_ret", "sharpe"]
            ].rename(
                columns={
                    "rank": "rank_prev",
                    "annual_ret": "annual_prev",
                    "sharpe": "sharpe_prev",
                }
            )
            merged = latest_small.merge(prev_small, on="combo", how="inner")
            if not merged.empty:
                merged["delta_sharpe"] = merged["sharpe_latest"] - merged["sharpe_prev"]
                merged["delta_annual"] = merged["annual_latest"] - merged["annual_prev"]
                merged["delta_rank"] = merged["rank_latest"] - merged["rank_prev"]

                logger.info("")
                logger.info("é‡å ç»„åˆå¯¹æ¯”ï¼ˆå‡å€¼ï¼‰:")
                logger.info(
                    f"- å¹³å‡ Sharpe å˜åŒ–: {merged['delta_sharpe'].mean():>6.3f}"
                )
                logger.info(f"- å¹³å‡ å¹´åŒ–  å˜åŒ–: {merged['delta_annual'].mean():>6.3%}")
                logger.info(
                    f"- å¹³å‡ æ’å  å˜åŒ–: {merged['delta_rank'].mean():>6.2f} (è´Ÿæ•°=æœ€æ–°æ’åæ›´é å‰)"
                )
                logger.info(
                    f"- æå‡å æ¯”(Sharpe>0): {(merged['delta_sharpe']>0).mean()*100:>5.1f}%  ({(merged['delta_sharpe']>0).sum()}/{len(merged)})"
                )

                compare_file = (
                    run_output_dir / f"compare_top100_{prev_ts}_vs_{latest_ts}.csv"
                )
                merged.sort_values("delta_sharpe", ascending=False).to_csv(
                    compare_file, index=False
                )
                logger.info(f"å¯¹æ¯”æ˜ç»†å·²ä¿å­˜: {compare_file}")
            else:
                logger.info("ä¸¤è½®Top100æ— é‡å ç»„åˆï¼Œè·³è¿‡é€ç»„åˆå¯¹æ¯”ä¿å­˜ã€‚")


if __name__ == "__main__":
    main()
