#!/usr/bin/env python3
"""
Step 4: WFOçª—å£å› å­ç­–ç•¥å›æµ‹ - åŸºäº WFO å› å­é€‰æ‹©

åŠŸèƒ½ï¼š
1. åŠ è½½ WFO ä¼˜åŒ–ç»“æœä¸­é€‰ä¸­çš„å› å­ç»„åˆ
2. å¯¹æ¯ä¸ªçª—å£è¿›è¡Œæ ·æœ¬å¤–TopNç­‰æƒå¤šå¤´ç»„åˆå›æµ‹
3. è®¡ç®—æ”¶ç›Šç‡ã€å¤æ™®æ¯”ã€æœ€å¤§å›æ’¤ç­‰æ€§èƒ½æŒ‡æ ‡
4. ç”Ÿæˆè¯¦ç»†çš„å›æµ‹æŠ¥å‘Šå’Œæ—¥å¿—

è¾“å…¥ï¼š
- WFO ç»“æœ: wfo/{timestamp}/wfo_results.pkl
- OHLCV æ•°æ®: cross_section/{date}/{timestamp}/ohlcv/
- æ ‡å‡†åŒ–å› å­: factor_selection/{date}/{timestamp}/standardized/

è¾“å‡ºï¼š
- backtest/{timestamp}/backtest_results.pkl
- backtest/{timestamp}/backtest_report.txt
- backtest/{timestamp}/performance_summary.csv
- backtest/{timestamp}/combination_performance.csv
"""

import json
import logging
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.ic_calculator import ICCalculator


def setup_logging(output_dir: Path):
    """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
    log_file = output_dir / "step4_backtest.log"

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s - %(message)s",
        datefmt="%H:%M:%S",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def find_latest_wfo(results_dir: Path):
    """æŸ¥æ‰¾æœ€æ–°çš„ WFO ç»“æœç›®å½•"""
    wfo_root = results_dir / "wfo"

    if not wfo_root.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•
    all_runs = []
    for timestamp_dir in wfo_root.iterdir():
        if not timestamp_dir.is_dir():
            continue
        # éªŒè¯æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
        if (timestamp_dir / "wfo_results.pkl").exists():
            all_runs.append(timestamp_dir)

    if not all_runs:
        return None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°
    all_runs.sort(key=lambda p: p.name, reverse=True)
    return all_runs[0]


def load_wfo_results(wfo_dir: Path, logger):
    """åŠ è½½ WFO ç»“æœ"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 1/4: åŠ è½½ WFO ä¼˜åŒ–ç»“æœ")
    logger.info("-" * 80)

    # åŠ è½½ WFO ç»“æœå¯¹è±¡
    wfo_results_path = wfo_dir / "wfo_results.pkl"
    with open(wfo_results_path, "rb") as f:
        wfo_results = pickle.load(f)

    logger.info(f"âœ… WFO ç»“æœå·²åŠ è½½: {wfo_results_path}")
    logger.info(f"  - æ€»çª—å£æ•°: {wfo_results['total_windows']}")
    logger.info(f"  - æœ‰æ•ˆçª—å£æ•°: {wfo_results['valid_windows']}")
    logger.info(f"  - å¹³å‡OOS IC: {wfo_results['avg_oos_ic']:.4f}")
    logger.info("")

    return wfo_results


def load_backtest_data(wfo_dir: Path, results_dir: Path, logger):
    """åŠ è½½å›æµ‹æ‰€éœ€çš„æ•°æ®"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 2/4: åŠ è½½å›æµ‹æ•°æ®ï¼ˆOHLCV + æ ‡å‡†åŒ–å› å­ï¼‰")
    logger.info("-" * 80)

    # åŠ è½½ WFO metadata æ¥è·å–æ•°æ®è·¯å¾„ä¿¡æ¯
    wfo_metadata_path = wfo_dir / "metadata.json"
    with open(wfo_metadata_path) as f:
        wfo_meta = json.load(f)

    # ä» WFO å…ƒæ•°æ®ä¸­æå–æ—¶é—´æˆ³ä¸æ—¥æœŸï¼Œç”¨äºç»‘å®šæ•°æ®ç‰ˆæœ¬
    wfo_ts = str(wfo_meta.get("timestamp", wfo_dir.name))
    wfo_date = wfo_ts.split("_")[0] if "_" in wfo_ts else wfo_ts[:8]
    logger.info(f"ç»‘å®šæ•°æ®ç‰ˆæœ¬ï¼šç›®æ ‡æ—¥æœŸ={wfo_date}ï¼ŒWFOæ—¶é—´æˆ³={wfo_ts}")

    # ä¼˜å…ˆæŒ‰ WFO æ—¶é—´æˆ³ç»‘å®š cross_section æ•°æ®ï¼ˆè‹¥ç¼ºå¤±åˆ™å›é€€åˆ°æœ€æ–°ï¼Œå¹¶ç»™å‡ºå‘Šè­¦ï¼‰
    cross_section_root = results_dir / "cross_section"
    bound_cross_section: Path = None
    try:
        target_date_dir = cross_section_root / wfo_date
        candidates = []
        if target_date_dir.exists():
            for ts_dir in target_date_dir.iterdir():
                if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                    # ä»…é€‰æ‹©æ—¶é—´æˆ³ä¸æ™šäº WFO çš„ç›®å½•ï¼Œé¿å…è·¨ç‰ˆæœ¬é”™é…
                    if ts_dir.name <= wfo_ts:
                        candidates.append(ts_dir)
        if candidates:
            candidates.sort(key=lambda p: p.name, reverse=True)
            bound_cross_section = candidates[0]
            logger.info(f"âœ… å·²æŒ‰WFOç»‘å®š cross_section ç›®å½•: {bound_cross_section}")
        else:
            # å›é€€ï¼šå…¨å±€æœ€æ–°å¯ç”¨
            all_cross_section = []
            for date_dir in cross_section_root.iterdir():
                if not date_dir.is_dir():
                    continue
                for ts_dir in date_dir.iterdir():
                    if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                        all_cross_section.append(ts_dir)
            all_cross_section.sort(key=lambda p: p.name, reverse=True)
            bound_cross_section = all_cross_section[0]
            logger.warning(
                f"âš ï¸ æœªæ‰¾åˆ°ä¸WFO({wfo_ts})åŒæ—¥ä¸”ä¸æ™šäºå®ƒçš„ cross_section ç‰ˆæœ¬ï¼Œå·²å›é€€è‡³æœ€æ–°: {bound_cross_section}"
            )
    except Exception as e:
        logger.warning(f"âš ï¸ ç»‘å®š cross_section ç‰ˆæœ¬æ—¶å‡ºé”™ï¼š{e}ï¼Œå°†å›é€€åˆ°æœ€æ–°å¯ç”¨ç‰ˆæœ¬")
        # å…œåº•ï¼šåŸæœ‰é€»è¾‘ï¼Œé€‰æ‹©å…¨å±€æœ€æ–°
        all_cross_section = []
        for date_dir in cross_section_root.iterdir():
            if not date_dir.is_dir():
                continue
            for ts_dir in date_dir.iterdir():
                if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                    all_cross_section.append(ts_dir)
        all_cross_section.sort(key=lambda p: p.name, reverse=True)
        bound_cross_section = all_cross_section[0]

    cross_section_dir = bound_cross_section

    # åŠ è½½ OHLCV
    ohlcv_data = {}
    ohlcv_dir = cross_section_dir / "ohlcv"
    for col in ["open", "high", "low", "close", "volume"]:
        ohlcv_data[col] = pd.read_parquet(ohlcv_dir / f"{col}.parquet")

    # ä¼˜å…ˆæŒ‰ WFO æ—¶é—´æˆ³ç»‘å®š factor_selection æ•°æ®ï¼ˆè‹¥ç¼ºå¤±åˆ™å›é€€åˆ°æœ€æ–°ï¼Œå¹¶ç»™å‡ºå‘Šè­¦ï¼‰
    factor_sel_root = results_dir / "factor_selection"
    bound_factor_sel: Path = None
    try:
        target_date_dir = factor_sel_root / wfo_date
        candidates = []
        if target_date_dir.exists():
            for ts_dir in target_date_dir.iterdir():
                if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                    if ts_dir.name <= wfo_ts:
                        candidates.append(ts_dir)
        if candidates:
            candidates.sort(key=lambda p: p.name, reverse=True)
            bound_factor_sel = candidates[0]
            logger.info(f"âœ… å·²æŒ‰WFOç»‘å®š factor_selection ç›®å½•: {bound_factor_sel}")
        else:
            # å›é€€ï¼šå…¨å±€æœ€æ–°å¯ç”¨
            all_factor_sel = []
            for date_dir in factor_sel_root.iterdir():
                if not date_dir.is_dir():
                    continue
                for ts_dir in date_dir.iterdir():
                    if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                        all_factor_sel.append(ts_dir)
            all_factor_sel.sort(key=lambda p: p.name, reverse=True)
            bound_factor_sel = all_factor_sel[0]
            logger.warning(
                f"âš ï¸ æœªæ‰¾åˆ°ä¸WFO({wfo_ts})åŒæ—¥ä¸”ä¸æ™šäºå®ƒçš„ factor_selection ç‰ˆæœ¬ï¼Œå·²å›é€€è‡³æœ€æ–°: {bound_factor_sel}"
            )
    except Exception as e:
        logger.warning(f"âš ï¸ ç»‘å®š factor_selection ç‰ˆæœ¬æ—¶å‡ºé”™ï¼š{e}ï¼Œå°†å›é€€åˆ°æœ€æ–°å¯ç”¨ç‰ˆæœ¬")
        # å…œåº•ï¼šåŸæœ‰é€»è¾‘ï¼Œé€‰æ‹©å…¨å±€æœ€æ–°
        all_factor_sel = []
        for date_dir in factor_sel_root.iterdir():
            if not date_dir.is_dir():
                continue
            for ts_dir in date_dir.iterdir():
                if ts_dir.is_dir() and (ts_dir / "metadata.json").exists():
                    all_factor_sel.append(ts_dir)
        all_factor_sel.sort(key=lambda p: p.name, reverse=True)
        bound_factor_sel = all_factor_sel[0]

    factor_sel_dir = bound_factor_sel

    # åŠ è½½æ ‡å‡†åŒ–å› å­
    std_factors = {}
    std_dir = factor_sel_dir / "standardized"
    for factor_file in std_dir.glob("*.parquet"):
        factor_name = factor_file.stem
        std_factors[factor_name] = pd.read_parquet(factor_file)

    logger.info(f"âœ… OHLCV æ•°æ®å·²åŠ è½½: {ohlcv_data['close'].shape}")
    logger.info(f"âœ… æ ‡å‡†åŒ–å› å­å·²åŠ è½½: {len(std_factors)} ä¸ªå› å­")
    logger.info(f"  - å› å­: {', '.join(list(std_factors.keys())[:5])}...")
    logger.info("")

    return ohlcv_data, std_factors, wfo_meta


def run_backtest_combinations(
    wfo_results: Dict,
    ohlcv_data: Dict,
    std_factors: Dict,
    wfo_meta: Dict,
    logger,
) -> Tuple[pd.DataFrame, Dict]:
    """
    è¿è¡Œæ‰€æœ‰çª—å£çš„ç»„åˆå›æµ‹

    å¯¹æ¯ä¸ª WFO çª—å£ä¸­é€‰ä¸­çš„å› å­ç»„åˆè¿›è¡Œå›æµ‹ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡
    """
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 3/4: è¿è¡Œ WFO çª—å£å› å­ç­–ç•¥å›æµ‹ï¼ˆTopN=5 æ—¥é¢‘å¤šå¤´ï¼‰")
    logger.info("-" * 80)
    logger.info("")

    # æå– WFO ç»“æœ
    results_df = wfo_results["results_df"]
    constraint_reports = wfo_results["constraint_reports"]
    total_windows = len(constraint_reports)

    # å‡†å¤‡å›æµ‹ç»“æœå®¹å™¨
    backtest_records = []

    # è·å– close ä»·æ ¼ç”¨äºè®¡ç®—æ”¶ç›Š
    close_prices = ohlcv_data["close"]
    returns = close_prices.pct_change(fill_method=None)  # (1399, 43) ä¿®å¤FutureWarning

    # è¯´æ˜ï¼šæ¨ªæˆªé¢ICåœ¨ä¸‹æ–¹ä½¿ç”¨scipyç›´æ¥è®¡ç®—ï¼Œè‹¥éœ€æ‰©å±•å¯åˆ‡æ¢ä¸ºICCalculator

    # å…¨å±€å‚æ•°
    TOPN = 5  # æ¯æ—¥é€‰å–Top5èµ„äº§
    COST_BPS = float(os.getenv("TRADING_COST_BPS", "0"))  # äº¤æ˜“æˆæœ¬ï¼ˆå•è¾¹ï¼Œbpsï¼‰
    COST_RATE = COST_BPS / 10000.0

    # ç”¨äºæ‹¼æ¥éé‡å  OOS æƒç›Šæ›²çº¿
    # ä¼°è®¡æ­¥é•¿ï¼ˆæ­¥è¿›å¤©æ•°ï¼‰ï¼Œç”¨äºæ‹¼æ¥æ¯ä¸ªçª—å£çš„å‰ step_len å¤©
    try:
        oos_starts = [int(r.oos_start) for r in constraint_reports]
        diffs = [b - a for a, b in zip(oos_starts[:-1], oos_starts[1:]) if (b - a) > 0]
        step_len = int(min(diffs)) if diffs else None
    except Exception:
        step_len = None
    if step_len is None:
        step_len = 20  # å›é€€ï¼šé»˜è®¤ 20 å¤©
    logger.info(f"æ‹¼æ¥éé‡å  OOS æƒç›Šçš„æ­¥é•¿(step_len)={step_len}")

    stitched_rows: List[Dict] = []

    # å¯¹æ¯ä¸ªçª—å£è¿›è¡Œå›æµ‹
    for window_idx, report in enumerate(constraint_reports, 1):
        is_end = report.is_end
        oos_start = report.oos_start
        oos_end = report.oos_end
        selected_factors = report.selected_factors

        # DEBUG: æ‰“å°å› å­åˆ—è¡¨
        logger.info(f"[çª—å£ {window_idx}/{total_windows}] é€‰ä¸­å› å­: {selected_factors}")

        if not selected_factors:
            logger.info(f"[çª—å£ {window_idx}/{total_windows}] æ— é€‰ä¸­å› å­ï¼Œè·³è¿‡")
            continue

        # ========== æ ¸å¿ƒä¿®å¤ï¼šåŸºäºå› å­ä¿¡å·çš„TopNé€‰è‚¡ ==========

        # å‡†å¤‡OOSæœŸçš„æ—¥æ”¶ç›Šç‡
        oos_returns = returns.iloc[oos_start:oos_end]  # (60, 43)
        n_oos_days = len(oos_returns)

        # å‡†å¤‡å› å­æ•°æ®ï¼šéœ€è¦åœ¨OOSå¼€å§‹å‰ä¸€å¤©åˆ°OOSç»“æŸå‰ä¸€å¤©ï¼ˆç”¨äºT-1é¢„æµ‹Tæ—¥ï¼‰
        # å› å­ç´¢å¼•èŒƒå›´ï¼š[oos_start-1, oos_end-1)
        factor_start = max(0, oos_start - 1)
        factor_end = max(1, oos_end - 1)

        # è·å–é€‰ä¸­å› å­çš„æ ‡å‡†åŒ–æ•°æ®
        factor_signals = []
        for factor_name in selected_factors:
            if factor_name not in std_factors:
                continue
            factor_data = std_factors[factor_name]
            # å–T-1åˆ°T-1+N-1çš„å› å­å€¼ï¼ˆç”¨äºé¢„æµ‹Tåˆ°T+Nï¼‰
            factor_slice = factor_data.iloc[factor_start:factor_end]
            factor_signals.append(factor_slice.values)

        if not factor_signals:
            logger.info(f"[çª—å£ {window_idx}/{total_windows}] æ— å¯ç”¨å› å­æ•°æ®ï¼Œè·³è¿‡")
            continue

        # ç­‰æƒå¹³å‡å¤šå› å­ä¿¡å· (n_days, 43)
        combined_signal = np.nanmean(factor_signals, axis=0)

        # é€æ—¥TopNé€‰è‚¡å¹¶è®¡ç®—ç»„åˆæ”¶ç›Šä¸æ¢æ‰‹/å‡€å€¼
        portfolio_daily_returns = []
        net_daily_returns = []
        daily_turnovers = []
        n_assets = oos_returns.shape[1]
        prev_weights = np.zeros(n_assets, dtype=float)

        for day_idx in range(n_oos_days):
            # Tæ—¥çš„æ”¶ç›Šç‡
            day_returns = oos_returns.iloc[day_idx].values  # (43,)

            # T-1æ—¥çš„å› å­ä¿¡å·ï¼ˆå·²ç»åœ¨combined_signalä¸­å¯¹é½ï¼‰
            if day_idx < len(combined_signal):
                day_signal = combined_signal[day_idx]  # (43,)
            else:
                # è¾¹ç•Œæƒ…å†µï¼šç”¨æœ€åä¸€ä¸ªä¿¡å·ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                day_signal = combined_signal[-1]

            # æ‰¾å‡ºæœ‰æ•ˆçš„ï¼ˆéNaNï¼‰å› å­å€¼å’Œæ”¶ç›Šç‡
            valid_mask = ~(np.isnan(day_signal) | np.isnan(day_returns))
            valid_indices = np.where(valid_mask)[0]

            if len(valid_indices) == 0:
                # å½“æ—¥æ— æœ‰æ•ˆæ•°æ®ï¼Œç»„åˆæ”¶ç›Šä¸º0
                portfolio_daily_returns.append(0.0)
                continue

            # å¯¹æœ‰æ•ˆèµ„äº§æŒ‰å› å­å€¼æ’åºï¼Œé€‰TopN
            valid_signals = day_signal[valid_indices]
            valid_rets = day_returns[valid_indices]

            # é™åºæ’åˆ—ï¼ˆå› å­å€¼è¶Šå¤§è¶Šå¥½ï¼‰
            sorted_idx = np.argsort(-valid_signals)
            topn_count = min(TOPN, len(sorted_idx))
            topn_idx = sorted_idx[:topn_count]

            # TopNç­‰æƒç»„åˆæ”¶ç›Š
            topn_returns = valid_rets[topn_idx]
            portfolio_ret = np.mean(topn_returns)
            portfolio_daily_returns.append(portfolio_ret)

            # è®¡ç®—å½“æ—¥ç›®æ ‡æƒé‡ï¼ˆå…¨å¸‚åœºç»´åº¦ï¼‰
            selected_global_idx = valid_indices[topn_idx]  # ç›¸å¯¹å…¨èµ„äº§çš„ç´¢å¼•
            weights = np.zeros(n_assets, dtype=float)
            if len(selected_global_idx) > 0:
                weights[selected_global_idx] = 1.0 / len(selected_global_idx)

            # è®¡ç®—å½“æ—¥æ¢æ‰‹ç‡ï¼ˆå•è¾¹ï¼‰ï¼š0.5 * L1èŒƒæ•°
            turnover = 0.5 * np.sum(np.abs(weights - prev_weights))
            daily_turnovers.append(float(turnover))

            # æ‰£é™¤æˆæœ¬åçš„å‡€æ”¶ç›Šï¼ˆå•è¾¹æˆæœ¬ï¼‰
            net_ret = portfolio_ret - turnover * COST_RATE
            net_daily_returns.append(net_ret)

            # æ›´æ–°æ˜¨æ—¥æƒé‡
            prev_weights = weights

        portfolio_daily_returns = np.array(portfolio_daily_returns)
        net_daily_returns = np.array(net_daily_returns)
        daily_turnovers = np.array(daily_turnovers) if daily_turnovers else np.array([])

        # ========== è®¡ç®—æ¨ªæˆªé¢ICï¼ˆæ¯æ—¥ICçš„å‡å€¼ï¼‰==========
        daily_ics = []
        for day_idx in range(n_oos_days):
            day_returns = oos_returns.iloc[day_idx].values
            if day_idx < len(combined_signal):
                day_signal = combined_signal[day_idx]
            else:
                day_signal = combined_signal[-1]

            # è®¡ç®—æ¨ªæˆªé¢Spearmanç›¸å…³
            valid_mask = ~(np.isnan(day_signal) | np.isnan(day_returns))
            if valid_mask.sum() < 2:
                continue

            from scipy.stats import spearmanr

            ic, _ = spearmanr(day_signal[valid_mask], day_returns[valid_mask])
            if not np.isnan(ic):
                daily_ics.append(ic)

        avg_oos_ic = np.mean(daily_ics) if daily_ics else 0.0

        # ========== æ€§èƒ½æŒ‡æ ‡è®¡ç®— ==========
        # ç´¯è®¡æ”¶ç›Šç‡ï¼ˆæ¯›/å‡€ï¼‰
        total_return = (1 + portfolio_daily_returns).prod() - 1
        net_total_return = (1 + net_daily_returns).prod() - 1

        # æ ·æœ¬æœŸå¤©æ•°
        n_days = len(portfolio_daily_returns)

        # æ³¢åŠ¨ç‡ï¼šæ—¥æ ‡å‡†å·®ï¼ˆæ¯›/å‡€ï¼‰
        daily_vol = portfolio_daily_returns.std()
        net_daily_vol = net_daily_returns.std()

        # ç»Ÿä¸€å£å¾„ï¼šå…¨éƒ¨æŒ‰å¹´åŒ–å£å¾„æŠ¥å‘Šï¼ˆå¹¶åŒæ—¶ä¿ç•™æœŸé—´æ”¶ç›Šå­—æ®µï¼Œé¿å…æ­§ä¹‰ï¼‰
        annual_return = (1 + total_return) ** (252 / max(1, n_days)) - 1
        annual_vol = daily_vol * np.sqrt(252)
        sharpe = (portfolio_daily_returns.mean() / (daily_vol + 1e-6)) * np.sqrt(252)

        net_annual_return = (1 + net_total_return) ** (252 / max(1, n_days)) - 1
        net_annual_vol = net_daily_vol * np.sqrt(252)
        net_sharpe = (net_daily_returns.mean() / (net_daily_vol + 1e-6)) * np.sqrt(252)

        # æœ€å¤§å›æ’¤ï¼šåŸºäºç´¯è®¡å‡€å€¼
        cumulative_returns = (1 + portfolio_daily_returns).cumprod()
        running_max = cumulative_returns.copy()
        for i in range(1, len(running_max)):
            running_max[i] = max(running_max[i], running_max[i - 1])
        drawdown = (cumulative_returns - running_max) / running_max
        max_dd = drawdown.min()

        # å‡€å€¼æœ€å¤§å›æ’¤
        net_cum = (1 + net_daily_returns).cumprod()
        net_running_max = net_cum.copy()
        for i in range(1, len(net_running_max)):
            net_running_max[i] = max(net_running_max[i], net_running_max[i - 1])
        net_dd = (net_cum - net_running_max) / net_running_max
        net_max_dd = net_dd.min()

        # è®°å½•ç»“æœï¼ˆåŒæ—¶ä¿ç•™æœŸé—´å£å¾„ä»¥ä¾¿äºŒæ¬¡åˆ†æï¼‰
        record = {
            "window_idx": window_idx,
            "is_start": report.is_start,
            "is_end": report.is_end,
            "oos_start": report.oos_start,
            "oos_end": report.oos_end,
            "factor_count": len(selected_factors),
            "selected_factors": "|".join(selected_factors),
            "avg_oos_ic": avg_oos_ic,
            "oos_annual_return": annual_return,
            "oos_annual_vol": annual_vol,
            "oos_sharpe": sharpe,
            "oos_max_dd": max_dd,
            "oos_period_return": total_return,  # æœŸé—´æ”¶ç›Šï¼ˆæœªå¹´åŒ–ï¼‰
            "oos_total_return": total_return,  # å‘åå…¼å®¹ï¼ˆç­‰åŒäºæœŸé—´æ”¶ç›Šï¼‰
            # å‡€å€¼ç›¸å…³
            "oos_net_period_return": net_total_return,
            "oos_net_annual_return": net_annual_return,
            "oos_net_annual_vol": net_annual_vol,
            "oos_net_sharpe": net_sharpe,
            "oos_net_max_dd": net_max_dd,
            # æˆæœ¬ä¸æ¢æ‰‹
            "avg_daily_turnover": (
                float(daily_turnovers.mean()) if len(daily_turnovers) else 0.0
            ),
            "cost_bps": COST_BPS,
        }
        backtest_records.append(record)

        if window_idx % 10 == 0 or window_idx == total_windows:
            logger.info(
                f"[çª—å£ {window_idx}/{total_windows}] "
                f"IC={avg_oos_ic:.4f} Sharpe={sharpe:.4f} AnnRet={annual_return:.4f}"
            )

        # è®°å½•ç”¨äºæ‹¼æ¥éé‡å  OOS æƒç›Šçš„å‰ step_len å¤©
        take_n = min(step_len, n_oos_days)
        if take_n > 0:
            dates_part = oos_returns.index[:take_n]
            gross_part = portfolio_daily_returns[:take_n]
            net_part = net_daily_returns[:take_n]
            for dt, g, n in zip(dates_part, gross_part, net_part):
                stitched_rows.append(
                    {
                        "date": str(dt),
                        "window_idx": window_idx,
                        "gross_ret": float(g),
                        "net_ret": float(n),
                    }
                )

    logger.info(f"\nâœ… å›æµ‹å®Œæˆ: {len(backtest_records)} ä¸ªçª—å£")
    logger.info("")

    backtest_df = pd.DataFrame(backtest_records)

    # ç”Ÿæˆæ‹¼æ¥åçš„æƒç›Šæ›²çº¿
    stitched_df = pd.DataFrame(stitched_rows)
    if not stitched_df.empty:
        stitched_df["cum_gross"] = (1 + stitched_df["gross_ret"]).cumprod()
        stitched_df["cum_net"] = (1 + stitched_df["net_ret"]).cumprod()
    extras = {"stitched_oos": stitched_df}
    return backtest_df, extras


def save_backtest_results(
    backtest_df: pd.DataFrame, output_dir: Path, logger, extras: Dict = None
):
    """ä¿å­˜å›æµ‹ç»“æœ"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 4/4: ä¿å­˜å›æµ‹ç»“æœ")
    logger.info("-" * 80)

    # æ€§èƒ½æ‘˜è¦ï¼ˆåˆ é™¤ä¸ç§‘å­¦çš„total_returnç»Ÿè®¡ï¼‰
    performance_summary = {
        "total_windows": len(backtest_df),
        "avg_ic": backtest_df["avg_oos_ic"].mean(),
        "avg_sharpe": backtest_df["oos_sharpe"].mean(),
        "avg_annual_return": backtest_df["oos_annual_return"].mean(),
        "avg_annual_vol": backtest_df["oos_annual_vol"].mean(),
        "avg_max_dd": backtest_df["oos_max_dd"].mean(),
        # å‡€å€¼æ‘˜è¦
        "avg_net_sharpe": (
            backtest_df.get("oos_net_sharpe", pd.Series(dtype=float)).mean()
            if "oos_net_sharpe" in backtest_df.columns
            else None
        ),
        "avg_net_annual_return": (
            backtest_df.get("oos_net_annual_return", pd.Series(dtype=float)).mean()
            if "oos_net_annual_return" in backtest_df.columns
            else None
        ),
        "avg_net_annual_vol": (
            backtest_df.get("oos_net_annual_vol", pd.Series(dtype=float)).mean()
            if "oos_net_annual_vol" in backtest_df.columns
            else None
        ),
        "avg_net_max_dd": (
            backtest_df.get("oos_net_max_dd", pd.Series(dtype=float)).mean()
            if "oos_net_max_dd" in backtest_df.columns
            else None
        ),
        # æˆæœ¬/æ¢æ‰‹
        "avg_daily_turnover": (
            backtest_df.get("avg_daily_turnover", pd.Series(dtype=float)).mean()
            if "avg_daily_turnover" in backtest_df.columns
            else None
        ),
    }

    # ä¿å­˜ç»„åˆæ€§èƒ½åˆ° CSV
    combo_csv = output_dir / "combination_performance.csv"
    backtest_df.to_csv(combo_csv, index=False)
    logger.info(f"âœ… ç»„åˆæ€§èƒ½å·²ä¿å­˜: {combo_csv}")

    # ä¿å­˜æ€§èƒ½æ‘˜è¦
    summary_csv = output_dir / "performance_summary.csv"
    pd.DataFrame([performance_summary]).to_csv(summary_csv, index=False)
    logger.info(f"âœ… æ€§èƒ½æ‘˜è¦å·²ä¿å­˜: {summary_csv}")

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡æœ¬
    report_path = output_dir / "backtest_report.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        lines = []
        lines.append("=" * 80 + "\n")
        lines.append("WFOçª—å£å› å­ç­–ç•¥å›æµ‹è¯¦ç»†æŠ¥å‘Š\n")
        lines.append("=" * 80 + "\n\n")

        lines.append("æ€§èƒ½æ‘˜è¦\n")
        lines.append("-" * 80 + "\n")
        lines.append(f"æ€»çª—å£æ•°: {performance_summary['total_windows']}\n")
        lines.append(f"å¹³å‡ OOS IC (æ¨ªæˆªé¢): {performance_summary['avg_ic']:.6f}\n")
        lines.append(f"å¹³å‡å¤æ™®æ¯”: {performance_summary['avg_sharpe']:.4f}\n")
        lines.append(f"å¹³å‡å¹´åŒ–æ”¶ç›Š: {performance_summary['avg_annual_return']:.4f}\n")
        lines.append(f"å¹³å‡å¹´åŒ–æ³¢åŠ¨: {performance_summary['avg_annual_vol']:.4f}\n")
        lines.append(f"å¹³å‡æœ€å¤§å›æ’¤: {performance_summary['avg_max_dd']:.4f}\n")
        if performance_summary.get("avg_net_annual_return") is not None:
            lines.append(
                f"å¹³å‡å‡€å¹´åŒ–æ”¶ç›Š: {performance_summary['avg_net_annual_return']:.4f}\n"
            )
        if performance_summary.get("avg_net_annual_vol") is not None:
            lines.append(
                f"å¹³å‡å‡€å¹´åŒ–æ³¢åŠ¨: {performance_summary['avg_net_annual_vol']:.4f}\n"
            )
        if performance_summary.get("avg_net_sharpe") is not None:
            lines.append(f"å¹³å‡å‡€å¤æ™®æ¯”: {performance_summary['avg_net_sharpe']:.4f}\n")
        if performance_summary.get("avg_net_max_dd") is not None:
            lines.append(
                f"å¹³å‡å‡€æœ€å¤§å›æ’¤: {performance_summary['avg_net_max_dd']:.4f}\n"
            )
        if performance_summary.get("avg_daily_turnover") is not None:
            lines.append(
                f"å¹³å‡æ—¥æ¢æ‰‹: {performance_summary['avg_daily_turnover']:.4f}\n"
            )
        lines.append("\n")

        # TOP 10 çª—å£ï¼ˆæŒ‰ Sharpeï¼‰
        lines.append("TOP 10 çª—å£ï¼ˆæŒ‰å¤æ™®æ¯”ï¼‰\n")
        lines.append("-" * 80 + "\n")
        top10 = backtest_df.nlargest(10, "oos_sharpe")
        for _, row in top10.iterrows():
            lines.append(
                f"çª—å£ {row['window_idx']}: "
                f"Sharpe={row['oos_sharpe']:.4f} "
                f"AnnReturn={row['oos_annual_return']:.4f} "
                f"AnnVol={row['oos_annual_vol']:.4f} "
                f"IC={row['avg_oos_ic']:.6f}\n"
            )
        lines.append("\n")

        # ç»Ÿè®¡åˆ†å¸ƒ
        lines.append("ç»Ÿè®¡åˆ†å¸ƒ\n")
        lines.append("-" * 80 + "\n")
        lines.append(
            f"IC èŒƒå›´: [{backtest_df['avg_oos_ic'].min():.6f}, "
            f"{backtest_df['avg_oos_ic'].max():.6f}]\n"
        )
        lines.append(
            f"Sharpe èŒƒå›´: [{backtest_df['oos_sharpe'].min():.4f}, "
            f"{backtest_df['oos_sharpe'].max():.4f}]\n"
        )
        lines.append(
            f"å¹´åŒ–æ”¶ç›ŠèŒƒå›´: [{backtest_df['oos_annual_return'].min():.4f}, "
            f"{backtest_df['oos_annual_return'].max():.4f}]\n\n"
        )

        f.writelines(lines)

    logger.info(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    logger.info("")

    # é¢å¤–äº§ç‰©ï¼šæ‹¼æ¥çš„éé‡å  OOS æƒç›Šæ›²çº¿
    if extras and isinstance(extras.get("stitched_oos"), pd.DataFrame):
        stitched: pd.DataFrame = extras["stitched_oos"]
        stitched_path = output_dir / "stitched_oos_equity.csv"
        stitched.to_csv(stitched_path, index=False)
        logger.info(f"âœ… éé‡å OOSæƒç›Šå·²ä¿å­˜: {stitched_path}")


def main(wfo_dir: Path = None):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # è¾“å‡ºç›®å½•
    output_root = PROJECT_ROOT / "results"
    backtest_dir = output_root / "backtest" / timestamp
    backtest_dir.mkdir(parents=True, exist_ok=True)

    logger = setup_logging(backtest_dir)

    logger.info("=" * 80)
    logger.info("Step 4: WFOçª—å£å› å­ç­–ç•¥å›æµ‹")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {backtest_dir}")
    logger.info(f"æ—¶é—´æˆ³: {timestamp}")
    logger.info("")

    # æŸ¥æ‰¾ WFO ç»“æœ
    if wfo_dir is None:
        logger.info("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ WFO ç»“æœ...")
        wfo_dir = find_latest_wfo(output_root)

        if wfo_dir is None:
            logger.error("âŒ æœªæ‰¾åˆ° WFO ç»“æœï¼è¯·å…ˆè¿è¡Œ step3_run_wfo.py")
            sys.exit(1)

        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–° WFO ç»“æœ: {wfo_dir}")
        logger.info("")

    # 1. åŠ è½½ WFO ç»“æœ
    wfo_results = load_wfo_results(wfo_dir, logger)

    # 2. åŠ è½½å›æµ‹æ•°æ®
    ohlcv_data, std_factors, wfo_meta = load_backtest_data(wfo_dir, output_root, logger)

    # 3. è¿è¡Œå›æµ‹
    backtest_df, extras = run_backtest_combinations(
        wfo_results, ohlcv_data, std_factors, wfo_meta, logger
    )

    # 4. ä¿å­˜ç»“æœ
    save_backtest_results(backtest_df, backtest_dir, logger, extras)

    # å®Œæˆ
    logger.info("=" * 80)
    logger.info("âœ… Step 4 å®Œæˆï¼WFOçª—å£å› å­ç­–ç•¥å›æµ‹å·²æ‰§è¡Œ")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {backtest_dir}")
    logger.info(f"  - combination_performance.csv: çª—å£æ€§èƒ½æ•°æ®")
    logger.info(f"  - performance_summary.csv: æ€§èƒ½æ‘˜è¦")
    logger.info(f"  - backtest_report.txt: è¯¦ç»†æŠ¥å‘Š")
    logger.info(f"  - step4_backtest.log: æ‰§è¡Œæ—¥å¿—")
    logger.info("")


if __name__ == "__main__":
    main()
