#!/usr/bin/env python3
"""
Step 1: æ¨ªæˆªé¢å»ºè®¾ - ç‹¬ç«‹æ‰§è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½43åªETFçš„OHLCVæ•°æ®
2. è®¡ç®—10ä¸ªç²¾ç¡®å› å­
3. ä¿å­˜åŽŸå§‹å› å­æ•°æ®åˆ° cross_section/
4. è¯¦ç»†çš„è¿›åº¦æ—¥å¿—å’ŒéªŒè¯

è¾“å‡ºï¼š
- cross_section/{date}/{timestamp}/ohlcv/*.parquet
- cross_section/{date}/{timestamp}/factors/*.parquet
- cross_section/{date}/{timestamp}/metadata.json
"""

import json
import logging
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.precise_factor_library_v2 import PreciseFactorLibrary
from utils.factor_cache import FactorCache

from scripts.standard_real_data_loader import StandardRealDataLoader


def setup_logging(output_dir: Path):
    """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
    log_file = output_dir / "step1_cross_section.log"

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s - %(message)s",
        datefmt="%H:%M:%S",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def main():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_date = timestamp[:8]

    # è¾“å‡ºç›®å½•
    output_root = PROJECT_ROOT / "results"
    cross_section_dir = output_root / "cross_section" / run_date / timestamp
    cross_section_dir.mkdir(parents=True, exist_ok=True)

    logger = setup_logging(cross_section_dir)

    logger.info("=" * 80)
    logger.info("Step 1: æ¨ªæˆªé¢å»ºè®¾ï¼ˆ43åªETFï¼Œå®Œæ•´å› å­è®¡ç®—ï¼‰")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {cross_section_dir}")
    logger.info(f"æ—¶é—´æˆ³: {timestamp}")
    logger.info("")

    # ========== 1. åŠ è½½æ•°æ® ==========
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 1/4: åŠ è½½ETFæ•°æ®")
    logger.info("-" * 80)

    etf_codes = [
        # æ·±åœ³ETF (19åª)
        "159801",
        "159819",
        "159859",
        "159883",
        "159915",
        "159920",
        "159928",
        "159949",
        "159992",
        "159995",
        "159998",
        # ä¸Šæµ·ETF (24åª)
        "510050",
        "510300",
        "510500",
        "511010",
        "511260",
        "511380",
        "512010",
        "512100",
        "512400",
        "512480",
        "512660",
        "512690",
        "512720",
        "512800",
        "512880",
        "512980",
        "513050",
        "513100",
        "513130",
        "513500",
        "515030",
        "515180",
        "515210",
        "515650",
        "515790",
        "516090",
        "516160",
        "516520",
        "518850",
        "518880",
        "588000",
        "588200",
    ]

    logger.info(f"ETFä»£ç : {len(etf_codes)}åª")
    logger.info(f"ä»£ç åˆ—è¡¨: {etf_codes[:5]}...ï¼ˆå…±{len(etf_codes)}åªï¼‰")

    loader = StandardRealDataLoader()
    ohlcv_data = loader.load_ohlcv(
        etf_codes=etf_codes, start_date="2020-01-01", end_date="2025-10-14"
    )

    data_summary = loader.get_summary(ohlcv_data)

    logger.info("âœ… æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"   æ—¥æœŸ: {data_summary['total_dates']} å¤©")
    logger.info(f"   æ ‡çš„: {data_summary['total_symbols']} åª")
    logger.info(
        f"   æ—¥æœŸèŒƒå›´: {data_summary['date_range'][0]} -> {data_summary['date_range'][1]}"
    )
    logger.info("")

    # è¦†ç›–çŽ‡ç»Ÿè®¡
    low_coverage = {
        code: ratio
        for code, ratio in data_summary["coverage_ratio"].items()
        if ratio < 0.95
    }
    if low_coverage:
        logger.warning(f"âš ï¸  {len(low_coverage)} åªETFè¦†ç›–çŽ‡ < 95%:")
        for code, ratio in sorted(low_coverage.items(), key=lambda x: x[1])[:10]:
            logger.warning(f"     {code}: {ratio*100:.2f}%")
    logger.info("")

    # ========== 2. è®¡ç®—å› å­ï¼ˆå¸¦ç¼“å­˜ï¼‰ ==========
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 2/4: è®¡ç®—ç²¾ç¡®å› å­ï¼ˆPreciseFactorLibrary v2ï¼‰")
    logger.info("-" * 80)

    cache_dir = PROJECT_ROOT / "cache" / "factors"
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache = FactorCache(cache_dir=cache_dir, use_timestamp=True)

    lib = PreciseFactorLibrary()

    # å°è¯•åŠ è½½ç¼“å­˜
    cached_factors = cache.load_factors(
        ohlcv=ohlcv_data, lib_class=lib.__class__, stage="raw"
    )

    if cached_factors is not None:
        logger.info("âœ… ä½¿ç”¨å› å­ç¼“å­˜ï¼ˆè·³è¿‡è®¡ç®—ï¼‰")
        factors_dict = cached_factors
    else:
        logger.info("ðŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹è®¡ç®—å› å­...")
        import time

        start_time = time.time()

        factors_df = lib.compute_all_factors(prices=ohlcv_data)

        elapsed = time.time() - start_time
        logger.info(f"âœ… å› å­è®¡ç®—å®Œæˆï¼ˆè€—æ—¶ {elapsed:.1f}ç§’ï¼‰")

        # è½¬æ¢ä¸ºå­—å…¸
        factors_dict = {}
        for factor_name in lib.list_factors():
            factors_dict[factor_name] = factors_df[factor_name]

        # ä¿å­˜ç¼“å­˜
        cache.save_factors(
            factors=factors_dict, ohlcv=ohlcv_data, lib_class=lib.__class__, stage="raw"
        )
        logger.info(f"ðŸ’¾ å› å­ç¼“å­˜å·²ä¿å­˜")

    logger.info("")
    logger.info(f"å› å­æ•°é‡: {len(factors_dict)}")
    for idx, (fname, fdata) in enumerate(factors_dict.items(), start=1):
        nan_ratio = fdata.isna().sum().sum() / fdata.size
        logger.info(f"  {idx:02d}. {fname:25s} NaNçŽ‡: {nan_ratio*100:.2f}%")
    logger.info("")

    # ========== 3. ä¿å­˜OHLCVæ•°æ® ==========
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 3/4: ä¿å­˜OHLCVæ•°æ®")
    logger.info("-" * 80)

    ohlcv_dir = cross_section_dir / "ohlcv"
    ohlcv_dir.mkdir(exist_ok=True)

    for col_name, df in ohlcv_data.items():
        output_path = ohlcv_dir / f"{col_name}.parquet"
        df.to_parquet(output_path)
        logger.info(f"  âœ… {col_name}.parquet ({df.shape[0]} Ã— {df.shape[1]})")

    logger.info("")

    # ========== 4. ä¿å­˜å› å­æ•°æ® ==========
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 4/4: ä¿å­˜åŽŸå§‹å› å­æ•°æ®")
    logger.info("-" * 80)

    factors_dir = cross_section_dir / "factors"
    factors_dir.mkdir(exist_ok=True)

    for fname, fdata in factors_dict.items():
        output_path = factors_dir / f"{fname}.parquet"
        # fdataå¯èƒ½æ˜¯Seriesæˆ–DataFrameï¼Œç»Ÿä¸€è½¬æ¢ä¸ºDataFrame
        if isinstance(fdata, pd.Series):
            df_to_save = fdata.to_frame(name=fname)
        else:
            df_to_save = fdata
        df_to_save.to_parquet(output_path)
        total_rows = len(df_to_save)
        logger.info(f"  âœ… {fname}.parquet ({total_rows} è¡Œ)")

    logger.info("")

    # ========== 5. ä¿å­˜å…ƒæ•°æ® ==========
    metadata = {
        "timestamp": timestamp,
        "step": "cross_section",
        "etf_count": len(etf_codes),
        "etf_codes": etf_codes,
        "date_range": data_summary["date_range"],
        "total_dates": data_summary["total_dates"],
        "factor_count": len(factors_dict),
        "factor_names": list(factors_dict.keys()),
        "coverage_ratio": data_summary["coverage_ratio"],
        "output_dir": str(cross_section_dir),
    }

    metadata_path = cross_section_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    logger.info("")

    # ========== å®Œæˆ ==========
    logger.info("=" * 80)
    logger.info("âœ… Step 1 å®Œæˆï¼æ¨ªæˆªé¢æ•°æ®å·²æž„å»º")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {cross_section_dir}")
    logger.info(f"  - ohlcv/: {len(ohlcv_data)} ä¸ªæ–‡ä»¶")
    logger.info(f"  - factors/: {len(factors_dict)} ä¸ªæ–‡ä»¶")
    logger.info(f"  - metadata.json")
    logger.info("")
    logger.info("ðŸ”œ ä¸‹ä¸€æ­¥: è¿è¡Œ step2_factor_selection.py è¿›è¡Œå› å­ç­›é€‰")
    logger.info("")

    # è¿”å›žè¾“å‡ºç›®å½•ï¼Œä¾›åŽç»­æ­¥éª¤ä½¿ç”¨
    return cross_section_dir


if __name__ == "__main__":
    main()
