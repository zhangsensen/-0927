"""
Step 3: Ensemble Walk-Forward Optimization

é›†æˆå‰å‘å›æµ‹ä¼˜åŒ– - 1000ç»„åˆé‡‡æ · + Top10é›†æˆ

è¾“å…¥: step2çš„æ ‡å‡†åŒ–å› å­æ•°æ®
è¾“å‡º: ensemble_wfoç»“æœ (CSV + JSON)

è¿è¡Œ:
    python scripts/step3_ensemble_wfo.py
    python scripts/step3_ensemble_wfo.py --factor-selection-dir results/factor_selection/20250128/20250128_120000
"""

import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.ensemble_wfo_optimizer import EnsembleWFOOptimizer


def setup_logging(output_dir: Path) -> logging.Logger:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_file = output_dir / "step3_ensemble_wfo.log"

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )

    return logging.getLogger(__name__)


def find_latest_factor_selection(output_root: Path) -> Path:
    """æŸ¥æ‰¾æœ€æ–°çš„å› å­é€‰æ‹©ç»“æœç›®å½•"""
    factor_selection_root = output_root / "factor_selection"

    if not factor_selection_root.exists():
        return None

    # æŸ¥æ‰¾æœ€æ–°çš„æ—¥æœŸç›®å½•
    date_dirs = sorted(
        [d for d in factor_selection_root.iterdir() if d.is_dir() and d.name.isdigit()],
        reverse=True,
    )

    if not date_dirs:
        return None

    # æŸ¥æ‰¾è¯¥æ—¥æœŸä¸‹æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
    latest_date_dir = date_dirs[0]
    timestamp_dirs = sorted(
        [d for d in latest_date_dir.iterdir() if d.is_dir()], reverse=True
    )

    if not timestamp_dirs:
        return None

    return timestamp_dirs[0]


def load_factor_selection_data(
    factor_selection_dir: Path, logger: logging.Logger
) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame], Dict]:
    """
    åŠ è½½step2çš„å› å­é€‰æ‹©æ•°æ®

    Returns:
        (ohlcv_data, factors_dict, metadata)
    """
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 1/4: åŠ è½½å› å­é€‰æ‹©æ•°æ®")
    logger.info("-" * 80)
    logger.info(f"æ•°æ®ç›®å½•: {factor_selection_dir}")

    # 1. åŠ è½½OHLCV
    ohlcv_path = factor_selection_dir / "standardized" / "OHLCV.parquet"
    if not ohlcv_path.exists():
        raise FileNotFoundError(f"OHLCVæ•°æ®ä¸å­˜åœ¨: {ohlcv_path}")

    ohlcv_data = pd.read_parquet(ohlcv_path)
    logger.info(f"âœ… OHLCVæ•°æ®: {ohlcv_data.shape}")

    # 2. åŠ è½½æ‰€æœ‰å› å­
    factors_dir = factor_selection_dir / "standardized"
    factors_dict = {}

    for factor_file in sorted(factors_dir.glob("*.parquet")):
        if factor_file.stem == "OHLCV":
            continue

        factor_name = factor_file.stem
        df = pd.read_parquet(factor_file)
        factors_dict[factor_name] = df

    logger.info(f"âœ… åŠ è½½ {len(factors_dict)} ä¸ªå› å­")

    # 3. åŠ è½½å…ƒæ•°æ®
    metadata_path = factor_selection_dir / "metadata.json"
    if metadata_path.exists():
        import json

        with open(metadata_path, encoding="utf-8") as f:
            metadata = json.load(f)
    else:
        metadata = {}

    logger.info(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ")
    logger.info("")

    return ohlcv_data, factors_dict, metadata


def prepare_wfo_data(
    ohlcv_data: pd.DataFrame, factors_dict: Dict[str, pd.DataFrame], logger: logging.Logger
) -> Tuple[np.ndarray, np.ndarray, list]:
    """
    å‡†å¤‡WFOæ•°æ®æ ¼å¼

    Returns:
        (factors_array, returns_array, factor_names)
        - factors_array: (T, N, K) ndarray
        - returns_array: (T, N) ndarray
        - factor_names: List[str]
    """
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 2/4: å‡†å¤‡WFOæ•°æ®æ ¼å¼")
    logger.info("-" * 80)

    # 1. æå–æ”¶ç›Šç‡
    if "RET_1D" in ohlcv_data.columns:
        returns_df = ohlcv_data["RET_1D"]
    else:
        logger.warning("âš ï¸  OHLCVä¸­æ— RET_1Dåˆ—ï¼Œä½¿ç”¨Closeè®¡ç®—æ”¶ç›Šç‡")
        returns_df = ohlcv_data["Close"].pct_change()

    returns_array = returns_df.values  # (T, N)
    logger.info(f"âœ… æ”¶ç›Šç‡æ•°ç»„: {returns_array.shape}")

    # 2. å †å å› å­ä¸º3Dæ•°ç»„
    factor_names = sorted(factors_dict.keys())
    factor_arrays = []

    for factor_name in factor_names:
        factor_df = factors_dict[factor_name]
        factor_arrays.append(factor_df.values)  # (T, N)

    # å †å : (K, T, N) â†’ (T, N, K)
    factors_array = np.stack(factor_arrays, axis=0)  # (K, T, N)
    factors_array = np.transpose(factors_array, (1, 2, 0))  # (T, N, K)

    logger.info(f"âœ… å› å­æ•°ç»„: {factors_array.shape}")
    logger.info(f"   - æ—¶é—´æ­¥: {factors_array.shape[0]}")
    logger.info(f"   - èµ„äº§æ•°: {factors_array.shape[1]}")
    logger.info(f"   - å› å­æ•°: {factors_array.shape[2]}")
    logger.info("")

    return factors_array, returns_array, factor_names


def load_constraints_config(logger: logging.Logger) -> Dict:
    """åŠ è½½å› å­çº¦æŸé…ç½®"""
    constraints_path = PROJECT_ROOT / "configs" / "FACTOR_SELECTION_CONSTRAINTS.yaml"

    if not constraints_path.exists():
        logger.warning(f"âš ï¸  çº¦æŸé…ç½®ä¸å­˜åœ¨: {constraints_path}")
        logger.warning("âš ï¸  å°†ä½¿ç”¨ç©ºçº¦æŸé…ç½®")
        return {"family_quotas": {}, "mutually_exclusive_pairs": []}

    with open(constraints_path, encoding="utf-8") as f:
        config = yaml.safe_load(f)

    # æ ¼å¼è½¬æ¢: family_quota â†’ family_quotas (å…¼å®¹EnsembleSampler)
    if "family_quota" in config:
        family_quota_data = config["family_quota"]
        
        # è½¬æ¢ä¸ºEnsembleSampleræœŸæœ›çš„æ ¼å¼
        family_quotas = {}
        for family_name, family_config in family_quota_data.items():
            if isinstance(family_config, dict) and family_config.get("enabled", True):
                family_quotas[family_name] = {
                    "max_count": family_config.get("max_count", 2),
                    "candidates": family_config.get("factors", [])
                }
        
        config["family_quotas"] = family_quotas
    else:
        config["family_quotas"] = {}
    
    # ç¡®ä¿mutually_exclusive_pairså­˜åœ¨
    if "mutually_exclusive_pairs" not in config:
        config["mutually_exclusive_pairs"] = []

    logger.info(f"âœ… åŠ è½½çº¦æŸé…ç½®: {constraints_path}")
    logger.info(f"   - å®¶æ—é…é¢: {len(config.get('family_quotas', {}))} ä¸ª")
    logger.info(f"   - äº’æ–¥å¯¹: {len(config.get('mutually_exclusive_pairs', []))} å¯¹")

    return config


def run_ensemble_wfo(
    factors_array: np.ndarray,
    returns_array: np.ndarray,
    factor_names: list,
    constraints_config: Dict,
    output_dir: Path,
    logger: logging.Logger,
    n_samples: int = 1000,
    combo_size: int = 5,
    top_k: int = 10,
    weighting_scheme: str = "gradient_decay",
    is_period: int = 100,
    oos_period: int = 20,
    step_size: int = 20,
) -> pd.DataFrame:
    """
    è¿è¡ŒEnsemble WFOä¼˜åŒ–

    Args:
        n_samples: æ¯çª—å£é‡‡æ ·ç»„åˆæ•° (é»˜è®¤1000)
        combo_size: æ¯ç»„åˆå› å­æ•° (é»˜è®¤5)
        top_k: é›†æˆçš„æœ€ä¼˜ç»„åˆæ•° (é»˜è®¤10)
        weighting_scheme: åŠ æƒæ–¹æ¡ˆ (é»˜è®¤gradient_decay)
        is_period: ISçª—å£é•¿åº¦ (é»˜è®¤100)
        oos_period: OOSçª—å£é•¿åº¦ (é»˜è®¤20)
        step_size: æ»‘åŠ¨æ­¥é•¿ (é»˜è®¤20)

    Returns:
        æ±‡æ€»DataFrame
    """
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 3/4: è¿è¡ŒEnsemble WFOä¼˜åŒ–")
    logger.info("-" * 80)
    logger.info(f"é‡‡æ ·é…ç½®: {n_samples}ä¸ªç»„åˆ Ã— {combo_size}å› å­")
    logger.info(f"é›†æˆé…ç½®: Top{top_k}, æƒé‡={weighting_scheme}")
    logger.info(f"çª—å£é…ç½®: IS={is_period}, OOS={oos_period}, step={step_size}")
    logger.info("")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = EnsembleWFOOptimizer(
        constraints_config=constraints_config,
        n_samples=n_samples,
        combo_size=combo_size,
        top_k=top_k,
        weighting_scheme=weighting_scheme,
        random_seed=42,
        verbose=True,
    )

    # è¿è¡ŒWFO
    summary_df = optimizer.run_ensemble_wfo(
        factors_data=factors_array,
        returns=returns_array,
        factor_names=factor_names,
        is_period=is_period,
        oos_period=oos_period,
        step_size=step_size,
    )

    # ä¿å­˜ç»“æœ
    optimizer.save_results(output_dir)

    logger.info("")
    logger.info(f"âœ… Ensemble WFOä¼˜åŒ–å®Œæˆ")
    logger.info(f"   - æ€»çª—å£æ•°: {len(summary_df)}")
    logger.info(f"   - å¹³å‡OOS IC: {summary_df['oos_ensemble_ic'].mean():.4f}")
    logger.info(
        f"   - å¹³å‡OOS Sharpe: {summary_df['oos_ensemble_sharpe'].mean():.2f}"
    )
    logger.info("")

    return summary_df


def generate_summary_report(
    summary_df: pd.DataFrame, output_dir: Path, logger: logging.Logger
):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 4/4: ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
    logger.info("-" * 80)

    # 1. æ€§èƒ½ç»Ÿè®¡
    stats = {
        "total_windows": len(summary_df),
        "mean_oos_ic": summary_df["oos_ensemble_ic"].mean(),
        "std_oos_ic": summary_df["oos_ensemble_ic"].std(),
        "mean_oos_sharpe": summary_df["oos_ensemble_sharpe"].mean(),
        "std_oos_sharpe": summary_df["oos_ensemble_sharpe"].std(),
        "positive_ic_ratio": (summary_df["oos_ensemble_ic"] > 0).mean(),
    }

    logger.info("ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    logger.info(f"   - æ€»çª—å£æ•°: {stats['total_windows']}")
    logger.info(
        f"   - OOS IC: {stats['mean_oos_ic']:.4f} Â± {stats['std_oos_ic']:.4f}"
    )
    logger.info(
        f"   - OOS Sharpe: {stats['mean_oos_sharpe']:.2f} Â± {stats['std_oos_sharpe']:.2f}"
    )
    logger.info(f"   - æ­£ICæ¯”ç‡: {stats['positive_ic_ratio']:.1%}")
    logger.info("")

    # 2. ä¿å­˜ç»Ÿè®¡
    stats_path = output_dir / "performance_stats.json"
    import json

    with open(stats_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… æ€§èƒ½ç»Ÿè®¡å·²ä¿å­˜: {stats_path}")

    # 3. ç»˜åˆ¶æ€§èƒ½æ›²çº¿ (å¯é€‰,å¦‚æœæœ‰matplotlib)
    try:
        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(2, 1, figsize=(12, 8))

        # ICæ›²çº¿
        axes[0].plot(summary_df.index, summary_df["oos_ensemble_ic"], marker="o")
        axes[0].axhline(0, color="red", linestyle="--", alpha=0.5)
        axes[0].set_title("OOS Ensemble IC")
        axes[0].set_ylabel("IC")
        axes[0].grid(True, alpha=0.3)

        # Sharpeæ›²çº¿
        axes[1].plot(
            summary_df.index, summary_df["oos_ensemble_sharpe"], marker="s", color="green"
        )
        axes[1].axhline(0, color="red", linestyle="--", alpha=0.5)
        axes[1].set_title("OOS Ensemble Sharpe")
        axes[1].set_xlabel("Window Index")
        axes[1].set_ylabel("Sharpe")
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plot_path = output_dir / "performance_curves.png"
        plt.savefig(plot_path, dpi=150, bbox_inches="tight")
        plt.close()

        logger.info(f"âœ… æ€§èƒ½æ›²çº¿å·²ä¿å­˜: {plot_path}")

    except ImportError:
        logger.warning("âš ï¸  matplotlibæœªå®‰è£…,è·³è¿‡æ€§èƒ½æ›²çº¿ç»˜åˆ¶")

    logger.info("")


def main(factor_selection_dir: Path = None):
    """ä¸»å…¥å£"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_date = timestamp[:8]

    # è¾“å‡ºç›®å½•
    output_root = PROJECT_ROOT / "results"
    ensemble_wfo_dir = output_root / "ensemble_wfo" / run_date / timestamp
    ensemble_wfo_dir.mkdir(parents=True, exist_ok=True)

    logger = setup_logging(ensemble_wfo_dir)

    logger.info("=" * 80)
    logger.info("Step 3: Ensemble Walk-Forward Optimization")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {ensemble_wfo_dir}")
    logger.info(f"æ—¶é—´æˆ³: {timestamp}")
    logger.info("")

    # æŸ¥æ‰¾è¾“å…¥æ•°æ®
    if factor_selection_dir is None:
        logger.info("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„å› å­é€‰æ‹©æ•°æ®...")
        factor_selection_dir = find_latest_factor_selection(output_root)

        if factor_selection_dir is None:
            logger.error("âŒ æœªæ‰¾åˆ°å› å­é€‰æ‹©æ•°æ®ï¼è¯·å…ˆè¿è¡Œ step2_factor_selection.py")
            sys.exit(1)

        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–°æ•°æ®: {factor_selection_dir}")
        logger.info("")

    # 1. åŠ è½½æ•°æ®
    ohlcv_data, factors_dict, metadata = load_factor_selection_data(
        factor_selection_dir, logger
    )

    # 2. å‡†å¤‡WFOæ•°æ®
    factors_array, returns_array, factor_names = prepare_wfo_data(
        ohlcv_data, factors_dict, logger
    )

    # 3. åŠ è½½çº¦æŸé…ç½®
    constraints_config = load_constraints_config(logger)
    logger.info("")

    # 4. è¿è¡ŒEnsemble WFO
    summary_df = run_ensemble_wfo(
        factors_array=factors_array,
        returns_array=returns_array,
        factor_names=factor_names,
        constraints_config=constraints_config,
        output_dir=ensemble_wfo_dir,
        logger=logger,
    )

    # 5. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(summary_df, ensemble_wfo_dir, logger)

    # å®Œæˆ
    logger.info("=" * 80)
    logger.info("âœ… Step 3 å®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"ç»“æœä¿å­˜è‡³: {ensemble_wfo_dir}")
    logger.info(f"   - ensemble_wfo_summary.csv")
    logger.info(f"   - ensemble_wfo_detailed.json")
    logger.info(f"   - performance_stats.json")
    logger.info("=" * 80)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Step 3: Ensemble WFO")
    parser.add_argument(
        "--factor-selection-dir",
        type=Path,
        default=None,
        help="å› å­é€‰æ‹©ç»“æœç›®å½• (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)",
    )

    args = parser.parse_args()

    main(factor_selection_dir=args.factor_selection_dir)
