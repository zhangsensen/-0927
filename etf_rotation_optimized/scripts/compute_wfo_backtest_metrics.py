#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WFOå›æµ‹æŒ‡æ ‡è®¡ç®— - Top-Nå› å­åŠ æƒç­–ç•¥

ä» wfo_results.pkl æå–é€‰ä¸­å› å­ä¸çª—å£æ—¶é—´ï¼ŒåŸºäºå› å­å€¼æ’åºTop-Nå»ºä»“ï¼š
- åˆæˆåˆ†æ•°ï¼šé€‰ä¸­å› å­ç­‰æƒåˆæˆcomposite_score
- æŒä»“æ„å»ºï¼šæŒ‰composite_scoreæ’åºå–Top-Nï¼ˆé»˜è®¤12åªETFï¼‰
- æ”¶ç›Šè®¡ç®—ï¼šæŒä»“æœŸå†…ç­‰æƒç»„åˆæ”¶ç›Š
- æŒ‡æ ‡è¾“å‡ºï¼šå¹´åŒ–æ”¶ç›Šã€å¤æ™®ã€æœ€å¤§å›æ’¤ã€æ¢æ‰‹ç‡ã€èƒœç‡

è¾“å…¥ï¼šwfo/{timestamp}/wfo_results.pkl + standardizedå› å­ + ohlcv
è¾“å‡ºï¼šæ‰©å±• wfo_report.txt ä¸ metadata.json
"""

import argparse
import json
import pickle
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ç­–ç•¥å‚æ•°
# ä¸ Step4 å›æµ‹è„šæœ¬ä¿æŒä¸€è‡´ï¼šé»˜è®¤TopN=5ï¼Œå¯é€šè¿‡CLIè¦†ç›–
TOP_N_HOLDINGS = 5  # Top-NæŒä»“æ•°é‡ï¼ˆé»˜è®¤ï¼Œå¯é€šè¿‡CLIè¦†ç›–ï¼‰


def load_ohlcv_data():
    """åŠ è½½OHLCVæ•°æ®ï¼ˆä½¿ç”¨æœ€æ–°cross_sectionï¼‰"""
    cross_section_root = PROJECT_ROOT / "results" / "cross_section"
    all_runs = []
    for date_dir in cross_section_root.iterdir():
        if not date_dir.is_dir():
            continue
        for ts_dir in date_dir.iterdir():
            if (ts_dir / "metadata.json").exists():
                all_runs.append(ts_dir)
    if not all_runs:
        raise FileNotFoundError("æ— æ³•æ‰¾åˆ°cross_sectionæ•°æ®")
    all_runs.sort(key=lambda p: p.name, reverse=True)
    latest = all_runs[0]
    ohlcv_dir = latest / "ohlcv"
    close_df = pd.read_parquet(ohlcv_dir / "close.parquet")
    return close_df


def load_standardized_factors():
    """åŠ è½½æ ‡å‡†åŒ–å› å­æ•°æ®ï¼ˆä½¿ç”¨æœ€æ–°factor_selectionï¼‰"""
    selection_root = PROJECT_ROOT / "results" / "factor_selection"
    all_runs = []
    for date_dir in selection_root.iterdir():
        if not date_dir.is_dir():
            continue
        for ts_dir in date_dir.iterdir():
            if (ts_dir / "metadata.json").exists():
                all_runs.append(ts_dir)
    if not all_runs:
        raise FileNotFoundError("æ— æ³•æ‰¾åˆ°factor_selectionæ•°æ®")
    all_runs.sort(key=lambda p: p.name, reverse=True)
    latest = all_runs[0]

    # è¯»å–å…ƒæ•°æ®è·å–å› å­åˆ—è¡¨
    with open(latest / "metadata.json", "r", encoding="utf-8") as f:
        meta = json.load(f)

    factor_names = meta["standardized_factor_names"]
    standardized_dir = latest / "standardized"

    factors_dict = {}
    for fname in factor_names:
        parquet_path = standardized_dir / f"{fname}.parquet"
        if parquet_path.exists():
            factors_dict[fname] = pd.read_parquet(parquet_path)

    return factors_dict


def compute_portfolio_returns_topn(
    close_df: pd.DataFrame,
    window_results: List[dict],
    factors_dict: Dict[str, pd.DataFrame],
    top_n: int = TOP_N_HOLDINGS,
    exclude_factors: Optional[List[str]] = None,
    weight_mode: str = "equal",
    constraint_reports: Optional[List] = None,
    tx_cost_bps: float = 0.0,
    max_turnover: float = 1.0,
):
    """
    è®¡ç®—Top-Nå› å­åŠ æƒç»„åˆæ”¶ç›Š

    æµç¨‹ï¼š
    1. æ¯çª—å£OOSèµ·å§‹ï¼Œè¯»å–é€‰ä¸­å› å­å€¼
    2. ç­‰æƒåˆæˆcomposite_score
    3. æŒ‰scoreæ’åºå–Top-NåªETFç­‰æƒå»ºä»“
    4. è®¡ç®—æŒä»“æœŸæ”¶ç›Šä¸æ¢æ‰‹
    """
    returns = close_df.pct_change(fill_method=None).iloc[1:]  # å¯¹é½
    gross_rets = []
    net_rets = []
    holdings_history = []
    turnover_list = []
    prev_holdings = set()
    etf_codes = close_df.columns.tolist()

    exclude_factors = set(exclude_factors or [])

    for w in window_results:
        oos_start = int(w["oos_start"])
        oos_end = int(w["oos_end"])
        selected_factors = [
            f for f in w["selected_factors"] if f not in exclude_factors
        ]  # è¿‡æ»¤ç¦ç”¨å› å­

        if oos_start >= len(returns) or oos_end > len(returns):
            print(f"âš ï¸  çª—å£{w['window_id']}: OOSç´¢å¼•è¶…ç•Œï¼Œè·³è¿‡")
            continue

        # æ„å»ºcomposite_scoreï¼ˆé€‰ä¸­å› å­åŠ æƒåˆæˆï¼‰
        # ä½¿ç”¨OOSèµ·å§‹æ—¥Tçš„å› å­å€¼ï¼ˆå·²å¯¹é½T-1â†’Té¢„æµ‹ï¼‰
        composite_scores = pd.Series(0.0, index=etf_codes)

        # å› å­æƒé‡ï¼šequal æˆ– icï¼ˆä½¿ç”¨IS ICä½œä¸ºæƒé‡ï¼Œè´ŸICæˆªæ–­ä¸º0ï¼‰
        factor_weights: Dict[str, float] = {}
        if weight_mode == "ic" and constraint_reports is not None:
            try:
                wid = int(w.get("window_id", 0))
                report = (
                    constraint_reports[wid - 1]
                    if wid and len(constraint_reports) >= wid
                    else None
                )
                if report is not None and hasattr(report, "is_ic_stats"):
                    is_ic_stats: Dict[str, float] = report.is_ic_stats or {}
                    for f in selected_factors:
                        icv = float(is_ic_stats.get(f, 0.0))
                        factor_weights[f] = max(icv, 0.0)
                    # å½’ä¸€åŒ–
                    s = sum(factor_weights.values())
                    if s > 0:
                        factor_weights = {k: v / s for k, v in factor_weights.items()}
                    else:
                        factor_weights = {}
            except Exception as e:
                print(f"âš ï¸  IS ICæƒé‡æ„å»ºå¤±è´¥ï¼Œå›é€€ç­‰æƒ: {e}")
                factor_weights = {}

        # ç­‰æƒå›é€€
        if not factor_weights:
            if len(selected_factors) > 0:
                eq_w = 1.0 / len(selected_factors)
                factor_weights = {f: eq_w for f in selected_factors}
            else:
                factor_weights = {}

        valid_factor_count = 0
        for factor_name in selected_factors:
            if factor_name not in factors_dict:
                continue
            factor_df = factors_dict[factor_name]
            if oos_start >= len(factor_df):
                continue
            # å–OOSèµ·å§‹æ—¥å› å­å€¼ï¼ˆè¡Œç´¢å¼•=oos_startå¯¹åº”æ—¥æœŸï¼‰
            factor_values = factor_df.iloc[oos_start]
            # å¡«å……NaNä¸º0ï¼ˆå› å­ç¼ºå¤±ETFä¸å‚ä¸æ’åºï¼‰
            factor_values = factor_values.fillna(0.0)
            wgt = factor_weights.get(factor_name, 0.0)
            if wgt <= 0:
                continue
            composite_scores += factor_values * wgt
            valid_factor_count += 1

        if valid_factor_count == 0:
            print(f"âš ï¸  çª—å£{w['window_id']}: æ— æœ‰æ•ˆå› å­ï¼ˆå¯èƒ½è¢«excludeè¿‡æ»¤ï¼‰ï¼Œè·³è¿‡")
            continue

        # å¦‚æœç”±äºæƒé‡è¿‡æ»¤å¯¼è‡´æ‰€æœ‰æƒé‡æ— æ•ˆï¼Œå›é€€ç­‰æƒä¸€æ¬¡
        if valid_factor_count == 0 and len(selected_factors) > 0:
            eq_w = 1.0 / len(selected_factors)
            for factor_name in selected_factors:
                if factor_name not in factors_dict:
                    continue
                factor_df = factors_dict[factor_name]
                if oos_start >= len(factor_df):
                    continue
                factor_values = factor_df.iloc[oos_start].fillna(0.0)
                composite_scores += factor_values * eq_w
            valid_factor_count = len(selected_factors)

        # æŒ‰scoreæ’åºå–Top-Nï¼ˆç›®æ ‡æŒä»“ï¼‰
        desired_ranked = (
            composite_scores.sort_values(ascending=False).index[:top_n].tolist()
        )
        desired_set = set(desired_ranked)

        # åº”ç”¨æ¢æ‰‹çº¦æŸï¼šé™åˆ¶æ¯æœŸæ›´æ¢çš„æ ‡çš„æ•°é‡ï¼ˆå¯¹åº”æ€»æ¢æ‰‹ â‰¤ max_turnoverï¼‰
        if prev_holdings and max_turnover < 1.0:
            # å…è®¸æ›´æ¢çš„æ•°é‡ï¼ˆåŒå‘å¯¹ç§°å·®/Top-N = 2*k/top_n â‰¤ max_turnoverï¼‰â†’ k â‰¤ floor(max_turnover*top_n/2)
            k_allowed = int(np.floor(max_turnover * top_n / 2))
            # ä¿ç•™å‰ä¸€æœŸä¸”ä»åœ¨ç›®æ ‡listä¸­çš„æ ‡çš„ï¼ŒæŒ‰ç›®æ ‡å¾—åˆ†é¡ºåº
            keep = [etf for etf in desired_ranked if etf in prev_holdings]
            # éœ€è¦æ–°å¢çš„å€™é€‰ï¼ˆæŒ‰ç›®æ ‡å¾—åˆ†é¡ºåºï¼‰
            adds = [etf for etf in desired_ranked if etf not in prev_holdings]
            adds = adds[:k_allowed]  # é™åˆ¶æ–°å¢æ•°é‡
            # è‹¥ä¿ç•™+æ–°å¢ä¸è¶³ä»¥å¡«æ»¡Top-Nï¼Œç”¨ä¸Šä¸€æœŸæœªå…¥é€‰ä½†å¾—åˆ†è¾ƒé«˜çš„æŒä»“å¡«å……
            remain_slots = top_n - (len(keep) + len(adds))
            fillers_candidates = [
                etf for etf in prev_holdings if etf not in desired_set
            ]
            # ç”¨å½“å‰åˆ†æ•°å¯¹ä¸Šä¸€æœŸå€™é€‰æ’åºï¼ˆé«˜åˆ†ä¼˜å…ˆï¼‰
            fillers_candidates.sort(
                key=lambda x: composite_scores.get(x, -np.inf), reverse=True
            )
            fillers = fillers_candidates[: max(0, remain_slots)]
            current_ranked = keep + adds + fillers
            # è‹¥ä»ä¸è¶³ï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œè¡¥é½ä¸ºTop-N
            if len(current_ranked) < top_n:
                extra = [
                    etf for etf in desired_ranked if etf not in set(current_ranked)
                ]
                current_ranked += extra[: (top_n - len(current_ranked))]
        else:
            current_ranked = desired_ranked

        curr_holdings = set(current_ranked)

        # è®¡ç®—æ¢æ‰‹ç‡ï¼ˆå•å‘ï¼Œä½¿ç”¨é›†åˆå¯¹ç§°å·®å®šä¹‰/Top-Nï¼‰
        if prev_holdings:
            turnover = len(curr_holdings.symmetric_difference(prev_holdings)) / max(
                len(curr_holdings), len(prev_holdings)
            )
        else:
            turnover = 1.0  # é¦–æ¬¡å…¨å»ºä»“
        turnover_list.append(turnover)

        # è®¡ç®—æŒä»“æœŸæ”¶ç›Šï¼ˆç­‰æƒTop-Nï¼‰
        window_rets = returns.iloc[oos_start:oos_end][current_ranked]
        eq_ret = window_rets.mean(axis=1)  # ç­‰æƒç»„åˆæ—¥æ”¶ç›Šï¼ˆæ¯›æ”¶ç›Šï¼‰
        eq_ret_gross = eq_ret.copy()

        # äº¤æ˜“æˆæœ¬ï¼ˆä»…åœ¨æ¯ä¸ªçª—å£å¼€å§‹æ—¥æ‰£é™¤ï¼‰
        if tx_cost_bps and tx_cost_bps > 0:
            cost = turnover * (tx_cost_bps / 1e4)
            if len(eq_ret) > 0:
                eq_ret.iloc[0] = eq_ret.iloc[0] - cost

        gross_rets.extend(eq_ret_gross.tolist())
        net_rets.extend(eq_ret.tolist())

        prev_holdings = curr_holdings
        holdings_history.append(
            {
                "window": w["window_id"],
                "oos_start": oos_start,
                "oos_end": oos_end,
                "holdings": current_ranked[:5],  # è®°å½•å‰5ä¸ªç¤ºä¾‹
                "avg_score": float(composite_scores[current_ranked].mean()),
            }
        )

    return np.array(gross_rets), np.array(net_rets), turnover_list, holdings_history


def compute_metrics(portfolio_rets):
    """è®¡ç®—ç»„åˆæŒ‡æ ‡"""
    cum_rets = (1 + portfolio_rets).cumprod()
    total_ret = cum_rets[-1] - 1

    # æœ€å¤§å›æ’¤
    running_max = np.maximum.accumulate(cum_rets)
    drawdowns = (cum_rets - running_max) / running_max
    max_dd = drawdowns.min()

    # å¹´åŒ–æ”¶ç›Šä¸æ³¢åŠ¨
    n_days = len(portfolio_rets)
    ann_ret = (1 + total_ret) ** (252 / n_days) - 1 if n_days > 0 else 0.0
    ann_vol = portfolio_rets.std() * np.sqrt(252)
    sharpe = ann_ret / ann_vol if ann_vol > 0 else 0.0

    # èƒœç‡
    win_rate = (
        (portfolio_rets > 0).sum() / len(portfolio_rets)
        if len(portfolio_rets) > 0
        else 0.0
    )

    return {
        "total_return": float(total_ret),
        "annualized_return": float(ann_ret),
        "annualized_volatility": float(ann_vol),
        "sharpe_ratio": float(sharpe),
        "max_drawdown": float(max_dd),
        "win_rate": float(win_rate),
        "trading_days": int(n_days),
    }


def main(
    wfo_dir: Path,
    top_n: int = TOP_N_HOLDINGS,
    exclude_factors=None,
    weight_mode: str = "equal",
    tx_cost_bps: float = 0.0,
    max_turnover: float = 1.0,
    target_vol: float = 0.0,
) -> None:
    """æ‰©å±•WFOç»“æœè¡¥å……å›æµ‹æŒ‡æ ‡ï¼ˆTop-Nå› å­åŠ æƒç­–ç•¥ï¼‰"""
    print(f"ğŸ” å¤„ç†WFOç»“æœ: {wfo_dir}")
    print(
        f"ğŸ“Š ç­–ç•¥: Top-{top_n}å› å­åŠ æƒç»„åˆ | æ’é™¤å› å­: {','.join(exclude_factors) if exclude_factors else 'æ— '}"
    )

    # åŠ è½½ç»“æœ
    pkl_path = wfo_dir / "wfo_results.pkl"
    with open(pkl_path, "rb") as f:
        results = pickle.load(f)

    window_results = results.get("window_results", [])
    constraint_reports = results.get("constraint_reports", None)
    if not window_results:
        print("âš ï¸  æ— çª—å£ç»“æœï¼Œè·³è¿‡")
        return

    # åŠ è½½æ•°æ®
    close_df = load_ohlcv_data()
    factors_dict = load_standardized_factors()

    # è®¡ç®—Top-Nç»„åˆæ”¶ç›Šï¼ˆæ¯›/å‡€ï¼‰
    gross_rets, net_rets, turnover_list, holdings = compute_portfolio_returns_topn(
        close_df,
        window_results,
        factors_dict,
        top_n=top_n,
        exclude_factors=exclude_factors,
        weight_mode=weight_mode,
        constraint_reports=constraint_reports,
        tx_cost_bps=tx_cost_bps,
        max_turnover=max_turnover,
    )

    # è®¡ç®—æŒ‡æ ‡ï¼ˆæ¯›æ”¶ç›Šï¼‰
    gross_metrics = compute_metrics(gross_rets)

    # è®¡ç®—æŒ‡æ ‡ï¼ˆå‡€æ”¶ç›Šï¼šå«äº¤æ˜“æˆæœ¬ï¼‰
    net_before_vol_metrics = compute_metrics(net_rets)

    # ç›®æ ‡æ³¢åŠ¨ç‡ï¼ˆåœ¨å‡€æ”¶ç›ŠåŸºç¡€ä¸Šè¿›è¡Œç¼©æ”¾ï¼‰
    final_rets = net_rets.copy()
    if target_vol and target_vol > 0:
        realized_vol = final_rets.std() * np.sqrt(252)
        if realized_vol > 0:
            scale = target_vol / realized_vol
            final_rets = final_rets * scale
    final_metrics = compute_metrics(final_rets)
    avg_turnover = float(np.mean(turnover_list)) if turnover_list else 0.0

    # æ‰©å±•metadata
    metadata_path = wfo_dir / "metadata.json"
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    metadata.update(
        {
            "config_overrides": {
                "tx_cost_bps": float(tx_cost_bps),
                "max_turnover": float(max_turnover),
                "target_vol": float(target_vol),
            },
            "gross_backtest_metrics": {
                **gross_metrics,
                "avg_single_turnover": avg_turnover,
            },
            "net_backtest_metrics": {
                **net_before_vol_metrics,
                "avg_single_turnover": avg_turnover,
            },
            # æœ€ç»ˆæŒ‡æ ‡ï¼ˆå«æˆæœ¬+ç›®æ ‡æ³¢åŠ¨ï¼‰ä¿æŒå…¼å®¹ä¸º backtest_metrics
            "backtest_metrics": {
                **final_metrics,
                "avg_single_turnover": avg_turnover,
                "strategy": f"Top-{top_n}å› å­åŠ æƒ({weight_mode})",
                "holdings_count": top_n,
                "excluded_factors": list(exclude_factors or []),
            },
        }
    )

    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    # æ‰©å±•æŠ¥å‘Š
    report_path = wfo_dir / "wfo_report.txt"
    with open(report_path, "a", encoding="utf-8") as f:
        f.write("\n" + "=" * 80 + "\n")
        f.write(f"å›æµ‹ç»„åˆæŒ‡æ ‡ï¼ˆTop-{TOP_N_HOLDINGS}å› å­åŠ æƒç­–ç•¥ï¼‰\n")
        f.write("=" * 80 + "\n\n")
        # æ¯›æ”¶ç›Š
        f.write("[æ¯›æ”¶ç›Š]ï¼ˆä¸å«æˆæœ¬/ä¸å«æ³¢åŠ¨ç‡ç›®æ ‡ï¼‰\n")
        f.write(f"  æ€»æ”¶ç›Šç‡: {gross_metrics['total_return']*100:.2f}%\n")
        f.write(f"  å¹´åŒ–æ”¶ç›Š: {gross_metrics['annualized_return']*100:.2f}%\n")
        f.write(f"  å¹´åŒ–æ³¢åŠ¨: {gross_metrics['annualized_volatility']*100:.2f}%\n")
        f.write(f"  å¤æ™®æ¯”ç‡: {gross_metrics['sharpe_ratio']:.2f}\n")
        f.write(f"  æœ€å¤§å›æ’¤: {gross_metrics['max_drawdown']*100:.2f}%\n")
        f.write(f"  èƒœç‡: {gross_metrics['win_rate']*100:.2f}%\n")

        # å‡€æ”¶ç›Šï¼ˆå«æˆæœ¬ï¼Œæœªåº”ç”¨ç›®æ ‡æ³¢åŠ¨ï¼‰
        f.write("\n[å‡€æ”¶ç›Š]ï¼ˆå«æˆæœ¬/ä¸å«æ³¢åŠ¨ç‡ç›®æ ‡ï¼‰\n")
        f.write(f"  å¹´åŒ–æ”¶ç›Š: {net_before_vol_metrics['annualized_return']*100:.2f}%\n")
        f.write(
            f"  å¹´åŒ–æ³¢åŠ¨: {net_before_vol_metrics['annualized_volatility']*100:.2f}%\n"
        )
        f.write(f"  å¤æ™®æ¯”ç‡: {net_before_vol_metrics['sharpe_ratio']:.2f}\n")
        f.write(f"  æœ€å¤§å›æ’¤: {net_before_vol_metrics['max_drawdown']*100:.2f}%\n")
        f.write(f"  èƒœç‡: {net_before_vol_metrics['win_rate']*100:.2f}%\n")

        # æœ€ç»ˆï¼ˆå«æˆæœ¬+ç›®æ ‡æ³¢åŠ¨ï¼‰
        f.write("\n[æœ€ç»ˆ]ï¼ˆå«æˆæœ¬/å«æ³¢åŠ¨ç‡ç›®æ ‡ï¼‰\n")
        f.write(f"  å¹´åŒ–æ”¶ç›Š: {final_metrics['annualized_return']*100:.2f}%\n")
        f.write(f"  å¹´åŒ–æ³¢åŠ¨: {final_metrics['annualized_volatility']*100:.2f}%\n")
        f.write(f"  å¤æ™®æ¯”ç‡: {final_metrics['sharpe_ratio']:.2f}\n")
        f.write(f"  æœ€å¤§å›æ’¤: {final_metrics['max_drawdown']*100:.2f}%\n")
        f.write(f"  èƒœç‡: {final_metrics['win_rate']*100:.2f}%\n")

        # å…¶ä»–ä¿¡æ¯
        f.write("\n")
        f.write(f"å¹³å‡æ¢æ‰‹: {avg_turnover*100:.2f}%\n")
        f.write(f"äº¤æ˜“å¤©æ•°: {final_metrics['trading_days']}\n")
        f.write(f"æŒä»“æ•°é‡: {top_n}\n")
        if exclude_factors:
            f.write(f"æ’é™¤å› å­: {','.join(exclude_factors)}\n")
        f.write(
            f"é…ç½®: tx_cost_bps={tx_cost_bps}, max_turnover={max_turnover}, target_vol={target_vol}\n"
        )
        f.write("\n")

    print("âœ… æŒ‡æ ‡å·²è¡¥å…… (å«æ¯›/å‡€/æœ€ç»ˆ)ï¼š")
    print(
        f"   æ¯›æ”¶ç›Š-å¤æ™®: {gross_metrics['sharpe_ratio']:.2f} | å¹´åŒ–: {gross_metrics['annualized_return']*100:.2f}%"
    )
    print(
        f"   å‡€æ”¶ç›Š-å¤æ™®: {net_before_vol_metrics['sharpe_ratio']:.2f} | å¹´åŒ–: {net_before_vol_metrics['annualized_return']*100:.2f}%"
    )
    print(
        f"   æœ€ç»ˆ  -å¤æ™®: {final_metrics['sharpe_ratio']:.2f} | å¹´åŒ–: {final_metrics['annualized_return']*100:.2f}%"
    )
    print(
        f"   å¹³å‡æ¢æ‰‹: {avg_turnover*100:.2f}% | é…ç½®: tx_cost_bps={tx_cost_bps}, max_turnover={max_turnover}, target_vol={target_vol}"
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®¡ç®—WFOå›æµ‹æŒ‡æ ‡ï¼ˆTop-Nå› å­åŠ æƒï¼‰")
    parser.add_argument(
        "wfo_timestamp_dir",
        type=str,
        help="WFOç»“æœç›®å½•ï¼Œä¾‹å¦‚ results/wfo/20251028_151333",
    )
    # æ”¯æŒ --topn ä¸ --top-nï¼Œä¸¤è€…ç­‰ä»·ï¼›é»˜è®¤å€¼ä¸Step4ä¸€è‡´ä¸º5
    parser.add_argument(
        "--topn",
        type=int,
        default=TOP_N_HOLDINGS,
        help="Top-NæŒä»“æ•°é‡ï¼Œé»˜è®¤5ï¼ˆä¸Step4ä¸€è‡´ï¼‰",
    )
    parser.add_argument(
        "--top-n", dest="topn", type=int, help="Top-NæŒä»“æ•°é‡ï¼Œä¸ --topn ç­‰ä»·"
    )
    parser.add_argument(
        "--weight-mode",
        type=str,
        default="equal",
        choices=["equal", "ic"],
        help="å› å­åˆæˆæƒé‡ï¼šç­‰æƒæˆ–ICæƒé‡",
    )
    parser.add_argument(
        "--exclude-factors",
        type=str,
        default="",
        help="ä»¥é€—å·åˆ†éš”çš„å› å­ååˆ—è¡¨ï¼Œåœ¨ç»„åˆæ„å»ºæ—¶æ’é™¤ä¸å‚ä¸åˆæˆï¼Œä¾‹å¦‚ 'RSI_14,OBV_SLOPE_10D'",
    )
    parser.add_argument(
        "--tx-cost-bps",
        type=float,
        default=0.0,
        help="æ¯æ¬¡è°ƒä»“çš„å•å‘æˆæœ¬ï¼ˆbpï¼‰ï¼Œä¾‹å¦‚5 è¡¨ç¤º 5bpï¼›é»˜è®¤0=ä¸è®¡æˆæœ¬",
    )
    parser.add_argument(
        "--max-turnover",
        type=float,
        default=1.0,
        help="å•æœŸæ€»æ¢æ‰‹ä¸Šé™ï¼ˆ0~1ï¼‰ï¼Œé»˜è®¤1.0=ä¸é™åˆ¶ï¼›ä¾‹å¦‚0.5 è¡¨ç¤ºæ€»æ¢æ‰‹â‰¤50%",
    )
    parser.add_argument(
        "--target-vol",
        type=float,
        default=0.0,
        help="ç›®æ ‡å¹´åŒ–æ³¢åŠ¨ç‡ï¼ˆ0=å…³é—­ï¼‰ï¼Œä¾‹å¦‚0.10 è¡¨ç¤ºç›®æ ‡10%",
    )

    args = parser.parse_args()
    wfo_dir = Path(args.wfo_timestamp_dir)
    if not wfo_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {wfo_dir}")
        sys.exit(1)

    exclude = (
        [s.strip() for s in args.exclude_factors.split(",") if s.strip()]
        if args.exclude_factors
        else []
    )
    main(
        wfo_dir,
        top_n=args.topn,
        exclude_factors=exclude,
        weight_mode=args.weight_mode,
        tx_cost_bps=args.tx_cost_bps,
        max_turnover=args.max_turnover,
        target_vol=args.target_vol,
    )
