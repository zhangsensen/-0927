#!/usr/bin/env python3
"""
Step 2: å› å­ç­›é€‰ï¼ˆæ ‡å‡†åŒ–ï¼‰ - ç‹¬ç«‹æ‰§è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å– Step 1 çš„æ¨ªæˆªé¢æ•°æ®
2. æ‰§è¡Œå› å­æ ‡å‡†åŒ–ï¼ˆæˆªé¢æ ‡å‡†åŒ–ï¼Œä¿ç•™NaNï¼‰
3. ä¿å­˜æ ‡å‡†åŒ–å› å­åˆ° factor_selection/
4. è¯¦ç»†çš„ç»Ÿè®¡éªŒè¯æ—¥å¿—

è¾“å…¥ï¼š
- cross_section/{date}/{timestamp}/

è¾“å‡ºï¼š
- factor_selection/{date}/{timestamp}/standardized/*.parquet
- factor_selection/{date}/{timestamp}/metadata.json
"""

import json
import logging
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.precise_factor_library_v2 import PreciseFactorLibrary
from utils.factor_cache import FactorCache


def setup_logging(output_dir: Path):
    """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
    log_file = output_dir / "step2_factor_selection.log"

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s - %(message)s",
        datefmt="%H:%M:%S",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def find_latest_cross_section(results_dir: Path):
    """æŸ¥æ‰¾æœ€æ–°çš„æ¨ªæˆªé¢æ•°æ®ç›®å½•"""
    cross_section_root = results_dir / "cross_section"

    if not cross_section_root.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•
    all_runs = []
    for date_dir in cross_section_root.iterdir():
        if not date_dir.is_dir():
            continue
        for timestamp_dir in date_dir.iterdir():
            if not timestamp_dir.is_dir():
                continue
            # éªŒè¯æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
            if (timestamp_dir / "metadata.json").exists():
                all_runs.append(timestamp_dir)

    if not all_runs:
        return None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°
    all_runs.sort(key=lambda p: p.name, reverse=True)
    return all_runs[0]


def load_cross_section_data(cross_section_dir: Path, logger):
    """åŠ è½½æ¨ªæˆªé¢æ•°æ®"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 1/4: åŠ è½½æ¨ªæˆªé¢æ•°æ®")
    logger.info("-" * 80)
    logger.info(f"è¾“å…¥ç›®å½•: {cross_section_dir}")
    logger.info("")

    # åŠ è½½å…ƒæ•°æ®
    metadata_path = cross_section_dir / "metadata.json"
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    logger.info("ğŸ“‹ æ¨ªæˆªé¢å…ƒæ•°æ®:")
    logger.info(f"  æ—¶é—´æˆ³: {metadata['timestamp']}")
    logger.info(f"  ETFæ•°é‡: {metadata['etf_count']}")
    logger.info(
        f"  æ—¥æœŸèŒƒå›´: {metadata['date_range'][0]} -> {metadata['date_range'][1]}"
    )
    logger.info(f"  æ€»äº¤æ˜“æ—¥: {metadata['total_dates']}")
    logger.info(f"  å› å­æ•°é‡: {metadata['factor_count']}")
    logger.info("")

    # åŠ è½½OHLCV
    ohlcv_dir = cross_section_dir / "ohlcv"
    ohlcv_data = {}
    for col_name in ["open", "high", "low", "close", "volume"]:
        parquet_path = ohlcv_dir / f"{col_name}.parquet"
        ohlcv_data[col_name] = pd.read_parquet(parquet_path)
        logger.info(f"  âœ… {col_name}.parquet: {ohlcv_data[col_name].shape}")

    logger.info("")

    # åŠ è½½å› å­
    factors_dir = cross_section_dir / "factors"
    factors_dict = {}
    for factor_name in metadata["factor_names"]:
        parquet_path = factors_dir / f"{factor_name}.parquet"
        factor_df = pd.read_parquet(parquet_path)
        # å› å­æ–‡ä»¶æ˜¯å®½è¡¨æ ¼å¼ï¼ˆæ—¥æœŸÃ—æ ‡çš„ï¼‰ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªDataFrame
        factors_dict[factor_name] = factor_df

        nan_ratio = factor_df.isna().sum().sum() / factor_df.size
        logger.info(f"  âœ… {factor_name:25s} NaNç‡: {nan_ratio*100:.2f}%")

    logger.info("")

    return ohlcv_data, factors_dict, metadata


def standardize_factors(ohlcv_data, factors_dict, cache_dir, logger):
    """æ ‡å‡†åŒ–å› å­ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 2/4: å› å­æ ‡å‡†åŒ–ï¼ˆæˆªé¢æ ‡å‡†åŒ–ï¼Œä¿ç•™NaNï¼‰")
    logger.info("-" * 80)

    cache = FactorCache(cache_dir=cache_dir, use_timestamp=True)
    lib = PreciseFactorLibrary()

    # å°è¯•åŠ è½½ç¼“å­˜
    cached_standardized = cache.load_factors(
        ohlcv=ohlcv_data, lib_class=lib.__class__, stage="standardized"
    )

    if cached_standardized is not None:
        logger.info("âœ… ä½¿ç”¨æ ‡å‡†åŒ–å› å­ç¼“å­˜")
        standardized_dict = cached_standardized
    else:
        logger.info("ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹æ ‡å‡†åŒ–...")
        import time

        start_time = time.time()

        standardized_dict = {}
        for factor_name, factor_df in factors_dict.items():
            # æˆªé¢æ ‡å‡†åŒ–ï¼ˆæŒ‰è¡Œ/æ—¥æœŸæ ‡å‡†åŒ–ï¼‰
            # factor_dfæ˜¯DataFrameï¼ˆæ—¥æœŸÃ—æ ‡çš„ï¼‰ï¼Œå¯¹æ¯ä¸€è¡Œè¿›è¡Œæ ‡å‡†åŒ–
            standardized = factor_df.apply(
                lambda row: (row - row.mean()) / row.std(), axis=1
            )
            standardized_dict[factor_name] = standardized

        elapsed = time.time() - start_time
        logger.info(f"âœ… æ ‡å‡†åŒ–å®Œæˆï¼ˆè€—æ—¶ {elapsed:.1f}ç§’ï¼‰")

        # ä¿å­˜ç¼“å­˜
        cache.save_factors(
            factors=standardized_dict,
            ohlcv=ohlcv_data,
            lib_class=lib.__class__,
            stage="standardized",
        )
        logger.info(f"ğŸ’¾ æ ‡å‡†åŒ–å› å­ç¼“å­˜å·²ä¿å­˜")

    logger.info("")

    # ç»Ÿè®¡éªŒè¯
    logger.info("ğŸ“Š æ ‡å‡†åŒ–éªŒè¯ï¼ˆæ¯ä¸ªå› å­çš„æˆªé¢ç»Ÿè®¡ï¼‰:")
    for factor_name, factor_df in standardized_dict.items():
        # è®¡ç®—æˆªé¢å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæ¯ä¸€è¡Œï¼‰
        row_means = factor_df.mean(axis=1)  # æ¯ä¸ªæ—¥æœŸçš„å‡å€¼
        row_stds = factor_df.std(axis=1)  # æ¯ä¸ªæ—¥æœŸçš„æ ‡å‡†å·®
        cross_sectional_mean = row_means.mean()
        cross_sectional_std = row_stds.mean()
        nan_ratio = factor_df.isna().sum().sum() / factor_df.size

        logger.info(
            f"  {factor_name:25s} "
            f"å‡å€¼={cross_sectional_mean:7.4f}  "
            f"æ ‡å‡†å·®={cross_sectional_std:7.4f}  "
            f"NaNç‡={nan_ratio*100:6.2f}%"
        )

    logger.info("")

    return standardized_dict


def save_standardized_factors(standardized_dict, output_dir, metadata, logger):
    """ä¿å­˜æ ‡å‡†åŒ–å› å­"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 3/4: ä¿å­˜æ ‡å‡†åŒ–å› å­")
    logger.info("-" * 80)

    standardized_dir = output_dir / "standardized"
    standardized_dir.mkdir(parents=True, exist_ok=True)

    for fname, fdata in standardized_dict.items():
        output_path = standardized_dir / f"{fname}.parquet"
        # fdataå·²ç»æ˜¯DataFrameï¼Œç›´æ¥ä¿å­˜
        fdata.to_parquet(output_path)
        logger.info(f"  âœ… {fname}.parquet")

    logger.info("")

    # ä¿å­˜å…ƒæ•°æ®
    selection_metadata = {
        **metadata,
        "step": "factor_selection",
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "standardized_factor_count": len(standardized_dict),
        "standardized_factor_names": list(standardized_dict.keys()),
        "output_dir": str(output_dir),
    }

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(selection_metadata, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    logger.info("")

    return selection_metadata


def main(cross_section_dir: Path = None):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_date = timestamp[:8]

    # è¾“å‡ºç›®å½•
    output_root = PROJECT_ROOT / "results"
    selection_dir = output_root / "factor_selection" / run_date / timestamp
    selection_dir.mkdir(parents=True, exist_ok=True)

    logger = setup_logging(selection_dir)

    logger.info("=" * 80)
    logger.info("Step 2: å› å­ç­›é€‰ï¼ˆæ ‡å‡†åŒ–å¤„ç†ï¼‰")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {selection_dir}")
    logger.info(f"æ—¶é—´æˆ³: {timestamp}")
    logger.info("")

    # æŸ¥æ‰¾è¾“å…¥æ•°æ®
    if cross_section_dir is None:
        logger.info("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ¨ªæˆªé¢æ•°æ®...")
        cross_section_dir = find_latest_cross_section(output_root)

        if cross_section_dir is None:
            logger.error("âŒ æœªæ‰¾åˆ°æ¨ªæˆªé¢æ•°æ®ï¼è¯·å…ˆè¿è¡Œ step1_cross_section.py")
            sys.exit(1)

        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–°æ•°æ®: {cross_section_dir}")
        logger.info("")

    # 1. åŠ è½½æ•°æ®
    ohlcv_data, factors_dict, metadata = load_cross_section_data(
        cross_section_dir, logger
    )

    # 2. æ ‡å‡†åŒ–
    cache_dir = PROJECT_ROOT / "cache" / "factors"
    cache_dir.mkdir(parents=True, exist_ok=True)
    standardized_dict = standardize_factors(ohlcv_data, factors_dict, cache_dir, logger)

    # 3. ä¿å­˜
    selection_metadata = save_standardized_factors(
        standardized_dict, selection_dir, metadata, logger
    )

    # å®Œæˆ
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 4/4: å®Œæˆæ‘˜è¦")
    logger.info("-" * 80)
    logger.info("=" * 80)
    logger.info("âœ… Step 2 å®Œæˆï¼å› å­å·²æ ‡å‡†åŒ–")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {selection_dir}")
    logger.info(f"  - standardized/: {len(standardized_dict)} ä¸ªæ–‡ä»¶")
    logger.info(f"  - metadata.json")
    logger.info("")
    logger.info("ğŸ”œ ä¸‹ä¸€æ­¥: è¿è¡Œ step3_run_wfo.py è¿›è¡ŒWFOä¼˜åŒ–")
    logger.info("")

    return selection_dir


if __name__ == "__main__":
    main()
