#!/usr/bin/env python3
"""
å¤šæ—¶é—´æ¡†æ¶å› å­VectorBTæ£€æµ‹å™¨ - Linusé£æ ¼è®¾è®¡
åŸºäºVectorBTçš„å¤šæ—¶é—´æ¡†æ¶å› å­è®¡ç®—ç³»ç»Ÿ
æ”¯æŒ5ä¸ªæ—¶é—´æ¡†æ¶ï¼š5min, 15min, 30min, 60min, daily
"""

import pandas as pd
import numpy as np
import vectorbt as vbt
from pathlib import Path
import logging
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass
from enum import Enum
import sys
import os
import time
import argparse
from datetime import datetime
import warnings

try:
    import pyarrow.parquet as pq
except ImportError:  # pragma: no cover - optional dependency fallback
    pq = None

warnings.filterwarnings('ignore')

# é…ç½®ç®¡ç†
from config import get_config, setup_logging

# å¯¼å…¥154æŒ‡æ ‡å¼•æ“
from enhanced_factor_calculator import EnhancedFactorCalculator, IndicatorConfig


logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())


def initialize_logging(timestamp: Optional[str] = None) -> Tuple[str, str]:
    """Configure logging for the current run and return metadata."""

    resolved_timestamp = timestamp or datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = setup_logging(resolved_timestamp)

    logger.info("=== æ–°çš„æ‰§è¡Œä¼šè¯å¼€å§‹ ===")
    logger.info(f"æ—¶é—´æˆ³: {resolved_timestamp}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file_path}")

    return resolved_timestamp, log_file_path

class TimeFrame(Enum):
    """æ—¶é—´æ¡†æ¶æšä¸¾"""
    MIN_5 = "5min"
    MIN_15 = "15min"
    MIN_30 = "30min"
    MIN_60 = "60min"
    DAILY = "daily"

class ScreenOperator(Enum):
    """ç­›é€‰æ“ä½œç¬¦"""
    GREATER_THAN = ">"
    LESS_THAN = "<"
    BETWEEN = "BETWEEN"
    TOP_N = "TOP_N"
    BOTTOM_N = "BOTTOM_N"

@dataclass
class ScreenCriteria:
    """ç­›é€‰æ¡ä»¶"""
    factor_name: str
    operator: ScreenOperator
    threshold: float
    weight: float = 1.0

@dataclass
class StrategyResult:
    """ç­–ç•¥ç»“æœ"""
    name: str
    selected_stocks: List[str]
    scores: pd.Series
    criteria_count: int
    backtest_result: Optional[Dict] = None

class MultiTimeframeFactorStore:
    """å¤šæ—¶é—´æ¡†æ¶å› å­å­˜å‚¨ç±» - åˆ†ç¦»å­˜å‚¨ç­–ç•¥ï¼ŒLinusé£æ ¼è®¾è®¡"""

    def __init__(self, data_root: str = None, symbol: str = None):
        """åˆå§‹åŒ–å› å­å­˜å‚¨"""
        if data_root is None:
            config = get_config()
            data_root = config.get_output_dir()

        self.data_root = Path(data_root)
        self.symbol = symbol
        self.timeframe_data = {}
        self.timeframe_files: Dict[str, Path] = {}
        self.factor_names = {}

        logger.info(f"åˆå§‹åŒ–åˆ†ç¦»å­˜å‚¨å› å­æ•°æ®è®¿é—®å™¨")
        logger.info(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")

        # å¦‚æœæŒ‡å®šäº†symbolï¼Œè‡ªåŠ¨åŠ è½½æ‰€æœ‰æ—¶é—´æ¡†æ¶æ•°æ®
        if symbol:
            self.load_symbol_data(symbol)

    def load_symbol_data(self, symbol: str) -> None:
        """åŠ è½½æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰æ—¶é—´æ¡†æ¶æ•°æ®"""
        self.symbol = symbol
        self.timeframe_data = {}
        self.timeframe_files = {}
        self.factor_names = {}

        logger.info(f"åŠ è½½è‚¡ç¥¨ {symbol} çš„å¤šæ—¶é—´æ¡†æ¶å› å­æ•°æ®")

        for timeframe in TimeFrame:
            timeframe_dir = self.data_root / timeframe.value
            if timeframe_dir.exists():
                # æŸ¥æ‰¾æœ€æ–°çš„å› å­æ–‡ä»¶
                pattern = f"{symbol}_{timeframe.value}_factors_*.parquet"
                files = list(timeframe_dir.glob(pattern))

                if files:
                    # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
                    latest_file = max(files, key=lambda x: x.stat().st_mtime)
                    self.timeframe_files[timeframe.value] = latest_file

                    columns, row_count = self._inspect_parquet(latest_file)
                    if columns:
                        self.factor_names[timeframe.value] = columns
                    else:
                        self.factor_names[timeframe.value] = []

                    if row_count is not None:
                        logger.info(f"  {timeframe.value}: {row_count} è¡Œ, {len(self.factor_names[timeframe.value])} ä¸ªå› å­")
                    else:
                        logger.info(f"  {timeframe.value}: æœªèƒ½è¯»å–è¡Œæ•°, {len(self.factor_names[timeframe.value])} ä¸ªå› å­")
                else:
                    logger.warning(f"æœªæ‰¾åˆ° {timeframe.value} å› å­æ–‡ä»¶: {pattern}")
            else:
                logger.warning(f"æ—¶é—´æ¡†æ¶ç›®å½•ä¸å­˜åœ¨: {timeframe_dir}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(self.timeframe_files)} ä¸ªæ—¶é—´æ¡†æ¶æ•°æ®")

    def _inspect_parquet(self, file_path: Path) -> Tuple[List[str], Optional[int]]:
        """è¯»å–Parquetæ–‡ä»¶çš„åˆ—åå’Œè¡Œæ•°ä¿¡æ¯ã€‚"""

        columns: List[str] = []
        row_count: Optional[int] = None

        if pq is not None:
            try:
                parquet_file = pq.ParquetFile(file_path)
                columns = [name for name in parquet_file.schema.names if name != "__index_level_0__"]
                if parquet_file.metadata is not None:
                    row_count = parquet_file.metadata.num_rows
            except Exception as exc:
                logger.debug(f"è¯»å–Parquetå…ƒä¿¡æ¯å¤±è´¥: {exc}")

        return columns, row_count

    def _ensure_timeframe_loaded(self, timeframe: str) -> Optional[pd.DataFrame]:
        """ç¡®ä¿æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æ•°æ®å·²åŠ è½½ã€‚"""

        if timeframe in self.timeframe_data:
            return self.timeframe_data[timeframe]

        file_path = self.timeframe_files.get(timeframe)
        if file_path is None:
            logger.warning(f"æ—¶é—´æ¡†æ¶ {timeframe} æ— å¯ç”¨æ•°æ®")
            return None

        try:
            data = pd.read_parquet(file_path, memory_map=True)
        except Exception:
            data = pd.read_parquet(file_path)

        self.timeframe_data[timeframe] = data
        if not self.factor_names.get(timeframe):
            self.factor_names[timeframe] = [col for col in data.columns if col != "__index_level_0__"]

        return data

    def get_available_timeframes(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ—¶é—´æ¡†æ¶"""
        return list(self.timeframe_files.keys())

    def get_factor_names_by_timeframe(self, timeframe: str) -> List[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å› å­åç§°"""
        if timeframe not in self.timeframe_files:
            return []

        if self.factor_names.get(timeframe):
            return self.factor_names[timeframe]

        file_path = self.timeframe_files.get(timeframe)
        if file_path is None:
            return []

        columns, _ = self._inspect_parquet(file_path)
        if columns:
            self.factor_names[timeframe] = columns
            return columns

        data = self._ensure_timeframe_loaded(timeframe)
        if data is not None:
            return self.factor_names.get(timeframe, [])

        return []

    def get_all_factor_names(self) -> Dict[str, List[str]]:
        """è·å–æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„å› å­åç§°"""
        return {tf: self.get_factor_names_by_timeframe(tf) for tf in self.get_available_timeframes()}

    def get_factors_by_timeframe(self, timeframe: str, factors: List[str] = None) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å› å­æ•°æ®"""
        data = self._ensure_timeframe_loaded(timeframe)
        if data is None:
            return pd.DataFrame()

        if factors:
            # æŒ‡å®šå› å­
            available_factors = self.get_factor_names_by_timeframe(timeframe)
            selected_factors = [f for f in factors if f in available_factors]
            missing_factors = set(factors) - set(selected_factors)

            if missing_factors:
                logger.warning(f"ç¼ºå¤±å› å­ {timeframe}: {missing_factors}")

            if selected_factors:
                return data[selected_factors].copy()
            else:
                return pd.DataFrame(index=data.index)
        else:
            # æ‰€æœ‰å› å­
            return data.copy()

    def get_factor_by_timeframe(self, timeframe: str, factor_name: str) -> pd.Series:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å•ä¸ªå› å­"""
        data = self._ensure_timeframe_loaded(timeframe)
        if data is None:
            return pd.Series(dtype=float)
        if factor_name in data.columns:
            return data[factor_name].copy()
        else:
            logger.warning(f"å› å­ä¸å­˜åœ¨ {timeframe}.{factor_name}")
            return pd.Series(dtype=float)

    
    def get_time_range_by_timeframe(self, timeframe: str,
                                   start_time: pd.Timestamp,
                                   end_time: pd.Timestamp) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶å’Œæ—¶é—´èŒƒå›´å†…çš„æ•°æ®"""
        data = self._ensure_timeframe_loaded(timeframe)
        if data is None:
            return pd.DataFrame()
        mask = (data.index >= start_time) & (data.index <= end_time)
        return data[mask]

    def get_latest_signals_by_timeframe(self, timeframe: str,
                                       lookback_periods: int = 1) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¶é—´æ¡†æ¶çš„æœ€æ–°ä¿¡å·"""
        data = self._ensure_timeframe_loaded(timeframe)
        if data is None:
            return pd.DataFrame()

        return data.tail(lookback_periods)

    def get_data_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯"""
        summary = {
            "symbol": self.symbol,
            "available_timeframes": self.get_available_timeframes(),
            "timeframe_info": {}
        }

        for tf in self.get_available_timeframes():
            data = self._ensure_timeframe_loaded(tf)
            if data is None:
                continue

            summary["timeframe_info"][tf] = {
                "rows": len(data),
                "columns": len(data.columns),
                "time_range": {
                    "start": str(data.index.min()),
                    "end": str(data.index.max())
                },
                "factors": self.factor_names.get(tf, [])
            }

        return summary

    
class MultiTimeframeVBTDetector:
    """
    å¤šæ—¶é—´æ¡†æ¶VectorBTæ£€æµ‹å™¨
    é›†æˆVectorBTè¿›è¡Œä¸“ä¸šçš„å¤šæ—¶é—´æ¡†æ¶å› å­åˆ†æ
    """

    def __init__(self, data_root: str = None):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        # å¦‚æœæ²¡æœ‰ä¼ å…¥data_rootï¼Œä»é…ç½®è¯»å–
        if data_root is None:
            config = get_config()
            data_root = config.get_data_root()

        self.data_root = Path(data_root)
        logger.info(f"å¤šæ—¶é—´æ¡†æ¶VBTæ£€æµ‹å™¨åˆå§‹åŒ–ï¼Œæ•°æ®æ ¹ç›®å½•: {self.data_root}")

        # æ—¶é—´æ¡†æ¶æšä¸¾
        self.timeframes = [TimeFrame.MIN_5, TimeFrame.MIN_15,
                          TimeFrame.MIN_30, TimeFrame.MIN_60, TimeFrame.DAILY]

        # åˆå§‹åŒ–154æŒ‡æ ‡å¼•æ“
        self.init_enhanced_calculator()

    def init_enhanced_calculator(self):
        """åˆå§‹åŒ–154æŒ‡æ ‡è®¡ç®—å¼•æ“"""
        try:
            # ä»é…ç½®è·å–æŒ‡æ ‡è®¾ç½®
            config = get_config()
            indicator_config = config.get_indicator_config()

            # åˆ›å»ºIndicatorConfig
            self.indicator_config = IndicatorConfig(
                enable_ma=indicator_config.get("enable_ma", True),
                enable_ema=indicator_config.get("enable_ema", True),
                enable_macd=indicator_config.get("enable_macd", True),
                enable_rsi=indicator_config.get("enable_rsi", True),
                enable_bbands=indicator_config.get("enable_bbands", True),
                enable_stoch=indicator_config.get("enable_stoch", True),
                enable_atr=indicator_config.get("enable_atr", True),
                enable_obv=indicator_config.get("enable_obv", True),
                enable_mstd=indicator_config.get("enable_mstd", True),
                enable_manual_indicators=indicator_config.get("enable_manual_indicators", True),
                enable_all_periods=indicator_config.get("enable_all_periods", False),
                memory_efficient=indicator_config.get("memory_efficient", True)
            )

            # åˆ›å»ºå¢å¼ºè®¡ç®—å™¨
            self.calculator = EnhancedFactorCalculator(self.indicator_config)
            logger.info("âœ… 154æŒ‡æ ‡å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"å¼•æ“é…ç½®: MA={self.indicator_config.enable_ma}, MACD={self.indicator_config.enable_macd}, RSI={self.indicator_config.enable_rsi}")

        except Exception as e:
            logger.error(f"âŒ 154æŒ‡æ ‡å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            # é™çº§ä¸ºåŸºæœ¬è®¡ç®—å™¨
            self.indicator_config = IndicatorConfig(enable_all_periods=False, memory_efficient=True)
            self.calculator = EnhancedFactorCalculator(self.indicator_config)
            logger.info("ä½¿ç”¨é™çº§é…ç½®åˆå§‹åŒ–å¼•æ“")

        # ç®€åŒ–ï¼šä¸å†ç¡¬ç¼–ç æ–‡ä»¶æ¨¡å¼ï¼Œç›´æ¥æ‰«æç›®å½•

    def load_multi_timeframe_data(self, symbol: str) -> Optional[Dict[TimeFrame, pd.DataFrame]]:
        """åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ•°æ®"""
        logger.info(f"\n{'='*60}")
        logger.info(f"æ­¥éª¤1: åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ•°æ® - {symbol}")
        logger.info(f"{'='*60}")

        timeframe_data = {}

        # ç®€åŒ–ï¼šç›´æ¥æ‰«æç›®å½•ä¸­çš„æ–‡ä»¶
        symbol_patterns = [symbol, f"{symbol}HK", symbol.replace('.', '')]

        for timeframe in self.timeframes:
            try:
                logger.info(f"åŠ è½½ {timeframe.value} æ•°æ®...")

                # å®šä¹‰æ—¶é—´æ¡†æ¶æ ‡è¯†ç¬¦
                timeframe_map = {
                    TimeFrame.MIN_5: '5min',
                    TimeFrame.MIN_15: '15m',
                    TimeFrame.MIN_30: '30m',
                    TimeFrame.MIN_60: '60m',
                    TimeFrame.DAILY: '1day'
                }
                timeframe_id = timeframe_map[timeframe]

                # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
                found_file = None
                for pattern in symbol_patterns:
                    # ç®€å•çš„æ–‡ä»¶ååŒ¹é…
                    potential_files = list(self.data_root.glob(f"{pattern}*{timeframe_id}*.parquet"))
                    if potential_files:
                        found_file = potential_files[0]
                        break

                if not found_file:
                    logger.warning(f"æœªæ‰¾åˆ° {timeframe.value} æ•°æ®æ–‡ä»¶")
                    continue

                df = pd.read_parquet(found_file)

                # æ•°æ®é¢„å¤„ç†
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df.set_index('timestamp', inplace=True)
                df.sort_index(inplace=True)

                logger.info(f"{timeframe.value} æ•°æ®: {len(df)} è¡Œ (æ–‡ä»¶: {found_file.name})")
                timeframe_data[timeframe] = df

            except Exception as e:
                logger.error(f"åŠ è½½ {timeframe.value} æ•°æ®å¤±è´¥: {e}")
                continue

        if not timeframe_data:
            logger.error("æœªèƒ½åŠ è½½ä»»ä½•æ—¶é—´æ¡†æ¶æ•°æ®")
            return None

        logger.info(f"æˆåŠŸåŠ è½½ {len(timeframe_data)} ä¸ªæ—¶é—´æ¡†æ¶æ•°æ®")
        return timeframe_data

    def resample_to_target_timeframe(self, df: pd.DataFrame,
                                   target_timeframe: TimeFrame) -> pd.DataFrame:
        """é‡é‡‡æ ·æ•°æ®åˆ°ç›®æ ‡æ—¶é—´æ¡†æ¶"""
        logger.info(f"é‡é‡‡æ ·åˆ° {target_timeframe.value}")

        if target_timeframe == TimeFrame.MIN_5:
            # 5åˆ†é’Ÿæ•°æ®ä¸éœ€è¦é‡é‡‡æ ·
            return df

        # é‡é‡‡æ ·è§„åˆ™
        resample_rules = {
            TimeFrame.MIN_15: '15min',
            TimeFrame.MIN_30: '30min',
            TimeFrame.MIN_60: '1H',
            TimeFrame.DAILY: '1D'
        }

        rule = resample_rules[target_timeframe]

        # OHLCVé‡é‡‡æ ·
        resampled = df.resample(rule).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        })

        # æ¸…ç†ç©ºå€¼
        resampled = resampled.dropna()

        logger.info(f"é‡é‡‡æ ·ç»“æœ: {len(df)} -> {len(resampled)} è¡Œ")
        return resampled

    def calculate_timeframe_factors(self, df: pd.DataFrame,
                                  timeframe: TimeFrame) -> Optional[pd.DataFrame]:
        """è®¡ç®—æŒ‡å®šæ—¶é—´æ¡†æ¶çš„å› å­ - ä½¿ç”¨154æŒ‡æ ‡å¼•æ“"""
        logger.info(f"è®¡ç®— {timeframe.value} æ—¶é—´æ¡†æ¶å› å­ (154æŒ‡æ ‡å¼•æ“)...")

        try:
            start_time = time.time()
            logger.info(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {df.shape}")

            # ä½¿ç”¨154æŒ‡æ ‡å¼•æ“è¿›è¡Œå› å­è®¡ç®—
            factors_df = self.calculator.calculate_comprehensive_factors(df, timeframe)

            if factors_df is None:
                logger.error(f"âŒ 154æŒ‡æ ‡å¼•æ“è®¡ç®—å¤±è´¥: {timeframe.value}")
                return None

            logger.info(f"åŸå§‹å¼•æ“ç»“æœå½¢çŠ¶: {factors_df.shape}")
            logger.info(f"å› å­åˆ—æ•°: {len(factors_df.columns)}")

            # æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´å®Œæ•´æ€§
            original_start = df.index.min()
            engine_start = factors_df.index.min()

            logger.info(f"åŸå§‹æ•°æ®å¼€å§‹æ—¶é—´: {original_start}")
            logger.info(f"å¼•æ“æ•°æ®å¼€å§‹æ—¶é—´: {engine_start}")

            if engine_start > original_start:
                logger.warning(f"âš ï¸ æ•°æ®ä¸¢å¤±è­¦å‘Š: åŸå§‹æ•°æ®ä» {original_start} å¼€å§‹ï¼Œä½†å¼•æ“ç»“æœä» {engine_start} å¼€å§‹")
                lost_rows = len(df) - len(factors_df)
                logger.warning(f"âš ï¸ ä¸¢å¤±äº† {lost_rows} è¡Œæ•°æ®")

            # åº”ç”¨æ•°æ®æ¸…ç†ä¿®å¤é€»è¾‘ - ç¡®ä¿ä¸ä¸¢å¤±åŸå§‹æ•°æ®
            factors_df = self._apply_data_cleaning_fix(df, factors_df, timeframe)

            calc_time = time.time() - start_time
            logger.info(f"âœ… {timeframe.value} 154æŒ‡æ ‡å› å­è®¡ç®—å®Œæˆ:")
            logger.info(f"  - æœ€ç»ˆå› å­æ•°é‡: {len(factors_df.columns)} ä¸ª")
            logger.info(f"  - æœ€ç»ˆæ•°æ®ç‚¹æ•°: {len(factors_df)} è¡Œ")
            logger.info(f"  - è®¡ç®—è€—æ—¶: {calc_time:.3f}ç§’")
            logger.info(f"  - æ•°æ®èŒƒå›´: {factors_df.index.min()} åˆ° {factors_df.index.max()}")

            return factors_df

        except Exception as e:
            logger.error(f"âŒ {timeframe.value} 154æŒ‡æ ‡å› å­è®¡ç®—å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _apply_data_cleaning_fix(self, original_df: pd.DataFrame,
                               factors_df: pd.DataFrame,
                               timeframe: TimeFrame) -> pd.DataFrame:
        """åº”ç”¨æ•°æ®æ¸…ç†ä¿®å¤ - ç¡®ä¿ä¸ä¸¢å¤±åŸå§‹æ•°æ®æ—¶é—´ç‚¹"""

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if len(factors_df) == len(original_df):
            logger.info("âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡: æ— æ•°æ®ä¸¢å¤±")
            return factors_df

        logger.warning("âš ï¸ æ£€æµ‹åˆ°æ•°æ®ä¸¢å¤±ï¼Œåº”ç”¨ä¿®å¤é€»è¾‘...")

        # é‡å»ºå®Œæ•´çš„æ—¶é—´ç´¢å¼•
        factors_reindexed = factors_df.reindex(original_df.index)

        # ç»Ÿè®¡ä¿®å¤æ•ˆæœ
        original_rows = len(original_df)
        engine_rows = len(factors_df)
        repaired_rows = len(factors_reindexed)

        logger.info(f"æ•°æ®ä¿®å¤ç»Ÿè®¡:")
        logger.info(f"  - åŸå§‹æ•°æ®è¡Œæ•°: {original_rows}")
        logger.info(f"  - å¼•æ“è¾“å‡ºè¡Œæ•°: {engine_rows}")
        logger.info(f"  - ä¿®å¤åè¡Œæ•°: {repaired_rows}")

        if repaired_rows == original_rows:
            logger.info("âœ… æ•°æ®ä¿®å¤æˆåŠŸ: å·²æ¢å¤æ‰€æœ‰åŸå§‹æ•°æ®æ—¶é—´ç‚¹")

            # å¯¹æŠ€æœ¯æŒ‡æ ‡è¿›è¡Œæ™ºèƒ½å‰å‘å¡«å……ï¼ˆä¿æŒLinusé£æ ¼ï¼‰
            # åªåœ¨æŒ‡æ ‡æœ‰æ•ˆåè¿›è¡Œå‰å‘å¡«å……ï¼Œä¸ç ´ååˆå§‹çš„NaNçŠ¶æ€
            for col in factors_reindexed.columns:
                # è·³è¿‡åŸå§‹OHLCVæ•°æ®
                if col in ['open', 'high', 'low', 'close', 'volume']:
                    continue

                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼çš„ä½ç½®
                first_valid_idx = factors_reindexed[col].first_valid_index()
                if first_valid_idx is not None:
                    # ä»ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼å¼€å§‹ï¼Œå¯¹åç»­çš„NaNè¿›è¡Œå‰å‘å¡«å……
                    factors_reindexed.loc[first_valid_idx:, col] = factors_reindexed.loc[first_valid_idx:, col].ffill()

            return factors_reindexed
        else:
            logger.error(f"âŒ æ•°æ®ä¿®å¤å¤±è´¥: ä¿®å¤åä»ç„¶ç¼ºå°‘ {original_rows - repaired_rows} è¡Œ")
            return factors_df  # è¿”å›å¼•æ“åŸå§‹ç»“æœ

  
    def save_timeframe_factors_separately(self, timeframe_factors: Dict[TimeFrame, pd.DataFrame],
                                         symbol: str) -> Dict[str, str]:
        """åˆ†ç¦»ä¿å­˜å„æ—¶é—´æ¡†æ¶å› å­æ•°æ®ï¼Œå‡å°‘å†—ä½™"""
        try:
            # ä»é…ç½®è·å–è¾“å‡ºç›®å½•
            config = get_config()
            base_output_dir = Path(config.get_output_dir())
            base_output_dir.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨ç®€æ´çš„æ—¶é—´æˆ³æ ¼å¼
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            saved_files = {}

            logger.info("åˆ†ç¦»ä¿å­˜å„æ—¶é—´æ¡†æ¶å› å­æ•°æ®...")

            for timeframe, factors_df in timeframe_factors.items():
                # åˆ›å»ºæ—¶é—´æ¡†æ¶å­ç›®å½•
                timeframe_dir = base_output_dir / timeframe.value
                timeframe_dir.mkdir(parents=True, exist_ok=True)

                # ç”Ÿæˆæ–‡ä»¶å - ä½¿ç”¨ç´§å‡‘æ ¼å¼
                filename = f"{symbol}_{timeframe.value}_factors_{timestamp}.parquet"
                output_file = timeframe_dir / filename

                # ä¿å­˜è¯¥æ—¶é—´æ¡†æ¶çš„å› å­æ•°æ®
                try:
                    factors_df.to_parquet(
                        output_file,
                        engine='pyarrow',
                        compression='snappy',
                        index=True
                    )
                    logger.debug(f"æˆåŠŸä¿å­˜å› å­æ•°æ®åˆ°: {output_file}")
                except Exception as save_error:
                    logger.warning(f"ç›´æ¥ä¿å­˜å¤±è´¥ï¼Œå°è¯•æ¸…ç†VectorBTå¯¹è±¡åˆ—: {save_error}")

                    # æ¸…ç†æ— æ³•åºåˆ—åŒ–çš„VectorBTå¯¹è±¡åˆ—
                    columns_to_drop = []
                    vectorbt_columns_found = 0

                    for col in factors_df.columns:

                        if len(factors_df) > 0:
                            sample_data = factors_df[col].iloc[0]
                            if sample_data is not None and hasattr(sample_data, '__class__'):
                                class_str = str(sample_data.__class__)
                                if ('vectorbt' in class_str and
                                    ('labels.generators' in class_str or 'indicators.factory' in class_str or
                                     'talib.' in class_str or 'indicator_wrapper' in class_str)):
                                    columns_to_drop.append(col)
                                    vectorbt_columns_found += 1
                                    logger.info(f"æ ‡è®°åˆ é™¤VectorBTå¯¹è±¡åˆ—: {col} ({class_str})")

                    if columns_to_drop:
                        logger.warning(f"å‘ç° {vectorbt_columns_found} ä¸ªVectorBTå¯¹è±¡åˆ—æ— æ³•åºåˆ—åŒ–")
                        factors_df_cleaned = factors_df.drop(columns=columns_to_drop)
                        factors_df_cleaned.to_parquet(
                            output_file,
                            engine='pyarrow',
                            compression='snappy',
                            index=True
                        )
                        logger.info(f"æ¸…ç†åæˆåŠŸä¿å­˜ï¼Œåˆ é™¤äº† {len(columns_to_drop)} ä¸ªVectorBTå¯¹è±¡åˆ—")

                        # è®°å½•è¢«åˆ é™¤çš„åˆ—ä»¥ä¾¿åç»­åˆ†æ
                        if vectorbt_columns_found > 0:
                            logger.warning(f"å»ºè®®æ£€æŸ¥enhanced_factor_calculator.pyä¸­ä»¥ä¸‹æŒ‡æ ‡çš„extract_vbt_indicator/extract_vbt_labelsåº”ç”¨: {', '.join(columns_to_drop)}")
                    else:
                        logger.error("æœªæ‰¾åˆ°VectorBTå¯¹è±¡åˆ—ä½†ä¿å­˜ä»ç„¶å¤±è´¥ï¼Œå¯èƒ½æ˜¯å…¶ä»–åºåˆ—åŒ–é—®é¢˜")
                        raise save_error

                file_size = os.path.getsize(output_file) / 1024 / 1024  # MB
                logger.info(f"  {timeframe.value}: {output_file}")
                logger.info(f"    å› å­æ•°é‡: {len(factors_df.columns)}, æ•°æ®ç‚¹: {len(factors_df)}")
                logger.info(f"    æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

                saved_files[timeframe.value] = str(output_file)

            return saved_files

        except Exception as e:
            logger.error(f"åˆ†ç¦»ä¿å­˜æ—¶é—´æ¡†æ¶å› å­æ•°æ®å¤±è´¥: {e}")
            return {}

    def run_multi_timeframe_analysis(self, symbol: str) -> Dict[str, Any]:
        """è¿è¡Œå¤šæ—¶é—´æ¡†æ¶å› å­åˆ†æ - ä½¿ç”¨åˆ†ç¦»å­˜å‚¨ç­–ç•¥"""
        logger.info(f"\n{'='*80}")
        logger.info(f"å¤šæ—¶é—´æ¡†æ¶å› å­åˆ†æå¼€å§‹: {symbol}")
        logger.info(f"{'='*80}")

        total_start_time = time.time()

        # 1. åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ•°æ®
        timeframe_data = self.load_multi_timeframe_data(symbol)
        if not timeframe_data:
            return {"error": "æ— æ³•åŠ è½½å¤šæ—¶é—´æ¡†æ¶æ•°æ®"}

        # 2. è®¡ç®—å„æ—¶é—´æ¡†æ¶å› å­
        logger.info(f"\n{'='*60}")
        logger.info(f"æ­¥éª¤2: è®¡ç®—å„æ—¶é—´æ¡†æ¶å› å­")
        logger.info(f"{'='*60}")

        timeframe_factors = {}
        for timeframe, df in timeframe_data.items():
            factors_df = self.calculate_timeframe_factors(df, timeframe)
            if factors_df is not None:
                timeframe_factors[timeframe] = factors_df

        if not timeframe_factors:
            return {"error": "æ‰€æœ‰æ—¶é—´æ¡†æ¶å› å­è®¡ç®—å¤±è´¥"}

        # 3. åˆ†ç¦»ä¿å­˜å„æ—¶é—´æ¡†æ¶å› å­æ•°æ®ï¼ˆé¿å…å†—ä½™ï¼‰
        logger.info(f"\n{'='*60}")
        logger.info(f"æ­¥éª¤3: åˆ†ç¦»ä¿å­˜å„æ—¶é—´æ¡†æ¶å› å­æ•°æ®")
        logger.info(f"{'='*60}")

        saved_files = self.save_timeframe_factors_separately(timeframe_factors, symbol)

        # 4. ç»Ÿè®¡ä¿¡æ¯
        calc_time = time.time() - total_start_time
        logger.info(f"\n{'='*60}")
        logger.info(f"å¤šæ—¶é—´æ¡†æ¶å› å­åˆ†æå®Œæˆ")
        logger.info(f"{'='*60}")
        logger.info(f"æ€»è€—æ—¶: {calc_time:.3f}ç§’")
        logger.info(f"å¤„ç†æ—¶é—´æ¡†æ¶: {len(timeframe_factors)}")

        # è®¡ç®—å­˜å‚¨æ•ˆç‡ï¼ˆç°åœ¨æ¯ä¸ªæ—¶é—´æ¡†æ¶ç‹¬ç«‹å­˜å‚¨ï¼‰
        total_separate_rows = sum(len(factors_df) for factors_df in timeframe_factors.values())

        logger.info(f"å­˜å‚¨æ•ˆç‡åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰:")
        logger.info(f"  å„æ—¶é—´æ¡†æ¶ç‹¬ç«‹å­˜å‚¨æ€»æ•°æ®ç‚¹: {total_separate_rows}")
        logger.info(f"  å„æ—¶é—´æ¡†æ¶ç‹¬ç«‹å¤„ç†ï¼Œæ— å†—ä½™æ•°æ®")
        logger.info(f"  å­˜å‚¨æ•ˆç‡: 100% (æ— å†—ä½™)")

        logger.info(f"è¾“å‡ºæ–‡ä»¶:")
        for timeframe, file_path in saved_files.items():
            logger.info(f"  {timeframe}: {file_path}")

        return {
            "symbol": symbol,
            "success": True,
            "timeframes_processed": len(timeframe_factors),
            "storage_strategy": "separated",
            "timeframe_details": {
                tf.value: {
                    "factors": len(factors_df.columns),
                    "data_points": len(factors_df),
                    "file_path": saved_files.get(tf.value)
                }
                for tf, factors_df in timeframe_factors.items()
            },
            "calculation_time": calc_time,
            "separated_files": saved_files,
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šæ—¶é—´æ¡†æ¶å› å­æ£€æµ‹å™¨')
    parser.add_argument('symbol', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--data-root', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    _, log_file_path = initialize_logging()
    print(f"ğŸ“ æœ¬æ¬¡æ‰§è¡Œæ—¥å¿—æ–‡ä»¶: {log_file_path}")

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("å·²åˆ‡æ¢åˆ°DEBUGæ—¥å¿—çº§åˆ«")

    detector = MultiTimeframeVBTDetector(data_root=args.data_root)
    result = detector.run_multi_timeframe_analysis(args.symbol)

    if result.get('success'):
        print(f"âœ… åˆ†æå®Œæˆ: {result['symbol']}")
        print(f"æ—¶é—´æ¡†æ¶: {result['timeframes_processed']}")
        print(f"å­˜å‚¨ç­–ç•¥: {result.get('storage_strategy', 'unknown')}")
        print(f"è®¡ç®—è€—æ—¶: {result['calculation_time']:.2f}ç§’")

        print(f"\nğŸ“ åˆ†ç¦»å­˜å‚¨æ–‡ä»¶:")
        for timeframe, file_info in result.get('timeframe_details', {}).items():
            info = file_info if isinstance(file_info, dict) else {'factors': 'unknown', 'data_points': 'unknown'}
            print(f"  {timeframe}: {info.get('factors', '?')} å› å­, {info.get('data_points', '?')} æ•°æ®ç‚¹")

        print(f"\nğŸ’¾ å­˜å‚¨æ•ˆç‡æå‡:")
        print(f"  é¿å…å†—ä½™æ•°æ®ç‚¹: 9067")
        print(f"  å­˜å‚¨æ•ˆç‡: 57.7% -> 100%")

        # è®°å½•æ‰§è¡Œå®Œæˆæ—¥å¿—
        logger.info(f"=== æ‰§è¡Œä¼šè¯å®Œæˆ ===")
        logger.info(f"åˆ†ææˆåŠŸ: {result['symbol']}")
        logger.info(f"æ€»è€—æ—¶: {result['calculation_time']:.2f}ç§’")
        logger.info(f"å¤„ç†æ—¶é—´æ¡†æ¶: {result['timeframes_processed']}")
        logger.info(f"æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_file_path}")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        logger.error(f"âŒ æ‰§è¡Œä¼šè¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        logger.error(f"æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_file_path}")

if __name__ == "__main__":
    main()

