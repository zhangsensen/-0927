#!/usr/bin/env python3
"""
8å¤© WFO å…¨é‡ + å…¨é¢‘å›æµ‹è‡ªåŠ¨åŒ–æµæ°´çº¿

å·¥ä½œæµ:
1. æ£€æŸ¥ 8å¤© WFO æ˜¯å¦å®Œæˆ
2. æå–é€šè¿‡é—¨æ§›çš„ç»„åˆç™½åå•
3. å¯åŠ¨å…¨é¢‘çœŸå®å›æµ‹ï¼ˆ30 é¢‘ç‡ï¼‰
4. ç”Ÿæˆé¢‘ç‡æ³›åŒ–åˆ†ææŠ¥å‘Š
"""

import json
import os
import sys
import time
from pathlib import Path
from subprocess import run, PIPE
import pandas as pd
import numpy as np
from scipy.stats import spearmanr

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))


def check_wfo_completion(ts: str) -> bool:
    """æ£€æŸ¥ WFO æ˜¯å¦å®Œæˆ"""
    pid_file = ROOT / f"results/.wfo_8d_full_{ts}.pid"
    if not pid_file.exists():
        return False
    
    pid = int(pid_file.read_text().strip())
    result = run(["ps", "-p", str(pid)], capture_output=True)
    return result.returncode != 0  # è¿›ç¨‹ä¸å­˜åœ¨=å®Œæˆ


def resolve_wfo_run_ts(ts: str) -> str | None:
    """æ ¹æ® PID æ´¾ç”Ÿæ—¶é—´æˆ³å°è¯•å®šä½çœŸå®è½ç›˜çš„ WFO ç»“æœ run_<ts> ç›®å½•ã€‚

    æœ‰æ—¶ WFO ä¸»è„šæœ¬æœªæ­£ç¡®åˆ›å»ºå¯¹åº”çš„ run_{ts} è¾“å‡ºç›®å½• (ä¾‹å¦‚å‡ºç°æ—¥å¿—ä½†æ—  run_2025* ç›®å½•)ï¼Œ
    å¯¼è‡´åç»­è‡ªåŠ¨åŒ–æ‰¾ä¸åˆ° `wfo_learned_ranking_{ts}.csv` è€Œæå‰é€€å‡ºã€‚

    å›é€€ç­–ç•¥:
    1. è‹¥ `results/run_{ts}/wfo_learned_ranking_{ts}.csv` å­˜åœ¨, ç›´æ¥ä½¿ç”¨ã€‚
    2. å¦åˆ™åœ¨ `results/run_*/wfo_learned_ranking_*.csv` ä¸­é€‰æ‹©æœ€è¿‘ä¿®æ”¹çš„ä¸€ä»½ä½œä¸ºæ›¿ä»£ã€‚
    3. è‹¥ä»ä¸å­˜åœ¨, è¿”å› Noneã€‚
    """
    run_dir = ROOT / "results" / f"run_{ts}"
    wfo_file = run_dir / f"wfo_learned_ranking_{ts}.csv"
    if wfo_file.exists():
        return ts
    candidates = sorted(
        (ROOT / "results").glob("run_*/wfo_learned_ranking_*.csv"),
        key=lambda p: p.stat().st_mtime,
        reverse=True,
    )
    if candidates:
        latest = candidates[0]
        latest_ts = latest.name.removeprefix("wfo_learned_ranking_").removesuffix(".csv")
        print(f"âš ï¸ æœªæ‰¾åˆ°é¢„æœŸè¾“å‡º {wfo_file}ï¼Œä½¿ç”¨æœ€è¿‘çš„æœ‰æ•ˆ WFO ç»“æœ: run_{latest_ts}")
        return latest_ts
    print("âŒ æœªæ‰¾åˆ°ä»»ä½• WFO ç»“æœæ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­åç»­é˜¶æ®µ")
    return None


def extract_qualified_whitelist(ts: str, ic_threshold: float = 0.03, min_stability: float = 0.5) -> Path:
    """
    ä» WFO ç»“æœæå–åˆæ ¼ç»„åˆç™½åå•
    
    ç­›é€‰æ ‡å‡†:
    - wfo_ic > ic_threshold (é»˜è®¤ 0.03)
    - stability > min_stability (é»˜è®¤ 0.5)
    - FDR æ ¡éªŒé€šè¿‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    run_dir = ROOT / "results" / f"run_{ts}"
    wfo_file = run_dir / f"wfo_learned_ranking_{ts}.csv"
    
    if not wfo_file.exists():
        print(f"âŒ WFO ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {wfo_file}")
        return None
    
    df = pd.read_csv(wfo_file)
    print(f"âœ… åŠ è½½ WFO ç»“æœ: {len(df)} ç»„åˆ")
    
    # ç­›é€‰åˆæ ¼ç»„åˆ
    qualified = df[
        (df.get('wfo_ic', df.get('mean_oos_ic', 0)) > ic_threshold) &
        (df.get('stability_score', 1.0) > min_stability)
    ].copy()
    
    print(f"âœ… ç­›é€‰ååˆæ ¼ç»„åˆ: {len(qualified)} (IC>{ic_threshold}, ç¨³å®šæ€§>{min_stability})")
    
    # ä¿å­˜ç™½åå•
    whitelist_file = run_dir / f"whitelist_8d_wfo_qualified_{ts}.txt"
    qualified['combo'].to_csv(whitelist_file, index=False, header=False)
    print(f"âœ… ç™½åå•å·²ä¿å­˜: {whitelist_file}")
    
    return whitelist_file, len(qualified)


def run_all_freq_backtest(whitelist_file: Path, ts: str) -> Path:
    """å¯åŠ¨å…¨é¢‘çœŸå®å›æµ‹"""
    log_file = ROOT / "results" / "logs" / f"all_freq_scan_8d_wfo_{ts}.log"
    
    env = {
        "RB_WHITELIST_FILE": str(whitelist_file),
        "RB_TEST_ALL_FREQS": "1",
        "RB_SKIP_PREV": "1",
        "RB_OUTPUT_PREFIX": f"8d_wfo_qualified_{ts}",
    }
    
    cmd = [sys.executable, "-u", "-m", "real_backtest.run_production_backtest"]
    
    print(f"âœ… å¯åŠ¨å…¨é¢‘å›æµ‹...")
    print(f"   æ—¥å¿—: {log_file}")
    
    with open(log_file, "w") as f:
        proc = run(cmd, cwd=ROOT, env={**os.environ, **env}, stdout=f, stderr=f)
    
    if proc.returncode != 0:
        print(f"âŒ å…¨é¢‘å›æµ‹å¤±è´¥ï¼ŒæŸ¥çœ‹æ—¥å¿—: {log_file}")
        return None
    
    print(f"âœ… å…¨é¢‘å›æµ‹å®Œæˆ")
    
    # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶ï¼ˆå…¼å®¹ä¸¤ç§å‘½åé£æ ¼ï¼‰
    output_dir = ROOT / "results_combo_wfo"
    scan_file = list(output_dir.glob(f"*/all_freq_scan_8d_wfo_qualified_{ts}.csv"))
    if not scan_file:
        # å›é€€ï¼šéƒ¨åˆ†å›æµ‹è„šæœ¬ä»¥ all_freq_scan_<ts>.csv å‘½å
        scan_file = list(output_dir.glob(f"*/all_freq_scan_{ts}.csv"))
    
    if not scan_file:
        print("âŒ æœªæ‰¾åˆ°å…¨é¢‘å›æµ‹ç»“æœæ–‡ä»¶")
        return None
    
    return scan_file[0]


def analyze_freq_generalization(scan_file: Path, ts: str) -> dict:
    """åˆ†æé¢‘ç‡æ³›åŒ–èƒ½åŠ›"""
    df = pd.read_csv(scan_file)
    df['test_freq'] = df['test_freq'].astype(int)
    
    print(f"âœ… åŠ è½½å…¨é¢‘æ‰«æç»“æœ: {len(df)} è¡Œ")
    
    report = {
        "run_ts": ts,
        "n_combos": len(df['combo'].unique()),
        "n_freqs": len(df['test_freq'].unique()),
    }
    
    # 1. å„é¢‘ç‡çš„ Sharpe åˆ†å¸ƒ
    freq_stats = {}
    for freq in sorted(df['test_freq'].unique()):
        subset = df[df['test_freq'] == freq]['sharpe']
        freq_stats[int(freq)] = {
            'n': int(len(subset)),
            'median': float(np.median(subset)),
            'p20': float(np.percentile(subset, 20)),
            'p80': float(np.percentile(subset, 80)),
            'iqr': float(np.percentile(subset, 75) - np.percentile(subset, 25)),
            'gt_1.0_share': float(np.mean(subset > 1.0)),
        }
    
    report['freq_stats'] = freq_stats
    
    # 2. æ¯ä¸ªç»„åˆçš„æœ€ä½³é¢‘ç‡åˆ†å¸ƒ
    best_freq_per_combo = df.loc[df.groupby('combo')['sharpe'].idxmax()]
    best_freq_dist = best_freq_per_combo['test_freq'].value_counts().to_dict()
    report['best_freq_distribution'] = {int(k): int(v) for k, v in best_freq_dist.items()}
    
    # 3. 8å¤© vs å…¶ä»–é¢‘ç‡çš„ç§©ç›¸å…³ï¼ˆæ³›åŒ–æ£€éªŒï¼‰
    D8 = df[df['test_freq'] == 8][['combo', 'sharpe']].rename(columns={'sharpe': 'sharpe_8'})
    corr_vs_8d = {}
    
    for freq in [6, 7, 9, 10, 12, 16, 21, 24]:
        Df = df[df['test_freq'] == freq][['combo', 'sharpe']].rename(columns={'sharpe': f'sharpe_{freq}'})
        merged = D8.merge(Df, on='combo', how='inner')
        
        if len(merged) > 10:
            sp = float(spearmanr(merged['sharpe_8'], merged[f'sharpe_{freq}']).correlation)
            corr_vs_8d[int(freq)] = sp
    
    report['spearman_8d_vs_other_freqs'] = corr_vs_8d
    
    # 4. æ³›åŒ–è´¨é‡åˆ¤æ–­
    median_corr = np.median(list(corr_vs_8d.values()))
    report['generalization_quality'] = {
        'median_spearman': float(median_corr),
        'judgment': 'ä¼˜ç§€' if median_corr > 0.7 else ('è‰¯å¥½' if median_corr > 0.5 else ('ä¸€èˆ¬' if median_corr > 0.3 else 'å·®'))
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = scan_file.parent / f"freq_generalization_report_{ts}.json"
    report_file.write_text(json.dumps(report, ensure_ascii=False, indent=2))
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report


def main():
    # è·å–æœ€æ–°çš„è¿è¡Œæ—¶é—´æˆ³ï¼ˆä» PID æ–‡ä»¶ï¼‰
    pid_files = list((ROOT / "results").glob(".wfo_8d_full_*.pid"))
    if not pid_files:
        print("âŒ æœªæ‰¾åˆ° 8å¤© WFO è¿è¡Œè®°å½•")
        return 1
    
    latest_pid_file = max(pid_files, key=lambda p: p.stat().st_mtime)
    ts = latest_pid_file.stem.replace(".wfo_8d_full_", "")
    
    print(f"ğŸ” æ£€æµ‹åˆ°è¿è¡Œæ—¶é—´æˆ³: {ts}")
    
    # Step 1: ç­‰å¾… WFO å®Œæˆ
    print("â³ ç­‰å¾… 8å¤© WFO å®Œæˆ...")
    while not check_wfo_completion(ts):
        time.sleep(60)
        print("   ä»åœ¨è¿è¡Œï¼Œ60ç§’åé‡è¯•...")
    
    print("âœ… 8å¤© WFO å·²å®Œæˆ")
    
    # æ—¶é—´æˆ³å®¹é”™ä¿®æ­£ï¼šè§£æçœŸå®å­˜åœ¨çš„ run_{ts}
    resolved_ts = resolve_wfo_run_ts(ts)
    if not resolved_ts:
        return 1
    ts = resolved_ts

    # Step 2: æå–ç™½åå•
    result = extract_qualified_whitelist(ts, ic_threshold=0.03, min_stability=0.5)
    if result is None:
        return 1
    
    whitelist_file, n_qualified = result
    
    if n_qualified < 100:
        print(f"âš ï¸  åˆæ ¼ç»„åˆæ•°é‡è¿‡å°‘ï¼ˆ{n_qualified}ï¼‰ï¼Œå»ºè®®é™ä½ç­›é€‰æ ‡å‡†")
        return 1
    
    # Step 3: å…¨é¢‘å›æµ‹
    scan_file = run_all_freq_backtest(whitelist_file, ts)
    if scan_file is None:
        return 1
    
    # Step 4: åˆ†ææŠ¥å‘Š
    report = analyze_freq_generalization(scan_file, ts)
    
    # æ‰“å°å…³é”®ç»“è®º
    print("\n" + "="*60)
    print("ğŸ¯ é¢‘ç‡æ³›åŒ–è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"åˆæ ¼ç»„åˆæ•°: {report['n_combos']}")
    print(f"æµ‹è¯•é¢‘ç‡æ•°: {report['n_freqs']}")
    print(f"\n8å¤© vs å…¶ä»–é¢‘ç‡ç§©ç›¸å…³ï¼ˆä¸­ä½æ•°ï¼‰: {report['generalization_quality']['median_spearman']:.3f}")
    print(f"æ³›åŒ–è´¨é‡: {report['generalization_quality']['judgment']}")
    
    print("\næœ€ä½³é¢‘ç‡åˆ†å¸ƒï¼ˆå‰5ï¼‰:")
    best_dist = sorted(report['best_freq_distribution'].items(), key=lambda x: x[1], reverse=True)[:5]
    for freq, count in best_dist:
        print(f"  {freq}å¤©: {count} ç»„åˆ ({count/report['n_combos']*100:.1f}%)")
    
    print("\nå„é¢‘ç‡ä¸­ä½ Sharpeï¼ˆå‰5ï¼‰:")
    freq_by_median = sorted(report['freq_stats'].items(), key=lambda x: x[1]['median'], reverse=True)[:5]
    for freq, stats in freq_by_median:
        print(f"  {freq}å¤©: {stats['median']:.3f} (P20={stats['p20']:.3f}, >1.0å æ¯”={stats['gt_1.0_share']:.1%})")
    
    print("="*60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
