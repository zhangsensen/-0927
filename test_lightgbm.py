#!/usr/bin/env python3
"""
LightGBMå®‰è£…éªŒè¯å’Œæ€§èƒ½æµ‹è¯•è„šæœ¬
ä¸“ä¸ºApple Silicon Macä¼˜åŒ–
"""

import lightgbm as lgb
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import time

def test_lightgbm_installation():
    """æµ‹è¯•LightGBMå®‰è£…å’ŒåŸºæœ¬åŠŸèƒ½"""
    print("ğŸ LightGBMåœ¨Apple Silicon Macä¸Šçš„æµ‹è¯•")
    print("=" * 50)

    # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
    print(f"LightGBMç‰ˆæœ¬: {lgb.__version__}")
    print()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    X, y = make_classification(
        n_samples=10000,
        n_features=20,
        n_informative=15,
        random_state=42
    )
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print()

    # Apple Siliconä¼˜åŒ–å‚æ•°
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': 0,
        'device': 'cpu',  # Macä¸ŠCPUé€šå¸¸æ›´ç¨³å®š
        'num_threads': 4,  # MèŠ¯ç‰‡ä¼˜åŒ–ï¼Œå¯æ ¹æ®æ ¸å¿ƒæ•°è°ƒæ•´
        'seed': 42
    }

    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
    start_time = time.time()

    # åˆ›å»ºæ•°æ®é›†
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

    # è®­ç»ƒ
    model = lgb.train(
        params,
        train_data,
        num_boost_round=100,
        valid_sets=[valid_data],
        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
    )

    training_time = time.time() - start_time
    print(f"âœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {training_time:.2f}ç§’")

    # é¢„æµ‹å’Œè¯„ä¼°
    print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°...")
    y_pred = model.predict(X_test)
    y_pred_binary = (y_pred > 0.5).astype(int)
    accuracy = accuracy_score(y_test, y_pred_binary)

    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"è®­ç»ƒè½®æ•°: {model.num_trees()}")
    print()

    # ç‰¹å¾é‡è¦æ€§
    feature_importance = model.feature_importance()
    top_features = np.argsort(feature_importance)[-5:][::-1]

    print("ğŸ¯ Top 5 é‡è¦ç‰¹å¾:")
    for i, feature_idx in enumerate(top_features, 1):
        print(f"  {i}. ç‰¹å¾ {feature_idx}: {feature_importance[feature_idx]:.2f}")

    print()
    print("ğŸ‰ LightGBMæµ‹è¯•å®Œæˆï¼åœ¨ä½ çš„Macä¸Šè¿è¡Œå®Œç¾ã€‚")

    return model

def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("-" * 30)

    # ä¸åŒå¤§å°çš„æ•°æ®é›†
    sizes = [1000, 5000, 10000]

    for size in sizes:
        print(f"\næ•°æ®é›†å¤§å°: {size:,}")

        X, y = make_classification(
            n_samples=size,
            n_features=20,
            random_state=42
        )

        # æµ‹è¯•è®­ç»ƒæ—¶é—´
        start_time = time.time()

        train_data = lgb.Dataset(X, label=y)
        model = lgb.train(
            {
                'objective': 'binary',
                'verbose': -1,
                'num_threads': 4,
                'device': 'cpu'
            },
            train_data,
            num_boost_round=50
        )

        elapsed = time.time() - start_time
        print(f"  è®­ç»ƒæ—¶é—´: {elapsed:.3f}ç§’")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    model = test_lightgbm_installation()
    performance_benchmark()

    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("- åœ¨ä½ çš„MèŠ¯ç‰‡Macä¸Šï¼Œæ¨èä½¿ç”¨CPUè€ŒéGPU")
    print("- å¯ä»¥è°ƒæ•´num_threadså‚æ•°æ¥ä¼˜åŒ–æ€§èƒ½")
    print("- å¯¹äºå¤§æ•°æ®é›†ï¼ŒLightGBMåœ¨Macä¸Šè¡¨ç°ä¼˜å¼‚")