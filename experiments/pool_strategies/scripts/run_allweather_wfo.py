#!/usr/bin/env python3
"""
å…¨å¤©å€™ç­–ç•¥ä¸“ç”¨ WFO ä¼˜åŒ–å™¨ | All-Weather WFO Optimizer
================================================================================
åŠŸèƒ½ï¼š
1. é’ˆå¯¹å…¨å¤©å€™ç­–ç•¥çš„ 7 ä¸ªå­æ± åˆ†åˆ«è¿è¡Œ WFO ä¼˜åŒ–
2. ä¸ºæ¯ä¸ªå­æ± å¯»æ‰¾æœ€ä½³çš„å› å­ç»„åˆ (åŸºäº ICIR)
3. ç”Ÿæˆ unified_config.json ä¾›å›æµ‹å¼•æ“ä½¿ç”¨

å­æ± å®šä¹‰ï¼š
- EQUITY_BROAD, EQUITY_GROWTH, EQUITY_CYCLICAL, EQUITY_DEFENSIVE
- BOND, COMMODITY, QDII

è¾“å‡ºï¼š
- results/allweather_wfo_YYYYMMDD_HHMMSS/best_combos.json
================================================================================
"""

import logging
import os
import sys
import json
from datetime import datetime
from pathlib import Path
from itertools import combinations
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import yaml
from tqdm import tqdm
from numba import njit, prange

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT))

from etf_rotation_optimized.core.data_loader import DataLoader
from etf_rotation_optimized.core.precise_factor_library_v2 import PreciseFactorLibrary
from etf_rotation_optimized.core.cross_section_processor import CrossSectionProcessor
from etf_rotation_optimized.core.category_factors import CategoryFactorManager

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)

# ============================================================================
# Numba åŠ é€Ÿå‡½æ•° (ä» run_unified_wfo.py ç§»æ¤)
# ============================================================================

@njit(cache=True)
def _compute_spearman_ic_single_day(scores: np.ndarray, returns: np.ndarray) -> float:
    mask = ~(np.isnan(scores) | np.isnan(returns))
    n_valid = np.sum(mask)
    
    if n_valid < 3:
        return np.nan
    
    s = scores[mask]
    r = returns[mask]
    
    s_rank = np.argsort(np.argsort(s)).astype(np.float64)
    r_rank = np.argsort(np.argsort(r)).astype(np.float64)
    
    s_mean = np.mean(s_rank)
    r_mean = np.mean(r_rank)
    
    numerator = np.sum((s_rank - s_mean) * (r_rank - r_mean))
    s_std = np.sqrt(np.sum((s_rank - s_mean) ** 2))
    r_std = np.sqrt(np.sum((r_rank - r_mean) ** 2))
    
    if s_std > 0 and r_std > 0:
        return numerator / (s_std * r_std)
    return np.nan

@njit(cache=True)
def _compute_combo_ic_series(
    factors_3d: np.ndarray,
    factor_indices: np.ndarray,
    forward_returns: np.ndarray,
    lookback: int,
    min_valid_days: int,
) -> Tuple[float, float, float, int]:
    T, N, _ = factors_3d.shape
    n_factors = len(factor_indices)
    
    ic_values = np.zeros(T - lookback)
    valid_count = 0
    
    for t in range(lookback, T):
        combo_score = np.zeros(N)
        for n in range(N):
            score = 0.0
            valid_factors = 0
            for i in range(n_factors):
                f_idx = factor_indices[i]
                val = factors_3d[t-1, n, f_idx]
                if not np.isnan(val):
                    score += val
                    valid_factors += 1
            
            # åªæœ‰å½“è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆå› å­æ—¶æ‰è®¡ç®—å¾—åˆ†
            if valid_factors > 0:
                combo_score[n] = score
            else:
                combo_score[n] = np.nan
        
        ic = _compute_spearman_ic_single_day(combo_score, forward_returns[t])
        ic_values[t - lookback] = ic
        if not np.isnan(ic):
            valid_count += 1
    
    if valid_count >= min_valid_days:
        valid_ics = np.zeros(valid_count)
        idx = 0
        for i in range(len(ic_values)):
            if not np.isnan(ic_values[i]):
                valid_ics[idx] = ic_values[i]
                idx += 1
        
        ic_mean = np.mean(valid_ics)
        ic_std = np.std(valid_ics)
        
        if ic_std > 0.001:
            ic_ir = ic_mean / ic_std
        else:
            ic_ir = 0.0
        
        return ic_mean, ic_std, ic_ir, valid_count
    
    return 0.0, 0.0, 0.0, valid_count

@njit(parallel=True, cache=True)
def _compute_all_combo_ics(
    factors_3d: np.ndarray,
    all_combo_indices: np.ndarray,
    combo_sizes: np.ndarray,
    forward_returns: np.ndarray,
    lookback: int,
    min_valid_days: int,
) -> np.ndarray:
    n_combos = all_combo_indices.shape[0]
    results = np.zeros((n_combos, 4))
    
    for i in prange(n_combos):
        size = combo_sizes[i]
        factor_indices = all_combo_indices[i, :size]
        
        ic_mean, ic_std, ic_ir, n_valid = _compute_combo_ic_series(
            factors_3d, factor_indices, forward_returns, lookback, min_valid_days
        )
        
        results[i, 0] = ic_mean
        results[i, 1] = ic_std
        results[i, 2] = ic_ir
        results[i, 3] = n_valid
    
    return results

# ============================================================================
# ä¸»é€»è¾‘
# ============================================================================

class AllWeatherWFO:
    def __init__(self):
        self.config_wfo = self._load_yaml(ROOT / "configs/combo_wfo_config.yaml")
        self.config_pools = self._load_yaml(ROOT / "configs/etf_pools.yaml")
        
        self.loader = DataLoader(
            data_dir=self.config_wfo["data"].get("data_dir"),
            cache_dir=self.config_wfo["data"].get("cache_dir"),
        )
        self.factor_lib = PreciseFactorLibrary()
        self.category_factor_mgr = CategoryFactorManager()
        self.processor = CrossSectionProcessor(verbose=False)
        
        self.ohlcv = None
        self.factors_dict = {}
        self.factor_names = []
        self.etf_codes = []
        
    def _load_yaml(self, path: Path) -> Dict:
        with open(path) as f:
            return yaml.safe_load(f)
            
    def load_and_prepare_data(self):
        """åŠ è½½æ•°æ®å¹¶è®¡ç®—æ‰€æœ‰å› å­"""
        logger.info("ğŸ“Š åŠ è½½å…¨é‡æ•°æ®...")
        
        # 1. è·å–æ‰€æœ‰æ± çš„å¹¶é›†ç¬¦å·
        all_symbols = []
        for pool in self.config_pools["pools"].values():
            all_symbols.extend(pool["symbols"])
        all_symbols = sorted(list(set(all_symbols)))
        
        # 2. åŠ è½½ OHLCV
        self.ohlcv = self.loader.load_ohlcv(
            etf_codes=all_symbols,
            start_date=self.config_wfo["data"]["start_date"],
            end_date=self.config_wfo["data"]["end_date"],
        )
        self.etf_codes = self.ohlcv["close"].columns.tolist()
        
        # 3. è®¡ç®—é€šç”¨å› å­
        logger.info("ğŸ”§ è®¡ç®—é€šç”¨å› å­...")
        raw_factors_df = self.factor_lib.compute_all_factors(self.ohlcv)
        
        # 4. è®¡ç®—åˆ†ç±»å› å­ (Bond, Commodity, QDII)
        logger.info("ğŸ”§ è®¡ç®—åˆ†ç±»å› å­...")
        bond_symbols = self.config_pools["pools"]["BOND"]["symbols"]
        comm_symbols = self.config_pools["pools"]["COMMODITY"]["symbols"]
        qdii_symbols = self.config_pools["pools"]["QDII"]["symbols"]
        
        market_proxy = "510300.SH" if "510300.SH" in self.ohlcv["close"].columns else self.ohlcv["close"].columns[0]
        
        bond_factors = self.category_factor_mgr.compute_factors_for_pool("BOND", self.ohlcv, bond_symbols)
        comm_factors = self.category_factor_mgr.compute_factors_for_pool("COMMODITY", self.ohlcv, comm_symbols, market_proxy)
        qdii_factors = self.category_factor_mgr.compute_factors_for_pool("QDII", self.ohlcv, qdii_symbols)
        
        # 5. åˆå¹¶æ‰€æœ‰å› å­
        all_dfs = [raw_factors_df]
        if not bond_factors.empty: all_dfs.append(bond_factors)
        if not comm_factors.empty: all_dfs.append(comm_factors)
        if not qdii_factors.empty: all_dfs.append(qdii_factors)
        
        combined_df = pd.concat(all_dfs, axis=1)
        
        # 6. æ ‡å‡†åŒ–
        logger.info("ğŸ“ æ ‡å‡†åŒ–å› å­...")
        factor_names_list = combined_df.columns.get_level_values(0).unique().tolist()
        raw_factors_dict = {fname: combined_df[fname] for fname in factor_names_list}
        
        processed_factors = self.processor.process_all_factors(raw_factors_dict)
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å› å­ DataFrame éƒ½å¯¹é½åˆ°å®Œæ•´çš„ etf_codes
        # è¿™æ ·åç»­ä½¿ç”¨ symbol_indices åˆ‡ç‰‡æ—¶æ‰ä¸ä¼šè¶Šç•Œ
        self.factors_dict = {}
        for fname, df in processed_factors.items():
            # Reindex columns to match self.etf_codes, filling with NaN
            aligned_df = df.reindex(columns=self.etf_codes)
            self.factors_dict[fname] = aligned_df
            
        self.factor_names = sorted(self.factors_dict.keys())
        
        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(self.etf_codes)} ETFs, {len(self.factor_names)} Factors")

    def run_pool_optimization(self, pool_name: str, pool_config: Dict) -> Dict:
        """å¯¹å•ä¸ªæ± è¿è¡Œ WFO ä¼˜åŒ–"""
        logger.info(f"ğŸš€ ä¼˜åŒ–å­æ± : {pool_name}")
        
        pool_symbols = pool_config["symbols"]
        # è¿‡æ»¤å‡ºå­˜åœ¨çš„ symbols
        valid_symbols = [s for s in pool_symbols if s in self.etf_codes]
        if not valid_symbols:
            logger.warning(f"âš ï¸ æ±  {pool_name} æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ï¼Œè·³è¿‡")
            return None
            
        # 1. å‡†å¤‡è¯¥æ± çš„æ•°æ®åˆ‡ç‰‡
        # æ‰¾å‡º valid_symbols åœ¨ self.etf_codes ä¸­çš„ç´¢å¼•
        symbol_indices = [self.etf_codes.index(s) for s in valid_symbols]
        
        # å‡†å¤‡å› å­ 3D æ•°ç»„ (T, N_subset, F)
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ç­›é€‰å‡ºå¯¹è¯¥æ± æœ‰æ„ä¹‰çš„å› å­
        # æ¯”å¦‚ Bond æ± ä¸éœ€è¦çœ‹ Equity å› å­ï¼Œåä¹‹äº¦ç„¶
        # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨æ‰€æœ‰å› å­ï¼ŒIC ä¼šè‡ªåŠ¨è¿‡æ»¤æ— æ•ˆå€¼ (NaN)
        
        # ä¼˜åŒ–ï¼šåªé€‰æ‹©åœ¨è¯¥æ± ä¸Šæœ‰é NaN å€¼çš„å› å­
        # è¿™ä¸€æ­¥å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡
        relevant_factors = []
        for fname in self.factor_names:
            # æ£€æŸ¥è¯¥å› å­åœ¨è¿™äº› symbol ä¸Šæ˜¯å¦å…¨ä¸º NaN
            f_data = self.factors_dict[fname].values[:, symbol_indices]
            if not np.isnan(f_data).all():
                relevant_factors.append(fname)
        
        logger.info(f"   æœ‰æ•ˆå› å­æ•°: {len(relevant_factors)} / {len(self.factor_names)}")
        
        factors_3d = np.stack([
            self.factors_dict[f].values[:, symbol_indices] 
            for f in relevant_factors
        ], axis=-1)
        
        # å‡†å¤‡æ”¶ç›Šç‡
        close_prices = self.ohlcv["close"][valid_symbols].ffill().bfill().values
        T, N = close_prices.shape
        forward_returns = np.zeros((T, N))
        for t in range(T - 1):
            for n in range(N):
                if close_prices[t, n] > 0:
                    forward_returns[t + 1, n] = (close_prices[t + 1, n] - close_prices[t, n]) / close_prices[t, n]
                else:
                    forward_returns[t + 1, n] = np.nan
                    
        # 2. ç”Ÿæˆç»„åˆ
        combo_sizes = [2, 3, 4] # é™åˆ¶ç»„åˆå¤§å°ä»¥åŠ å¿«é€Ÿåº¦
        all_combos = []
        for size in combo_sizes:
            combos = list(combinations(range(len(relevant_factors)), size))
            all_combos.extend([(c, size) for c in combos])
            
        # é™åˆ¶ç»„åˆæ•°é‡ (éšæœºé‡‡æ ·å¦‚æœå¤ªå¤š)
        MAX_COMBOS = 5000
        if len(all_combos) > MAX_COMBOS:
            import random
            random.seed(42)
            all_combos = random.sample(all_combos, MAX_COMBOS)
            logger.info(f"   ç»„åˆè¿‡å¤šï¼Œé‡‡æ ·è‡³ {MAX_COMBOS} ä¸ª")
            
        # 3. è¿è¡Œ IC è®¡ç®—
        n_combos = len(all_combos)
        max_size = max(combo_sizes)
        all_combo_indices = np.full((n_combos, max_size), -1, dtype=np.int64)
        combo_sizes_arr = np.zeros(n_combos, dtype=np.int64)
        
        for i, (combo, size) in enumerate(all_combos):
            combo_sizes_arr[i] = size
            for j, idx in enumerate(combo):
                all_combo_indices[i, j] = idx
                
        ic_results = _compute_all_combo_ics(
            factors_3d,
            all_combo_indices,
            combo_sizes_arr,
            forward_returns,
            lookback=252,
            min_valid_days=20
        )
        
        # 4. æ•´ç†ç»“æœ
        results = []
        for i, (combo, size) in enumerate(all_combos):
            ic_mean, ic_std, ic_ir, n_valid = ic_results[i]
            if n_valid >= 20:
                combo_names = [relevant_factors[idx] for idx in combo]
                results.append({
                    "combo": combo_names,
                    "icir": ic_ir,
                    "ic_mean": ic_mean
                })
        
        # 5. é€‰å‡ºæœ€ä½³
        if not results:
            logger.warning(f"âš ï¸ æ±  {pool_name} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆç»„åˆ")
            return None
            
        results.sort(key=lambda x: x["icir"], reverse=True)
        best = results[0]
        logger.info(f"   ğŸ† æœ€ä½³ç»„åˆ: {best['combo']} (ICIR: {best['icir']:.2f})")
        
        return best["combo"]

    def run(self):
        self.load_and_prepare_data()
        
        best_combos = {}
        
        # éå†æ‰€æœ‰æ± 
        pools = self.config_pools["pools"]
        # æ’é™¤ A_SHARE_LIVE
        target_pools = [p for p in pools if p != "A_SHARE_LIVE"]
        
        for pool_name in target_pools:
            pool_config = pools[pool_name]
            best_combo = self.run_pool_optimization(pool_name, pool_config)
            if best_combo:
                best_combos[pool_name] = best_combo
                
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = ROOT / "results" / f"allweather_wfo_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / "best_combos.json"
        with open(output_file, "w") as f:
            json.dump(best_combos, f, indent=2)
            
        logger.info(f"âœ… ä¼˜åŒ–å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_file}")
        
        # åŒæ—¶æ›´æ–° latest è½¯é“¾æ¥æˆ–å›ºå®šè·¯å¾„ä¾›å›æµ‹è„šæœ¬è¯»å–
        latest_file = ROOT / "results" / "allweather_best_combos_latest.json"
        with open(latest_file, "w") as f:
            json.dump(best_combos, f, indent=2)
            
        return best_combos

if __name__ == "__main__":
    optimizer = AllWeatherWFO()
    optimizer.run()
