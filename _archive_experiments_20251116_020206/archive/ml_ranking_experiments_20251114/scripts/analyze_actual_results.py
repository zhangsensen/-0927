#!/usr/bin/env python3
"""æ·±åº¦åˆ†æç›‘ç£å­¦ä¹ å®é™…è¿è¡Œç»“æœ"""

import json
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold, train_test_split

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from ml_ranking import evaluation, models

# é…ç½®
DATASET_PATH = Path(__file__).parent.parent / "ml_ranking/data/training_dataset.parquet"
OUTPUT_PATH = Path(__file__).parent.parent / "ml_ranking/ACTUAL_RESULTS_ANALYSIS.json"

def analyze_results():
    """æ‰§è¡Œå®Œæ•´åˆ†æ"""
    
    print("="*80)
    print("ğŸ” ç›‘ç£å­¦ä¹ å®é™…ç»“æœæ·±åº¦åˆ†æ")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®é›†
    print("\n[1/6] åŠ è½½æ•°æ®é›†...")
    df = pd.read_parquet(DATASET_PATH)
    label_col = "oos_compound_sharpe"
    
    dataset_stats = {
        "rows": len(df),
        "features": len(df.columns) - 1,
        "columns": df.columns.tolist(),
        "label_stats": df[label_col].describe().to_dict(),
        "missing_rates": {
            col: float(df[col].isnull().mean()) 
            for col in df.columns if df[col].isnull().mean() > 0
        }
    }
    
    print(f"   âœ… æ ·æœ¬æ•°: {dataset_stats['rows']}")
    print(f"   âœ… ç‰¹å¾æ•°: {dataset_stats['features']}")
    print(f"   âœ… æ ‡ç­¾å‡å€¼: {dataset_stats['label_stats']['mean']:.4f}")
    print(f"   âœ… æ ‡ç­¾èŒƒå›´: [{dataset_stats['label_stats']['min']:.4f}, {dataset_stats['label_stats']['max']:.4f}]")
    
    # 2. æ•°æ®è´¨é‡æ£€æŸ¥
    print("\n[2/6] æ•°æ®è´¨é‡æ£€æŸ¥...")
    
    quality_checks = {
        "label_in_features": label_col in [c for c in df.columns if c != label_col],
        "duplicates": int(df.duplicated().sum()),
        "infinite_values": int(np.isinf(df.select_dtypes(include=[np.number])).sum().sum()),
        "overall_missing_rate": float(df.isnull().mean().mean()),
        "high_missing_features": [
            col for col in df.columns 
            if col != label_col and df[col].isnull().mean() > 0.05
        ]
    }
    
    print(f"   {'âœ…' if not quality_checks['label_in_features'] else 'ğŸš¨'} æ ‡ç­¾æ³„éœ²: {quality_checks['label_in_features']}")
    print(f"   âœ… é‡å¤æ ·æœ¬: {quality_checks['duplicates']}")
    print(f"   âœ… ç¼ºå¤±ç‡: {quality_checks['overall_missing_rate']*100:.2f}%")
    print(f"   âš ï¸  é«˜ç¼ºå¤±ç‰¹å¾æ•°: {len(quality_checks['high_missing_features'])}")
    
    # 3. å•æ¬¡åˆ†å‰²è®­ç»ƒ
    print("\n[3/6] å•æ¬¡80/20åˆ†å‰²è®­ç»ƒ...")
    X = df.drop(columns=[label_col])
    y = df[label_col].to_numpy(dtype=float)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # é¢„å¤„ç†
    X_train_clean = X_train.apply(pd.to_numeric, errors="coerce")
    X_test_clean = X_test.apply(pd.to_numeric, errors="coerce")
    median_vals = X_train_clean.median()
    X_train_clean = X_train_clean.fillna(median_vals).fillna(0.0)
    X_test_clean = X_test_clean.fillna(median_vals).fillna(0.0)
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    model_registry = models.baseline_model_registry()
    single_split_results = {}
    
    for name, model in model_registry.items():
        print(f"   è®­ç»ƒ {name:15s}...", end=" ")
        try:
            model.fit(X_train_clean, y_train)
            y_pred = model.predict(X_test_clean)
            
            eval_result = evaluation.evaluate_predictions(
                y_true=y_test, y_pred=y_pred, model_name=name
            )
            
            single_split_results[name] = eval_result.metrics
            
            spearman = eval_result.metrics['spearman']
            top50 = eval_result.metrics['top50_overlap']
            ndcg50 = eval_result.metrics['ndcg@50']
            
            print(f"Spearman={spearman:.4f}, Top50={top50:.2%}, NDCG@50={ndcg50:.4f}")
            
            # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸
            if spearman > 0.90:
                print(f"      ğŸš¨ è­¦å‘Š: Spearmanè¿‡é«˜ ({spearman:.4f}), å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ•°æ®æ³„éœ²!")
            elif spearman > 0.75:
                print(f"      âš ï¸  æ³¨æ„: Spearmanè¾ƒé«˜ ({spearman:.4f}), å»ºè®®éªŒè¯")
            elif spearman >= 0.60:
                print(f"      âœ… æ­£å¸¸: Spearmanåœ¨åˆç†èŒƒå›´")
            else:
                print(f"      âš ï¸  åä½: Spearman<0.60, ç‰¹å¾å·¥ç¨‹å¯èƒ½éœ€è¦æ”¹è¿›")
                
        except Exception as e:
            print(f"å¤±è´¥: {e}")
    
    # 4. 5æŠ˜äº¤å‰éªŒè¯
    print("\n[4/6] 5æŠ˜äº¤å‰éªŒè¯...")
    X_all = X.apply(pd.to_numeric, errors="coerce")
    median_all = X_all.median()
    X_all_clean = X_all.fillna(median_all).fillna(0.0)
    
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_results = {}
    
    for name in model_registry.keys():
        fold_metrics = []
        
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_all_clean)):
            X_tr = X_all_clean.iloc[train_idx]
            y_tr = y[train_idx]
            X_val = X_all_clean.iloc[val_idx]
            y_val = y[val_idx]
            
            if name == "elasticnet":
                fold_model = models.make_linear_model(models.ModelConfig())
            elif name == "decision_tree":
                fold_model = models.make_tree_model(models.ModelConfig())
            elif name == "lgbm_regressor":
                fold_model = models.make_lgbm_regressor(models.ModelConfig())
            else:
                continue
            
            try:
                fold_model.fit(X_tr, y_tr)
                y_pred_val = fold_model.predict(X_val)
                eval_res = evaluation.evaluate_predictions(
                    y_true=y_val, y_pred=y_pred_val, model_name=f"{name}_f{fold_idx}"
                )
                fold_metrics.append(eval_res.metrics)
            except Exception:
                continue
        
        if fold_metrics:
            aggregated = {}
            for metric_name in fold_metrics[0].keys():
                vals = [m[metric_name] for m in fold_metrics]
                aggregated[f"{metric_name}_mean"] = float(np.mean(vals))
                aggregated[f"{metric_name}_std"] = float(np.std(vals))
            
            cv_results[name] = {
                "n_folds": len(fold_metrics),
                "aggregated": aggregated,
            }
            
            sp_mean = aggregated['spearman_mean']
            sp_std = aggregated['spearman_std']
            top50_mean = aggregated['top50_overlap_mean']
            
            print(f"   {name:15s}: Spearman={sp_mean:.4f}Â±{sp_std:.4f}, Top50={top50_mean:.2%}")
    
    # 5. Top-Kåˆ†æ
    print("\n[5/6] Top-Ké¢„æµ‹è´¨é‡åˆ†æ...")
    top_ks = [50, 100, 200, 500, 1000, 2000]
    topk_analysis = {}
    
    for name, model in model_registry.items():
        if name not in single_split_results:
            continue
        
        try:
            model.fit(X_all_clean, y)
            y_pred_all = model.predict(X_all_clean)
            
            ranked_idx = np.argsort(-y_pred_all)
            true_ranked_idx = np.argsort(-y)
            
            model_topk = {}
            for k in top_ks:
                if k > len(y):
                    k = len(y)
                
                pred_topk_idx = ranked_idx[:k]
                pred_topk_actual = y[pred_topk_idx]
                
                true_topk_idx = true_ranked_idx[:k]
                oracle_actual = y[true_topk_idx]
                
                model_topk[f"top{k}"] = {
                    "mean_actual": float(np.mean(pred_topk_actual)),
                    "median_actual": float(np.median(pred_topk_actual)),
                    "std_actual": float(np.std(pred_topk_actual)),
                    "oracle_mean": float(np.mean(oracle_actual)),
                    "oracle_median": float(np.median(oracle_actual)),
                    "gap": float(np.mean(pred_topk_actual) - np.mean(oracle_actual)),
                }
            
            topk_analysis[name] = model_topk
            
            # æ‰“å°å…³é”®Kå€¼
            for k_name in ['top50', 'top2000']:
                if k_name in model_topk:
                    stats = model_topk[k_name]
                    print(f"   {name:15s} {k_name:8s}: "
                          f"pred={stats['mean_actual']:.4f}, "
                          f"oracle={stats['oracle_mean']:.4f}, "
                          f"gap={stats['gap']:.4f}")
        
        except Exception as e:
            print(f"   {name}: å¤±è´¥ - {e}")
    
    # 6. å¯¹æ¯”åˆ†æ
    print("\n[6/6] ä¸è§„åˆ’ç›®æ ‡å¯¹æ¯”...")
    
    plan_targets = {
        "phase1": {"spearman": 0.60, "top50_overlap": 0.18},
        "phase2_linear": {"spearman": 0.65, "top50_overlap": 0.25},
        "phase2_gbm": {"spearman": 0.70, "top50_overlap": 0.35},
        "phase2_lambdamart": {"spearman": 0.70, "top50_overlap": 0.40},
        "mvp": {"spearman": 0.70, "top50_overlap": 0.50},
        "ideal": {"spearman": 0.75, "top50_overlap": 0.70},
    }
    
    comparison = {}
    for name, metrics in single_split_results.items():
        actual_sp = metrics['spearman']
        actual_top50 = metrics['top50_overlap']
        
        status = "unknown"
        if actual_sp >= plan_targets["ideal"]["spearman"]:
            status = "è¶…è¶Šç†æƒ³ç›®æ ‡"
        elif actual_sp >= plan_targets["mvp"]["spearman"]:
            status = "è¾¾åˆ°MVPæ ‡å‡†"
        elif actual_sp >= plan_targets["phase2_gbm"]["spearman"]:
            status = "ç¬¦åˆPhase2é¢„æœŸ"
        elif actual_sp >= plan_targets["phase1"]["spearman"]:
            status = "ç¬¦åˆPhase1åŸºçº¿"
        else:
            status = "ä½äºPhase1åŸºçº¿"
        
        comparison[name] = {
            "actual_spearman": actual_sp,
            "actual_top50": actual_top50,
            "status": status,
            "vs_mvp_spearman": actual_sp - plan_targets["mvp"]["spearman"],
            "vs_mvp_top50": actual_top50 - plan_targets["mvp"]["top50_overlap"],
        }
        
        print(f"   {name:15s}: {status}")
        print(f"      Spearman: {actual_sp:.4f} (MVPç›®æ ‡: 0.70, å·®è·: {actual_sp-0.70:+.4f})")
        print(f"      Top50:    {actual_top50:.2%} (MVPç›®æ ‡: 50%, å·®è·: {actual_top50-0.50:+.2%})")
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Š
    report = {
        "dataset_stats": dataset_stats,
        "quality_checks": quality_checks,
        "single_split_results": single_split_results,
        "cross_validation_results": cv_results,
        "top_k_analysis": topk_analysis,
        "comparison_to_plan": comparison,
    }
    
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_PATH}")
    
    # æœ€ç»ˆåˆ¤æ–­
    print("\n" + "="*80)
    print("ğŸ¯ æœ€ç»ˆåˆ¤æ–­")
    print("="*80)
    
    best_model = max(single_split_results.items(), key=lambda x: x[1]['spearman'])
    best_name, best_metrics = best_model
    
    print(f"\næœ€ä½³æ¨¡å‹: {best_name}")
    print(f"  Spearman: {best_metrics['spearman']:.4f}")
    print(f"  Top50 overlap: {best_metrics['top50_overlap']:.2%}")
    print(f"  NDCG@50: {best_metrics['ndcg@50']:.4f}")
    
    if best_metrics['spearman'] > 0.95:
        print("\nğŸš¨ ä¸¥é‡è­¦å‘Š: ç»“æœå¼‚å¸¸ä¼˜ç§€ (Spearman>0.95)")
        print("   å¯èƒ½åŸå› :")
        print("   1. æ•°æ®æ³„éœ² (æ ‡ç­¾ä¿¡æ¯æ³„éœ²åˆ°ç‰¹å¾ä¸­)")
        print("   2. æ ·æœ¬é‡è¿‡å°å¯¼è‡´è¿‡æ‹Ÿåˆ")
        print("   3. ç‰¹å¾ä¸æ ‡ç­¾é«˜åº¦ç›¸å…³ä½†ç¼ºä¹æ³›åŒ–æ€§")
        print("   å»ºè®®: ç«‹å³å®¡è®¡ç‰¹å¾å·¥ç¨‹ä»£ç ï¼Œä½¿ç”¨ç‹¬ç«‹æ•°æ®é›†éªŒè¯")
    elif best_metrics['spearman'] > 0.85:
        print("\nâš ï¸  è­¦å‘Š: ç»“æœä¼˜äºé¢„æœŸ (Spearman>0.85)")
        print("   è¿™è¶…è¶Šäº†Phase5ç†æƒ³ç›®æ ‡(0.75)")
        print("   å»ºè®®: éªŒè¯CVç»“æœç¨³å®šæ€§ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆ")
    elif best_metrics['spearman'] >= 0.70:
        print("\nâœ… ä¼˜ç§€: å·²è¾¾åˆ°MVPæ ‡å‡†!")
        print("   å¯ä»¥è¿›å…¥Phase3ç‰¹å¾å·¥ç¨‹æ·±åŒ–")
    elif best_metrics['spearman'] >= 0.60:
        print("\nâœ… åˆæ ¼: ç¬¦åˆPhase1-2é¢„æœŸ")
        print("   å»ºè®®ç»§ç»­Phase2ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–")
    else:
        print("\nâš ï¸  å¾…æ”¹è¿›: Spearman<0.60")
        print("   å»ºè®®: æ£€æŸ¥ç‰¹å¾å·¥ç¨‹å’Œæ•°æ®é¢„å¤„ç†")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    analyze_results()
