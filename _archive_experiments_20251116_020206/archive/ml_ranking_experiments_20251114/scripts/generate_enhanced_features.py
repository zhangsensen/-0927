# -*- coding: utf-8 -*-
"""
@author: Copilot
@created: 2025-11-13
@description: ä»å›æµ‹æ‘˜è¦æ–‡ä»¶ä¸­æå–å¢å¼ºç‰¹å¾ï¼Œç”¨äº GBDT æ¨¡å‹ä¼˜åŒ–ã€‚
"""
import pandas as pd
import numpy as np
import glob
import os
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore', category=RuntimeWarning)


def main():
    """ä¸»å‡½æ•°"""
    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç›®å½•
    run_dirs = sorted([d for d in glob.glob('results_combo_wfo/2025*') if os.path.isdir(d)])
    if not run_dirs:
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° 'results_combo_wfo/2025*' å¼€å¤´çš„è¿è¡Œç›®å½•ã€‚")
        return
        
    # ä»æœ€æ–°çš„ç›®å½•ä¸­æå– run_idï¼Œä¾‹å¦‚ '20251112_223854'
    latest_dir_basename = os.path.basename(run_dirs[-1])
    run_id = '_'.join(latest_dir_basename.split('_')[:2])
    
    print(f"ğŸ” æ­£åœ¨å¤„ç†æœ€æ–°çš„è¿è¡Œ: {run_id}")
    
    # æ„å»ºæ­£ç¡®çš„æ–‡ä»¶æœç´¢æ¨¡å¼ - æŸ¥æ‰¾å›æµ‹æ‘˜è¦æ–‡ä»¶
    file_pattern = f"results_combo_wfo/{run_id}_*/*.csv"
    print(f"ğŸ“‚ ä½¿ç”¨æ–‡ä»¶æ¨¡å¼: {file_pattern}")
    
    backtest_files = glob.glob(file_pattern)
    if not backtest_files:
        print(f"é”™è¯¯ï¼šåœ¨æ¨¡å¼ {file_pattern} ä¸­æ‰¾ä¸åˆ°ä»»ä½• CSV æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(backtest_files)} ä¸ªå›æµ‹æ‘˜è¦æ–‡ä»¶ã€‚")
    
    # è¯»å–æ‰€æœ‰å›æµ‹æ‘˜è¦æ–‡ä»¶å¹¶åˆå¹¶
    all_results = []
    for file in tqdm(backtest_files, desc="è¯»å–å›æµ‹æ‘˜è¦"):
        try:
            df = pd.read_csv(file)
            if 'combo' in df.columns and 'sharpe' in df.columns:
                all_results.append(df)
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {os.path.basename(file)} æ—¶å‡ºé”™: {e}")
            
    if not all_results:
        print("é”™è¯¯ï¼šæœªèƒ½ä»ä»»ä½•æ–‡ä»¶ä¸­è¯»å–æ•°æ®ã€‚")
        return

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    combined_df = pd.concat(all_results, ignore_index=True)
    
    # å»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
    combined_df = combined_df.drop_duplicates(subset=['combo'], keep='last')
    
    print(f"âœ… å·²åˆå¹¶ {len(combined_df)} ä¸ªå”¯ä¸€ç­–ç•¥çš„æ•°æ®ã€‚")
    
    # é€‰æ‹©æœ‰ç”¨çš„ç‰¹å¾åˆ—
    useful_features = [
        'combo',
        'sharpe',
        'max_dd',
        'vol',
        'annual_ret',
        'calmar_ratio',
        'sortino_ratio',
        'profit_factor',
        'win_rate',
        'avg_turnover',
        'avg_n_holdings',
        'max_consecutive_wins',
        'max_consecutive_losses'
    ]
    
    # æ£€æŸ¥å“ªäº›ç‰¹å¾å­˜åœ¨
    available_features = [f for f in useful_features if f in combined_df.columns]
    missing_features = [f for f in useful_features if f not in combined_df.columns]
    
    if missing_features:
        print(f"âš ï¸  ä»¥ä¸‹ç‰¹å¾ä¸å¯ç”¨: {missing_features}")
    
    # æå–å¯ç”¨ç‰¹å¾
    features_df = combined_df[available_features].copy()
    
    # è®¡ç®—æ´¾ç”Ÿç‰¹å¾
    print("\nğŸ“Š è®¡ç®—æ´¾ç”Ÿç‰¹å¾...")
    
    # 1. é£é™©è°ƒæ•´æ”¶ç›Šå¤åˆæŒ‡æ ‡
    features_df['sharpe_calmar_product'] = features_df['sharpe'] * features_df.get('calmar_ratio', 1)
    
    # 2. å›æ’¤æ¢å¤èƒ½åŠ› (annual_ret / max_dd)
    features_df['dd_recovery_ratio'] = features_df['annual_ret'] / (features_df['max_dd'].abs() + 1e-6)
    
    # 3. ç¨³å¥æ€§æŒ‡æ ‡ (sortino / sharpe)
    features_df['sortino_sharpe_ratio'] = features_df.get('sortino_ratio', 0) / (features_df['sharpe'] + 1e-6)
    
    # 4. èƒœç‡-ç›ˆäºæ¯”å¤åˆæŒ‡æ ‡
    if 'win_rate' in features_df.columns and 'profit_factor' in features_df.columns:
        features_df['win_rate_profit_composite'] = features_df['win_rate'] * np.log1p(features_df['profit_factor'])
    
    # 5. è¿èƒœè¿è´¥æ¯”
    if 'max_consecutive_wins' in features_df.columns and 'max_consecutive_losses' in features_df.columns:
        features_df['win_loss_streak_ratio'] = features_df['max_consecutive_wins'] / (features_df['max_consecutive_losses'] + 1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = f"results/{run_id}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜åˆ° Parquet æ–‡ä»¶
    output_path = os.path.join(output_dir, 'enhanced_features.parquet')
    features_df.to_parquet(output_path, index=False)
    
    print("\nâœ… å¢å¼ºç‰¹å¾æå–å®Œæˆï¼")
    print(f"   - å…±å¤„ç† {len(features_df)} ä¸ªç­–ç•¥ã€‚")
    print(f"   - å¯ç”¨ç‰¹å¾æ•°: {len(features_df.columns)}")
    print(f"   - ç‰¹å¾å·²ä¿å­˜åˆ°: {output_path}")
    
    print("\nğŸ“‹ ç‰¹å¾åˆ—è¡¨:")
    for i, col in enumerate(features_df.columns, 1):
        print(f"   {i:2d}. {col}")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. å°†æ­¤ 'enhanced_features.parquet' æ–‡ä»¶ä¸ 'ranking_baseline.parquet' åˆå¹¶ã€‚")
    print("   2. ä½¿ç”¨åˆå¹¶åçš„æ•°æ®é›†é‡æ–°è®­ç»ƒ GBDT æ¨¡å‹ã€‚")
    print("   3. è¿è¡Œ 'validate_ml_ranking_accuracy.py' éªŒè¯æ–°æ¨¡å‹çš„æ•ˆæœã€‚")

if __name__ == '__main__':
    main()
