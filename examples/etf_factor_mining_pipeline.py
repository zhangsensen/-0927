#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ¨ªæˆªé¢å› å­æŒ–æ˜å®Œæ•´æµæ°´çº¿
æ•´åˆå€™é€‰å› å­ç”Ÿæˆã€æ‰¹é‡è®¡ç®—ã€ICåˆ†æã€ç¨³å®šæ€§æµ‹è¯•ã€å¤šç»´ç­›é€‰å’Œåˆ†ç±»æ ‡æ³¨
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional

from factor_system.factor_engine.providers.etf_cross_section_provider import ETFCrossSectionDataManager
from factor_system.factor_engine.factors.etf_cross_section.candidate_factor_generator import ETFCandidateFactorGenerator
from factor_system.factor_engine.factors.etf_cross_section.batch_factor_calculator import BatchFactorCalculator, calculate_all_etf_factors
from factor_system.factor_engine.factors.etf_cross_section.ic_analyzer import ICAnalyzer
from factor_system.factor_engine.factors.etf_cross_section.stability_analyzer import StabilityAnalyzer
from factor_system.factor_engine.factors.etf_cross_section.factor_screener import FactorScreener, ScreeningCriteria, screen_etf_factors
from factor_system.factor_engine.factors.etf_cross_section.factor_classifier import classify_etf_factors

from factor_system.utils import safe_operation, FactorSystemError

logger = logging.getLogger(__name__)


class ETFFactorMiningPipeline:
    """ETFå› å­æŒ–æ˜æµæ°´çº¿"""

    def __init__(self, output_base_dir: str = None):
        """
        åˆå§‹åŒ–æŒ–æ˜æµæ°´çº¿

        Args:
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        """
        self.output_base_dir = Path(output_base_dir) if output_base_dir else Path("factor_system/factor_output/etf_cross_section/mining_results")
        self.output_base_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_dir = self.output_base_dir / f"run_{self.timestamp}"
        self.run_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        self.data_manager = ETFCrossSectionDataManager()
        self.factor_generator = ETFCandidateFactorGenerator()
        self.calculator = BatchFactorCalculator()
        self.ic_analyzer = ICAnalyzer()
        self.stability_analyzer = StabilityAnalyzer(self.ic_analyzer)
        self.screener = FactorScreener()

        logger.info(f"ETFå› å­æŒ–æ˜æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è¿è¡Œç›®å½•: {self.run_dir}")

    def load_price_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        åŠ è½½ä»·æ ¼æ•°æ®

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            ä»·æ ¼æ•°æ®
        """
        logger.info(f"åŠ è½½ä»·æ ¼æ•°æ®: {start_date} ~ {end_date}")

        try:
            # è·å–ETFåˆ—è¡¨
            etf_list = self.data_manager.get_etf_list()
            logger.info(f"æ‰¾åˆ° {len(etf_list)} åªETF")

            # åŠ è½½ä»·æ ¼æ•°æ®
            price_data = self.data_manager.load_price_data(etf_list, start_date, end_date)

            if price_data is None or price_data.empty:
                raise ValueError("æ— æ³•åŠ è½½ä»·æ ¼æ•°æ®")

            logger.info(f"æˆåŠŸåŠ è½½ä»·æ ¼æ•°æ®: {len(price_data)} æ¡è®°å½•")
            logger.info(f"æ—¥æœŸèŒƒå›´: {price_data['date'].min()} ~ {price_data['date'].max()}")
            logger.info(f"ETFæ•°é‡: {price_data['symbol'].nunique()}")

            # ä¿å­˜ä»·æ ¼æ•°æ®
            price_file = self.run_dir / "price_data.parquet"
            price_data.to_parquet(price_file, index=False)
            logger.info(f"ä»·æ ¼æ•°æ®å·²ä¿å­˜åˆ°: {price_file}")

            return price_data

        except Exception as e:
            logger.error(f"åŠ è½½ä»·æ ¼æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def generate_candidate_factors(self) -> List:
        """
        ç”Ÿæˆå€™é€‰å› å­

        Returns:
            å€™é€‰å› å­åˆ—è¡¨
        """
        logger.info("ç”Ÿæˆå€™é€‰å› å­...")

        try:
            variants = self.factor_generator.generate_all_variants()

            logger.info(f"æˆåŠŸç”Ÿæˆ {len(variants)} ä¸ªå€™é€‰å› å­")

            # ä¿å­˜å€™é€‰å› å­åˆ—è¡¨
            variants_file = self.run_dir / "candidate_factors.csv"
            self.factor_generator.save_variants_to_file(variants, str(variants_file))

            return variants

        except Exception as e:
            logger.error(f"ç”Ÿæˆå€™é€‰å› å­å¤±è´¥: {str(e)}")
            raise

    def calculate_factors(self, variants: List, price_data: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        è®¡ç®—å› å­æ•°æ®

        Args:
            variants: å€™é€‰å› å­åˆ—è¡¨
            price_data: ä»·æ ¼æ•°æ®

        Returns:
            å› å­æ•°æ®å­—å…¸
        """
        logger.info(f"å¼€å§‹è®¡ç®— {len(variants)} ä¸ªå› å­...")

        try:
            # å‡†å¤‡å‚æ•°
            symbols = price_data['symbol'].unique().tolist()
            start_date = price_data['date'].min().strftime('%Y-%m-%d')
            end_date = price_data['date'].max().strftime('%Y-%m-%d')

            # åˆ›å»ºå› å­è®¡ç®—ç›®å½•
            factor_dir = self.run_dir / "calculated_factors"
            factor_dir.mkdir(exist_ok=True)

            # æ‰¹é‡è®¡ç®—å› å­
            factors_data = self.calculator.calculate_factors_batch(
                variants=variants,
                symbols=symbols,
                timeframe="daily",
                start_date=pd.to_datetime(start_date),
                end_date=pd.to_datetime(end_date),
                output_dir=str(factor_dir)
            )

            logger.info(f"å› å­è®¡ç®—å®Œæˆ: {len(factors_data)}/{len(variants)} ä¸ªå› å­æˆåŠŸ")

            return factors_data

        except Exception as e:
            logger.error(f"å› å­è®¡ç®—å¤±è´¥: {str(e)}")
            raise

    def analyze_factors(self, factors_data: Dict[str, pd.DataFrame],
                       price_data: pd.DataFrame) -> tuple:
        """
        åˆ†æå› å­ï¼ˆICåˆ†æå’Œç¨³å®šæ€§åˆ†æï¼‰

        Args:
            factors_data: å› å­æ•°æ®
            price_data: ä»·æ ¼æ•°æ®

        Returns:
            (ICåˆ†æç»“æœ, ç¨³å®šæ€§åˆ†æç»“æœ)
        """
        logger.info("å¼€å§‹å› å­åˆ†æ...")

        try:
            # ICåˆ†æ
            logger.info("æ‰§è¡ŒICåˆ†æ...")
            ic_results = self.ic_analyzer.batch_analyze_factors(factors_data, price_data)

            # ä¿å­˜ICåˆ†æç»“æœ
            ic_file = self.run_dir / "ic_analysis.csv"
            self.ic_analyzer.save_ic_analysis_results(ic_results, str(ic_file))

            logger.info(f"ICåˆ†æå®Œæˆ: {len(ic_results)} ä¸ªå› å­")

            # ç¨³å®šæ€§åˆ†æ
            logger.info("æ‰§è¡Œç¨³å®šæ€§åˆ†æ...")
            stability_results = self.stability_analyzer.batch_analyze_stability(factors_data, price_data)

            # ä¿å­˜ç¨³å®šæ€§åˆ†æç»“æœ
            stability_file = self.run_dir / "stability_analysis.csv"
            self.stability_analyzer.save_stability_results(stability_results, str(stability_file))

            logger.info(f"ç¨³å®šæ€§åˆ†æå®Œæˆ: {len(stability_results)} ä¸ªå› å­")

            return ic_results, stability_results

        except Exception as e:
            logger.error(f"å› å­åˆ†æå¤±è´¥: {str(e)}")
            raise

    def screen_factors(self, factors_data: Dict[str, pd.DataFrame],
                      price_data: pd.DataFrame,
                      criteria: Optional[ScreeningCriteria] = None) -> Dict:
        """
        ç­›é€‰å› å­

        Args:
            factors_data: å› å­æ•°æ®
            price_data: ä»·æ ¼æ•°æ®
            criteria: ç­›é€‰æ ‡å‡†

        Returns:
            ç­›é€‰ç»“æœ
        """
        logger.info("å¼€å§‹å› å­ç­›é€‰...")

        try:
            screening_results = screen_etf_factors(
                factors_data=factors_data,
                price_data=price_data,
                criteria=criteria,
                output_dir=str(self.run_dir)
            )

            passed_count = sum(1 for r in screening_results.values() if r.screening_reason == "é€šè¿‡ç­›é€‰")
            logger.info(f"å› å­ç­›é€‰å®Œæˆ: {passed_count}/{len(factors_data)} ä¸ªå› å­é€šè¿‡ç­›é€‰")

            return screening_results

        except Exception as e:
            logger.error(f"å› å­ç­›é€‰å¤±è´¥: {str(e)}")
            raise

    def classify_factors(self, screening_results: Dict) -> Dict:
        """
        åˆ†ç±»å› å­

        Args:
            screening_results: ç­›é€‰ç»“æœ

        Returns:
            åˆ†ç±»ç»“æœ
        """
        logger.info("å¼€å§‹å› å­åˆ†ç±»...")

        try:
            classification_results = classify_etf_factors(
                screening_results=screening_results,
                output_dir=str(self.run_dir)
            )

            logger.info(f"å› å­åˆ†ç±»å®Œæˆ: {len(classification_results)} ä¸ªå› å­å·²åˆ†ç±»")

            return classification_results

        except Exception as e:
            logger.error(f"å› å­åˆ†ç±»å¤±è´¥: {str(e)}")
            raise

    def generate_final_report(self, screening_results: Dict,
                            classification_results: Dict) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

        Args:
            screening_results: ç­›é€‰ç»“æœ
            classification_results: åˆ†ç±»ç»“æœ

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        logger.info("ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")

        try:
            report_file = self.run_dir / "final_report.md"

            # ç»Ÿè®¡ä¿¡æ¯
            total_factors = len(screening_results)
            passed_factors = [r for r in screening_results.values() if r.screening_reason == "é€šè¿‡ç­›é€‰"]
            classified_factors = classification_results

            # ç”ŸæˆæŠ¥å‘Šå†…å®¹
            report_content = f"""# ETFæ¨ªæˆªé¢å› å­æŒ–æ˜æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- è¿è¡ŒID: {self.timestamp}

## æŒ–æ˜ç»“æœç»Ÿè®¡
- æ€»å€™é€‰å› å­æ•°: {total_factors}
- é€šè¿‡ç­›é€‰å› å­æ•°: {len(passed_factors)}
- æœ€ç»ˆåˆ†ç±»å› å­æ•°: {len(classified_factors)}
- é€šè¿‡ç‡: {len(passed_factors)/total_factors:.1%}

## ç­›é€‰æ ‡å‡†
- æœ€å°ICå‡å€¼: {self.screener.criteria.min_ic_mean}
- æœ€å¤§IC på€¼: {self.screener.criteria.max_ic_pvalue}
- æœ€å°ICèƒœç‡: {self.screener.criteria.min_ic_win_rate}
- æœ€å°ç¨³å®šæ€§è¯„åˆ†: {self.screener.criteria.min_stability_score}
- æœ€å¤§ç›¸å…³æ€§é˜ˆå€¼: {self.screener.criteria.max_correlation}

## åˆ†ç±»ç»Ÿè®¡
"""

            # åˆ†ç±»ç»Ÿè®¡
            category_stats = {}
            for factor in classification_results.values():
                category = factor.category
                category_stats[category] = category_stats.get(category, 0) + 1

            for category, count in sorted(category_stats.items()):
                report_content += f"- {category}: {count} ä¸ªå› å­\n"

            # é¡¶çº§å› å­åˆ—è¡¨
            report_content += f"""
## é¡¶çº§å› å­ï¼ˆå‰20åï¼‰

| æ’å | å› å­ID | ç±»åˆ« | ICå‡å€¼ | ç¨³å®šæ€§è¯„åˆ† | ç»¼åˆè¯„åˆ† |
|------|--------|------|--------|------------|----------|
"""

            # æ’åºå¹¶æ˜¾ç¤ºå‰20å
            passed_factors_sorted = sorted(
                [r for r in screening_results.values() if r.screening_reason == "é€šè¿‡ç­›é€‰"],
                key=lambda x: x.overall_score,
                reverse=True
            )

            for i, result in enumerate(passed_factors_sorted[:20]):
                category = classification_results.get(result.variant_id)
                category_name = category.category if category else "æœªåˆ†ç±»"

                report_content += f"| {i+1} | {result.variant_id} | {category_name} | {result.ic_mean:.4f} | {result.stability_score:.3f} | {result.overall_score:.2f} |\n"

            # è¯¦ç»†åˆ†æ
            report_content += f"""
## è¯¦ç»†åˆ†æ

### ICåˆ†æç»“æœ
- å¹³å‡ICå‡å€¼: {np.mean([r.ic_mean for r in passed_factors]):.4f}
- å¹³å‡ICèƒœç‡: {np.mean([r.ic_win_rate for r in passed_factors]):.2%}
- æœ€å¤§ICå‡å€¼: {max([r.ic_mean for r in passed_factors]):.4f}
- æœ€å°ICå‡å€¼: {min([r.ic_mean for r in passed_factors]):.4f}

### ç¨³å®šæ€§åˆ†æç»“æœ
- å¹³å‡ç¨³å®šæ€§è¯„åˆ†: {np.mean([r.stability_score for r in passed_factors]):.3f}
- æœ€é«˜ç¨³å®šæ€§è¯„åˆ†: {max([r.stability_score for r in passed_factors]):.3f}
- æœ€ä½ç¨³å®šæ€§è¯„åˆ†: {min([r.stability_score for r in passed_factors]):.3f}

### ç±»åˆ«åˆ†å¸ƒ
"""

            for category, count in sorted(category_stats.items()):
                percentage = count / len(classified_factors) * 100
                report_content += f"- {category}: {count} ä¸ª ({percentage:.1f}%)\n"

            report_content += f"""
## æ–‡ä»¶è¾“å‡º
æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°: {self.run_dir}

ä¸»è¦æ–‡ä»¶:
- price_data.parquet: åŸå§‹ä»·æ ¼æ•°æ®
- candidate_factors.csv: å€™é€‰å› å­åˆ—è¡¨
- calculated_factors/: è®¡ç®—åçš„å› å­æ•°æ®
- ic_analysis.csv: ICåˆ†æç»“æœ
- stability_analysis.csv: ç¨³å®šæ€§åˆ†æç»“æœ
- factor_screening_*.csv: ç­›é€‰ç»“æœ
- factor_classification_*.csv: åˆ†ç±»ç»“æœ
- final_report.md: æœ¬æŠ¥å‘Š

## ä½¿ç”¨å»ºè®®
1. ä¼˜å…ˆä½¿ç”¨ç»¼åˆè¯„åˆ†é«˜çš„å› å­
2. æ ¹æ®æŠ•èµ„é£æ ¼é€‰æ‹©ä¸åŒç±»åˆ«çš„å› å­
3. æ³¨æ„å› å­çš„é€‚ç”¨åœºæ™¯å’Œé£é™©ç‰¹å¾
4. å»ºè®®ç»“åˆå¤šä¸ªä¸åŒç±»åˆ«çš„å› å­ä½¿ç”¨

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

            # ä¿å­˜æŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            logger.info(f"æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            return str(report_file)

        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            raise

    def run_pipeline(self, start_date: str, end_date: str,
                    criteria: Optional[ScreeningCriteria] = None) -> Dict:
        """
        è¿è¡Œå®Œæ•´æŒ–æ˜æµæ°´çº¿

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            criteria: ç­›é€‰æ ‡å‡†

        Returns:
            æµæ°´çº¿ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹è¿è¡ŒETFå› å­æŒ–æ˜æµæ°´çº¿...")
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")

        pipeline_results = {}

        try:
            # æ­¥éª¤1: åŠ è½½ä»·æ ¼æ•°æ®
            price_data = self.load_price_data(start_date, end_date)
            pipeline_results['price_data'] = price_data

            # æ­¥éª¤2: ç”Ÿæˆå€™é€‰å› å­
            variants = self.generate_candidate_factors()
            pipeline_results['variants'] = variants

            # æ­¥éª¤3: è®¡ç®—å› å­
            factors_data = self.calculate_factors(variants, price_data)
            pipeline_results['factors_data'] = factors_data

            # æ­¥éª¤4: åˆ†æå› å­
            ic_results, stability_results = self.analyze_factors(factors_data, price_data)
            pipeline_results['ic_results'] = ic_results
            pipeline_results['stability_results'] = stability_results

            # æ­¥éª¤5: ç­›é€‰å› å­
            screening_results = self.screen_factors(factors_data, price_data, criteria)
            pipeline_results['screening_results'] = screening_results

            # æ­¥éª¤6: åˆ†ç±»å› å­
            classification_results = self.classify_factors(screening_results)
            pipeline_results['classification_results'] = classification_results

            # æ­¥éª¤7: ç”ŸæˆæŠ¥å‘Š
            report_file = self.generate_final_report(screening_results, classification_results)
            pipeline_results['report_file'] = report_file

            logger.info("ETFå› å­æŒ–æ˜æµæ°´çº¿è¿è¡Œå®Œæˆï¼")
            logger.info(f"ç»“æœä¿å­˜åœ¨: {self.run_dir}")
            logger.info(f"æœ€ç»ˆæŠ¥å‘Š: {report_file}")

            return pipeline_results

        except Exception as e:
            logger.error(f"æµæ°´çº¿è¿è¡Œå¤±è´¥: {str(e)}")
            raise


@safe_operation
def main():
    """ä¸»å‡½æ•° - è¿è¡ŒETFå› å­æŒ–æ˜æµæ°´çº¿"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('etf_factor_mining.log'),
            logging.StreamHandler()
        ]
    )

    # è¿è¡Œå‚æ•°
    start_date = "2024-01-01"
    end_date = "2025-10-14"

    # è‡ªå®šä¹‰ç­›é€‰æ ‡å‡†ï¼ˆå¯é€‰ï¼‰
    criteria = ScreeningCriteria(
        min_ic_mean=0.015,          # ç¨å¾®é™ä½ICè¦æ±‚
        max_ic_pvalue=0.1,          # ç¨å¾®æ”¾å®½æ˜¾è‘—æ€§è¦æ±‚
        min_ic_win_rate=0.45,       # é™ä½èƒœç‡è¦æ±‚
        min_stability_score=0.65,   # é™ä½ç¨³å®šæ€§è¦æ±‚
        max_correlation=0.9,        # æ”¾å®½ç›¸å…³æ€§è¦æ±‚
        min_monotonicity_r2=0.7,    # é™ä½å•è°ƒæ€§è¦æ±‚
        min_sample_size=20          # é™ä½æ ·æœ¬æ•°è¦æ±‚
    )

    try:
        # åˆå§‹åŒ–å¹¶è¿è¡Œæµæ°´çº¿
        pipeline = ETFFactorMiningPipeline()
        results = pipeline.run_pipeline(start_date, end_date, criteria)

        # æ‰“å°æ‘˜è¦
        screening_results = results['screening_results']
        classification_results = results['classification_results']

        passed_count = sum(1 for r in screening_results.values() if r.screening_reason == "é€šè¿‡ç­›é€‰")
        total_count = len(screening_results)

        print(f"\n{'='*60}")
        print("ğŸ‰ ETFå› å­æŒ–æ˜æµæ°´çº¿è¿è¡Œå®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ğŸ“Š æŒ–æ˜ç»Ÿè®¡:")
        print(f"   æ€»å€™é€‰å› å­: {total_count}")
        print(f"   é€šè¿‡ç­›é€‰: {passed_count}")
        print(f"   æœ€ç»ˆä¿ç•™: {len(classification_results)}")
        print(f"   é€šè¿‡ç‡: {passed_count/total_count:.1%}")
        print(f"\nğŸ“ ç»“æœç›®å½•: {pipeline.run_dir}")
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Š: {results['report_file']}")
        print(f"{'='*60}")

    except Exception as e:
        logger.error(f"æµæ°´çº¿è¿è¡Œå¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    main()