#!/usr/bin/env python3
"""
ç­‰å¾…å›æµ‹å®Œæˆå¹¶è¿è¡Œæ¯”è¾ƒåˆ†æçš„è„šæœ¬
"""
import subprocess
import time
import sys
from pathlib import Path
import pandas as pd
import json
import numpy as np

def wait_for_backtest_completion():
    """ç­‰å¾…å›æµ‹å®Œæˆ"""
    pid_file = Path("results/logs/rb_full_all_combos.pid")

    if not pid_file.exists():
        print("âŒ PIDæ–‡ä»¶ä¸å­˜åœ¨")
        return False

    pid = int(pid_file.read_text().strip())
    print(f"ğŸ” ç›‘æ§å›æµ‹è¿›ç¨‹ PID: {pid}")

    while True:
        try:
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
            result = subprocess.run(['ps', '-p', str(pid)],
                                      capture_output=True, text=True)
            if str(pid) not in result.stdout:
                print(f"âœ… å›æµ‹è¿›ç¨‹ {pid} å·²å®Œæˆ")
                return True

            # æ˜¾ç¤ºè¿›åº¦
            log_files = list(Path("results/logs").glob("rb_full_all_combos_8d_*.log"))
            if log_files:
                latest_log = max(log_files, key=lambda p: p.stat().st_mtime)
                try:
                    with open(latest_log, 'r') as f:
                        lines = f.readlines()
                        if lines:
                            last_line = lines[-1].strip()
                            if "Done" in last_line:
                                # æå–è¿›åº¦ä¿¡æ¯
                                import re
                                match = re.search(r'Done (\d+) tasks', last_line)
                                if match:
                                    completed = int(match.group(1))
                                    progress = completed / 12597 * 100
                                    print(f"ğŸ“Š è¿›åº¦: {progress:.1f}% ({completed}/12597)")
                except:
                    pass

            print(f"â³ ç­‰å¾…å›æµ‹å®Œæˆ... (PID {pid} ä»åœ¨è¿è¡Œ)")
            time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç›‘æ§")
            return False
        except Exception as e:
            print(f"âŒ ç›‘æ§é”™è¯¯: {e}")
            return False

def run_comparison():
    """è¿è¡Œæ¯”è¾ƒåˆ†æ"""
    print("ğŸ” å¼€å§‹è¿è¡Œæ¯”è¾ƒåˆ†æ...")

    root = Path("results")

    # è·å–æœ€æ–°è¿è¡Œç›®å½•
    link = root / "run_latest"
    if link.exists():
        latest_dir = link.resolve()
    else:
        latest_dir = sorted([p for p in root.glob("run_*") if p.is_dir()],
                           key=lambda p: p.name)[-1]

    # è·å–å‰ä¸€è¿è¡Œç›®å½•
    cands = sorted([p for p in root.glob("run_*") if p.is_dir()],
                   key=lambda p: p.name)
    prev_dir = None
    for p in reversed(cands):
        if p.name != latest_dir.name:
            prev_dir = p
            break

    print(f"ğŸ“ æœ€æ–°è¿è¡Œ: {latest_dir.name}")
    if prev_dir:
        print(f"ğŸ“ å‰ä¸€è¿è¡Œ: {prev_dir.name}")

    # åŠ è½½å¹¶è®¡ç®—æ¯”è¾ƒ
    new_df = pd.read_parquet(latest_dir / "all_combos.parquet")

    # æ’åºè¾…åŠ©å‡½æ•°
    sort_old = lambda df: df.sort_values(['mean_oos_ic', 'stability_score'],
                                        ascending=[False, False])

    def sort_new(df):
        if 'calibrated_sharpe_pred' in df.columns:
            return df.sort_values(['calibrated_sharpe_pred', 'stability_score'],
                                   ascending=[False, False])
        elif 'calibrated_sharpe_full' in df.columns:
            return df.sort_values(['calibrated_sharpe_full', 'stability_score'],
                                   ascending=[False, False])
        else:
            return sort_old(df)

    new_sorted = sort_new(new_df)
    ks = [100, 500, 1000, 2000]

    res = {
        'latest': latest_dir.name,
        'new_mean_ic': float(new_df['mean_oos_ic'].mean())
    }

    if 'calibrated_sharpe_pred' in new_df.columns:
        res['calibrated_mean'] = float(new_df['calibrated_sharpe_pred'].mean())
    elif 'calibrated_sharpe_full' in new_df.columns:
        res['calibrated_mean'] = float(new_df['calibrated_sharpe_full'].mean())

    thr_new = np.percentile(new_df['mean_oos_ic'], 80)
    res['precision_ic_latest'] = {
        k: float((new_sorted.head(k)['mean_oos_ic'] > thr_new).mean())
        for k in ks
    }

    if prev_dir and (prev_dir / "all_combos.parquet").exists():
        old_df = pd.read_parquet(prev_dir / "all_combos.parquet")
        old_sorted = sort_old(old_df)

        overlaps = {}
        for k in ks:
            old_top = set(old_sorted.head(k)['combo'])
            new_top = set(new_sorted.head(k)['combo'])
            ov = len(old_top & new_top)
            overlaps[k] = {
                'overlap_count': ov,
                'overlap_ratio': ov / max(1, len(old_top))
            }

        res['overlap'] = overlaps
        res['previous'] = prev_dir.name
        res['old_mean_ic'] = float(old_df['mean_oos_ic'].mean())

        thr_old = np.percentile(old_df['mean_oos_ic'], 80)
        res['precision_ic_prev'] = {
            k: float((old_sorted.head(k)['mean_oos_ic'] > thr_old).mean())
            for k in ks
        }

    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    cmp_dir = latest_dir / "comparison"
    cmp_dir.mkdir(exist_ok=True)

    (cmp_dir / "comparison_metrics.json").write_text(
        json.dumps(res, indent=2, ensure_ascii=False)
    )

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    lines = []
    lines.append(f"# æ’åºå¯¹æ¯”æŠ¥å‘Š ({latest_dir.name} vs {res.get('previous', 'N/A')})\n")
    lines.append("## æ‘˜è¦\n")
    lines.append(f"- æœ€æ–° run: {latest_dir.name}\n")
    if 'previous' in res:
        lines.append(f"- å‰ä¸€ run: {res['previous']}\n")
    lines.append(f"- æœ€æ–° mean_oos_ic: {res['new_mean_ic']:.6f}\n")
    if 'calibrated_mean' in res:
        lines.append(f"- æ ¡å‡†åˆ†å‡å€¼: {res['calibrated_mean']:.6f}\n")
    lines.append("\n## Overlap & Precision@K\n")

    if 'overlap' in res:
        for k, v in res['overlap'].items():
            lp = res['precision_ic_latest'].get(k, float('nan'))
            op = res.get('precision_ic_prev', {}).get(k, float('nan'))
            lines.append(f"- K={k}: overlap={v['overlap_count']} ({v['overlap_ratio']*100:.1f}%), "
                         f"prev_P@K={op:.3f}, new_P@K={lp:.3f}\n")
    else:
        for k, lp in res['precision_ic_latest'].items():
            lines.append(f"- K={k}: new_P@K={lp:.3f}\n")

    lines.append("\n## å¤‡æ³¨\n")
    lines.append("- æœ¬æ¬¡ç”Ÿäº§å›æµ‹é‡‡ç”¨æ ¡å‡†ä¼˜å…ˆæ’åºé€»è¾‘\n")
    lines.append("- è‹¥æ ¡å‡†åˆ—ç¼ºå¤±åˆ™å›é€€åˆ°ICæ’åº\n")

    (cmp_dir / "FINAL_REPORT.md").write_text(
        ''.join(lines), encoding='utf-8'
    )

    print(f"âœ… æ¯”è¾ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {cmp_dir}")
    print(f"ğŸ“„ JSON: {cmp_dir / 'comparison_metrics.json'}")
    print(f"ğŸ“„ Markdown: {cmp_dir / 'FINAL_REPORT.md'}")

    return True

def main():
    print("ğŸš€ å¯åŠ¨å›æµ‹ç›‘æ§å’Œæ¯”è¾ƒåˆ†æ...")

    # ç­‰å¾…å›æµ‹å®Œæˆ
    if wait_for_backtest_completion():
        # è¿è¡Œæ¯”è¾ƒåˆ†æ
        if run_comparison():
            print("ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
        else:
            print("âŒ æ¯”è¾ƒåˆ†æå¤±è´¥")
            sys.exit(1)
    else:
        print("âŒ å›æµ‹ç›‘æ§å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()