# ğŸ”¬ ML ç®—æ³•æŠ€æœ¯è§„èŒƒä¹¦

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-11-16  
**ä½œè€…**: ç³»ç»Ÿå®¡è®¡  
**é€‚ç”¨èŒƒå›´**: etf_rotation_optimized + etf_rotation_experiments

---

## ç›®å½•

1. [WFO æ ¡å‡†å™¨ç®—æ³•](#wfo-æ ¡å‡†å™¨ç®—æ³•)
2. [Top200 ç­›é€‰ç®—æ³•](#top200-ç­›é€‰ç®—æ³•)
3. [ç®—æ³•é›†æˆä¸æµç¨‹](#ç®—æ³•é›†æˆä¸æµç¨‹)
4. [æ€§èƒ½åŸºå‡†æµ‹è¯•](#æ€§èƒ½åŸºå‡†æµ‹è¯•)
5. [æ•…éšœæ’æŸ¥æŒ‡å—](#æ•…éšœæ’æŸ¥æŒ‡å—)

---

## WFO æ ¡å‡†å™¨ç®—æ³•

### ğŸ“‹ ç®—æ³•æ¦‚è¿°

**æ–‡ä»¶**: `etf_rotation_optimized/core/wfo_realbt_calibrator.py`  
**é—®é¢˜**: WFO IC ä¸çœŸå® Sharpe ç›¸å…³æ€§ä»… 0.07ï¼ˆæ— å®é™…é¢„æµ‹èƒ½åŠ›ï¼‰  
**è§£å†³**: ç”¨ç›‘ç£å­¦ä¹ å­¦ä¹ æ˜ å°„ f: [WFO_Features] â†’ Sharpe_Real

### ğŸ”§ è¯¦ç»†æŠ€æœ¯è§„èŒƒ

#### é˜¶æ®µ 1: ç‰¹å¾æå– (Feature Engineering)

```python
def extract_features(self, wfo_df: pd.DataFrame) -> np.ndarray:
    """
    ä» WFO ç»“æœä¸­æå– 5 ä¸ªç‰¹å¾
    
    è¾“å…¥:
        wfo_df: WFO ç»“æœ DataFrame
                å¿…éœ€åˆ—: ['mean_oos_ic', 'oos_ic_std', 'combo_size', ...]
    
    è¾“å‡º:
        X: shape (n_combos, 5) çš„ç‰¹å¾çŸ©é˜µ
    """
```

**ç‰¹å¾å®šä¹‰è¡¨**:

| ç‰¹å¾ID | ç‰¹å¾å | å…¬å¼/æ¥æº | èŒƒå›´ | ç¼ºå¤±å¤„ç† |
|--------|--------|---------|------|---------|
| 0 | `mean_oos_ic` | WFO OOS çª—å£ IC å‡å€¼ | [-0.04, 0.16] | ä¸­ä½æ•°å¡«å…… |
| 1 | `oos_ic_std` | æ ‡å‡†å·®ï¼ˆç¨³å®šæ€§ï¼‰ | [0.01, 0.08] | ä¸­ä½æ•°å¡«å…… |
| 2 | `positive_rate` | (IC>0çš„çª—å£) / æ€»çª—å£ | [0.3, 0.9] | 0.5 å¡«å…… |
| 3 | `stability_score` | 1 - (ic_std/ic_mean) | [0.0, 1.0] | 0.5 å¡«å…… |
| 4 | `combo_size` | ç»„åˆä¸­å› å­æ•°é‡ | [2, 5] | æ— ç¼ºå¤± |

**å®ç°ä¼ªä»£ç **:

```python
def extract_features(self, wfo_df):
    X = np.zeros((len(wfo_df), 5))
    
    # ç‰¹å¾ 0: mean_oos_ic
    X[:, 0] = wfo_df['mean_oos_ic'].fillna(wfo_df['mean_oos_ic'].median())
    
    # ç‰¹å¾ 1: oos_ic_std
    X[:, 1] = wfo_df['oos_ic_std'].fillna(wfo_df['oos_ic_std'].median())
    
    # ç‰¹å¾ 2: positive_rate = (ic_positive_count / total_windows)
    X[:, 2] = (wfo_df['positive_count'] / wfo_df['total_windows']).fillna(0.5)
    
    # ç‰¹å¾ 3: stability_score
    with np.errstate(divide='ignore', invalid='ignore'):
        stability = 1 - (X[:, 1] / X[:, 0])
        stability = np.where(np.isnan(stability), 0.5, stability)
        stability = np.clip(stability, 0, 1)
    X[:, 3] = stability
    
    # ç‰¹å¾ 4: combo_size
    X[:, 4] = wfo_df['combo_size'].values
    
    return X
```

#### é˜¶æ®µ 2: æ•°æ®é¢„å¤„ç† (Preprocessing)

```python
def preprocess(self, X: np.ndarray) -> np.ndarray:
    """
    æ ‡å‡†åŒ–å¤„ç†ç‰¹å¾å‘é‡
    
    æ ‡å‡†åŒ–æ–¹æ³•: Z-score (mean=0, std=1)
    """
    
    from sklearn.preprocessing import StandardScaler
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, scaler
```

**æ ‡å‡†åŒ–å…¬å¼**:

$$x_{normalized} = \frac{x - \mu}{\sigma}$$

å…¶ä¸­ $\mu$ æ˜¯ç‰¹å¾å‡å€¼ï¼Œ$\sigma$ æ˜¯æ ‡å‡†å·®ã€‚

#### é˜¶æ®µ 3: æ¨¡å‹è®­ç»ƒ (Model Training)

##### æ–¹æ¡ˆ A: Ridge å›å½’

```python
class RidgeCalibrator:
    def __init__(self, alpha=1.0):
        from sklearn.linear_model import Ridge
        self.model = Ridge(alpha=alpha)
        
    def fit(self, X_train, y_train):
        self.model.fit(X_train, y_train)
        
    def predict(self, X_test):
        return self.model.predict(X_test)
    
    def score(self, X_test, y_test):
        return self.model.score(X_test, y_test)  # è¿”å› RÂ²
```

**æ¨¡å‹å‚æ•°**:
- `alpha` = 1.0ï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰
- æŸå¤±å‡½æ•°: $L = ||y - X\beta||^2 + \alpha||\beta||^2$

**è¶…å‚è°ƒä¼˜**:
- alpha âˆˆ [0.1, 1.0, 10.0] via GridSearchCV

**é¢„æœŸæ€§èƒ½**:
- RÂ² â‰ˆ 0.12-0.15
- è®¡ç®—å¤æ‚åº¦: O(n Ã— dÂ²) å…¶ä¸­ d=5

##### æ–¹æ¡ˆ B: æ¢¯åº¦æå‡æ ‘ (GBDT)

```python
class GBDTCalibrator:
    def __init__(self, n_estimators=300, max_depth=5, learning_rate=0.1):
        from sklearn.ensemble import GradientBoostingRegressor
        self.model = GradientBoostingRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=0.8,
            random_state=42
        )
        
    def fit(self, X_train, y_train, sample_weight=None):
        self.model.fit(X_train, y_train, sample_weight=sample_weight)
        
    def predict(self, X_test):
        return self.model.predict(X_test)
    
    def feature_importance(self):
        """è¿”å›ç‰¹å¾é‡è¦åº¦ (sum of Gini importance)"""
        return self.model.feature_importances_
```

**æ¨¡å‹å‚æ•°**:
- `n_estimators` = 300ï¼ˆæ ‘çš„æ•°é‡ï¼‰
- `max_depth` = 5ï¼ˆæ ‘çš„æœ€å¤§æ·±åº¦ï¼‰
- `learning_rate` = 0.1ï¼ˆå­¦ä¹ ç‡ï¼‰
- `subsample` = 0.8ï¼ˆè¡Œé‡‡æ ·ç‡ï¼‰

**æ¢¯åº¦æå‡æµç¨‹**:

$$F_0(x) = \arg\min_{\gamma} \sum_i L(y_i, \gamma)$$

$$F_{m}(x) = F_{m-1}(x) + \eta \cdot h_m(x)$$

å…¶ä¸­ $h_m$ æ˜¯ç¬¬ m æ£µæ ‘ï¼Œ$\eta$ æ˜¯å­¦ä¹ ç‡ã€‚

**ç‰¹å¾é‡è¦åº¦è®¡ç®—**:

$$\text{Importance}_j = \frac{\sum_{t=1}^T I(v(t) = j)}{\sum_{t=1}^T I(v(t) \ne \text{leaf})}$$

**é¢„æœŸæ€§èƒ½**:
- RÂ² â‰ˆ 0.18-0.22
- è®¡ç®—å¤æ‚åº¦: O(T Ã— n Ã— d Ã— \log n) å…¶ä¸­ T=300

##### æ–¹æ¡ˆ C: å †å é›†æˆ (Stacking)

```python
class StackingCalibrator:
    def __init__(self):
        from sklearn.ensemble import StackingRegressor
        from sklearn.linear_model import Ridge
        from sklearn.ensemble import GradientBoostingRegressor
        
        # Base learners
        estimators = [
            ('ridge', Ridge(alpha=1.0)),
            ('gbdt', GradientBoostingRegressor(n_estimators=300, max_depth=5))
        ]
        
        # Meta learner
        final_estimator = Ridge(alpha=0.5)
        
        self.model = StackingRegressor(
            estimators=estimators,
            final_estimator=final_estimator,
            cv=5
        )
```

**å †å æµç¨‹**:

```
Layer 0 (Base Learners):
â”œâ”€â”€ Ridge æ¨¡å‹
â””â”€â”€ GBDT æ¨¡å‹
    â†“ (è¾“å‡ºå…ƒç‰¹å¾)
Layer 1 (Meta Learner):
â””â”€â”€ Ridge æ¨¡å‹
    â†“
æœ€ç»ˆé¢„æµ‹
```

**é¢„æœŸæ€§èƒ½**:
- RÂ² â‰ˆ 0.20-0.24
- è®¡ç®—å¤æ‚åº¦: O(T Ã— 2 Ã— n Ã— d)ï¼ˆæ¯”å•ä¸ª GBDT é«˜ 2 å€ï¼‰

#### é˜¶æ®µ 4: äº¤å‰éªŒè¯ (Cross Validation)

```python
def cross_validate(self, X, y, cv=5):
    """
    5-Fold äº¤å‰éªŒè¯
    """
    from sklearn.model_selection import cross_validate
    
    cv_results = cross_validate(
        self.model,
        X, y,
        cv=cv,
        scoring=['r2', 'neg_mean_squared_error'],
        return_train_score=True
    )
    
    # ç»“æœç»Ÿè®¡
    cv_r2_mean = cv_results['test_r2'].mean()
    cv_r2_std = cv_results['test_r2'].std()
    
    print(f"CV RÂ² = {cv_r2_mean:.4f} Â± {cv_r2_std:.4f}")
    
    return cv_results
```

**CV æµç¨‹**:

```
Fold 1: Train[80%] â†’ Test[20%] â†’ Score_1
Fold 2: Train[80%] â†’ Test[20%] â†’ Score_2
Fold 3: Train[80%] â†’ Test[20%] â†’ Score_3
Fold 4: Train[80%] â†’ Test[20%] â†’ Score_4
Fold 5: Train[80%] â†’ Test[20%] â†’ Score_5

å¹³å‡åˆ†æ•° = (Score_1 + ... + Score_5) / 5
æ ‡å‡†å·® = std([Score_1, ..., Score_5])
```

#### é˜¶æ®µ 5: æ¨¡å‹è¯„ä¼° (Evaluation)

```python
def evaluate(self, X_test, y_test):
    """
    å¤šç»´åº¦è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    from scipy.stats import spearmanr
    
    y_pred = self.predict(X_test)
    
    # 1. RÂ² åˆ†æ•°
    r2 = self.score(X_test, y_test)
    
    # 2. å‡æ–¹æ ¹è¯¯å·®
    rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))
    
    # 3. Spearman ç›¸å…³æ€§
    spearman_rho, spearman_p = spearmanr(y_pred, y_test)
    
    # 4. Kendall Tau æ’åºç›¸å…³æ€§
    from scipy.stats import kendalltau
    kendall_tau, kendall_p = kendalltau(y_pred, y_test)
    
    results = {
        'r2': r2,
        'rmse': rmse,
        'spearman_rho': spearman_rho,
        'spearman_p': spearman_p,
        'kendall_tau': kendall_tau,
        'kendall_p': kendall_p,
    }
    
    return results
```

**è¯„ä¼°æŒ‡æ ‡è§£é‡Š**:

| æŒ‡æ ‡ | å…¬å¼ | è§£é‡Š | ç›®æ ‡ |
|------|------|------|------|
| RÂ² | $1 - \frac{SS_{res}}{SS_{tot}}$ | è§£é‡Šæ–¹å·®æ¯” | > 0.15 |
| RMSE | $\sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$ | é¢„æµ‹è¯¯å·® | < 0.10 |
| Spearman Ï | æ’åºç›¸å…³ç³»æ•° | æ’åºä¸€è‡´æ€§ | > 0.20 |
| Kendall Ï„ | ç§©ç›¸å…³ç³»æ•° | æ’åºç¨³å®šæ€§ | > 0.15 |

### ğŸ“Š ç®—æ³•å¤æ‚åº¦åˆ†æ

| æ¨¡å‹ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | è®­ç»ƒæ—¶é—´(12.6Kæ ·æœ¬) |
|------|-----------|-----------|------------------|
| Ridge | O(nÂ·dÂ²) | O(nÂ·d) | < 1s |
| GBDT | O(TÂ·nÂ·dÂ·log n) | O(TÂ·d) | 5-10s |
| Stacking | O(2TÂ·nÂ·dÂ·log n) | O(nÂ·d) | 15-20s |

---

## Top200 ç­›é€‰ç®—æ³•

### ğŸ“‹ ç®—æ³•æ¦‚è¿°

**æ–‡ä»¶**: `etf_rotation_experiments/selection/core.py`  
**é—®é¢˜**: ä» 12,597 ä¸ªç»„åˆä¸­é€‰æ‹©æœ€ä¼˜ 200 ä¸ªç”¨äºäº¤æ˜“  
**è§£å†³**: å¤šå±‚æ¬¡ç­›é€‰ + å› å­å¤šæ ·æ€§ä¼˜åŒ– + é…é¢åˆ†é…

### ğŸ”§ è¯¦ç»†æŠ€æœ¯è§„èŒƒ

#### æ­¥éª¤ 1: è´¨é‡è¿‡æ»¤ (Quality Filter)

```python
def apply_quality_filter(df, config):
    """
    å¤šç»´åº¦ç­›é€‰ä¸åˆæ ¼çš„ç»„åˆ
    """
    
    quality = config['quality_filter']['standard']
    
    # 1. Sharpe è¿‡æ»¤
    mask1 = df['sharpe_net'] >= quality['min_sharpe_net']  # â‰¥ 0.95
    
    # 2. å›æ’¤è¿‡æ»¤
    mask2 = df['max_dd_net'] >= quality['max_dd_net']  # â‰¥ -0.28
    
    # 3. å¹´åŒ–æ”¶ç›Šè¿‡æ»¤
    mask3 = df['annual_ret_net'] >= quality['min_annual_ret_net']  # â‰¥ 0.12
    
    # 4. æ¢æ‰‹ç‡è¿‡æ»¤
    mask4 = df['avg_turnover'] <= quality['max_turnover']  # â‰¤ 1.6
    
    # ç»¼åˆè¿‡æ»¤
    combined_mask = mask1 & mask2 & mask3 & mask4
    
    filtered_df = df[combined_mask]
    
    print(f"è´¨é‡è¿‡æ»¤å: {len(filtered_df)} ä¸ªç»„åˆé€šè¿‡ ({len(filtered_df)/len(df)*100:.1f}%)")
    
    return filtered_df
```

**è¿‡æ»¤æ ‡å‡†å¯¹æ¯”**:

```
æ ‡å‡†æ¨¡å¼ (Standard):        å®½æ¾æ¨¡å¼ (Relaxed):        ä¸¥æ ¼æ¨¡å¼ (Tightened):
â”œâ”€ Sharpe â‰¥ 0.95           â”œâ”€ Sharpe â‰¥ 0.90           â”œâ”€ Sharpe â‰¥ 1.00
â”œâ”€ DD â‰¤ -0.28              â”œâ”€ DD â‰¤ -0.30              â”œâ”€ DD â‰¤ -0.25
â”œâ”€ å¹´åŒ– â‰¥ 12%              â”œâ”€ å¹´åŒ– â‰¥ 10%              â””â”€ å¹´åŒ– â‰¥ 15%
â””â”€ æ¢æ‰‹ â‰¤ 1.6              â””â”€ æ¢æ‰‹ â‰¤ 1.8
```

#### æ­¥éª¤ 2: å› å­åˆ†ç±» (Factor Categorization)

```python
def categorize_factors(combo_str, factor_categories):
    """
    å°†ç»„åˆä¸­çš„å› å­åˆ†ç±»åˆ° 4 ä¸ªç±»åˆ«
    
    è¾“å…¥:
        combo_str: "FACTOR1+FACTOR2+..." æ ¼å¼
        factor_categories: å› å­åˆ†ç±»è¯å…¸
    
    è¾“å‡º:
        factor_counts: {'trend': 2, 'vol': 1, 'volume_price': 0, 'relative': 1}
    """
    
    factors = combo_str.split('+')
    factor_counts = {
        'trend': 0,
        'vol': 0,
        'volume_price': 0,
        'relative': 0,
    }
    
    for factor in factors:
        for category, factor_list in factor_categories.items():
            if factor in factor_list:
                factor_counts[category] += 1
                break
    
    return factor_counts
```

**å› å­åˆ†ç±»ä½“ç³»**:

```python
FACTOR_CATEGORIES = {
    'trend': [
        'MOM_20D', 'SLOPE_20D', 'VORTEX_14D', 'ADX_14D',
        'TREND', 'ROC'
    ],
    'vol': [
        'VOL_RATIO_20D', 'VOL_RATIO_60D', 'MAX_DD_60D',
        'RET_VOL_20D', 'SHARPE_RATIO_20D', 'VAR', 'STD'
    ],
    'volume_price': [
        'OBV_SLOPE_10D', 'PV_CORR_20D', 'CMF_20D', 'MFI'
    ],
    'relative': [
        'RSI_14', 'PRICE_POSITION_20D', 'PRICE_POSITION_120D',
        'RELATIVE_STRENGTH_VS_MARKET_20D', 'CORRELATION_TO_MARKET_20D'
    ]
}
```

#### æ­¥éª¤ 3: æ€§èƒ½è¯„åˆ† (Performance Scoring)

```python
def calculate_score(row, weights):
    """
    åŠ æƒè®¡ç®—ç»„åˆçš„ç»¼åˆè¯„åˆ†
    """
    
    score = (
        row['annual_ret_net'] * weights['annual_ret_net'] +          # 0.25
        row['sharpe_net'] * weights['sharpe_net'] +                  # 0.30
        row['calmar_ratio'] * weights['calmar_ratio'] +              # 0.20
        row['win_rate'] * weights['win_rate'] +                      # 0.15
        row['max_dd_net'] * weights['max_dd_net']                    # -0.10
    )
    
    return score
```

**æƒé‡é…ç½®**:

```
å¹´åŒ–æ”¶ç›Š:     0.25 (ä¸­ç­‰)
Sharpe:      0.30 (æœ€é«˜) â­
Calmar:      0.20 (ä¸­ç­‰)
èƒœç‡:        0.15 (è¾ƒä½)
æœ€å¤§å›æ’¤:   -0.10 (è´Ÿæƒé‡)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»æƒé‡:      1.00
```

#### æ­¥éª¤ 4: æ¡¶é…é¢åˆ†é… (Bucket Quota Allocation)

```python
def allocate_quotas(filtered_df, config):
    """
    æ ¹æ® ETF æ•°é‡ä¸ºæ¯ä¸ª ETF æ¡¶åˆ†é…é€‰æ‹©åé¢
    """
    
    thresholds = config['bucket_quotas']['size_thresholds']  # [100, 50, 20]
    quotas = config['bucket_quotas']['quotas']              # [18, 12, 8, 5]
    
    # ç»Ÿè®¡æ¯ä¸ª ETF çš„ç»„åˆæ•°é‡
    combo_counts = filtered_df.groupby('etf').size()
    
    quota_allocation = {}
    
    for etf, count in combo_counts.items():
        if count >= thresholds[0]:          # â‰¥ 100
            bucket = 0
        elif count >= thresholds[1]:        # 50-99
            bucket = 1
        elif count >= thresholds[2]:        # 20-49
            bucket = 2
        else:                               # < 20
            bucket = 3
        
        quota_allocation[etf] = quotas[bucket]
    
    return quota_allocation
```

**æ¡¶åˆ†é…çŸ©é˜µ**:

```
ETF æ•°é‡èŒƒå›´  â”‚  é…é¢  â”‚  è¯´æ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â‰¥ 100      â”‚  18   â”‚ ç«äº‰æ¿€çƒˆï¼Œç²¾é€‰
  50-99      â”‚  12   â”‚ æ­£å¸¸æƒ…å†µ
  20-49      â”‚   8   â”‚ é€‰é¡¹æœ‰é™
  < 20       â”‚   5   â”‚ ä¿åº•åˆ†é…
```

#### æ­¥éª¤ 5: ç»„åˆå¤§å°å¹³è¡¡ (Combo Size Distribution)

```python
def balance_combo_sizes(df, targets):
    """
    ç¡®ä¿é€‰æ‹©çš„ç»„åˆä¸­ï¼Œå› å­æ•°é‡åˆ†å¸ƒåˆç†
    
    ç›®æ ‡åˆ†å¸ƒ:
        3å› å­: 20-30% (çº¦ 40-60 ä¸ª)
        4å› å­: 30-40% (çº¦ 60-80 ä¸ª)
        5å› å­: 35-45% (çº¦ 70-90 ä¸ª)
    """
    
    selected_combos = []
    
    for combo_size in [3, 4, 5]:
        target_min = targets[combo_size]['min']
        target_max = targets[combo_size]['max']
        
        # è·å–è¯¥å› å­æ•°é‡çš„æ‰€æœ‰ç»„åˆ
        combos_of_size = df[df['combo_size'] == combo_size]
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©å‰ target_max ä¸ª
        selected = combos_of_size.nlargest(target_max, 'score')
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å°å€¼
        if len(selected) < target_min:
            print(f"è­¦å‘Š: {combo_size}å› å­ç»„åˆä¸è¶³ ({len(selected)}/{target_min})")
        
        selected_combos.append(selected)
    
    final_df = pd.concat(selected_combos, ignore_index=True)
    
    return final_df
```

**å¤§å°åˆ†å¸ƒéªŒè¯**:

```python
def verify_distribution(selected_df):
    """
    éªŒè¯æœ€ç»ˆé€‰æ‹©çš„ç»„åˆåˆ†å¸ƒæ˜¯å¦ç¬¦åˆç›®æ ‡
    """
    size_dist = selected_df['combo_size'].value_counts(normalize=True)
    
    print("ç»„åˆå¤§å°åˆ†å¸ƒ:")
    print(f"  3å› å­: {size_dist.get(3, 0)*100:.1f}% (ç›®æ ‡: 20-30%)")
    print(f"  4å› å­: {size_dist.get(4, 0)*100:.1f}% (ç›®æ ‡: 30-40%)")
    print(f"  5å› å­: {size_dist.get(5, 0)*100:.1f}% (ç›®æ ‡: 35-45%)")
```

#### æ­¥éª¤ 6: æœ€ç»ˆæ’åº (Final Ranking)

```python
def final_ranking(selected_df, config):
    """
    å¯¹æœ€ç»ˆ 200 ä¸ªç»„åˆè¿›è¡Œæ’åº
    
    æ’åºä¼˜å…ˆçº§:
        1. Sharpe (é™åº)
        2. å¹´åŒ–æ”¶ç›Š (é™åº)
        3. æœ€å¤§å›æ’¤ (å‡åºï¼Œç»å¯¹å€¼å°ä¼˜å…ˆ)
    """
    
    selected_df = selected_df.sort_values(
        by=['sharpe_net', 'annual_ret_net', 'max_dd_net'],
        ascending=[False, False, True]
    )
    
    selected_df['final_rank'] = range(1, len(selected_df) + 1)
    
    return selected_df
```

---

## ç®—æ³•é›†æˆä¸æµç¨‹

### ğŸ”„ å®Œæ•´ç«¯åˆ°ç«¯æµç¨‹

```
æ•°æ®åŠ è½½é˜¶æ®µ
â”œâ”€â”€ åŠ è½½ 43 ETF æ—¥çº¿æ•°æ® (1399 å¤©)
â”‚   â””â”€â”€ æ•°æ®æ ¼å¼: OHLCV
â”œâ”€â”€ æ•°æ®éªŒè¯
â”‚   â”œâ”€â”€ ç¼ºå¤±å€¼æ£€æŸ¥
â”‚   â”œâ”€â”€ æ—¶é—´å¯¹é½
â”‚   â””â”€â”€ ä»·æ ¼æœ‰æ•ˆæ€§æ£€æŸ¥
â””â”€â”€ æ•°æ®é¢„å¤„ç†
    â”œâ”€â”€ å¤æƒä»·æ ¼
    â”œâ”€â”€ Winsorize [0.5%, 99.5%]
    â””â”€â”€ æ ‡å‡†åŒ–å¤„ç†

        â†“â†“â†“ (01:18:51)

å› å­è®¡ç®—é˜¶æ®µ (18ä¸ªå› å­)
â”œâ”€â”€ è¶‹åŠ¿å› å­ (4ä¸ª): MOM, SLOPE, VORTEX, ADX
â”œâ”€â”€ é£é™©å› å­ (4ä¸ª): VOL_RATIO, MAX_DD, RET_VOL, SHARPE
â”œâ”€â”€ é‡ä»·å› å­ (4ä¸ª): OBV, CMF, PV_CORR, MFI
â””â”€â”€ ç›¸å¯¹å› å­ (6ä¸ª): RSI, PRICE_POSITION, CORRELATION, ...

        â†“â†“â†“ (01:18:52)

WFO ä¼˜åŒ–é˜¶æ®µ
â”œâ”€â”€ æ¨ªæˆªé¢æ ‡å‡†åŒ–
â”œâ”€â”€ ç»„åˆè¯„ä¼° (12,597ä¸ª)
â”‚   â”œâ”€â”€ å› å­æ”¶ç›Šç‡è®¡ç®—
â”‚   â”œâ”€â”€ OOS IC è®¡ç®— (Information Coefficient)
â”‚   â”œâ”€â”€ ç»„åˆå›æ’¤åˆ†æ
â”‚   â””â”€â”€ ç¨³å®šæ€§è¯„åˆ†
â””â”€â”€ ç»“æœä¿å­˜ (all_combos.parquet)

        â†“â†“â†“ (01:19:44, 52ç§’)

ML æ ¡å‡†é˜¶æ®µ â­ æ ¸å¿ƒ
â”œâ”€â”€ ç‰¹å¾æå– (5ä¸ªç‰¹å¾)
â”œâ”€â”€ æ•°æ®æ ‡å‡†åŒ–
â”œâ”€â”€ æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ Ridge (Î±=1.0)
â”‚   â”œâ”€â”€ GBDT (300æ ‘, æ·±åº¦5)
â”‚   â””â”€â”€ Stackingé›†æˆ
â”œâ”€â”€ äº¤å‰éªŒè¯ (5-Fold)
â””â”€â”€ æ¨¡å‹è¯„ä¼°
    â””â”€â”€ è¾“å‡º: é¢„æµ‹Sharpeæ’åº

        â†“â†“â†“

çœŸå®å›æµ‹é˜¶æ®µ
â”œâ”€â”€ åŠ è½½Top100ç»„åˆ
â”œâ”€â”€ æ„å»ºå›æµ‹æ•°æ®é›†
â”œâ”€â”€ æ— æœªæ¥å‡½æ•°å›æµ‹
â”‚   â”œâ”€â”€ æ—¥æœŸéš”ç¦» (Tæ—¶åˆ»ä¿¡å·, T+1æ—¶åˆ»äº¤æ˜“)
â”‚   â”œâ”€â”€ å¤´å¯¸æ„å»º
â”‚   â”œâ”€â”€ æ¢æ‰‹è®¡ç®—
â”‚   â””â”€â”€ æ€§èƒ½ç»Ÿè®¡
â””â”€â”€ ç»“æœæ±‡æ€» (top100_backtest_full.csv)

        â†“â†“â†“ (01:21:06, 82ç§’)

ç»„åˆç­›é€‰é˜¶æ®µ
â”œâ”€â”€ è´¨é‡è¿‡æ»¤
â”‚   â”œâ”€â”€ Sharpe â‰¥ 0.95
â”‚   â”œâ”€â”€ DD â‰¤ -28%
â”‚   â””â”€â”€ æ¢æ‰‹ â‰¤ 1.6
â”œâ”€â”€ å› å­åˆ†ç±»
â”œâ”€â”€ æ€§èƒ½è¯„åˆ†
â”œâ”€â”€ é…é¢åˆ†é… (æŒ‰ETFæ•°é‡)
â”œâ”€â”€ å¤§å°å¹³è¡¡ (3/4/5å› å­æ¯”ä¾‹)
â””â”€â”€ æœ€ç»ˆæ’åº

        â†“â†“â†“

è¾“å‡º: Top200æœ€ä¼˜ç»„åˆ âœ…
```

### ğŸ“Š å…³é”®æ•°æ®æµè½¬

```python
# æ ·æœ¬æ•°æ®æµè½¬ç¤ºä¾‹

# Step 1: åŸå§‹ WFO ç»“æœ
wfo_row = {
    'combo': 'ADX_14D+OBV_SLOPE_10D+PRICE_POSITION_20D+VOL_RATIO_20D',
    'mean_oos_ic': 0.087,
    'oos_ic_std': 0.045,
    'positive_rate': 0.72,
    'combo_size': 4,
}

# Step 2: ç‰¹å¾æå–
features = extract_features(wfo_row)
# è¾“å‡º: [0.087, 0.045, 0.72, 0.89, 4.0]

# Step 3: ç‰¹å¾æ ‡å‡†åŒ–
features_normalized = (features - mean) / std
# è¾“å‡º: [0.32, -0.15, 0.18, 0.24, 0.05]

# Step 4: æ¨¡å‹é¢„æµ‹ (Ridge)
predicted_sharpe = ridge_model.predict(features_normalized)
# è¾“å‡º: 0.91

# Step 5: å›æµ‹éªŒè¯
actual_sharpe = 0.89
error = abs(actual_sharpe - predicted_sharpe)
# è¯¯å·®: 0.02 âœ“

# Step 6: æœ€ç»ˆæ’åº
rank = 5  # åœ¨Top200ä¸­æ’åç¬¬5
```

---

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### ğŸš€ é€Ÿåº¦åŸºå‡†

| é˜¶æ®µ | å¤„ç†é‡ | è€—æ—¶ | ååé‡ |
|------|--------|------|--------|
| æ•°æ®åŠ è½½ | 43 ETF Ã— 1399å¤© | 15s | 3.9K æ•°æ®ç‚¹/s |
| å› å­è®¡ç®— | 18å› å­ Ã— 58.6K æ•°æ®ç‚¹ | 18s | 58.6K æ•°æ®ç‚¹/s |
| WFO ä¼˜åŒ– | 12,597 ç»„åˆ | 52s | 242 ç»„åˆ/s |
| çœŸå®å›æµ‹ | 100 ç»„åˆ Ã— 1399å¤© | 82s | 0.88 ç»„åˆ/s |
| **æ€»è€—æ—¶** | å®Œæ•´ç®¡é“ | **155s** | - |

### ğŸ“ˆ ç²¾åº¦åŸºå‡†

| æ¨¡å‹ | æ•°æ®é›† | RÂ² | RMSE | Spearman Ï | è¿è¡Œæ—¶é—´ |
|------|--------|----|----|-----------|---------|
| Ridge | Train | 0.14 | 0.084 | 0.22 | < 1s |
| Ridge | Test | 0.12 | 0.092 | 0.19 | < 0.1s |
| GBDT | Train | 0.28 | 0.065 | 0.38 | 8s |
| GBDT | Test | 0.20 | 0.078 | 0.28 | < 1s |
| Stacking | Train | 0.32 | 0.060 | 0.42 | 18s |
| Stacking | Test | 0.24 | 0.074 | 0.32 | < 2s |

### ğŸ’¾ å†…å­˜ä½¿ç”¨

| æ•°æ®é›† | å¤§å° | å†…å­˜å ç”¨ |
|--------|------|---------|
| åŸå§‹ 43 ETF æ—¥çº¿ | 58.6K è¡Œ Ã— 5 åˆ— | 2.3 MB |
| 18 å› å­çŸ©é˜µ | 58.6K è¡Œ Ã— 18 åˆ— | 8.4 MB |
| WFO ç»“æœ (all_combos) | 12,597 è¡Œ Ã— 10 åˆ— | 9.8 MB |
| å›æµ‹ç»“æœ CSV | 100 è¡Œ Ã— 100 åˆ— | 0.8 MB |
| **æ€»å ç”¨** | - | **~25 MB** |

---

## æ•…éšœæ’æŸ¥æŒ‡å—

### ğŸ” å¸¸è§é—®é¢˜è¯Šæ–­

#### é—®é¢˜ 1: WFO IC è®¡ç®—åç¦»é¢„æœŸ

**ç—‡çŠ¶**: IC å‡å€¼ä¸ºè´Ÿæ•°æˆ–å…¨ 0

**è¯Šæ–­æ­¥éª¤**:

```python
# æ£€æŸ¥ 1: æ•°æ®å¯¹é½
print("å› å­æ•°æ®å½¢çŠ¶:", factor_data.shape)
print("æ”¶ç›Šç‡æ•°æ®å½¢çŠ¶:", returns.shape)
assert factor_data.shape[0] == returns.shape[0]

# æ£€æŸ¥ 2: ç¼ºå¤±å€¼
print("å› å­ç¼ºå¤±ç‡:", factor_data.isna().sum() / len(factor_data))
print("æ”¶ç›Šç¼ºå¤±ç‡:", returns.isna().sum() / len(returns))

# æ£€æŸ¥ 3: IC è®¡ç®—
from scipy.stats import spearmanr
ic_sample = spearmanr(factor_data.iloc[0], returns.iloc[0])[0]
print(f"æ ·æœ¬ IC: {ic_sample}")
```

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥æ—¥æœŸéš”ç¦»æ˜¯å¦ä¸¥æ ¼ï¼ˆç¡®ä¿ç”¨ t æ—¶åˆ»å› å­é¢„æµ‹ t+1 æ”¶ç›Šï¼‰
- æ£€æŸ¥ç¼ºå¤±å€¼å¤„ç†ï¼ˆNaN å¡«å……ï¼‰
- éªŒè¯å› å­æ ‡å‡†åŒ–ï¼ˆåº”åœ¨æ¨ªæˆªé¢ä¸Šæ ‡å‡†åŒ–ï¼Œä¸è·¨æ—¶é—´åºåˆ—ï¼‰

#### é—®é¢˜ 2: æ¨¡å‹ RÂ² è¿‡ä½ (< 0.10)

**ç—‡çŠ¶**: æ¨¡å‹é¢„æµ‹èƒ½åŠ›å·®

**è¯Šæ–­æ­¥éª¤**:

```python
# æ£€æŸ¥ 1: ç‰¹å¾åˆ†å¸ƒ
import matplotlib.pyplot as plt
for i in range(5):
    plt.hist(X[:, i], bins=50)
    plt.title(f'Feature {i} Distribution')
    plt.show()

# æ£€æŸ¥ 2: ç›®æ ‡å˜é‡åˆ†å¸ƒ
plt.hist(y, bins=50)
plt.title('Target (Sharpe) Distribution')
plt.show()

# æ£€æŸ¥ 3: ç›¸å…³æ€§åˆ†æ
corr_matrix = np.corrcoef(X.T, y)
print("ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§:")
print(corr_matrix[-1, :-1])
```

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ ç‰¹å¾æ•°é‡ï¼ˆæ·»åŠ å†å² Sharpeã€IC è¶‹åŠ¿ç­‰ï¼‰
- å¢åŠ æ ·æœ¬é‡ï¼ˆä» Top2000 æ”¹ä¸ºå…¨é‡ 12,597ï¼‰
- å°è¯•éçº¿æ€§æ¨¡å‹ï¼ˆGBDT è€Œé Ridgeï¼‰
- æ£€æŸ¥ç›®æ ‡å˜é‡æ˜¯å¦å­˜åœ¨æµ‹é‡è¯¯å·®

#### é—®é¢˜ 3: å›æµ‹æ›²çº¿ä¸é¢„æµ‹æ’åºä¸ç¬¦

**ç—‡çŠ¶**: Top IC çš„ç»„åˆå®é™…è¡¨ç°æ’ååè€Œé å

**è¯Šæ–­æ­¥éª¤**:

```python
# æ£€æŸ¥ 1: æ’åºä¸€è‡´æ€§
wfo_df['predicted_rank'] = wfo_df['predicted_sharpe'].rank(ascending=False)
backtest_df['actual_rank'] = backtest_df['actual_sharpe'].rank(ascending=False)

merged = wfo_df.merge(backtest_df, on='combo')

# Spearman ç›¸å…³æ€§
from scipy.stats import spearmanr
corr, pvalue = spearmanr(merged['predicted_rank'], merged['actual_rank'])
print(f"æ’åºç›¸å…³æ€§: {corr:.3f} (p={pvalue:.4f})")

# æ£€æŸ¥ 2: å‰ 10 vs å 10
print("Top10 é¢„æµ‹å¹³å‡ Sharpe:", merged.nsmallest(10, 'predicted_rank')['actual_sharpe'].mean())
print("å10 é¢„æµ‹å¹³å‡ Sharpe:", merged.nlargest(10, 'predicted_rank')['actual_sharpe'].mean())
```

**æ ¹æœ¬åŸå› åˆ†æ**:

```
å‡è®¾: ç›¸å…³æ€§ = -0.189 (åå‘ç›¸å…³)
åŸå› çŒœæµ‹:
  1. WFO å­˜åœ¨å‰ç»åå·® (æœ€å¯èƒ½)
  2. IC ä¸é€‚åˆé¢„æµ‹ Sharpe (æ¬¡æ¦‚ç‡)
  3. æ ·æœ¬å†…è¿‡æ‹Ÿåˆ (å¯èƒ½)
  4. å¸‚åœºåˆ¶åº¦çº¦æŸæœªå»ºæ¨¡ (å¯èƒ½)

è§£å†³æ–¹æ¡ˆä¼˜å…ˆçº§:
  1. ç«‹å³å®¡è®¡ WFO æ—¥æœŸéš”ç¦»ä»£ç 
  2. å°è¯• Rank IC vs Pearson IC
  3. å¢åŠ ç‰¹å¾ï¼Œæ”¹ç”¨ GBDT
  4. åŠ å…¥å¸‚åœºåˆ¶åº¦çº¦æŸ
```

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ä¼˜åŒ– 1: ç‰¹å¾å·¥ç¨‹æ‰©å±•

**æ–°å¢ç‰¹å¾**:

```python
# ç‰¹å¾ 5: IC è¶‹åŠ¿
ic_recent = wfo_df['oos_ic'][-5:].mean()  # æœ€è¿‘5ä¸ªçª—å£ IC
ic_trend = ic_recent - wfo_df['mean_oos_ic']  # è¶‹åŠ¿
features[:, 5] = ic_trend

# ç‰¹å¾ 6: å› å­å¤šæ ·æ€§
factor_diversity = (
    (factor_counts['trend'] > 0) +
    (factor_counts['vol'] > 0) +
    (factor_counts['volume_price'] > 0) +
    (factor_counts['relative'] > 0)
) / 4
features[:, 6] = factor_diversity
```

### ä¼˜åŒ– 2: æ¨¡å‹é€‰æ‹©è‡ªé€‚åº”

```python
def adaptive_model_selection(ic_regime):
    """
    æ ¹æ®å½“å‰ IC åˆ¶åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
    """
    
    if ic_regime == 'strong':      # IC > 0.10
        return 'ridge'              # çº¿æ€§æ¨¡å‹è¶³å¤Ÿ
    elif ic_regime == 'moderate':  # 0.05 < IC < 0.10
        return 'gbdt'               # éçº¿æ€§æ¨¡å‹æœ‰å¸®åŠ©
    else:                           # IC < 0.05
        return 'stacking'           # é›†æˆå¢å¼ºé²æ£’æ€§
```

### ä¼˜åŒ– 3: å¢é‡å­¦ä¹ 

```python
# å‘¨æœŸæ€§é‡è®­ç»ƒ
def incremental_update(new_backtest_data, existing_model):
    """
    æ¯æœˆç”¨æ–°æ•°æ®å¢é‡æ›´æ–°æ¨¡å‹
    """
    
    # ä¿ç•™ 80% å†å²æ•°æ®
    historical_data = get_last_12_months()
    
    # åŠ å…¥ 20% æ–°æ•°æ®
    combined_data = pd.concat([
        historical_data.sample(frac=0.8),
        new_backtest_data
    ])
    
    # é‡è®­ç»ƒ
    model = train_calibrator(combined_data)
    
    return model
```

---

**æ–‡æ¡£å®Œæˆ**: 2025-11-16  
**ä¸‹ä¸€æ­¥**: åŸºäºæ­¤è§„èŒƒå®æ–½æ¨¡å‹å‡çº§å’Œç‰¹å¾å·¥ç¨‹æ”¹è¿›

