#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""é…ç½®åŒ–å¹¶è¡Œå›æµ‹å¼•æ“ - å‘é‡åŒ–ç‰ˆæœ¬

ç»“åˆé«˜æ€§èƒ½å‘é‡åŒ–è®¡ç®—ä¸å®Œæ•´é…ç½®æŠ½è±¡çš„ETFè½®åŠ¨å›æµ‹å¼•æ“
"""

import argparse
import itertools
import json
import logging
import multiprocessing as mp
import os
import sys
import time
from datetime import datetime
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

try:
    import vectorbt as vbt

    HAS_VBT = True
except ImportError:
    HAS_VBT = False
    print("è­¦å‘Š: æœªå®‰è£… vectorbtï¼Œè¯·è¿è¡Œ: pip install vectorbt")

from config_loader_parallel import FastConfig, load_fast_config_from_args


class ConfigurableParallelBacktestEngine:
    """é…ç½®åŒ–å¹¶è¡Œå›æµ‹å¼•æ“ - å®Œå…¨å‘é‡åŒ–å®ç°"""

    def __init__(self, config: FastConfig):
        """
        åˆå§‹åŒ–é…ç½®åŒ–å¹¶è¡Œå›æµ‹å¼•æ“

        Args:
            config: å›æµ‹é…ç½®å¯¹è±¡
        """
        self.config = config

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåŸºäºé…ç½®ï¼‰
        os.environ.setdefault("OMP_NUM_THREADS", str(config.omp_num_threads))
        os.environ.setdefault(
            "VECLIB_MAXIMUM_THREADS", str(config.veclib_maximum_threads)
        )
        os.environ.setdefault("MKL_NUM_THREADS", str(config.mkl_num_threads))

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, config.log_level.upper()),
            format="%(asctime)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

    def _validate_config(self):
        """å¯åŠ¨æ—¶éªŒè¯é…ç½®åˆç†æ€§"""
        # ...existing code...

    def _validate_config(self):
        """å¯åŠ¨æ—¶éªŒè¯é…ç½®åˆç†æ€§"""
        # æ£€æŸ¥æƒé‡ç½‘æ ¼ä¸å› å­æ•°çš„é€‚é…æ€§
        n_grid = len(self.config.weight_grid_points)

        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if not Path(self.config.panel_file).exists():
            self.logger.warning(f"âš ï¸  é¢æ¿æ–‡ä»¶ä¸å­˜åœ¨: {self.config.panel_file}")

        if not Path(self.config.price_dir).exists():
            self.logger.warning(f"âš ï¸  ä»·æ ¼ç›®å½•ä¸å­˜åœ¨: {self.config.price_dir}")

        # æ£€æŸ¥ç­›é€‰æ–‡ä»¶ï¼ˆå¦‚æœæä¾›ï¼‰
        if self.config.screening_file and not Path(self.config.screening_file).exists():
            self.logger.warning(f"âš ï¸  ç­›é€‰æ–‡ä»¶ä¸å­˜åœ¨: {self.config.screening_file}")

        # æ£€æŸ¥å¹¶è¡Œé…ç½®
        import multiprocessing

        max_cores = multiprocessing.cpu_count()
        if self.config.n_workers > max_cores:
            self.logger.warning(
                f"âš ï¸  é…ç½®çš„å·¥ä½œè¿›ç¨‹æ•°({self.config.n_workers})è¶…è¿‡CPUæ ¸å¿ƒæ•°({max_cores})"
            )

    def _load_factor_panel(self) -> pd.DataFrame:
        """åŠ è½½å› å­é¢æ¿"""
        self.logger.info(f"åŠ è½½å› å­é¢æ¿: {self.config.panel_file}")
        panel = pd.read_parquet(self.config.panel_file)
        if not isinstance(panel.index, pd.MultiIndex):
            raise ValueError("é¢æ¿å¿…é¡»æ˜¯ (symbol, date) MultiIndex")

        # ç»Ÿè®¡é¢æ¿ä¿¡æ¯
        n_symbols = panel.index.get_level_values(0).nunique()
        dates = panel.index.get_level_values(1).unique()
        date_range = (
            f"{dates.min().strftime('%Y-%m-%d')}~{dates.max().strftime('%Y-%m-%d')}"
        )
        self.logger.info(
            f"é¢æ¿å½¢çŠ¶: {panel.shape}, æ—¥æœŸ: {date_range} ({len(dates)}æ—¥), æ ‡çš„: {n_symbols}ä¸ª"
        )

        return panel

    def _load_price_data(self) -> pd.DataFrame:
        """åŠ è½½ä»·æ ¼æ•°æ® - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        import glob

        # ç¼“å­˜è·¯å¾„
        cache_dir = Path(self.config.output_dir) / "cache"
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = cache_dir / "prices.parquet"

        # æ£€æŸ¥æºæ–‡ä»¶æœ€æ–°ä¿®æ”¹æ—¶é—´
        source_files = sorted(glob.glob(f"{self.config.price_dir}/*.parquet"))
        if not source_files:
            raise ValueError(f"æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®æ–‡ä»¶: {self.config.price_dir}")

        latest_mtime = max(Path(f).stat().st_mtime for f in source_files)

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        cache_hit = False
        if cache_file.exists():
            cache_mtime = cache_file.stat().st_mtime
            if cache_mtime >= latest_mtime:
                self.logger.info(f"å‘½ä¸­ä»·æ ¼ç¼“å­˜: {cache_file}")
                pivot = pd.read_parquet(cache_file)
                cache_hit = True

        if not cache_hit:
            # é‡æ–°åŠ è½½å¹¶ç¼“å­˜
            self.logger.info(f"åˆ·æ–°ä»·æ ¼ç¼“å­˜ï¼ŒåŠ è½½ {len(source_files)} ä¸ªæ–‡ä»¶...")
            prices = []
            for f in source_files:
                df = pd.read_parquet(f)
                symbol = f.split("/")[-1].split("_")[0]
                df["symbol"] = symbol
                df["date"] = pd.to_datetime(df["trade_date"])
                prices.append(df[["date", "close", "symbol"]])

            price_df = pd.concat(prices, ignore_index=True)
            pivot = price_df.pivot(index="date", columns="symbol", values="close")
            pivot = pivot.sort_index()

            # ï¿½ ä¸å¡«å……ï¼šä¿ç•™ NaN ç”¨äºæ ‡è®°åœç‰Œ/é€€å¸‚æ—¥æœŸ
            # åŸå› ï¼š
            #   1. ETF åœç‰Œæ—¥æœŸå°±æ˜¯ NaNï¼Œåæ˜ ç°å®
            #   2. æƒé‡è®¡ç®—æ—¶ NaNÃ—æƒé‡=NaNï¼Œè‡ªåŠ¨æ’é™¤åœç‰Œæ—¥æœŸ
            #   3. æ— éœ€äººå·¥å¡«å……ï¼Œè§„é¿è™šå‡ä»·æ ¼é£é™©

            # å†™å…¥ç¼“å­˜
            pivot.to_parquet(cache_file, compression="snappy")
            self.logger.info(f"ä»·æ ¼ç¼“å­˜å·²æ›´æ–°: {cache_file}")

        # ç»Ÿè®¡ä»·æ ¼æ•°æ®è´¨é‡ï¼ˆå¢å¼ºç‰ˆï¼‰
        missing_pct = pivot.isna().sum().sum() / (pivot.shape[0] * pivot.shape[1])
        cache_status = "ç¼“å­˜å‘½ä¸­" if cache_hit else "é‡æ–°åŠ è½½"

        # ğŸ” æ–°å¢ï¼šè¯¦ç»†ç¼ºå¤±å€¼ç»Ÿè®¡
        max_consecutive_gaps = {}
        for col in pivot.columns:
            mask = pivot[col].isna()
            if mask.any():
                consecutive = (mask != mask.shift()).cumsum()
                gap_lens = consecutive[mask].value_counts()
                max_consecutive_gaps[col] = (
                    gap_lens.index.max() if len(gap_lens) > 0 else 0
                )

        if max_consecutive_gaps:
            worst_symbol = max(max_consecutive_gaps, key=max_consecutive_gaps.get)
            worst_gap = max_consecutive_gaps[worst_symbol]
            self.logger.warning(
                f"âš ï¸  æœ€é•¿è¿ç»­ç¼ºå¤±: {worst_symbol} = {worst_gap}å¤© "
                f"(limit=3, è¶…å‡ºçš„å°†ä¿æŒNaN)"
            )

        self.logger.info(
            f"ä»·æ ¼çŸ©é˜µ: {pivot.shape}, æ€»ç¼ºå¤±ç‡: {missing_pct:.2%}, çŠ¶æ€: {cache_status}"
        )

        return pivot

    def _load_top_factors(self) -> List[str]:
        """ä»ç­›é€‰ç»“æœåŠ è½½Top Kå› å­ - æ·»åŠ IC/IRè¿‡æ»¤"""
        df = pd.read_csv(self.config.screening_file)
        col_name = "factor" if "factor" in df.columns else "panel_factor"

        # æ·»åŠ IC/IRè´¨é‡è¿‡æ»¤ï¼ˆå¦‚æœåˆ—å­˜åœ¨ï¼‰
        if "ic_mean" in df.columns and "ic_ir" in df.columns:
            original_count = len(df)
            # è¿‡æ»¤ä½è´¨é‡å› å­
            df = df[
                (df["ic_mean"].abs() >= 0.01)  # ICç»å¯¹å€¼è‡³å°‘0.01
                & (df["ic_ir"].abs() >= 0.05)  # IRç»å¯¹å€¼è‡³å°‘0.05
            ]
            filtered_count = len(df)
            if filtered_count < original_count:
                self.logger.info(
                    f"IC/IRè¿‡æ»¤: {original_count}ä¸ª â†’ {filtered_count}ä¸ªå› å­ "
                    f"(ç§»é™¤{original_count - filtered_count}ä¸ªä½è´¨å› å­)"
                )

        # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†å› å­åˆ—è¡¨ï¼Œä½¿ç”¨é…ç½®çš„å› å­
        if self.config.factors:
            factors = self.config.factors
            # éªŒè¯å› å­æ˜¯å¦å­˜åœ¨äºç­›é€‰ç»“æœä¸­
            available_factors = df[col_name].tolist()
            missing_factors = [f for f in factors if f not in available_factors]
            if missing_factors:
                self.logger.warning(f"ä»¥ä¸‹å› å­ä¸åœ¨ç­›é€‰ç»“æœä¸­: {missing_factors}")
                factors = [f for f in factors if f in available_factors]
            self.logger.info(f"ä½¿ç”¨é…ç½®çš„ {len(factors)} ä¸ªå› å­: {factors}")
        else:
            factors = df.head(self.config.top_k)[col_name].tolist()
            self.logger.info(f"åŠ è½½Top {len(factors)}å› å­: {factors}")

            # æ‰“å°å› å­è´¨é‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if "ic_mean" in df.columns and "ic_ir" in df.columns:
                for factor in factors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    row = df[df[col_name] == factor].iloc[0]
                    self.logger.info(
                        f"  {factor}: IC={row['ic_mean']:.3f}, IR={row['ic_ir']:.3f}"
                    )

        return factors

    def _calculate_composite_score(
        self,
        panel: pd.DataFrame,
        factors: List[str],
        weights: Dict[str, float],
    ) -> pd.DataFrame:
        """è®¡ç®—å¤åˆå› å­å¾—åˆ† - å®Œå…¨å‘é‡åŒ–å®ç°"""
        # é‡å¡‘ä¸º (date, symbol) ç»“æ„
        factor_data = panel[factors].unstack(level="symbol")

        # å‘é‡åŒ–æ ‡å‡†åŒ–
        if self.config.standardization_method == "zscore":
            normalized = (
                factor_data
                - factor_data.mean(axis=1, skipna=True).values[:, np.newaxis]
            ) / (
                factor_data.std(axis=1, skipna=True).values[:, np.newaxis]
                + self.config.numerical_epsilon
            )
        else:  # rank
            normalized = factor_data.rank(axis=1, pct=True) * 2 - 1

        # è·å–ç»´åº¦ä¿¡æ¯
        n_dates, n_total = normalized.shape
        n_factors = len(factors)
        n_symbols = n_total // n_factors

        # ä¿®å¤ï¼šunstackååˆ—åºæ˜¯ (factor1,sym1), (factor1,sym2), ..., (factor2,sym1), ...
        # éœ€è¦è½¬ç½®ä¸º (sym1,factor1), (sym1,factor2), ..., (sym2,factor1), ... æ‰èƒ½æ­£ç¡®reshape
        # æ–¹æ³•ï¼šreshapeä¸º (n_dates, n_factors, n_symbols) ç„¶åè½¬ç½®ä¸º (n_dates, n_symbols, n_factors)
        reshaped = normalized.values.reshape(n_dates, n_factors, n_symbols).transpose(
            0, 2, 1
        )

        # å‘é‡åŒ–åŠ æƒæ±‚å’Œ
        weight_array = np.array([weights.get(f, 0) for f in factors])
        scores_array = np.sum(
            reshaped * weight_array[np.newaxis, np.newaxis, :], axis=2
        )

        # åˆ›å»ºç»“æœDataFrame
        symbols = [col[1] for col in normalized.columns[::n_factors]]
        scores = pd.DataFrame(scores_array, index=normalized.index, columns=symbols)

        # ğŸ”§ ä¿®æ­£æœªæ¥å‡½æ•°ï¼šä¿¡å·å»¶è¿Ÿ1å¤©ï¼ˆä½¿ç”¨T-1æ—¥å› å­å†³ç­–Tæ—¥æŒä»“ï¼‰
        scores = scores.shift(1)

        return scores

    def _build_target_weights(self, scores: pd.DataFrame, top_n: int) -> pd.DataFrame:
        """æ„å»ºTop-Nç›®æ ‡æƒé‡"""
        ranks = scores.rank(axis=1, ascending=False, method="first")
        selection = ranks <= top_n
        weights = selection.astype(float)
        weights = weights.div(weights.sum(axis=1), axis=0).fillna(0.0)
        return weights

    def _process_weight_chunk(
        self,
        weight_chunk: List[Tuple[float, ...]],
        factors: List[str],
        panel: pd.DataFrame,
        prices: pd.DataFrame,
        top_n_list: List[int],
        rebalance_freq: int,
    ) -> List[Dict[str, Any]]:
        """å¤„ç†ä¸€ä¸ªæƒé‡ç»„åˆå— - å®Œå…¨å‘é‡åŒ–æ¶ˆé™¤æ‰€æœ‰å¾ªç¯"""
        results = []

        try:
            # === å®Œå…¨å‘é‡åŒ–æ­¥éª¤1: æ‰¹é‡è®¡ç®—æ‰€æœ‰æƒé‡ç»„åˆçš„å¾—åˆ†çŸ©é˜µ ===
            # é‡å¡‘å› å­æ•°æ®ä¸º3DçŸ©é˜µ: (dates, symbols, factors)
            factor_data = panel[factors].unstack(level="symbol")
            n_dates, n_total = factor_data.shape
            n_factors = len(factors)
            n_symbols = n_total // n_factors

            # æ ‡å‡†åŒ–å› å­æ•°æ® (ä¸€æ¬¡æ€§è®¡ç®—)
            factor_mean = factor_data.mean(axis=1, skipna=True).values[:, np.newaxis]
            factor_std = (
                factor_data.std(axis=1, skipna=True).values[:, np.newaxis] + 1e-8
            )
            normalized = (factor_data - factor_mean) / factor_std

            # é‡å¡‘ä¸º3DçŸ©é˜µç”¨äºæ‰¹é‡çŸ©é˜µä¹˜æ³•
            factor_matrix_3d = normalized.values.reshape(n_dates, n_symbols, n_factors)

            # === å®Œå…¨å‘é‡åŒ–æ­¥éª¤2: æ‰¹é‡è®¡ç®—æ‰€æœ‰æƒé‡ç»„åˆçš„å¾—åˆ† ===
            weight_array = np.array(weight_chunk)  # (n_combinations, n_factors)
            n_combinations = len(weight_chunk)

            # ä½¿ç”¨çŸ©é˜µä¹˜æ³•æ‰¹é‡è®¡ç®—æ‰€æœ‰å¾—åˆ†: (n_combinations, n_dates, n_symbols)
            scores_3d = np.einsum("cf,dsf->cds", weight_array, factor_matrix_3d)

            # === å®Œå…¨å‘é‡åŒ–æ­¥éª¤3: æ‰¹é‡å¤„ç†æ‰€æœ‰Top-Nå›æµ‹ ===
            symbol_list = [col[1] for col in normalized.columns[::n_factors]]
            date_index = normalized.index

            # ğŸ”§ ä¿®å¤ï¼šç¦ç”¨ä¿¡å·è´¨é‡é˜ˆå€¼ï¼Œåªä½¿ç”¨TopNæ’åç­›é€‰
            # åŸé˜ˆå€¼0.5è¿‡é«˜ï¼Œå¯¼è‡´TopNå‚æ•°å¤±æ•ˆï¼ˆæ‰€æœ‰TopNç»“æœç›¸åŒï¼‰
            # Z-scoreæ ‡å‡†åŒ–åï¼Œ0.5è¡¨ç¤ºè¶…è¿‡0.5ä¸ªæ ‡å‡†å·®ï¼Œè¿‡äºä¸¥æ ¼
            min_score_threshold = -999.0  # å®é™…ç¦ç”¨

            # ä¸ºæ¯ä¸ªTop-Nå€¼æ‰¹é‡å¤„ç†æ‰€æœ‰æƒé‡ç»„åˆ
            for top_n in top_n_list:
                try:
                    # å‘é‡åŒ–æ„å»ºæ‰€æœ‰æƒé‡ç»„åˆçš„ç›®æ ‡æƒé‡çŸ©é˜µ
                    # ğŸ”§ ä¿®å¤ï¼šä»…ä½¿ç”¨ç›¸å¯¹æ’åï¼Œç§»é™¤ç»å¯¹é˜ˆå€¼ç­›é€‰
                    ranks_3d = (
                        np.argsort(np.argsort(-scores_3d, axis=2), axis=2) + 1
                    )  # æ’åä»1å¼€å§‹

                    # ä»…ä½¿ç”¨TopNæ’åç­›é€‰
                    selection_3d = ranks_3d <= top_n
                    weights_3d = selection_3d.astype(float)

                    # å½’ä¸€åŒ–æƒé‡ (æ¯è¡Œå’Œä¸º1)
                    # å¦‚æœæŸå¤©æ‰€æœ‰ä¿¡å·éƒ½ä¸æ»¡è¶³é˜ˆå€¼ï¼Œæƒé‡ä¸ºå…¨0ï¼ˆç©ºä»“ï¼‰
                    weight_sums = weights_3d.sum(axis=2, keepdims=True)
                    weight_sums[weight_sums == 0] = 1  # é¿å…é™¤é›¶ï¼ˆç©ºä»“æ—¥è‡ªåŠ¨å˜ä¸ºå…¨0ï¼‰
                    weights_3d = weights_3d / weight_sums

                    # æ‰¹é‡å›æµ‹æ‰€æœ‰æƒé‡ç»„åˆ
                    chunk_results = self._vectorized_batch_backtest(
                        scores_3d,
                        weights_3d,
                        prices,
                        symbol_list,
                        date_index,
                        top_n,
                        rebalance_freq,
                        weight_chunk,
                        factors,
                    )
                    results.extend(chunk_results)

                except Exception as e:
                    if self.config.log_errors:
                        self.logger.error(f"å¤„ç†Top-N={top_n}æ—¶å‡ºé”™: {e}")
                        if self.config.verbose:
                            import traceback

                            self.logger.debug(traceback.format_exc())
                    continue  # è·³è¿‡å¤±è´¥çš„Top-Nå€¼

        except Exception as e:
            if self.config.log_errors:
                self.logger.error(f"å¤„ç†æƒé‡å—æ—¶å‡ºé”™: {e}")
                if self.config.verbose:
                    import traceback

                    self.logger.debug(traceback.format_exc())
            pass  # è·³è¿‡å¤±è´¥çš„å—

        return results

    def _vectorized_batch_backtest(
        self,
        scores_3d: np.ndarray,
        weights_3d: np.ndarray,
        prices: pd.DataFrame,
        symbol_list: List[str],
        date_index: pd.DatetimeIndex,
        top_n: int,
        rebalance_freq: int,
        weight_chunk: List[Tuple[float, ...]],
        factors: List[str],
    ) -> List[Dict[str, Any]]:
        """å®Œå…¨å‘é‡åŒ–æ‰¹é‡å›æµ‹ - æ¶ˆé™¤æ‰€æœ‰å¾ªç¯"""
        n_combinations, n_dates, n_symbols = scores_3d.shape

        # å¯¹é½ä»·æ ¼æ•°æ®
        common_dates = prices.index.intersection(date_index)
        price_aligned = prices.loc[common_dates, symbol_list]

        # æ‰¾åˆ°æ—¶é—´ç´¢å¼•å¯¹é½
        date_mask = np.isin(date_index, common_dates)
        scores_aligned = scores_3d[:, date_mask, :]
        weights_aligned = weights_3d[:, date_mask, :]

        # è®¡ç®—æ”¶ç›Šç‡çŸ©é˜µ (åªè®¡ç®—ä¸€æ¬¡ï¼Œå¤ç”¨æ‰€æœ‰ç»„åˆ)
        # ä¿®å¤pct_changeçš„FutureWarning
        returns = (
            price_aligned.pct_change(fill_method=None).fillna(0.0).values
        )  # (n_dates_aligned, n_symbols)
        n_aligned_dates = len(common_dates)

        # å‘é‡åŒ–è°ƒä»“å¤„ç†
        rebalance_indices = np.arange(n_aligned_dates)[
            np.arange(n_aligned_dates) % rebalance_freq == 0
        ]
        if len(rebalance_indices) == 0:
            rebalance_indices = [0]

        # åˆ›å»ºè°ƒä»“æƒé‡çŸ©é˜µ
        final_weights = np.zeros_like(weights_aligned)
        final_weights[:, rebalance_indices, :] = weights_aligned[
            :, rebalance_indices, :
        ]

        # å‘å‰å¡«å……æƒé‡ (å®Œå…¨å‘é‡åŒ–)
        if len(rebalance_indices) > 0:
            # ä½¿ç”¨np.maximum.accumulateè¿›è¡Œå‘é‡åŒ–å¡«å……
            # åˆ›å»ºè°ƒä»“çŸ©é˜µï¼Œè°ƒä»“æ—¥ä¸º1ï¼Œå…¶ä»–ä¸º0
            rebalance_matrix = np.zeros((n_aligned_dates,), dtype=int)
            rebalance_matrix[rebalance_indices] = 1

            # å‘å‰ä¼ æ’­æœ€è¿‘çš„è°ƒä»“ç´¢å¼•
            cumsum_rebalance = np.maximum.accumulate(
                np.arange(n_aligned_dates) * rebalance_matrix
            )

            # å°†0å€¼æ›¿æ¢ä¸ºæœ€è¿‘çš„è°ƒä»“ç´¢å¼•
            valid_mask = cumsum_rebalance > 0
            last_valid = np.maximum.accumulate(
                np.where(valid_mask, cumsum_rebalance, 0)
            )
            cumsum_rebalance = np.where(valid_mask, cumsum_rebalance, last_valid)

            # å‘é‡åŒ–å¡«å……æƒé‡
            final_weights = final_weights[:, cumsum_rebalance, :]

        # è®¡ç®—æŠ•èµ„ç»„åˆæ”¶ç›Š (å®Œå…¨å‘é‡åŒ–)
        # ğŸ”§ ä¿®å¤åŒé‡å»¶è¿ŸBUG:
        # scoreså·²ç»åœ¨ç¬¬276è¡Œshift(1)å»¶è¿Ÿï¼Œweights[T]å·²ç»å¯¹åº”Tæ—¥æŒä»“
        # ä¸åº”è¯¥å†ä½¿ç”¨prev_weightsï¼Œç›´æ¥ç”¨final_weightsè®¡ç®—æ”¶ç›Š
        # åŸé”™è¯¯é€»è¾‘: prev_weights[:, 1:, :] = final_weights[:, :-1, :] å¯¼è‡´é¢å¤–å»¶è¿Ÿ1å¤©
        # æ­£ç¡®é€»è¾‘: Tæ—¥æƒé‡ Ã— Tæ—¥æ”¶ç›Šç‡ = Tæ—¥ç»„åˆæ”¶ç›Š
        portfolio_returns = np.sum(
            final_weights * returns[np.newaxis, :, :], axis=2
        )  # (n_combinations, n_dates)

        # äº¤æ˜“æˆæœ¬è®¡ç®— (å®Œå…¨å‘é‡åŒ–)
        # ğŸ”§ ä¿®å¤ï¼šæƒé‡å˜åŒ–åº”è¯¥ä¸portfolio_returnsçš„ç»´åº¦å¯¹é½
        # ç”±äºportfolio_returnsç°åœ¨ä½¿ç”¨final_weightsï¼ˆæ— é¢å¤–å»¶è¿Ÿï¼‰ï¼Œéœ€è¦æ­£ç¡®è®¡ç®—æ¢æ‰‹
        weight_changes = np.abs(final_weights[:, 1:, :] - final_weights[:, :-1, :]).sum(
            axis=2
        )
        turnover = 0.5 * weight_changes  # (n_combinations, n_dates-1)
        trading_costs = self.config.fees * turnover  # ä»é…ç½®è¯»å–è´¹ç”¨ç‡

        # å‡€æ”¶ç›Šï¼šç¬¬ä¸€å¤©æ— äº¤æ˜“æˆæœ¬ï¼Œåç»­å¤©æ•°æ‰£é™¤æˆæœ¬
        net_returns = portfolio_returns.copy()
        net_returns[:, 1:] = portfolio_returns[:, 1:] - trading_costs

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ (å®Œå…¨å‘é‡åŒ–)
        init_cash = self.config.init_cash  # ä»é…ç½®è¯»å–åˆå§‹èµ„é‡‘
        equity_matrix = (1 + net_returns).cumprod(axis=1) * init_cash

        # æœ€ç»ˆç»“æœç»Ÿè®¡
        final_values = equity_matrix[:, -1]
        total_returns = (final_values / init_cash - 1) * 100

        # å¤æ™®æ¯”ç‡è®¡ç®— (ä½¿ç”¨ nanmean/nanstd å¿½ç•¥åœç‰Œæ—¥æœŸçš„ NaN)
        mean_returns = np.nanmean(net_returns, axis=1)
        std_returns = np.nanstd(net_returns, axis=1)
        sharpe_ratios = np.where(
            std_returns > 0,
            mean_returns / std_returns * np.sqrt(self.config.periods_per_year),
            0,
        )

        # æœ€å¤§å›æ’¤è®¡ç®—
        running_max = np.maximum.accumulate(equity_matrix, axis=1)
        drawdowns = (equity_matrix / running_max - 1) * 100
        max_drawdowns = drawdowns.min(axis=1)

        # æ¢æ‰‹ç‡
        total_turnover = turnover.sum(axis=1)

        # æ„å»ºç»“æœåˆ—è¡¨ (å®Œå…¨å‘é‡åŒ–)
        weight_dicts = [dict(zip(factors, chunk)) for chunk in weight_chunk]

        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ‰¹é‡æ„å»ºç»“æœï¼ŒåŒ…å«rebalance_freq
        results = [
            {
                "weights": str(weight_dicts[i]),
                "top_n": top_n,
                "rebalance_freq": rebalance_freq,
                "total_return": float(total_returns[i]),
                "sharpe_ratio": float(sharpe_ratios[i]),
                "max_drawdown": float(max_drawdowns[i]),
                "final_value": float(final_values[i]),
                "turnover": float(total_turnover[i]),
            }
            for i in range(n_combinations)
        ]

        return results

    def _generate_weight_combinations(self) -> List[Tuple[float, ...]]:
        """ç”Ÿæˆæœ‰æ•ˆçš„æƒé‡ç»„åˆ - æµå¼åŒ–é¿å…æŒ‡æ•°çº§å†…å­˜å ç”¨"""
        valid_combos = []
        weight_sum_min, weight_sum_max = self.config.weight_sum_range
        max_combos = self.config.max_combinations

        # è®¡ç®—ç†è®ºç»„åˆæ•°
        n_grid_points = len(self.config.weight_grid_points)
        n_factors = len(self.config.factors)
        theoretical_combos = n_grid_points**n_factors

        self.logger.info(
            f"æƒé‡ç½‘æ ¼: {n_grid_points}ç‚¹ Ã— {n_factors}å› å­, ç†è®ºç»„åˆ: {theoretical_combos:,}"
        )

        # è‡ªé€‚åº”æƒé‡çº¦æŸè­¦å‘Š
        avg_weight_if_equal = 1.0 / n_factors
        if n_factors > 10 and weight_sum_max > 1.5:
            self.logger.warning(
                f"âš ï¸  å› å­æ•°({n_factors})è¾ƒå¤šï¼Œä½†æƒé‡å’Œä¸Šé™({weight_sum_max})è¾ƒå®½æ¾"
            )
            self.logger.warning(
                f"    å¹³å‡æƒé‡={avg_weight_if_equal:.3f}ï¼Œå»ºè®®è°ƒæ•´weight_sum_range=[0.9, 1.1]"
            )

        if theoretical_combos > 1e9:
            # ç»„åˆæ•° > 10äº¿ï¼šä½¿ç”¨Dirichletæ™ºèƒ½é‡‡æ ·
            self.logger.warning(
                f"âš ï¸  ç†è®ºç»„åˆæ•° {theoretical_combos:.2e}ï¼Œé‡‡ç”¨Dirichletæ™ºèƒ½é‡‡æ ·"
            )
            np.random.seed(42)
            seen = set()  # ä½¿ç”¨setåŠ é€Ÿå»é‡

            # å…ˆä½¿ç”¨Dirichletç”Ÿæˆç¬¦åˆæƒé‡å’Œçº¦æŸçš„ç»„åˆ
            target_sum = (weight_sum_min + weight_sum_max) / 2  # ç›®æ ‡æƒé‡å’Œï¼ˆä¸­ç‚¹ï¼‰
            alpha = np.ones(n_factors) * 2.0  # Dirichletå‚æ•°ï¼ˆæ§åˆ¶åˆ†æ•£åº¦ï¼‰

            for attempt in range(max_combos * 20):  # å¢åŠ åˆ°20å€è¿‡é‡‡æ ·
                # ç”Ÿæˆå½’ä¸€åŒ–æƒé‡å‘é‡
                raw_weights = np.random.dirichlet(alpha)
                raw_weights *= target_sum  # ç¼©æ”¾åˆ°ç›®æ ‡æƒé‡å’Œ

                # æ˜ å°„åˆ°æœ€è¿‘çš„ç½‘æ ¼ç‚¹
                combo = tuple(
                    [
                        min(self.config.weight_grid_points, key=lambda x: abs(x - w))
                        for w in raw_weights
                    ]
                )
                weight_sum = sum(combo)

                if weight_sum_min <= weight_sum <= weight_sum_max:
                    if combo not in seen:
                        seen.add(combo)
                        valid_combos.append(combo)

                    if len(valid_combos) >= max_combos:
                        break

                # æ¯5000æ¬¡å°è¯•æŠ¥å‘Šè¿›åº¦
                if (attempt + 1) % 5000 == 0:
                    self.logger.info(
                        f"  é‡‡æ ·è¿›åº¦: {attempt+1:,}, æœ‰æ•ˆ: {len(valid_combos):,}"
                    )

            self.logger.info(
                f"é‡‡æ ·å®Œæˆ: {len(valid_combos):,} ä¸ªæœ‰æ•ˆç»„åˆ (å‘½ä¸­ç‡ {len(valid_combos)/(attempt+1)*100:.2f}%)"
            )
        else:
            # ç»„åˆæ•° â‰¤ 10äº¿ï¼šç›´æ¥éå†ï¼ˆå•ä¸ªè®¡ç®—å¿«é€Ÿï¼‰
            self.logger.info(f"ç†è®ºç»„åˆæ•° {theoretical_combos:.2e}ï¼Œç›´æ¥éå†ç”Ÿæˆ")
            combo_generator = itertools.product(
                self.config.weight_grid_points, repeat=n_factors
            )

            for combo in combo_generator:
                weight_sum = sum(combo)

                if weight_sum_min <= weight_sum <= weight_sum_max:
                    valid_combos.append(combo)

                    if len(valid_combos) >= max_combos:
                        break

        filter_rate = (
            (1 - len(valid_combos) / theoretical_combos) * 100
            if theoretical_combos > 0
            else 0
        )
        self.logger.info(
            f"æœ‰æ•ˆç»„åˆ: {len(valid_combos):,} (è¿‡æ»¤ç‡: {filter_rate:.3f}%), æƒé‡å’Œ: [{weight_sum_min}, {weight_sum_max}]"
        )
        return valid_combos

    def _chunk_weight_combinations(
        self, weight_combos: List[Tuple[float, ...]]
    ) -> List[List[Tuple[float, ...]]]:
        """å°†æƒé‡ç»„åˆåˆ†å—"""
        chunks = []
        for i in range(0, len(weight_combos), self.config.chunk_size):
            chunk = weight_combos[i : i + self.config.chunk_size]
            chunks.append(chunk)
        return chunks

    def parallel_grid_search(
        self, panel=None, prices=None, factors=None
    ) -> pd.DataFrame:
        """å¹¶è¡Œç½‘æ ¼æœç´¢æƒé‡ç»„åˆ - æ”¯æŒå¤šå‘¨æœŸè°ƒä»“ï¼Œæ•°æ®é¢„åŠ è½½å’Œç¼“å­˜

        Args:
            panel: å¤–éƒ¨ä¼ å…¥çš„å› å­é¢æ¿(WFOåœºæ™¯ç”¨äºIS/OOSæ•°æ®åˆ‡åˆ†)
            prices: å¤–éƒ¨ä¼ å…¥çš„ä»·æ ¼çŸ©é˜µ(WFOåœºæ™¯ç”¨äºIS/OOSæ•°æ®åˆ‡åˆ†)
            factors: å¤–éƒ¨ä¼ å…¥çš„å› å­åˆ—è¡¨
        """

        try:
            mp.set_start_method("fork", force=True)
        except RuntimeError:
            pass

        self.logger.info("å¼€å§‹é…ç½®åŒ–å¹¶è¡Œç½‘æ ¼æœç´¢...")
        start_time = time.time()

        # åŠ è½½æ•°æ® - æ”¯æŒå¤–éƒ¨ä¼ å…¥(WFOåœºæ™¯)
        self.logger.info("=== æ•°æ®åŠ è½½é˜¶æ®µ ===")

        stage_start = time.time()
        if panel is None:
            panel = self._load_factor_panel()
            self.logger.info(
                f"âœ“ å› å­é¢æ¿ä»æ–‡ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - stage_start:.2f}ç§’"
            )
        else:
            self.logger.info(f"âœ“ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å› å­é¢æ¿ (WFOæ¨¡å¼)ï¼Œå½¢çŠ¶: {panel.shape}")

        stage_start = time.time()
        if prices is None:
            prices = self._load_price_data()
            self.logger.info(
                f"âœ“ ä»·æ ¼çŸ©é˜µä»æ–‡ä»¶åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - stage_start:.2f}ç§’"
            )
        else:
            self.logger.info(
                f"âœ“ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ä»·æ ¼çŸ©é˜µ (WFOæ¨¡å¼)ï¼Œå½¢çŠ¶: {prices.shape}"
            )

        stage_start = time.time()
        if factors is None:
            factors = self._load_top_factors()
        # frozen dataclasséœ€è¦ç”¨replaceåˆ›å»ºæ–°å®ä¾‹
        from dataclasses import replace

        self.config = replace(self.config, factors=factors)
        self.logger.info(f"âœ“ å› å­åˆ—è¡¨åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - stage_start:.2f}ç§’")
        self.logger.info(f"  å®é™…ä½¿ç”¨å› å­æ•°: {len(factors)}")

        # ç”Ÿæˆæƒé‡ç»„åˆ
        self.logger.info("\n=== æƒé‡ç»„åˆç”Ÿæˆ ===")
        stage_start = time.time()
        weight_combos = self._generate_weight_combinations()
        self.logger.info(f"âœ“ æƒé‡ç»„åˆç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time.time() - stage_start:.2f}ç§’")

        # åˆ†å—
        chunks = self._chunk_weight_combinations(weight_combos)
        total_tasks = len(chunks)
        total_rebalance_freqs = len(self.config.rebalance_freq_list)
        total_strategies = (
            len(weight_combos) * len(self.config.top_n_list) * total_rebalance_freqs
        )
        strategies_per_worker = total_strategies / self.config.n_workers

        self.logger.info(f"\n=== å¹¶è¡Œæ‰§è¡Œ ===")
        self.logger.info(
            f"ä»»åŠ¡åˆ†å—: {total_tasks}å— Ã— {self.config.chunk_size}ç»„åˆ/å—, {self.config.n_workers}è¿›ç¨‹å¹¶è¡Œ"
        )
        self.logger.info(
            f"é¢„è®¡å¤„ç†: {len(weight_combos):,}ç»„åˆ Ã— {len(self.config.top_n_list)}ä¸ªTop-N Ã— {total_rebalance_freqs}ä¸ªè°ƒä»“å‘¨æœŸ = {total_strategies:,}ç­–ç•¥"
        )
        self.logger.info(f"è°ƒä»“å‘¨æœŸ: {self.config.rebalance_freq_list}æ—¥")
        self.logger.info(f"æ¯è¿›ç¨‹è´Ÿè½½: ~{strategies_per_worker:.0f}ç­–ç•¥")

        # å¯¹æ¯ä¸ªrebalance_freqæ‰§è¡Œå›æµ‹
        all_results = []
        for rebalance_freq in self.config.rebalance_freq_list:
            self.logger.info(f"\n--- å¼€å§‹å›æµ‹è°ƒä»“å‘¨æœŸ: {rebalance_freq}æ—¥ ---")
            freq_start = time.time()

            # åˆ›å»ºå·¥ä½œå‡½æ•°ï¼ˆå¯¹å½“å‰rebalance_freqï¼‰
            work_func = partial(
                self._process_weight_chunk,
                factors=factors,
                panel=panel,
                prices=prices,
                top_n_list=self.config.top_n_list,
                rebalance_freq=rebalance_freq,
            )

            # å¹¶è¡Œæ‰§è¡Œå½“å‰rebalance_freqçš„æ‰€æœ‰å—
            freq_results = []
            try:
                with mp.Pool(processes=self.config.n_workers) as pool:
                    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                    results_iter = pool.imap_unordered(work_func, chunks)

                    progress_bar = tqdm(
                        results_iter,
                        total=total_tasks,
                        desc=f"å¹¶è¡Œå¤„ç† ({self.config.n_workers}è¿›ç¨‹, rebalance={rebalance_freq}æ—¥)",
                        disable=not self.config.enable_progress_bar,
                    )

                    for chunk_results in progress_bar:
                        freq_results.extend(chunk_results)

            except Exception as e:
                self.logger.error(f"è°ƒä»“å‘¨æœŸ{rebalance_freq}æ—¥çš„å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
                raise

            # è®°å½•å½“å‰rebalance_freqçš„ç»“æœç»Ÿè®¡
            freq_time = time.time() - freq_start
            self.logger.info(
                f"âœ“ è°ƒä»“å‘¨æœŸ{rebalance_freq}æ—¥å®Œæˆ: {len(freq_results):,}ç»“æœ, è€—æ—¶: {freq_time:.2f}ç§’"
            )
            all_results.extend(freq_results)

        # å¤„ç†å…¨éƒ¨ç»“æœ
        processing_time = time.time() - start_time
        n_failed = total_strategies - len(all_results)
        self.logger.info(
            f"âœ“ æ‰€æœ‰è°ƒä»“å‘¨æœŸå¤„ç†å®Œæˆ: {len(all_results):,}ç»“æœ, å¤±è´¥: {n_failed}, æ€»è€—æ—¶: {processing_time:.2f}ç§’"
        )

        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        df = pd.DataFrame(all_results)
        if len(df) > 0:
            df = df.sort_values("sharpe_ratio", ascending=False)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_strategies = (
                len(weight_combos)
                * len(self.config.top_n_list)
                * len(self.config.rebalance_freq_list)
            )
            speed = total_strategies / processing_time
            estimated_sequential_time = total_strategies / 142  # åŸºçº¿é€Ÿåº¦142ç­–/ç§’
            speedup = estimated_sequential_time / processing_time
            efficiency = speedup / self.config.n_workers * 100

            self.logger.info(
                f"âœ“ é€Ÿåº¦: {speed:.1f}ç­–ç•¥/ç§’, åŠ é€Ÿæ¯”: {speedup:.1f}x, å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%"
            )

            # ç»“æœç»Ÿè®¡
            self.logger.info(f"\n=== ç»“æœç»Ÿè®¡ ===")
            best = df.iloc[0]
            self.logger.info(
                f"æœ€ä¼˜ç­–ç•¥: sharpe={best['sharpe_ratio']:.3f}, æ”¶ç›Š={best['total_return']:.2f}%, å›æ’¤={best['max_drawdown']:.2f}%"
            )

            # æœ‰æ•ˆç­–ç•¥ç»Ÿè®¡
            valid_strategies = df[df["sharpe_ratio"] > 0]
            good_strategies = df[df["sharpe_ratio"] > 0.5]
            self.logger.info(
                f"æœ‰æ•ˆç­–ç•¥: {len(valid_strategies):,} (sharpe>0.5: {len(good_strategies):,})"
            )

            if len(df) >= 10:
                top10_sharpe_range = f"[{df.iloc[9]['sharpe_ratio']:.3f}, {df.iloc[0]['sharpe_ratio']:.3f}]"
                self.logger.info(f"Top10å¤æ™®èŒƒå›´: {top10_sharpe_range}")

        return df

    def backtest_specific_strategies(
        self, strategy_params: List[Dict], panel: pd.DataFrame, prices: pd.DataFrame
    ) -> pd.DataFrame:
        """å›æµ‹æŒ‡å®šçš„ç­–ç•¥åˆ—è¡¨(ç”¨äºWFOçš„OOSéªŒè¯)

        Args:
            strategy_params: ISé˜¶æ®µé€‰å‡ºçš„ç­–ç•¥å‚æ•°åˆ—è¡¨
                [{'weights': {...}, 'top_n': 3, 'rebalance_freq': 10}, ...]
            panel: OOSæœŸçš„å› å­é¢æ¿
            prices: OOSæœŸçš„ä»·æ ¼çŸ©é˜µ

        Returns:
            DataFrame with OOS performance metrics
        """
        try:
            mp.set_start_method("fork", force=True)
        except RuntimeError:
            pass

        self.logger.info(f"å¼€å§‹å›æµ‹{len(strategy_params)}ä¸ªæŒ‡å®šç­–ç•¥ (OOSéªŒè¯æ¨¡å¼)...")
        start_time = time.time()

        # åŠ è½½å› å­åˆ—è¡¨
        factors = self._load_top_factors()
        from dataclasses import replace

        self.config = replace(self.config, factors=factors)

        # æŒ‰è°ƒä»“é¢‘ç‡åˆ†ç»„ä»¥ä¾¿å¹¶è¡Œå¤„ç†
        freq_groups = {}
        for params in strategy_params:
            freq = params["rebalance_freq"]
            if freq not in freq_groups:
                freq_groups[freq] = []
            freq_groups[freq].append(params)

        self.logger.info(f"ç­–ç•¥æŒ‰{len(freq_groups)}ä¸ªè°ƒä»“é¢‘ç‡åˆ†ç»„")

        # å¯¹æ¯ä¸ªé¢‘ç‡ç»„å¹¶è¡Œå›æµ‹
        all_results = []
        for freq_idx, (freq, params_list) in enumerate(freq_groups.items(), 1):
            self.logger.info(
                f"\nå¤„ç†é¢‘ç‡ç»„ {freq_idx}/{len(freq_groups)}: è°ƒä»“={freq}å¤©, ç­–ç•¥æ•°={len(params_list)}"
            )

            # æ„é€ æˆchunkæ ¼å¼å¤ç”¨ç°æœ‰æ¶æ„
            # æ¯ä¸ªchunkåŒ…å«è¯¥é¢‘ç‡çš„æ‰€æœ‰æƒé‡ç»„åˆ
            weights_list = [p["weights"] for p in params_list]
            top_n_list = [p["top_n"] for p in params_list]

            # å‡†å¤‡workerå‡½æ•°å‚æ•°
            work_func = partial(
                self._process_specific_strategies,
                factors=factors,
                panel=panel,
                prices=prices,
                rebalance_freq=freq,
            )

            # å°†ç­–ç•¥å‚æ•°åˆ†ç»„æ‰“åŒ…
            strategy_chunks = []
            for params in params_list:
                strategy_chunks.append(
                    {"weights": params["weights"], "top_n": params["top_n"]}
                )

            # åˆ†å—å¹¶è¡Œå¤„ç†
            chunk_size = self.config.chunk_size
            chunks = [
                strategy_chunks[i : i + chunk_size]
                for i in range(0, len(strategy_chunks), chunk_size)
            ]

            # å¹¶è¡Œå›æµ‹
            chunk_start_count = len(all_results)
            with mp.Pool(processes=self.config.n_workers) as pool:
                chunk_results = pool.map(work_func, chunks)
                for chunk_result in chunk_results:
                    all_results.extend(chunk_result)

            # âœ… ä¿®å¤ï¼šç®€åŒ–æ—¥å¿—ï¼Œç›´æ¥è®¡ç®—æœ¬æ¬¡å¢é‡
            completed_count = len(all_results) - chunk_start_count
            self.logger.info(f"  âœ“ å®Œæˆ{completed_count}ä¸ªç­–ç•¥å›æµ‹")

        # æ•´ç†ç»“æœ
        total_time = time.time() - start_time

        if not all_results:
            self.logger.error(f"\næŒ‡å®šç­–ç•¥å›æµ‹å¤±è´¥ï¼šæ²¡æœ‰è¿”å›ä»»ä½•ç»“æœï¼")
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame

        df = pd.DataFrame(all_results)
        df = df.sort_values("sharpe_ratio", ascending=False).reset_index(drop=True)

        self.logger.info(f"\næŒ‡å®šç­–ç•¥å›æµ‹å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        self.logger.info(f"æœ‰æ•ˆç­–ç•¥æ•°: {len(df[df['sharpe_ratio'] > 0])}/{len(df)}")

        if len(df) > 0:
            best = df.iloc[0]
            self.logger.info(
                f"æœ€ä¼˜ç­–ç•¥: sharpe={best['sharpe_ratio']:.3f}, "
                f"æ”¶ç›Š={best['total_return']:.2f}%, "
                f"å›æ’¤={best['max_drawdown']:.2f}%"
            )

        return df

    def _process_specific_strategies(
        self,
        strategy_chunk: List[Dict],
        factors: List[str],
        panel: pd.DataFrame,
        prices: pd.DataFrame,
        rebalance_freq: int,
    ) -> List[Dict]:
        """å¤„ç†æŒ‡å®šçš„ç­–ç•¥chunk (OOSéªŒè¯ç”¨) - å¤ç”¨_process_weight_chunk"""
        # å°†weightsä»dictè½¬ä¸ºtuple (ä¿è¯é¡ºåºä¸factorsä¸€è‡´)
        weight_list = []
        for s in strategy_chunk:
            weights_dict = s["weights"]
            # æŒ‰ç…§factorsé¡ºåºæ„é€ tuple
            weight_tuple = tuple(weights_dict[f] for f in factors)
            weight_list.append(weight_tuple)

        # âœ… ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„å…¨éƒ¨top_nå€¼ï¼Œè€Œä¸æ˜¯ä»chunkæå–
        # è¿™æ ·æ¯ä¸ªchunkéƒ½ä¼šæµ‹è¯•æ‰€æœ‰top_nï¼Œä¿è¯ç»“æœæ•°é‡ä¸€è‡´
        top_n_list = self.config.top_n_list

        # è°ƒç”¨ç°æœ‰çš„_process_weight_chunkæ–¹æ³•
        try:
            results = self._process_weight_chunk(
                weight_chunk=weight_list,
                factors=factors,
                panel=panel,
                prices=prices,
                top_n_list=top_n_list,
                rebalance_freq=rebalance_freq,
            )
            # ç›´æ¥è¿”å›æ‰€æœ‰ç»“æœï¼Œä¸åšç­›é€‰ (å› ä¸ºweightså·²ç»å¯¹åº”)
            return results
        except Exception as e:
            # âœ… ä¿®å¤ï¼šå¼ºåˆ¶è®°å½•é”™è¯¯ï¼Œä¸ä¾èµ–config.log_errors
            self.logger.error(f"OOSå›æµ‹chunkå¤±è´¥ (len={len(strategy_chunk)}): {e}")
            import traceback

            self.logger.error(traceback.format_exc())
            return []

    def run_parallel_backtest(self) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´çš„é…ç½®åŒ–å¹¶è¡Œå›æµ‹"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        preset_info = (
            f" (é¢„è®¾: {self.config.current_preset})"
            if self.config.current_preset
            else ""
        )

        print("=" * 80)
        print("ETFè½®åŠ¨å›æµ‹å¼•æ“ - é…ç½®åŒ–å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬")
        print("=" * 80)
        print(f"æ—¶é—´æˆ³: {timestamp}")
        print(f"é¢„è®¾: {self.config.current_preset or 'é»˜è®¤'}")
        print(f"å·¥ä½œè¿›ç¨‹æ•°: {self.config.n_workers}")
        print(f"å—å¤§å°: {self.config.chunk_size}")
        print(f"æœ€å¤§ç»„åˆæ•°: {self.config.max_combinations}")
        print(f"å†…å­˜é™åˆ¶: {self.config.max_memory_usage_gb}GB")
        print(f"é¢æ¿: {self.config.panel_file}")
        print(f"ç­›é€‰: {self.config.screening_file}")

        # å¹¶è¡Œç½‘æ ¼æœç´¢
        print(
            f"\nå¼€å§‹é…ç½®åŒ–å¹¶è¡Œå›æµ‹: {self.config.max_combinations}ä¸ªæƒé‡ç»„åˆ{preset_info}"
        )
        start_time = time.time()

        results = self.parallel_grid_search()

        total_time = time.time() - start_time
        print(f"\né…ç½®åŒ–å¹¶è¡Œå›æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

        # ä¿å­˜ç»“æœ - åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼Œä¿å­˜Top Né…ç½®
        output_path = Path(self.config.output_dir)
        timestamp_folder = output_path / f"backtest_{timestamp}"
        timestamp_folder.mkdir(parents=True, exist_ok=True)

        # é™åˆ¶ä¿å­˜Top Nï¼Œå‡å°‘æ–‡ä»¶å¤§å°
        top_results = results.head(self.config.save_top_results)
        csv_file = timestamp_folder / "results.csv"
        top_results.to_csv(csv_file, index=False)
        print(
            f"ç»“æœä¿å­˜è‡³: {csv_file} (Top{self.config.save_top_results}/{len(results)}ç­–ç•¥)"
        )

        # è¾“å‡ºTop N
        top_n = min(self.config.save_top_results, len(results))
        print(f"\nTop {top_n} ç­–ç•¥:")
        print(results.head(top_n).to_string(index=False))

        # ä¿å­˜æœ€ä¼˜ç­–ç•¥é…ç½®
        if len(results) > 0:
            best = results.iloc[0]
            best_config = {
                "timestamp": timestamp,
                "engine_type": "configurable_parallel",
                "preset": self.config.current_preset,
                "config": {
                    "n_workers": self.config.n_workers,
                    "chunk_size": self.config.chunk_size,
                    "max_combinations": self.config.max_combinations,
                    "top_n_list": self.config.top_n_list,
                    "rebalance_freq_list": self.config.rebalance_freq_list,
                    "fees": self.config.fees,
                    "init_cash": self.config.init_cash,
                    "weight_grid_points": self.config.weight_grid_points,
                    "weight_sum_range": self.config.weight_sum_range,
                },
                "weights": best["weights"],
                "top_n": int(best["top_n"]),
                "rebalance_freq": int(
                    best.get("rebalance_freq", self.config.rebalance_freq_list[0])
                ),
                "performance": {
                    "total_return": float(best["total_return"]),
                    "sharpe_ratio": float(best["sharpe_ratio"]),
                    "max_drawdown": float(best["max_drawdown"]),
                },
                "factors": self._load_top_factors(),
                "timing": {
                    "total_time": total_time,
                    "strategies_tested": len(results),
                    "speed_per_second": len(results) / total_time,
                },
                "data_source": {
                    "panel": self.config.panel_file,
                    "screening": self.config.screening_file,
                    "price_dir": self.config.price_dir,
                },
            }

            if self.config.save_best_config:
                config_file = timestamp_folder / "best_config.json"
                with open(config_file, "w") as f:
                    json.dump(best_config, f, indent=2, ensure_ascii=False)
                print(f"æœ€ä¼˜é…ç½®ä¿å­˜è‡³: {config_file}")

            # ä¿å­˜æ—¥å¿—æ–‡ä»¶
            log_file = timestamp_folder / "backtest.log"
            with open(log_file, "w") as f:
                f.write(f"ETFè½®åŠ¨å›æµ‹å¼•æ“ - é…ç½®åŒ–å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬\n")
                f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
                f.write(f"é¢„è®¾: {self.config.current_preset or 'é»˜è®¤'}\n")
                f.write(f"\n{'='*80}\n")
                f.write(f"ğŸ“Š å›æµ‹é…ç½®\n")
                f.write(f"{'='*80}\n")
                f.write(
                    f"å·¥ä½œè¿›ç¨‹: {self.config.n_workers} | å—å¤§å°: {self.config.chunk_size} | æœ€å¤§ç»„åˆ: {self.config.max_combinations:,}\n"
                )
                f.write(
                    f"å› å­æ•°: {len(best_config.get('factors', []))} | Top-NèŒƒå›´: {self.config.top_n_list} | è°ƒä»“å‘¨æœŸ: {self.config.rebalance_freq_list}æ—¥\n"
                )
                f.write(
                    f"è´¹ç‡æ¨¡å‹: ä½£é‡‘0.2% + å°èŠ±ç¨0.1% + æ»‘ç‚¹0.01% = {self.config.fees*100:.1f}% å¾€è¿”\n"
                )
                f.write(
                    f"æƒé‡ç½‘æ ¼: {len(self.config.weight_grid_points)}ä¸ªç‚¹ (ç†è®º{len(best_config.get('factors', []))**len(self.config.weight_grid_points):,}ç»„åˆ)\n"
                )

                f.write(f"\n{'='*80}\n")
                f.write(f"ğŸ“ æ•°æ®æº\n")
                f.write(f"{'='*80}\n")
                panel_name = Path(self.config.panel_file).parent.name
                screening_name = Path(self.config.screening_file).parent.name
                f.write(f"å› å­é¢æ¿: {panel_name}\n")
                f.write(f"å› å­ç­›é€‰: {screening_name}\n")
                f.write(f"ä»·æ ¼æ•°æ®: {Path(self.config.price_dir).name}\n")

                f.write(f"\n{'='*80}\n")
                f.write(f"âš¡ æ‰§è¡Œç»Ÿè®¡\n")
                f.write(f"{'='*80}\n")
                f.write(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)\n")
                f.write(f"æ€»ç­–ç•¥: {len(results):,}ä¸ª\n")
                f.write(f"å¤„ç†é€Ÿåº¦: {len(results)/total_time:.1f}ç­–ç•¥/ç§’\n")
                f.write(
                    f"ç»“æœä¿å­˜: Top {self.config.save_top_results} / {len(results):,}\n"
                )

                f.write(f"\n{'='*80}\n")
                f.write(f"ğŸ† æœ€ä¼˜ç­–ç•¥ (Rank 1)\n")
                f.write(f"{'='*80}\n")
                f.write(f"å¤æ™®æ¯”ç‡: {best['sharpe_ratio']:.4f}\n")
                f.write(f"æ€»æ”¶ç›Šç‡: {best['total_return']:.2f}%\n")
                f.write(f"æœ€å¤§å›æ’¤: {best['max_drawdown']:.2f}%\n")
                f.write(
                    f"Calmaræ¯”ç‡: {best['total_return'] / abs(best['max_drawdown']):.2f}\n"
                )
                f.write(f"æŒä»“æ•°é‡: {int(best['top_n'])}åª\n")

                # æƒé‡åˆ†æ
                import ast

                weights = (
                    best["weights"]
                    if isinstance(best["weights"], dict)
                    else ast.literal_eval(best["weights"])
                )
                sorted_weights = sorted(
                    [(k, v) for k, v in weights.items() if v > 0],
                    key=lambda x: x[1],
                    reverse=True,
                )
                f.write(f"\næƒé‡åˆ†é…:\n")
                for factor, weight in sorted_weights:
                    f.write(f"  â€¢ {factor}: {weight:.2f}\n")

                f.write(f"\n{'='*80}\n")
                f.write(f"ğŸ“ˆ æ€§èƒ½åˆ†å¸ƒ\n")
                f.write(f"{'='*80}\n")
                f.write(f"å¹³å‡Sharpe: {results['sharpe_ratio'].mean():.4f}\n")
                f.write(f"ä¸­ä½æ•°Sharpe: {results['sharpe_ratio'].median():.4f}\n")
                f.write(f"æœ€é«˜Sharpe: {results['sharpe_ratio'].max():.4f}\n")
                f.write(f"æœ€ä½Sharpe: {results['sharpe_ratio'].min():.4f}\n")
                f.write(
                    f"æ­£æœŸæœ›(Sharpe>0): {len(results[results['sharpe_ratio'] > 0]):,} ({len(results[results['sharpe_ratio'] > 0])/len(results)*100:.1f}%)\n"
                )
                f.write(
                    f"ä¼˜ç§€ç­–ç•¥(Sharpe>0.5): {len(results[results['sharpe_ratio'] > 0.5]):,} ({len(results[results['sharpe_ratio'] > 0.5])/len(results)*100:.1f}%)\n"
                )
                f.write(
                    f"é«˜è´¨é‡(Sharpe>0.7): {len(results[results['sharpe_ratio'] > 0.7]):,} ({len(results[results['sharpe_ratio'] > 0.7])/len(results)*100:.1f}%)\n"
                )

                f.write(f"\n{'='*80}\n")
                f.write(f"âœ… Top 5 ç­–ç•¥\n")
                f.write(f"{'='*80}\n")
                for idx in range(min(5, len(results))):
                    row = results.iloc[idx]
                    f.write(
                        f"#{idx+1}: Sharpe={row['sharpe_ratio']:.4f} | Return={row['total_return']:.2f}% | DD={row['max_drawdown']:.2f}% | Top_N={int(row['top_n'])}\n"
                    )

        return results, best_config if len(results) > 0 else {}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="é…ç½®åŒ–å¹¶è¡ŒETFè½®åŠ¨å›æµ‹å¼•æ“",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®
  python parallel_backtest_configurable.py

  # ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶
  python parallel_backtest_configurable.py --config-file my_config.yaml

  # ä½¿ç”¨é¢„è®¾åœºæ™¯
  python parallel_backtest_configurable.py --preset comprehensive

  # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
  python parallel_backtest_configurable.py --create-config
        """,
    )

    parser.add_argument(
        "--config-file",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: parallel_backtest_config.yaml)",
    )

    parser.add_argument("--preset", type=str, help="ä½¿ç”¨çš„é¢„è®¾åœºæ™¯åç§°")

    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")

    args = parser.parse_args()

    # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
    if args.create_config:
        from config_loader_parallel import create_default_parallel_config

        config_path = args.config_file or "parallel_backtest_config.yaml"
        create_default_parallel_config(config_path)
        print(f"é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
        return

    # åŠ è½½é…ç½®ï¼ˆé›¶å¼€é”€å¿«é€Ÿé…ç½®ï¼‰
    try:
        config = load_fast_config_from_args(args)
    except Exception as e:
        print(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    # åˆ›å»ºå¼•æ“å¹¶è¿è¡Œ
    engine = ConfigurableParallelBacktestEngine(config)

    try:
        results, best_config = engine.run_parallel_backtest()

        print("\nğŸ¯ é…ç½®åŒ–å¹¶è¡Œä¼˜åŒ–æ€»ç»“:")
        print(f"å¤„ç†æ—¶é—´: {best_config['timing']['total_time']:.2f}ç§’")
        print(f"å¤„ç†é€Ÿåº¦: {best_config['timing']['speed_per_second']:.1f}ç­–ç•¥/ç§’")
        print(f"å·¥ä½œè¿›ç¨‹: {best_config['config']['n_workers']}ä¸ª")
        print(f"æœ€å¤§ç»„åˆ: {best_config['config']['max_combinations']}")
        print(f"å½“å‰é¢„è®¾: {best_config['preset'] or 'é»˜è®¤'}")
        print(f"æœ€ä¼˜å¤æ™®æ¯”ç‡: {best_config['performance']['sharpe_ratio']:.3f}")
        print(f"æœ€ä¼˜æ”¶ç›Šç‡: {best_config['performance']['total_return']:.2f}%")

    except Exception as e:
        print(f"å›æµ‹æ‰§è¡Œå¤±è´¥: {e}")
        if config.log_errors:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
