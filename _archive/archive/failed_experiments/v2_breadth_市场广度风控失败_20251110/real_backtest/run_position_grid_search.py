"""
Top 500 æ”¶ç›Šå‚æ•°çš„æŒä»“æ•°ç½‘æ ¼æœç´¢

è®¾è®¡ç†å¿µï¼š
1. æå– all_freq_scan ä¸­æ”¶ç›Šæœ€é«˜çš„ 500 ä¸ªå‚æ•°é…ç½®
2. åªæµ‹è¯•æŒä»“æ•° 1-10 çš„å˜åŒ–
3. å¤§å¹…é™ä½è®¡ç®—é‡ï¼ˆä» 30000+ é™åˆ° 5000ï¼‰
4. æ‰¾åˆ°æœ€ä¼˜çš„æŒä»“æ•°é…ç½®
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

# å¯é€‰ï¼štqdm è¿›åº¦æ¡ï¼ˆå»ºè®®å®‰è£… tqdm ä¸ tqdm-joblibï¼‰
try:
    from tqdm.auto import tqdm  # type: ignore
    try:
        from tqdm_joblib import tqdm_joblib  # type: ignore
    except Exception:  # pragma: no cover
        tqdm_joblib = None  # type: ignore
except Exception:  # pragma: no cover
    tqdm = None  # type: ignore
    tqdm_joblib = None  # type: ignore
from joblib import Parallel, delayed
from scipy.stats import spearmanr

from run_production_backtest import backtest_no_lookahead


def extract_top500_params():
    """ä»all_freq_scanä¸­æå–Top 500æ”¶ç›Šçš„å‚æ•°"""

    # è¯»å–å…¨éƒ¨æ‰«æç»“æœ(ä»çˆ¶ç›®å½•)
    freq_scan_file = (
        "../results_combo_wfo/20251106_021606/all_freq_scan_20251106_021606.csv"
    )

    if not Path(freq_scan_file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {freq_scan_file}")
        return None

    df = pd.read_csv(freq_scan_file)

    # æŒ‰annual_retæ’åºï¼Œå–Top 500
    df_sorted = df.nlargest(500, "annual_ret")

    print(f"ğŸ“Š å·²ä»æ‰«æç»“æœä¸­æå– Top 500 å‚æ•°")
    print(
        f"   æ”¶ç›ŠèŒƒå›´: {df_sorted['annual_ret'].min():.4f} ~ {df_sorted['annual_ret'].max():.4f}"
    )
    print(
        f"   SharpeèŒƒå›´: {df_sorted['sharpe'].min():.4f} ~ {df_sorted['sharpe'].max():.4f}"
    )
    print()

    # æå–å”¯ä¸€çš„å› å­ç»„åˆå’Œé¢‘ç‡
    unique_configs = []
    seen = set()

    for idx, row in df_sorted.iterrows():
        # ä½¿ç”¨combo + wfo_freqä½œä¸ºå”¯ä¸€æ ‡è¯†
        config_key = (row["combo"], row["wfo_freq"])
        if config_key not in seen:
            seen.add(config_key)
            unique_configs.append(
                {
                    "combo": row["combo"],
                    "wfo_freq": row["wfo_freq"],
                    "test_freq": row["test_freq"],
                    "top_annual_ret": row["annual_ret"],
                }
            )

    print(f"âœ… æå–äº† {len(unique_configs)} ä¸ªå”¯ä¸€å‚æ•°é…ç½®")
    print()

    return unique_configs


def create_grid_search_task_list(unique_configs, position_size_range=range(1, 11)):
    """åˆ›å»ºç½‘æ ¼æœç´¢ä»»åŠ¡åˆ—è¡¨

    å¯¹æ¯ä¸ªå”¯ä¸€çš„å› å­ç»„åˆ + é¢‘ç‡é…ç½®ï¼Œæµ‹è¯•æ‰€æœ‰æŒä»“æ•°
    """

    tasks = []
    for config in unique_configs:
        for pos_size in position_size_range:
            tasks.append(
                {
                    "combo": config["combo"],
                    "wfo_freq": config["wfo_freq"],
                    "test_freq": config["test_freq"],
                    "position_size": pos_size,
                }
            )

    print(f"ğŸ“‹ ç”Ÿæˆäº† {len(tasks)} ä¸ªä»»åŠ¡")
    print(f"   é…ç½®æ•°: {len(unique_configs)}")
    print(f"   æŒä»“æ•°èŒƒå›´: {min(position_size_range)}-{max(position_size_range)}")
    print()

    return tasks


def load_data_and_config():
    """åŠ è½½æ•°æ®å’Œé…ç½®"""

    from core.cross_section_processor import CrossSectionProcessor
    from core.data_loader import DataLoader
    from core.precise_factor_library_v2 import PreciseFactorLibrary

    # åŠ è½½é…ç½®
    with open("configs/combo_wfo_config.yaml", "r") as f:
        config = yaml.safe_load(f)

    # åŠ è½½æ•°æ®
    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=config["data"]["end_date"],
        use_cache=True,
    )

    # è®¡ç®—å› å­
    factor_lib = PreciseFactorLibrary()
    factors_df = factor_lib.compute_all_factors(prices=ohlcv)
    factors_dict = {name: factors_df[name] for name in factor_lib.list_factors()}

    # æ¨ªæˆªé¢æ ‡å‡†åŒ–
    processor = CrossSectionProcessor(
        lower_percentile=config["cross_section"]["winsorize_lower"] * 100,
        upper_percentile=config["cross_section"]["winsorize_upper"] * 100,
        verbose=False,
    )
    standardized_factors = processor.process_all_factors(factors_dict)

    # ç»„ç»‡æ•°æ®
    factor_names = sorted(standardized_factors.keys())
    factor_arrays = [standardized_factors[name].values for name in factor_names]
    factors_data = np.stack(factor_arrays, axis=-1)

    returns_df = ohlcv["close"].pct_change(fill_method=None)
    returns = returns_df.values
    etf_names = list(ohlcv["close"].columns)
    dates = returns_df.index.strftime("%Y-%m-%d").tolist()

    return config, factors_data, returns, etf_names, dates


def run_grid_search(output_dir="../results_combo_wfo"):
    """æ‰§è¡Œ Top 500 æ”¶ç›Šå‚æ•°çš„æŒä»“æ•°ç½‘æ ¼æœç´¢"""

    print("\n" + "=" * 80)
    print("ğŸš€ Top 500 æ”¶ç›Šå‚æ•° - æŒä»“æ•°ç½‘æ ¼æœç´¢")
    print("=" * 80)
    print()

    # ç¬¬1æ­¥ï¼šæå–Top 500å‚æ•°
    unique_configs = extract_top500_params()
    if unique_configs is None:
        return

    # ç¬¬2æ­¥ï¼šåˆ›å»ºä»»åŠ¡åˆ—è¡¨
    position_size_range = range(1, 11)
    tasks = create_grid_search_task_list(unique_configs, position_size_range)

    # ç¬¬3æ­¥ï¼šåŠ è½½æ•°æ®å’Œé…ç½®
    print("ğŸ“¥ åŠ è½½æ•°æ®å’Œé…ç½®...")
    config, factors_data, returns, etf_names, dates = load_data_and_config()

    print(f"âœ… å·²åŠ è½½æ•°æ®:")
    print(f"   æ—¶é—´èŒƒå›´: {dates[0]} ~ {dates[-1]}")
    print(f"   äº¤æ˜“æ—¥æ•°: {len(dates)}")
    print(f"   ETFæ•°é‡: {len(etf_names)}")
    print(f"   å› å­æ•°é‡: {factors_data.shape[2]}")
    print()

    # ç¬¬4æ­¥ï¼šæ‰§è¡Œä»»åŠ¡
    print("âš™ï¸  å¼€å§‹æ‰§è¡Œä»»åŠ¡...")
    print("-" * 80)

    def run_task(task):
        """è¿è¡Œå•ä¸ªä»»åŠ¡"""
        try:
            result = backtest_no_lookahead(
                factors_data=factors_data,
                returns=returns,
                etf_names=etf_names,
                rebalance_freq=task["test_freq"],
                lookback_window=config["backtest"]["lookback_window"],
                position_size=task["position_size"],
                commission_rate=config["backtest"].get("commission_rate", 0.00005),
                initial_capital=config["backtest"]["initial_capital"],
            )

            # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
            result["combo"] = task["combo"]
            result["wfo_freq"] = task["wfo_freq"]
            result["test_freq"] = task["test_freq"]
            result["position_size"] = task["position_size"]
            result["test_position_size"] = task["position_size"]

            return result
        except Exception as e:
            print(
                f"âŒ é”™è¯¯ - {task['combo'][:30]}... pos={task['position_size']}: {str(e)[:50]}"
            )
            return None

    # å¹¶è¡Œæ‰§è¡Œï¼ˆé›†æˆ tqdm è¿›åº¦æ¡ï¼‰
    use_tqdm = (tqdm is not None) and (tqdm_joblib is not None)

    if use_tqdm:
        print("ğŸ“Ÿ ä½¿ç”¨ tqdm è¿›åº¦æ¡ç›‘æ§ä»»åŠ¡è¿›åº¦ï¼ˆå¦‚éœ€å…³é—­è¯·å¸è½½ tqdm-joblib æˆ–æ”¹ç”¨ verboseï¼‰")
        with tqdm_joblib(tqdm(total=len(tasks), desc="å›æµ‹è¿›åº¦", dynamic_ncols=True)):
            results = Parallel(
                n_jobs=config["backtest"]["max_workers"],
                backend="loky",
                verbose=0,
            )(delayed(run_task)(task) for task in tasks)
    else:
        if tqdm is None or tqdm_joblib is None:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ° tqdm/tqdm-joblibï¼Œå›é€€ä¸º joblib è‡ªå¸¦è¿›åº¦æ—¥å¿—ã€‚\n   å®‰è£…å»ºè®®: pip install tqdm tqdm-joblib")
        results = Parallel(
            n_jobs=config["backtest"]["max_workers"],
            backend="loky",
            verbose=10,
        )(delayed(run_task)(task) for task in tasks)

    print()
    print("-" * 80)

    # ç¬¬5æ­¥ï¼šå¤„ç†ç»“æœ
    valid_results = [r for r in results if r is not None]
    print(f"âœ… æˆåŠŸå®Œæˆ {len(valid_results)}/{len(tasks)} ä¸ªä»»åŠ¡")
    print()

    # ç¬¬6æ­¥ï¼šä¿å­˜ç»“æœ
    df_results = pd.DataFrame(valid_results)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_dir = Path(output_dir) / timestamp
    result_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜CSV
    output_file = result_dir / f"top500_pos_scan_{timestamp}.csv"
    df_results.to_csv(output_file, index=False)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")

    # ç¬¬7æ­¥ï¼šåˆ†æç»“æœ
    print()
    print("ğŸ“Š åˆ†æç»“æœ...")
    print("=" * 80)

    stats = (
        df_results.groupby("test_position_size")
        .agg(
            {
                "sharpe": ["mean", "std", "min", "max"],
                "annual_ret": ["mean", "std", "min", "max"],
                "max_dd": ["mean"],
                "win_rate": ["mean"],
            }
        )
        .round(4)
    )

    print(stats)
    print()

    # æ‰¾åˆ°æœ€ä¼˜æŒä»“æ•°
    optimal_pos = df_results.groupby("test_position_size")["sharpe"].mean().idxmax()
    optimal_sharpe = df_results.groupby("test_position_size")["sharpe"].mean().max()
    optimal_annual = df_results[df_results["test_position_size"] == optimal_pos][
        "annual_ret"
    ].mean()

    print("ğŸ¯ æœ€ä¼˜æŒä»“æ•°åˆ†æ:")
    print(f"   æœ€ä¼˜æŒä»“æ•°: {optimal_pos}")
    print(f"   å¹³å‡Sharpe: {optimal_sharpe:.4f}")
    print(f"   å¹³å‡å¹´åŒ–: {optimal_annual:.4f}")
    print()

    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report_file = result_dir / f"top500_analysis_{timestamp}.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("Top 500 æ”¶ç›Šå‚æ•° - æŒä»“æ•°ç½‘æ ¼æœç´¢åˆ†æ\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ‰§è¡Œæ—¶é—´: {timestamp}\n")
        f.write(f"ä»»åŠ¡æ•°: {len(tasks)}\n")
        f.write(f"æˆåŠŸç‡: {len(valid_results)}/{len(tasks)}\n\n")
        f.write("æ€§èƒ½ç»Ÿè®¡:\n")
        f.write(str(stats) + "\n\n")
        f.write(f"æœ€ä¼˜æŒä»“æ•°: {optimal_pos}\n")
        f.write(f"æœ€ä¼˜Sharpe: {optimal_sharpe:.4f}\n")
        f.write(f"æœ€ä¼˜å¹´åŒ–: {optimal_annual:.4f}\n")

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return df_results, result_dir


if __name__ == "__main__":
    df, output_dir = run_grid_search()
    print()
    print(f"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼è¾“å‡ºç›®å½•: {output_dir}")
