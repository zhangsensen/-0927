"""
ç”Ÿäº§å›æµ‹ä¸»è„šæœ¬ï¼ˆæ— æœªæ¥å‡½æ•°ä¿éšœ + å¯é€‰æ€§èƒ½/è¯Šæ–­å¼€å…³ï¼‰
================================================================================
ä¸¥æ ¼æ—¶é—´éš”ç¦»ï¼šæ¯ä¸ªè°ƒä»“æ—¥åªä½¿ç”¨æˆªè‡³å‰ä¸€æ—¥çš„å†å²æ•°æ®ã€‚

æ ¸å¿ƒåŸåˆ™
---------
1) å› å­è®¡ç®—ï¼šé€æ—¥è®¡ç®—ï¼Œä¸æå‰è®¡ç®—å…¨éƒ¨æ—¶é—´åºåˆ—ã€‚
2) æƒé‡è®¡ç®—ï¼šæ¯ä¸ªè°ƒä»“æ—¥ç”¨å†å²çª—å£è®¡ç®— IC æƒé‡ï¼ˆå¯èµ°â€œæ—¥çº§ICé¢„è®¡ç®—â€è·¯å¾„ï¼‰ã€‚
3) ä¿¡å·è®¡ç®—ï¼šæ¯ä¸ªè°ƒä»“æ—¥ç”¨å½“æ—¥å› å­å€¼è®¡ç®—ä¿¡å·ã€‚
4) é€‰è‚¡å†³ç­–ï¼šåŸºäºå½“æ—¥ä¿¡å·ï¼Œä¸çŸ¥é“æœªæ¥ä¿¡å·ã€‚

å…³é”®ç¯å¢ƒå˜é‡ï¼ˆåªåˆ—æœ€å¸¸ç”¨ï¼‰
--------------------------
- RB_DAILY_IC_PRECOMP=1    å¯ç”¨â€œæ—¥çº§ICé¢„è®¡ç®— + å‰ç¼€å’Œ O(1) æ»‘çª—â€ã€‚
- RB_DAILY_IC_MEMMAP=1     é€šè¿‡ np.memmap åœ¨å¤šè¿›ç¨‹é—´å…±äº«æ—¥çº§ICçŸ©é˜µã€‚
- RB_STABLE_RANK=1         Spearman ä½¿ç”¨â€œå¹³å‡ tiesâ€çš„ç¨³å®šæ’åï¼ˆæ›´é²æ£’ï¼‰ã€‚
- RB_PRELOAD_IC=1          é¢„è£…å¸¸ç”¨ (freqÃ—factor) é…å¯¹ä»¥æé«˜ç¼“å­˜å‘½ä¸­ã€‚
- RB_NUMBA_WARMUP=1        è¿›ç¨‹å¯åŠ¨æ—¶å¯¹å…³é”® numba è·¯å¾„åšä¸€æ¬¡é¢„çƒ­ã€‚
- RB_ENFORCE_NO_LOOKAHEAD  å¼€å¯æŠ½æ ·é‡ç®—åšè‡ªæ£€ï¼ˆä¸ç¨³å®šæ’åè·¯å¾„å­˜åœ¨å¾®å°æ•°å€¼å·®å¼‚ï¼‰ã€‚
- RB_NL_CHECK_TOL          è‡ªæ£€æƒé‡å·®å¼‚å®¹å·®ï¼ˆç¨³å®šæ’åå»ºè®® 1e-2ï¼‰ã€‚
- RB_OUTLIER_REPORT        æ‰“å°ç»„åˆçº§è€—æ—¶ outlier è¯Šæ–­ï¼ˆä»…è¯Šæ–­æœŸå¼€ï¼‰ã€‚
- RB_PROFILE_BACKTEST      è¾“å‡ºåˆ†é˜¶æ®µè€—æ—¶ç»Ÿè®¡ï¼ˆmean/median/p95/p99ï¼‰ã€‚

è¯´æ˜
----
ç¨³å®šæ’åè·¯å¾„ï¼ˆRB_STABLE_RANK=1ï¼‰åœ¨æ—¥çº§ICé¢„è®¡ç®—ä¸æ—§çš„â€œçª—å£å†…å³æ—¶é‡ç®—â€ä¹‹é—´å¯èƒ½å­˜åœ¨è½»å¾®æ•°å€¼å·®å¼‚ï¼Œ
å±äºå¹¶åˆ—ç§©å¤„ç†æ–¹å¼ä¸åŒå¯¼è‡´çš„å¯è§£é‡Šåå·®ã€‚ä¸¥æ ¼å®¡è®¡æ—¶è¯·é€‚å½“æ”¾å®½ RB_NL_CHECK_TOLï¼ˆå¦‚ 1e-2ï¼‰ã€‚
"""

import logging
import os
import hashlib
from multiprocessing import Manager
import sys
import time
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import yaml
from joblib import Parallel, delayed
from numba import njit, prange

# --- ensure package import works when launched from repo root or any cwd ---
_HERE = Path(__file__).resolve().parent
_PKG_ROOT = _HERE.parent  # etf_rotation_optimized
for p in (_HERE, _PKG_ROOT):
    sp = str(p)
    if sp not in sys.path:
        sys.path.append(sp)

from core.cross_section_processor import CrossSectionProcessor
from core.data_loader import DataLoader
from core.ic_calculator_numba import compute_spearman_ic_numba
from core.precise_factor_library_v2 import PreciseFactorLibrary

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)

# å…¨å±€æ»šåŠ¨ICæƒé‡ç¼“å­˜ï¼šé¿å…è·¨ç»„åˆé‡å¤è®¡ç®—
# key = (rebalance_key: bytes, lookback: int) -> {
#   "ic": np.ndarray (n_rebalance, F_total),
#   "filled": np.ndarray (F_total,) bool
# }
IC_CACHE = {}
# é¢„è®¡ç®—çš„â€œæ¯æ—¥ICçŸ©é˜µâ€(T, F_total) åŠå…¶ç´¢å¼•ç¼“å­˜
# key é‡‡ç”¨åº•å±‚å†…å­˜åœ°å€ä¸shapeæ„æˆï¼Œå°½å¯èƒ½é¿å…è¯¯ç”¨ï¼ˆè¿›ç¨‹å†…æœ‰æ•ˆï¼‰
PRECOMP_DAILY_IC = {}
NUMBA_WARMED_UP = False


def _arr_mem_key(arr: np.ndarray) -> tuple:
    try:
        # ä½¿ç”¨åº•å±‚dataæŒ‡é’ˆåœ°å€+shape+dtypeä½œä¸ºé”®ï¼Œä»¥å‡å°‘è¯¯åˆ¤
        return (int(arr.__array_interface__["data"][0]), arr.shape, str(arr.dtype))
    except Exception:
        # å›é€€ï¼šä»…ç”¨idå’Œshape
        return (id(arr), arr.shape, str(arr.dtype))


@njit(cache=True)
def _spearman_single_day_simple(x: np.ndarray, y: np.ndarray) -> float:
    """ç®€å•ç§©å®ç°ï¼ˆå½“å‰é»˜è®¤ï¼‰ï¼Œå¹¶åˆ—ä½¿ç”¨æ¬¡åºç§©ï¼Œä¸åšå¹³å‡ã€‚"""
    mask = ~(np.isnan(x) | np.isnan(y))
    n_valid = np.sum(mask)
    if n_valid <= 2:
        return np.nan
    xv = x[mask]
    yv = y[mask]
    xr = np.argsort(np.argsort(xv)).astype(np.float64)
    yr = np.argsort(np.argsort(yv)).astype(np.float64)
    xm = np.mean(xr)
    ym = np.mean(yr)
    num = np.sum((xr - xm) * (yr - ym))
    xs = np.sqrt(np.sum((xr - xm) ** 2))
    ys = np.sqrt(np.sum((yr - ym) ** 2))
    if xs > 0 and ys > 0:
        return num / (xs * ys)
    return np.nan


@njit(cache=True)
def _average_ranks(values: np.ndarray) -> np.ndarray:
    """å¹³å‡å¹¶åˆ—ç§©ï¼ˆstable rankï¼‰ã€‚
    ç®—æ³•: æ’åºåé¡ºåºæ‰«æï¼Œå°†ç›¸ç­‰å€¼åŒºé—´èµ‹äºˆ (start+end)/2 å¹³å‡ç§©ã€‚
    è¿”å›: float64 ranks æ•°ç»„ã€‚
    """
    n = values.shape[0]
    order = np.argsort(values)
    ranks = np.empty(n, dtype=np.float64)
    i = 0
    while i < n:
        j = i + 1
        v = values[order[i]]
        while j < n and values[order[j]] == v:
            j += 1
        # å¹³å‡ç§©: (i + j - 1)/2
        avg = (i + j - 1) / 2.0
        for k in range(i, j):
            ranks[order[k]] = avg
        i = j
    return ranks

def _average_ranks_py(values: np.ndarray) -> np.ndarray:
    """Python fallback ä½¿ç”¨ scipy.stats.rankdata(method='average').
    ä»…åœ¨éœ€è¦ä¸å¤–éƒ¨åº“å¯¹é½æˆ–è°ƒè¯•æ—¶ä½¿ç”¨ï¼Œé¿å…åœ¨ numba ä¸‹å¼•å…¥é¢å¤–ä¾èµ–ã€‚
    """
    try:
        from scipy.stats import rankdata
        return rankdata(values, method='average').astype(np.float64) - 1.0  # è½¬ä¸º0åŸº
    except Exception:
        return _average_ranks(values)


@njit(cache=True)
def _spearman_single_day_stable(x: np.ndarray, y: np.ndarray) -> float:
    """ç¨³å®šç§© Spearmanï¼šå¹¶åˆ—å–å¹³å‡ç§©ã€‚"""
    mask = ~(np.isnan(x) | np.isnan(y))
    n_valid = np.sum(mask)
    if n_valid <= 2:
        return np.nan
    xv = x[mask]
    yv = y[mask]
    xr = _average_ranks(xv)
    yr = _average_ranks(yv)
    xm = np.mean(xr)
    ym = np.mean(yr)
    num = np.sum((xr - xm) * (yr - ym))
    xs = np.sqrt(np.sum((xr - xm) ** 2))
    ys = np.sqrt(np.sum((yr - ym) ** 2))
    if xs > 0 and ys > 0:
        return num / (xs * ys)
    return np.nan


@njit(cache=True)
def _window_ic_for_factor_stable(factors_hist_2d: np.ndarray, returns_hist_2d: np.ndarray) -> float:
    """åœ¨å†å²çª—å£å†…ï¼ŒæŒ‰æ—¥è®¡ç®—ç¨³å®šç§© Spearman å†å–å‡å€¼ã€‚"""
    T_hist = factors_hist_2d.shape[0]
    s = 0.0
    c = 0
    for t in range(T_hist):
        ic = _spearman_single_day_stable(factors_hist_2d[t], returns_hist_2d[t])
        if not np.isnan(ic):
            s += ic
            c += 1
    return s / c if c > 0 else 0.0


@njit(cache=True)
def _compute_ic_for_all_factors_stable(factors_hist: np.ndarray, returns_hist: np.ndarray) -> np.ndarray:
    """è®¡ç®—çª—å£å†…æ‰€æœ‰å› å­çš„ç¨³å®šç§©ICï¼ˆæŒ‰æ—¥Spearmanå‡å€¼ï¼‰ã€‚"""
    F_sel = factors_hist.shape[2]
    ics = np.zeros(F_sel, dtype=np.float64)
    for f in range(F_sel):
        ics[f] = _window_ic_for_factor_stable(factors_hist[:, :, f], returns_hist)
    return ics


@njit(parallel=True, cache=True)
def _compute_daily_ic_all_factors_simple(factors_data_full: np.ndarray, returns: np.ndarray) -> np.ndarray:
    """æ¯æ—¥ICçŸ©é˜µï¼ˆç®€å•ç§©ï¼‰ã€‚"""
    T, N, F_total = factors_data_full.shape
    out = np.empty((T, F_total), dtype=np.float64)
    for f in prange(F_total):
        for t in range(T):
            out[t, f] = _spearman_single_day_simple(factors_data_full[t, :, f], returns[t])
    return out


@njit(parallel=True, cache=True)
def _compute_daily_ic_all_factors_stable(factors_data_full: np.ndarray, returns: np.ndarray) -> np.ndarray:
    """æ¯æ—¥ICçŸ©é˜µï¼ˆç¨³å®šå¹³å‡å¹¶åˆ—ç§©ï¼‰ã€‚"""
    T, N, F_total = factors_data_full.shape
    out = np.empty((T, F_total), dtype=np.float64)
    for f in prange(F_total):
        for t in range(T):
            out[t, f] = _spearman_single_day_stable(factors_data_full[t, :, f], returns[t])
    return out


def _numba_warmup():
    """ä¸€æ¬¡æ€§å°æ ·æœ¬è°ƒç”¨ä»¥è§¦å‘numbaç¼–è¯‘ï¼Œé¿å…é¦–æ‰¹ä»»åŠ¡æŠ–åŠ¨ã€‚"""
    global NUMBA_WARMED_UP
    if NUMBA_WARMED_UP:
        return
    try:
        T, N, F = 16, 8, 4
        factors = np.random.rand(T, N, F).astype(np.float64)
        rets = np.random.randn(T, N).astype(np.float64) * 0.001
        _compute_daily_ic_all_factors_simple(factors, rets)
        _compute_daily_ic_all_factors_stable(factors, rets)
        # æ„é€ ä¸€ä¸ª (T,N) ä¿¡å·ç”¨äº compute_spearman_ic_numba
        from core.ic_calculator_numba import compute_spearman_ic_numba as _csi
        sig = factors[:, :, 0]
        _ = _csi(sig, rets)
        NUMBA_WARMED_UP = True
    except Exception:
        # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»æµç¨‹
        NUMBA_WARMED_UP = True


def _compute_or_load_daily_ic_memmap(factors_data_full: np.ndarray, returns: np.ndarray, stable_rank: bool) -> np.ndarray:
    """
    åŸºäº memmap çš„è·¨è¿›ç¨‹å…±äº«æ¯æ—¥ IC çŸ©é˜µã€‚
    ç¯å¢ƒå˜é‡:
        RB_DAILY_IC_MEMMAP_DIR: ç›®å½•ï¼ˆé»˜è®¤ .cacheï¼‰
        RB_DAILY_IC_MEMMAP_FP32: =1 æ—¶å­˜ float32 (èŠ‚çœ IO/å†…å­˜)
        RB_DAILY_IC_MEMMAP_KEY: è‡ªå®šä¹‰æ–‡ä»¶ keyï¼ˆé¿å…åŒå½¢ä¸åŒå†…å®¹å†²çªï¼‰
    æ–‡ä»¶å‘½å: daily_ic_{T}_{N}_{F}_{key}_{dtype}.mmap
    """
    memmap_dir = os.environ.get("RB_DAILY_IC_MEMMAP_DIR", ".cache").strip()
    os.makedirs(memmap_dir, exist_ok=True)
    T, N, F_total = factors_data_full.shape
    use_fp32 = os.environ.get("RB_DAILY_IC_MEMMAP_FP32", "0").strip().lower() in ("1", "true", "yes")
    custom_key = os.environ.get("RB_DAILY_IC_MEMMAP_KEY", "").strip()
    if not custom_key:
        # æ¨¡å¼æ•æ„Ÿ keyï¼šåŠ å…¥æ’åæ¨¡å¼ + ç®—æ³•ç‰ˆæœ¬ï¼Œé¿å… stable/simple äº¤å‰æ±¡æŸ“
        algo_version = "v2stable" if stable_rank else "v1simple"
        try:
            sample = returns.ravel()[:256]
            h = hashlib.sha1(sample.tobytes()).hexdigest()[:12]
        except Exception:
            h = "nohash"
        custom_key = f"auto_{algo_version}_{T}_{N}_{F_total}_{h}"
    dtype_str = "fp32" if use_fp32 else "fp64"
    file_name = f"daily_ic_{custom_key}_{dtype_str}.mmap"
    path = os.path.join(memmap_dir, file_name)

    if os.path.exists(path):
        mm = np.memmap(path, dtype=np.float32 if use_fp32 else np.float64, mode="r", shape=(T, F_total))
        logger.info(f"[daily_ic_memmap] reuse {path} mode={'stable' if stable_rank else 'simple'}")
        return np.asarray(mm, dtype=np.float64)

    # ç®€å•æ–‡ä»¶é”é¿å…å¹¶å‘ç«äº‰
    lock_path = path + ".lock"
    got_lock = False
    for _ in range(50):  # æœ€å¤šç­‰å¾… ~5s
        try:
            fd = os.open(lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
            os.close(fd)
            got_lock = True
            break
        except FileExistsError:
            time.sleep(0.1)
    if not got_lock and os.path.exists(path):
        mm = np.memmap(path, dtype=np.float32 if use_fp32 else np.float64, mode="r", shape=(T, F_total))
        logger.info(f"[daily_ic_memmap] locked â€“ fallback reuse {path} mode={'stable' if stable_rank else 'simple'}")
        return np.asarray(mm, dtype=np.float64)
    if not got_lock:
        # ä»æœªè·å–é”ï¼šé€€åŒ–ä¸ºå†…å­˜è®¡ç®—ï¼ˆä¸å†™ç›˜ï¼Œä¿è¯éš”ç¦»ï¼‰
        logger.info("[daily_ic_memmap] lock wait exhausted, compute in-memory only")
        daily_ic_mat = _compute_daily_ic_all_factors_stable(factors_data_full, returns) if stable_rank else _compute_daily_ic_all_factors_simple(factors_data_full, returns)
        return daily_ic_mat

    try:
        daily_ic_mat = _compute_daily_ic_all_factors_stable(factors_data_full, returns) if stable_rank else _compute_daily_ic_all_factors_simple(factors_data_full, returns)
        arr_to_store = daily_ic_mat.astype(np.float32 if use_fp32 else np.float64)
        mm = np.memmap(path, dtype=arr_to_store.dtype, mode="w+", shape=(T, F_total))
        mm[:] = arr_to_store[:]
        del mm  # å…³é—­æ–‡ä»¶å¥æŸ„
        logger.info(f"[daily_ic_memmap] built {path} mode={'stable' if stable_rank else 'simple'}")
    finally:
        try:
            os.remove(lock_path)
        except Exception:
            pass
    return daily_ic_mat


@njit(cache=True)
def compute_signal_single_day(factors_day, weights):
    """
    è®¡ç®—å•æ—¥ä¿¡å·ï¼ˆæ¨ªæˆªé¢ï¼‰

    å‚æ•°:
        factors_day: (N, F) å•æ—¥å› å­å€¼
        weights: (F,) å› å­æƒé‡

    è¿”å›:
        signal: (N,) å•æ—¥ä¿¡å·
    """
    N, F = factors_day.shape
    signal = np.zeros(N)

    for n in range(N):
        s = 0.0
        w_sum = 0.0
        for f in range(F):
            val = factors_day[n, f]
            if not np.isnan(val):
                s += val * weights[f]
                w_sum += weights[f]
        if w_sum > 0:
            signal[n] = s / w_sum
        else:
            signal[n] = np.nan

    return signal


@njit(cache=True)
def compute_weights_from_ic(factors_hist, returns_hist):
    """
    åŸºäºå†å²ICè®¡ç®—å› å­æƒé‡

    å‚æ•°:
        factors_hist: (T_hist, N, F) å†å²å› å­æ•°æ®
        returns_hist: (T_hist, N) å†å²æ”¶ç›Šæ•°æ®

    è¿”å›:
        weights: (F,) å› å­æƒé‡
    """
    F = factors_hist.shape[2]
    ics = np.zeros(F)

    for f in range(F):
        # è®¡ç®—æ¯ä¸ªå› å­çš„IC
        ic = compute_spearman_ic_numba(factors_hist[:, :, f], returns_hist)
        ics[f] = ic

    # ç»å¯¹å€¼åŠ æƒ
    abs_ics = np.abs(ics)
    if np.sum(abs_ics) > 0:
        weights = abs_ics / np.sum(abs_ics)
    else:
        weights = np.ones(F) / F

    return weights


@njit(cache=True)
def _compute_ic_column_for_factor(factors_data_full, returns, rebalance_indices, lookback_window, factor_idx):
    """
    è®¡ç®—æŸä¸ªå› å­åœ¨æ‰€æœ‰è°ƒä»“æ—¥çš„æ»šåŠ¨ICåºåˆ—ï¼ˆæ— æœªæ¥æ³„éœ²ï¼‰ã€‚

    è¿”å›: (n_rebalance,) å¯¹åº”æ¯ä¸ªè°ƒä»“æ—¥çš„IC
    """
    n_rebalance = len(rebalance_indices)
    ics = np.empty(n_rebalance, dtype=np.float64)
    for i in range(n_rebalance):
        day_idx = rebalance_indices[i]
        hist_start = 0 if day_idx - lookback_window < 0 else day_idx - lookback_window
        hist_end = day_idx
        ics[i] = compute_spearman_ic_numba(
            factors_data_full[hist_start:hist_end, :, factor_idx],
            returns[hist_start:hist_end],
        )
    return ics

@njit(parallel=True, cache=True)
def _compute_ic_columns_for_factors(
    factors_data_full,
    returns,
    rebalance_indices,
    lookback_window,
    factor_indices,
):
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªå› å­çš„æ»šåŠ¨ICåˆ—ï¼Œè¿”å›å½¢çŠ¶ (n_rebalance, K) çš„çŸ©é˜µï¼Œå…¶ä¸­ K=len(factor_indices)ã€‚
    å¤–å±‚å¯¹å› å­å¹¶è¡Œï¼Œå†…å±‚å¯¹è°ƒä»“æ—¥é¡ºåºå¾ªç¯ã€‚
    """
    n_rebalance = len(rebalance_indices)
    K = len(factor_indices)
    out = np.empty((n_rebalance, K), dtype=np.float64)

    for k in prange(K):  # å¹¶è¡Œéå†å› å­
        fi = factor_indices[k]
        for i in range(n_rebalance):
            day_idx = rebalance_indices[i]
            hist_start = 0 if day_idx - lookback_window < 0 else day_idx - lookback_window
            hist_end = day_idx
            out[i, k] = compute_spearman_ic_numba(
                factors_data_full[hist_start:hist_end, :, fi],
                returns[hist_start:hist_end],
            )
    return out

def get_ic_weights_matrix_cached(
    factors_data_full: np.ndarray,
    returns: np.ndarray,
    rebalance_indices: np.ndarray,
    lookback_window: int,
    factor_indices: np.ndarray,
):
    """
    åŸºäºå…¨å±€ç¼“å­˜è¿”å›æŸç»„åˆæ‰€éœ€çš„ICæƒé‡çŸ©é˜µï¼ˆn_rebalance, F_selectedï¼‰ã€‚
    ä»…åœ¨ç¬¬ä¸€æ¬¡è¯·æ±‚æŸä¸ª(è°ƒä»“æ—¥, å›çœ‹çª—å£, å› å­)æ—¶è®¡ç®—å¹¶ç¼“å­˜å¯¹åº”åˆ—ï¼Œåç»­å¤ç”¨ã€‚
    """
    F_total = factors_data_full.shape[2]
    n_reb = len(rebalance_indices)
    # ä»¥rebalance_indiceså’Œlookbackçª—å£æ„é€ ç¼“å­˜key
    rebalance_key = rebalance_indices.tobytes()
    key = (rebalance_key, int(lookback_window))

    cache_entry = IC_CACHE.get(key)
    if cache_entry is None:
        ic_mat = np.empty((n_reb, F_total), dtype=np.float64)
        ic_mat[:] = np.nan
        filled = np.zeros(F_total, dtype=bool)
        IC_CACHE[key] = {"ic": ic_mat, "filled": filled}
        cache_entry = IC_CACHE[key]

    ic_mat = cache_entry["ic"]
    filled = cache_entry["filled"]

    # ============ æ–°å¢ï¼šåŸºäºâ€œæ¯æ—¥ICçŸ©é˜µâ€çš„å¿«é€Ÿæ»šåŠ¨å¹³å‡è·¯å¾„ ============
    enable_daily_ic = (
        os.environ.get("RB_DAILY_IC_PRECOMP", "0").strip().lower() in ("1", "true", "yes")
    )
    enable_memmap = (
        os.environ.get("RB_DAILY_IC_MEMMAP", "0").strip().lower() in ("1", "true", "yes")
    )
    enable_warmup = (
        os.environ.get("RB_NUMBA_WARMUP", "1").strip().lower() in ("1", "true", "yes")
    )
    daily_ic = None
    stable_rank_enabled = os.environ.get("RB_STABLE_RANK", "0").strip().lower() in ("1", "true", "yes")
    if enable_daily_ic:
        if enable_warmup:
            _numba_warmup()
        # ç”Ÿæˆ / å¤ç”¨å…¨å±€ daily ic çŸ©é˜µ
        if enable_memmap:
            daily_ic = _compute_or_load_daily_ic_memmap(factors_data_full, returns, stable_rank_enabled)
        else:
            factors_key = _arr_mem_key(factors_data_full)
            returns_key = _arr_mem_key(returns)
            main_key = (factors_key, returns_key)
            daily_entry = PRECOMP_DAILY_IC.get(main_key)
            if daily_entry is None:
                # è®¡ç®— (T, F_total) è·¨æˆªé¢ spearman (å½“æ—¥)
                daily_ic_mat = _compute_daily_ic_all_factors_stable(factors_data_full, returns) if stable_rank_enabled else _compute_daily_ic_all_factors_simple(factors_data_full, returns)
                PRECOMP_DAILY_IC[main_key] = {"daily_ic": daily_ic_mat}
                daily_entry = PRECOMP_DAILY_IC[main_key]
            daily_ic = daily_entry["daily_ic"]  # (T, F_total)

    # éœ€è¦è¡¥é½çš„åˆ—
    need_list = [int(fi) for fi in factor_indices if not filled[int(fi)]]
    if len(need_list) > 0:
        need_arr = np.asarray(need_list, dtype=np.int64)

        if enable_daily_ic and daily_ic is not None:
            # ä½¿ç”¨æ¯æ—¥ ic çš„æ»šåŠ¨å¹³å‡ï¼šic_window_mean = mean(daily_ic[hist_start:hist_end, fi])
            # é¢„å…ˆæ„é€  cumulative sum ä»¥ O(1) è·å–çª—å£å‡å€¼
            # æ³¨æ„ï¼šrebalance_indices ä¸­ day_idx ä¸åŒ…å«å½“æ—¥ -> hist_end = day_idx
            sel_daily = daily_ic[:, need_arr]  # (T, K)
            # å°† NaN è§†ä¸ºç¼ºå¤±ï¼Œä½¿ç”¨è®¡æ•°çŸ©é˜µ
            valid_mask = ~np.isnan(sel_daily)
            daily_filled = np.where(valid_mask, sel_daily, 0.0)
            cumsum = np.vstack([np.zeros((1, daily_filled.shape[1])), np.cumsum(daily_filled, axis=0)])  # (T+1,K)
            cnt = np.vstack([np.zeros((1, daily_filled.shape[1])), np.cumsum(valid_mask.astype(np.int64), axis=0)])
            for i, day_idx in enumerate(rebalance_indices):
                hist_start = 0 if day_idx - lookback_window < 0 else day_idx - lookback_window
                hist_end = day_idx
                win_sum = cumsum[hist_end, :] - cumsum[hist_start, :]
                win_cnt = cnt[hist_end, :] - cnt[hist_start, :]
                win_mean = np.zeros_like(win_sum)
                nz = win_cnt > 0
                win_mean[nz] = win_sum[nz] / win_cnt[nz]
                ic_mat[i, need_arr] = win_mean
            for fi in need_arr:
                filled[int(fi)] = True
        else:
            # å›é€€ï¼šåŸæœ‰å•åˆ—/æ‰¹é‡ numba è®¡ç®—è·¯å¾„
            if len(need_arr) == 1:
                fi = int(need_arr[0])
                ic_col = _compute_ic_column_for_factor(
                    factors_data_full, returns, rebalance_indices, lookback_window, fi
                )
                ic_mat[:, fi] = ic_col
                filled[fi] = True
            else:
                batch = _compute_ic_columns_for_factors(
                    factors_data_full, returns, rebalance_indices, lookback_window, need_arr
                )  # (n_rebalance, K)
                for idx_k in range(len(need_arr)):
                    fi = int(need_arr[idx_k])
                    ic_mat[:, fi] = batch[:, idx_k]
                    filled[fi] = True

    # æ„é€ å¹¶è¿”å›æ‰€éœ€å› å­æƒé‡çŸ©é˜µ (n_rebalance, len(factor_indices))
    sel_ic = ic_mat[:, factor_indices]  # å¯èƒ½å«NaN
    abs_ic = np.abs(sel_ic)
    weights = np.zeros_like(sel_ic)
    row_sums = np.nansum(abs_ic, axis=1)
    F_sel = len(factor_indices)
    for i in range(len(row_sums)):
        s = row_sums[i]
        if s > 0:
            w = abs_ic[i] / s
        else:
            w = np.full(F_sel, 1.0 / F_sel)
        weights[i] = w
    return weights


def backtest_no_lookahead(
    factors_data,
    returns,
    etf_names,
    rebalance_freq,
    lookback_window=252,
    position_size=4,
    initial_capital=1_000_000.0,
    commission_rate=0.00005,
    commission_min=0.0,
    *,
    factors_data_full=None,
    factor_indices_for_cache=None,
):
    """
    âš ï¸ ä¸¥æ ¼æ— æœªæ¥å‡½æ•°çš„å›æµ‹ (ä¼˜åŒ–ç‰ˆ)

    å‚æ•°:
        factors_data: (T, N, F) å…¨éƒ¨å› å­æ•°æ®
        returns: (T, N) å…¨éƒ¨æ”¶ç›˜åˆ°æ”¶ç›˜æ”¶ç›Š (å®šä¹‰: close[t]/close[t-1]-1)
        etf_names: list, ETFåç§°
        rebalance_freq: int, è°ƒä»“é¢‘ç‡(å¤©)
        lookback_window: int, è®¡ç®—æƒé‡çš„å›çœ‹çª—å£
        position_size: int, æŒä»“æ•°é‡ï¼ˆé»˜è®¤æŒ‰Top Nï¼‰
        initial_capital: float, åˆå§‹èµ„é‡‘
    commission_rate: float, ä½£é‡‘ç‡ï¼ˆåŒè¾¹ï¼Œä¹°å…¥å’Œå–å‡ºéƒ½æ”¶å–ï¼ŒETFé»˜è®¤ä¾‹0.5ï¼‰
    commission_min: float, ä½£é‡‘æœ€ä½è´¹ç”¨ï¼ˆç»å¯¹é‡‘é¢ï¼Œé»˜è®¤0è¡¨ç¤ºä¸å¯ç”¨ï¼‰

    è¿”å›:
        dict: å›æµ‹ç»“æœ
    """
    profile_enabled = os.environ.get("RB_PROFILE_BACKTEST", "0").strip().lower() in ("1", "true", "yes")
    profile_log = logger.info if profile_enabled else (lambda *args, **kwargs: None)
    profile_data = {} if profile_enabled else None

    enforce_nl = os.environ.get("RB_ENFORCE_NO_LOOKAHEAD", "0").strip().lower() in ("1", "true", "yes")
    nl_check_max = int(os.environ.get("RB_NL_CHECK_MAX", "5") or 5)
    try:
        nl_tol = float(os.environ.get("RB_NL_CHECK_TOL", "1e-9") or 1e-9)
    except Exception:
        nl_tol = 1e-9
    try:
        nl_rtol = float(os.environ.get("RB_NL_CHECK_RTOL", "0"))
    except Exception:
        nl_rtol = 0.0
    try:
        nl_atol = float(os.environ.get("RB_NL_CHECK_ATOL", str(nl_tol)))
    except Exception:
        nl_atol = nl_tol
    nl_checks_done = 0

    total_timer_start = time.perf_counter() if profile_enabled else None

    # ä¼˜å…ˆçº§2ï¼šç¡®ä¿å†…å­˜å¸ƒå±€è¿ç»­ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´ä¸ cache missï¼ˆä¸æ”¹å˜ dtype/æ•°å€¼ï¼‰
    factors_data = np.ascontiguousarray(factors_data)
    returns = np.ascontiguousarray(returns)
    if factors_data_full is not None:
        factors_data_full = np.ascontiguousarray(factors_data_full)

    # ä¼˜å…ˆçº§3ï¼šé»˜è®¤å¯ç”¨æ—¥çº§ICé¢„è®¡ç®—+memmapï¼ˆä»…å½“å¯ç”¨ï¼Œä¸”ä¸è¦†ç›–ç”¨æˆ·æ˜¾å¼è®¾ç½®ï¼‰
    os.environ.setdefault("RB_DAILY_IC_PRECOMP", "1")
    os.environ.setdefault("RB_DAILY_IC_MEMMAP", "1")
    os.environ.setdefault("RB_NUMBA_WARMUP", "1")

    T, N, F = factors_data.shape

    start_idx = lookback_window + 1  # +1 å›  returns ç¬¬1å¤©ä¸å¯ç”¨

    rebalance_indices = np.arange(start_idx, T, rebalance_freq, dtype=np.int32)
    n_rebalance = len(rebalance_indices)

    profile_log(
        f"  å›æµ‹å‚æ•°: {rebalance_freq}å¤©æ¢ä»“, Top{position_size}æŒä»“, å›çœ‹{lookback_window}å¤©"
    )
    profile_log(f"  èµ·å§‹æ—¥: ç¬¬{start_idx}å¤©, è°ƒä»“æ¬¡æ•°: {n_rebalance}æ¬¡")

    profile_log("  é¢„è®¡ç®—ICæƒé‡...")
    ic_timer_start = time.perf_counter() if profile_enabled else None
    disable_cache = os.environ.get("RB_DISABLE_IC_CACHE", "0").strip().lower() in ("1", "true", "yes")

    # è‹¥æœªæä¾›å…¨é‡æ•°ç»„/å› å­ç´¢å¼•ï¼Œé€€åŒ–ä¸ºä½¿ç”¨å½“å‰è¾“å…¥ï¼ˆä¸æ”¹å˜è¯­ä¹‰ï¼Œä»…ä½¿é¢„è®¡ç®—è·¯å¾„å¯ç”¨ï¼‰
    if factors_data_full is None:
        factors_data_full = factors_data
    if factor_indices_for_cache is None:
        factor_indices_for_cache = np.arange(F, dtype=np.int64)

    if not disable_cache:
        ic_weights_matrix = get_ic_weights_matrix_cached(
            factors_data_full=factors_data_full,
            returns=returns,
            rebalance_indices=rebalance_indices,
            lookback_window=lookback_window,
            factor_indices=np.asarray(factor_indices_for_cache, dtype=np.int64),
        )
        ic_path_type = (
            "daily_stable"
            if (
                os.environ.get("RB_DAILY_IC_PRECOMP", "0").strip().lower() in ("1", "true", "yes")
                and os.environ.get("RB_STABLE_RANK", "0").strip().lower() in ("1", "true", "yes")
            )
            else (
                "daily_simple"
                if (os.environ.get("RB_DAILY_IC_PRECOMP", "0").strip().lower() in ("1", "true", "yes"))
                else "cached_batch"
            )
        )
    else:
        F_sel = factors_data.shape[2]
        tmp_ic = np.zeros((n_rebalance, F_sel), dtype=np.float64)
        for i in range(n_rebalance):
            day_idx = rebalance_indices[i]
            hist_start = max(0, day_idx - lookback_window)
            hist_end = day_idx
            factors_hist = factors_data[hist_start:hist_end]
            returns_hist = returns[hist_start:hist_end]
            ics = np.zeros(F_sel)
            for f in range(F_sel):
                ics[f] = compute_spearman_ic_numba(factors_hist[:, :, f], returns_hist)
            abs_ics = np.abs(ics)
            tmp_ic[i] = abs_ics / abs_ics.sum() if abs_ics.sum() > 0 else np.ones(F_sel) / F_sel
        ic_weights_matrix = tmp_ic
        ic_path_type = "fallback_simple"

    if profile_enabled:
        profile_data["time_precompute_ic"] = time.perf_counter() - ic_timer_start if ic_timer_start is not None else 0.0
        profile_data["n_rebalance"] = int(n_rebalance)
        profile_data["n_days"] = int(T - start_idx)

    n_days = T - start_idx
    portfolio_values = np.zeros(n_days + 1)
    portfolio_values[0] = initial_capital
    daily_returns_arr = np.zeros(n_days)

    # é¢„åˆ†é…ï¼šæŒ‰â€œè°ƒä»“äº‹ä»¶â€æ¬¡æ•°ï¼Œè€ŒéæŒ‰æ—¥
    turnover_arr = np.empty(n_rebalance, dtype=float) if n_rebalance > 0 else np.empty(0, dtype=float)
    cost_rate_arr = np.empty(n_rebalance, dtype=float) if n_rebalance > 0 else np.empty(0, dtype=float)
    cost_amount_arr = np.empty(n_rebalance, dtype=float) if n_rebalance > 0 else np.empty(0, dtype=float)
    n_holdings_arr = np.empty(n_rebalance, dtype=np.int32) if n_rebalance > 0 else np.empty(0, dtype=np.int32)

    current_weights = np.zeros(N)
    rebalance_counter = 0

    loop_timer_start = time.perf_counter() if profile_enabled else None

    for offset, day_idx in enumerate(range(start_idx, T)):
        is_rebalance_day = (
            rebalance_counter < n_rebalance and day_idx == rebalance_indices[rebalance_counter]
        )

        if is_rebalance_day:
            factor_weights = ic_weights_matrix[rebalance_counter]
            if enforce_nl and nl_checks_done < nl_check_max:
                try:
                    stride = max(1, n_rebalance // max(1, nl_check_max))
                    if (rebalance_counter % stride) == 0:
                        hist_start = max(0, day_idx - lookback_window)
                        hist_end = day_idx
                        F_sel = factors_data.shape[2]
                        ics = np.zeros(F_sel, dtype=np.float64)
                        for f in range(F_sel):
                            ics[f] = compute_spearman_ic_numba(
                                factors_data[hist_start:hist_end, :, f],
                                returns[hist_start:hist_end],
                            )
                        abs_ics = np.abs(ics)
                        w_chk = abs_ics / abs_ics.sum() if abs_ics.sum() > 0 else np.full(F_sel, 1.0 / F_sel)
                        stable_rank_enabled = os.environ.get("RB_STABLE_RANK", "0").strip().lower() in ("1", "true", "yes")
                        daily_precomp_enabled = os.environ.get("RB_DAILY_IC_PRECOMP", "0").strip().lower() in ("1", "true", "yes")
                        can_use_daily = (
                            stable_rank_enabled and daily_precomp_enabled and (factors_data_full is not None) and (factor_indices_for_cache is not None)
                        )
                        if can_use_daily:
                            try:
                                daily_ic_full = _compute_or_load_daily_ic_memmap(factors_data_full, returns, stable_rank=True)
                                cols = np.asarray(factor_indices_for_cache, dtype=np.int64)
                                di_slice = daily_ic_full[hist_start:hist_end][:, cols]
                                window_mean = np.nanmean(di_slice, axis=0)
                                abs_ics_local = np.abs(window_mean)
                                if np.isfinite(abs_ics_local).any() and np.nansum(abs_ics_local) > 0:
                                    w_chk = abs_ics_local / np.nansum(abs_ics_local)
                                else:
                                    w_chk = np.full(F_sel, 1.0 / F_sel)
                            except Exception:
                                ics = np.zeros(F_sel, dtype=np.float64)
                                for f in range(F_sel):
                                    ics[f] = compute_spearman_ic_numba(
                                        factors_data[hist_start:hist_end, :, f],
                                        returns[hist_start:hist_end],
                                    )
                                abs_ics = np.abs(ics)
                                w_chk = (abs_ics / abs_ics.sum()) if abs_ics.sum() > 0 else np.full(F_sel, 1.0 / F_sel)
                        else:
                            ics = np.zeros(F_sel, dtype=np.float64)
                            for f in range(F_sel):
                                ics[f] = compute_spearman_ic_numba(
                                    factors_data[hist_start:hist_end, :, f],
                                    returns[hist_start:hist_end],
                                )
                            abs_ics = np.abs(ics)
                            w_chk = (abs_ics / abs_ics.sum()) if abs_ics.sum() > 0 else np.full(F_sel, 1.0 / F_sel)
                        if not np.allclose(w_chk, factor_weights, rtol=nl_rtol, atol=nl_atol, equal_nan=True):
                            diff = np.nanmax(np.abs(w_chk - factor_weights))
                            raise RuntimeError(
                                f"NO_LOOKAHEAD_CHECK_FAILED: day_idx={day_idx}, max_weight_diff={diff:.3e} (rtol={nl_rtol}, atol={nl_atol})"
                            )
                        nl_checks_done += 1
                except Exception:
                    raise

            # è®°å½•å½“å‰è°ƒä»“äº‹ä»¶ç´¢å¼•ï¼ˆè‡ªå¢å‰ï¼‰
            idx_rb = rebalance_counter
            rebalance_counter += 1

            prev_weights = current_weights.copy()

            factors_yesterday = factors_data[day_idx - 1]
            signal_yesterday = compute_signal_single_day(
                factors_yesterday, factor_weights
            )

            valid_mask = ~np.isnan(signal_yesterday)

            if np.sum(valid_mask) < position_size:
                target_weights = np.zeros(N)
                n_holdings_arr[idx_rb] = 0
            else:
                sig_valid = signal_yesterday.copy()
                sig_valid[~valid_mask] = -np.inf
                kth_val = np.partition(sig_valid, -position_size)[-position_size]
                candidates = np.where(sig_valid >= kth_val)[0]
                if len(candidates) > position_size:
                    order = np.lexsort((candidates, -sig_valid[candidates]))
                    chosen = candidates[order][:position_size]
                else:
                    chosen = candidates[:position_size]
                top_indices = chosen
                target_weights = np.zeros(N)
                target_weights[top_indices] = 1.0 / position_size
                n_holdings_arr[idx_rb] = len(top_indices)

            # æ¢æ‰‹ä¸æˆæœ¬ï¼ˆæŒ‰è°ƒä»“äº‹ä»¶ï¼‰
            delta_weights = target_weights - prev_weights
            buy_turnover = float(np.sum(delta_weights[delta_weights > 0]))
            sell_turnover = float(np.sum(-delta_weights[delta_weights < 0]))
            turnover = buy_turnover + sell_turnover
            turnover_arr[idx_rb] = turnover

            portfolio_before_cost = portfolio_values[offset]
            trade_notional = (buy_turnover + sell_turnover) * portfolio_before_cost
            commission_value = trade_notional * commission_rate
            if commission_min > 0 and turnover > 1e-12:
                commission_value = max(commission_value, commission_min)
            total_cost_amount = commission_value

            if portfolio_before_cost > 1e-12 and total_cost_amount > 0:
                total_cost_amount = min(total_cost_amount, portfolio_before_cost)
                cost_rate = total_cost_amount / portfolio_before_cost
                portfolio_values[offset] = portfolio_before_cost - total_cost_amount
            else:
                cost_rate = 0.0
                total_cost_amount = 0.0

            cost_rate_arr[idx_rb] = cost_rate
            cost_amount_arr[idx_rb] = total_cost_amount

            current_weights = target_weights

        # === æ¯æ—¥æ”¶ç›Šè®¡ç®— ===
        # æ˜¾å¼ä½¿ç”¨æ”¶ç›˜åˆ°æ”¶ç›˜å®šä¹‰ (close[t]/close[t-1]-1)
        close_to_close_ret = returns[day_idx]  # ç­‰ä»·äº (close[t]/close[t-1]-1)
        daily_ret = np.nansum(current_weights * close_to_close_ret)
        daily_returns_arr[offset] = daily_ret

        portfolio_values[offset + 1] = portfolio_values[offset] * (1 + daily_ret)

    if profile_enabled and loop_timer_start is not None:
        profile_data["time_main_loop"] = time.perf_counter() - loop_timer_start

    final = portfolio_values[-1]
    total_ret = final / initial_capital - 1

    days_elapsed = len(daily_returns_arr)
    annual_ret = (1 + total_ret) ** (252 / days_elapsed) - 1

    vol = np.std(daily_returns_arr) * np.sqrt(252)
    sharpe = annual_ret / vol if vol > 0 else 0

    cummax = np.maximum.accumulate(portfolio_values)
    dd = (portfolio_values - cummax) / cummax
    max_dd = np.min(dd)

    positive_returns = daily_returns_arr[daily_returns_arr > 0]
    negative_returns = daily_returns_arr[daily_returns_arr < 0]

    win_rate = (
        len(positive_returns) / len(daily_returns_arr)
        if len(daily_returns_arr) > 0
        else 0.0
    )
    winning_days = len(positive_returns)
    losing_days = len(negative_returns)

    avg_win = float(np.mean(positive_returns)) if len(positive_returns) > 0 else 0.0
    avg_loss = float(np.mean(negative_returns)) if len(negative_returns) > 0 else 0.0

    profit_factor = 0.0
    if losing_days > 0 and abs(np.sum(negative_returns)) > 1e-10:
        profit_factor = float(np.sum(positive_returns) / abs(np.sum(negative_returns)))

    calmar_ratio = annual_ret / abs(max_dd) if abs(max_dd) > 1e-10 else 0.0

    downside_returns = daily_returns_arr[daily_returns_arr < 0]
    downside_vol = (
        np.sqrt(np.mean(downside_returns**2)) * np.sqrt(252)
        if len(downside_returns) > 0
        else 1e-6
    )
    sortino_ratio = annual_ret / downside_vol if downside_vol > 1e-10 else 0.0

    if len(daily_returns_arr) > 0:
        max_consecutive_wins, max_consecutive_losses = calculate_streaks_vectorized(
            daily_returns_arr
        )
    else:
        max_consecutive_wins = 0
        max_consecutive_losses = 0

    avg_n_holdings = float(np.mean(n_holdings_arr)) if n_rebalance > 0 else 0.0

    result = {
        "freq": rebalance_freq,
        "final": final,
        "total_ret": total_ret,
        "annual_ret": annual_ret,
        "vol": vol,
        "sharpe": sharpe,
        "max_dd": max_dd,
        "n_rebalance": n_rebalance,
        "avg_turnover": float(np.mean(turnover_arr)) if n_rebalance > 0 else 0.0,
        # èƒœç‡ç›¸å…³
        "win_rate": win_rate,
        "winning_days": winning_days,
        "losing_days": losing_days,
        "avg_win": avg_win,
        "avg_loss": avg_loss,
        "profit_factor": profit_factor,
        # é«˜çº§é£é™©æŒ‡æ ‡
        "calmar_ratio": calmar_ratio,
        "sortino_ratio": sortino_ratio,
        "max_consecutive_wins": max_consecutive_wins,
        "max_consecutive_losses": max_consecutive_losses,
        # æŒä»“æ•°ç»Ÿè®¡ï¼ˆæŒ‰è°ƒä»“äº‹ä»¶ï¼‰
        "avg_n_holdings": avg_n_holdings,
        # è¯¦ç»†æ•°æ®
        "nav": portfolio_values,
        "daily_returns": daily_returns_arr,
        "turnover_series": turnover_arr,
        "cost_rate_series": cost_rate_arr,
        "cost_amount_series": cost_amount_arr,
    }

    if profile_enabled and total_timer_start is not None:
        profile_data["time_total"] = time.perf_counter() - total_timer_start
        profile_data["rebalance_executed"] = int(rebalance_counter)
        profile_data["loop_iterations"] = int(n_days)
        profile_data["avg_turnover"] = float(result["avg_turnover"])
        profile_data["ic_path"] = ic_path_type
        profile_data["stable_rank"] = os.environ.get("RB_STABLE_RANK", "0").strip().lower() in ("1", "true", "yes")
        result["profile"] = profile_data

    return result


def calculate_streaks_vectorized(daily_returns_arr: np.ndarray):
    """å‘é‡åŒ–è®¡ç®—æœ€é•¿è¿ç»­ç›ˆåˆ©ä¸äºæŸå¤©æ•° (0 æ”¶ç›Šè§†ä¸ºä¸­æ–­)ã€‚

    Args:
        daily_returns_arr: (T,) æ—¥æ”¶ç›Šåºåˆ—
    Returns:
        (max_consecutive_wins, max_consecutive_losses)
    """
    if daily_returns_arr.size == 0:
        return 0, 0
    signs = np.sign(daily_returns_arr).astype(np.int32)
    # æ­£æ•°->1, è´Ÿæ•°->-1, 0 ä¿æŒ0ä½œä¸ºåˆ†éš”
    signs[signs > 0] = 1
    signs[signs < 0] = -1
    max_win = 0
    max_loss = 0
    cur_len = 0
    cur_sign = 0
    for s in signs:
        if s == 0 or s != cur_sign:
            # ç»“ç®—ä¸Šä¸€ä¸ªåºåˆ—
            if cur_sign == 1:
                if cur_len > max_win:
                    max_win = cur_len
            elif cur_sign == -1:
                if cur_len > max_loss:
                    max_loss = cur_len
            # é‡ç½®
            cur_sign = s
            cur_len = 1 if s != 0 else 0
        else:
            cur_len += 1
    # ç»“ç®—æœ€åä¸€ä¸ªåºåˆ—
    if cur_sign == 1:
        if cur_len > max_win:
            max_win = cur_len
    elif cur_sign == -1:
        if cur_len > max_loss:
            max_loss = cur_len
    return max_win, max_loss


def load_top_combos_from_run(run_dir: Path, top_n: int = 100, load_all: bool = False):
    """
    åŠ è½½æŸä¸ª run_ ç›®å½•ä¸‹çš„ç»„åˆåˆ—è¡¨ã€‚
    
    Args:
        run_dir: WFO runç›®å½•
        top_n: åŠ è½½TopNç»„åˆï¼ˆå½“load_all=Falseæ—¶ç”Ÿæ•ˆï¼‰
        load_all: æ˜¯å¦åŠ è½½å…¨é‡ç»„åˆï¼ˆå¿½ç•¥top_né™åˆ¶ï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. è‹¥load_all=Trueï¼Œç›´æ¥åŠ è½½all_combos.parquetå…¨é‡æ•°æ®
    2. å¦åˆ™ï¼Œä¼˜å…ˆè¯»å– top100_by_ic.parquetï¼›
    3. è‹¥ä¸å­˜åœ¨ï¼Œåˆ™è¯»å– top_combos.parquetï¼›
    4. è‹¥ä»ä¸å­˜åœ¨ï¼Œé€€åŒ–ä¸º all_combos.parquet å¹¶æŒ‰ IC/ç¨³å®šæ€§æ’åºå– TopNã€‚

    è¿”å›:
        (df, sort_method_str)
    """
    top_by_ic_file = run_dir / "top100_by_ic.parquet"
    top_combos_file = run_dir / "top_combos.parquet"
    all_combos_file = run_dir / "all_combos.parquet"

    # ğŸ”¥ æ–°å¢ï¼šæ”¯æŒåŠ è½½å…¨é‡ç»„åˆï¼ˆç”¨äºå®Œæ•´æ ·æœ¬è®­ç»ƒï¼‰
    # è‹¥å­˜åœ¨æ ¡å‡†åˆ†æˆ–é¢„æµ‹åˆ—åˆ™ä¼˜å…ˆä½¿ç”¨æ ¡å‡†æ’åºï¼›å¦åˆ™æŒ‰IC/ç¨³å®šæ€§
    def _sort_df(df: pd.DataFrame) -> pd.DataFrame:
        if "calibrated_sharpe_pred" in df.columns:
            return df.sort_values(
                by=["calibrated_sharpe_pred", "stability_score"], ascending=[False, False]
            )
        if "calibrated_sharpe_full" in df.columns:
            return df.sort_values(
                by=["calibrated_sharpe_full", "stability_score"], ascending=[False, False]
            )
        return df.sort_values(
            by=["mean_oos_ic", "stability_score"], ascending=[False, False]
        )

    if load_all:
        if all_combos_file.exists():
            df = pd.read_parquet(all_combos_file)
            df = _sort_df(df)
            sort_label = "ALL calibrated" if ("calibrated_sharpe_pred" in df.columns or "calibrated_sharpe_full" in df.columns) else "ALL IC"
            return df.reset_index(drop=True), f"ALL ({len(df)} combos from all_combos, {sort_label})"
        else:
            raise FileNotFoundError(f"å…¨é‡å›æµ‹æ¨¡å¼éœ€è¦ all_combos.parquetï¼Œä½†æœªæ‰¾åˆ°: {all_combos_file}")

    if top_by_ic_file.exists():
        df = pd.read_parquet(top_by_ic_file).reset_index(drop=True)
        df = _sort_df(df)
        if len(df) >= top_n:
            lbl = "calibrated (top100_by_ic)" if ("calibrated_sharpe_pred" in df.columns or "calibrated_sharpe_full" in df.columns) else "IC (top100_by_ic)"
            return df.head(top_n), lbl
        elif all_combos_file.exists():
            df2 = pd.read_parquet(all_combos_file)
            df2 = _sort_df(df2).head(top_n)
            lbl = "calibrated (from all_combos)" if ("calibrated_sharpe_pred" in df2.columns or "calibrated_sharpe_full" in df2.columns) else "IC (from all_combos)"
            return df2.reset_index(drop=True), lbl
        else:
            return df, "IC (top100_by_ic)"
    if top_combos_file.exists():
        df = pd.read_parquet(top_combos_file)
        df = _sort_df(df)
        lbl = "calibrated (top_combos)" if ("calibrated_sharpe_pred" in df.columns or "calibrated_sharpe_full" in df.columns) else "IC (top_combos)"
        return df.reset_index(drop=True), lbl
    if all_combos_file.exists():
        df = pd.read_parquet(all_combos_file)
        df = _sort_df(df).head(top_n)
        lbl = "calibrated (from all_combos)" if ("calibrated_sharpe_pred" in df.columns or "calibrated_sharpe_full" in df.columns) else "IC (from all_combos)"
    # ä¿®å¤å‚æ•°åé”™è¯¯: drop_more -> drop
    return df.reset_index(drop=True), lbl
    raise FileNotFoundError(
        f"æœªæ‰¾åˆ° {run_dir} ä¸‹çš„ top100_by_ic/top_combos/all_combos æ–‡ä»¶"
    )


def summarize_results(results_df: pd.DataFrame):
    """ç”Ÿæˆæ±‡æ€»æŒ‡æ ‡å­—å…¸ï¼Œç”¨äºæ‰“å°/å¯¹æ¯”ã€‚"""
    from scipy.stats import spearmanr  # å¦‚æœç¼ºå¤±ï¼Œä¼šåœ¨è°ƒç”¨å¤„æ•è·

    summary = {
        "mean_annual": (
            float(results_df["annual_ret"].mean())
            if not results_df.empty
            else float("nan")
        ),
        "mean_sharpe": (
            float(results_df["sharpe"].mean()) if not results_df.empty else float("nan")
        ),
        "mean_max_dd": (
            float(results_df["max_dd"].mean()) if not results_df.empty else float("nan")
        ),
    }
    if {"rank", "sharpe", "annual_ret"}.issubset(results_df.columns):
        corr_sharpe, p_sharpe = spearmanr(results_df["rank"], results_df["sharpe"])
        corr_ret, p_ret = spearmanr(results_df["rank"], results_df["annual_ret"])
        summary.update(
            {
                "spearman_rank_sharpe": float(corr_sharpe),
                "spearman_rank_sharpe_p": float(p_sharpe),
                "spearman_rank_annual": float(corr_ret),
                "spearman_rank_annual_p": float(p_ret),
            }
        )
    return summary


def format_pct(x: float) -> str:
    try:
        return f"{x:>6.1%}"
    except Exception:
        return str(x)


def main():
    """ä¸»å‡½æ•° - è¯»å–æœ€æ–°ä¸ä¸Šä¸€æ¬¡ run çš„ Top100 ç»„åˆï¼Œåˆ†åˆ«å›æµ‹å¹¶è¾“å‡ºå¯¹æ¯”"""

    # åŠ è½½é…ç½®ï¼ˆå¢å¼ºè·¯å¾„é²æ£’æ€§ï¼‰
    cfg_candidates = []
    # 1) CWD ç›¸å¯¹è·¯å¾„
    cfg_candidates.append(Path("configs/combo_wfo_config.yaml").resolve())
    # 2) è„šæœ¬æ‰€åœ¨å·¥ç¨‹è·¯å¾„ä¸Šä¸€çº§çš„ configs
    try:
        cfg_candidates.append((Path(__file__).resolve().parent.parent / "configs" / "combo_wfo_config.yaml").resolve())
    except Exception:
        pass
    # 3) ç¯å¢ƒå˜é‡è¦†ç›–
    env_cfg = os.environ.get("RB_CONFIG_FILE")
    if env_cfg:
        cfg_candidates.insert(0, Path(env_cfg).expanduser().resolve())
    config_path = next((p for p in cfg_candidates if p.exists()), None)
    if not config_path:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå·²å°è¯•: {cfg_candidates}. å¯è®¾ç½® RB_CONFIG_FILE æŒ‡å®šç»å¯¹è·¯å¾„")
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    commission_rate_cfg = config["backtest"].get("commission_rate", 0.00005)

    # è¿è¡Œå‚æ•°è¦†ç›–ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼Œå¯ä¸æ”¹é…ç½®å¿«é€Ÿåˆ‡æ¢æ‰¹é‡è§„æ¨¡ä¸æ‰«ææ¨¡å¼ï¼‰
    # RB_TOPK: è¦†ç›– combo_wfo.top_nï¼Œä¾‹å¦‚ 1000
    # RB_BACKTEST_ALL: "1"/"true" å›æµ‹å…¨é‡ç»„åˆï¼ˆå¿½ç•¥RB_TOPKé™åˆ¶ï¼‰
    # RB_TEST_ALL_FREQS: "1"/"true" å¼€å¯å…¨é¢‘æ‰«æ
    # RB_FREQ_SUBSET: é€—å·åˆ†éš”çš„é¢‘ç‡å­é›†ï¼Œå¦‚ "6,7,8,9,10,11,12,13,21"ï¼›è®¾ç½®åä»…æ‰«æè¯¥å­é›†
    # RB_SKIP_PREV: "1" è·³è¿‡ä¸Šä¸€è½® run çš„å¯¹æ¯”ä»¥èŠ‚çœæ—¶é—´
    
    # è§£æRB_BACKTEST_ALL
    backtest_all = os.environ.get("RB_BACKTEST_ALL", "0").strip().lower() in ("1", "true", "yes")
    if backtest_all:
        logger.info("âš™ï¸  å…¨é‡å›æµ‹æ¨¡å¼å·²å¯ç”¨ (RB_BACKTEST_ALL=1)ï¼Œå°†å›æµ‹WFOæ‰€æœ‰ç»„åˆ")
    
    env_topk = os.environ.get("RB_TOPK")
    if env_topk is not None:
        try:
            config["combo_wfo"]["top_n"] = int(env_topk)
        except Exception:
            logger.warning(f"RB_TOPK æ— æ³•è§£æä¸ºæ•´æ•°: {env_topk}")

    env_test_all_freqs = os.environ.get("RB_TEST_ALL_FREQS")
    if env_test_all_freqs is not None:
        val = env_test_all_freqs.strip().lower()
        config.setdefault("backtest", {})["test_all_frequencies"] = val in ("1", "true", "yes")
    # å¼ºåˆ¶é”å®šé¢‘ç‡ä¸º8å¤©ï¼ˆå·²éªŒè¯æœ€ä¼˜ï¼‰ï¼Œè‹¥å…³é—­ test_all_frequencies åˆ™ä½¿ç”¨ combo_wfo.rebalance_frequencies=[8]
    if not config.get("backtest", {}).get("test_all_frequencies", False):
        config.setdefault("combo_wfo", {})["rebalance_frequencies"] = [8]

    env_freq_subset = os.environ.get("RB_FREQ_SUBSET")
    freq_subset_list = None
    if env_freq_subset:
        try:
            freq_subset_list = [int(x) for x in env_freq_subset.split(",") if x.strip()]
        except Exception:
            logger.warning(f"RB_FREQ_SUBSET è§£æå¤±è´¥: {env_freq_subset}")

    skip_prev = os.environ.get("RB_SKIP_PREV", "0").strip() in ("1", "true", "yes")

    # ğŸ”¥ ç™½åå•æœºåˆ¶å·²ç§»é™¤ï¼šç›´æ¥ä½¿ç”¨WFO TopKç»“æœï¼Œæ— éœ€é¢å¤–çº¦æŸ
    # ç§»é™¤åŸå› ï¼šç™½åå•ä¾èµ–ä¸ç¨³å®šçš„WFOç»“æœï¼Œä¸”æ— æ³•æå‡é¢„æµ‹å‡†ç¡®æ€§
    # æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨å›å½’æ¨¡å‹å­¦ä¹ WFOâ†’çœŸå®å›æµ‹çš„æ˜ å°„å…³ç³»

    # åŠ è½½æ•°æ®
    logger.info("=" * 100)
    logger.info("åŠ è½½æ•°æ®...")
    logger.info("=" * 100)

    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=config["data"]["end_date"],
        use_cache=True,
    )

    # è®¡ç®—å› å­
    logger.info("è®¡ç®—å› å­...")
    factor_lib = PreciseFactorLibrary()
    factors_df = factor_lib.compute_all_factors(prices=ohlcv)
    factors_dict = {name: factors_df[name] for name in factor_lib.list_factors()}

    # æ¨ªæˆªé¢æ ‡å‡†åŒ–
    logger.info("æ¨ªæˆªé¢æ ‡å‡†åŒ–...")
    processor = CrossSectionProcessor(
        lower_percentile=config["cross_section"]["winsorize_lower"] * 100,
        upper_percentile=config["cross_section"]["winsorize_upper"] * 100,
        verbose=False,
    )
    standardized_factors = processor.process_all_factors(factors_dict)

    # ç»„ç»‡æ•°æ®
    factor_names = sorted(standardized_factors.keys())
    factor_arrays = [standardized_factors[name].values for name in factor_names]
    factors_data = np.stack(factor_arrays, axis=-1)

    returns_df = ohlcv["close"].pct_change(fill_method=None)
    returns = returns_df.values
    etf_names = list(ohlcv["close"].columns)

    logger.info(
        f"æ•°æ®ç»´åº¦: {factors_data.shape[0]}å¤© Ã— {factors_data.shape[1]}åªETF Ã— {factors_data.shape[2]}ä¸ªå› å­"
    )

    # ========== è¯»å–WFO Top 100ç»„åˆï¼ˆæœ€æ–° ä¸ ä¸Šä¸€æ¬¡ï¼‰ ==========
    logger.info("")
    logger.info("=" * 100)
    logger.info("è¯»å–WFO Top 100ç»„åˆï¼ˆæŒ‰ICæ’åºï¼‰...")
    logger.info("=" * 100)

    # æŸ¥æ‰¾æœ€æ–°çš„è¿è¡Œç»“æœ (å¢å¼º: å¤šè·¯å¾„/ç¯å¢ƒå˜é‡å›é€€)
    candidate_roots = []
    env_root = os.environ.get("RB_WFO_ROOT")
    if env_root:
        p = Path(env_root).expanduser().resolve()
        if p.name.startswith("run_") and p.is_dir():
            candidate_roots.append(p.parent)
        else:
            candidate_roots.append(p)
    # å½“å‰å·¥ä½œç›®å½•ä¸‹ results
    candidate_roots.append(Path("results").resolve())
    # è„šæœ¬ä¸Šçº§(etf_rotation_optimized)ä¸‹ results
    try:
        script_results = (Path(__file__).resolve().parent.parent / "results").resolve()
        candidate_roots.append(script_results)
    except Exception:
        pass
    # ğŸ”¥ ç§»é™¤ç™½åå•è·¯å¾„æ¨æ–­é€»è¾‘ï¼ˆå·²æ— ç™½åå•æœºåˆ¶ï¼‰
    # å»é‡
    unique_roots = []
    seen = set()
    for r in candidate_roots:
        if r and r not in seen and r.exists():
            seen.add(r)
            unique_roots.append(r)
    run_dirs = []
    for root in unique_roots:
        run_dirs.extend([d for d in root.glob("run_*") if d.is_dir()])
    run_dirs = sorted({d.resolve() for d in run_dirs}, reverse=True)
    if not run_dirs:
        logger.error("æœªæ‰¾åˆ°WFOè¿è¡Œç»“æœï¼è¯·å…ˆè¿è¡Œ run_combo_wfo.py æˆ–è®¾ç½® RB_WFO_ROOT æŒ‡å‘å« run_* çš„ç›®å½•")
        logger.debug(f"å·²å°è¯•ç›®å½•: {unique_roots}")
        return

    latest_run = run_dirs[0]
    prev_run = run_dirs[1] if (not skip_prev and len(run_dirs) > 1) else None

    # è¯»å–"æœ€æ–°"ç»„åˆï¼ˆæ”¯æŒå…¨é‡æˆ–TopNæ¨¡å¼ï¼‰
    logger.info("")
    logger.info("=" * 100)
    if backtest_all:
        logger.info("è¯»å–WFOå…¨é‡ç»„åˆï¼ˆALLæ¨¡å¼ï¼‰...")
    else:
        logger.info(f"è¯»å–WFO Top {config['combo_wfo']['top_n']} ç»„åˆï¼ˆæœ€æ–° runï¼‰...")
    logger.info("=" * 100)
    
    latest_top_df, latest_sort_method = load_top_combos_from_run(
        latest_run, 
        top_n=config["combo_wfo"]["top_n"],
        load_all=backtest_all
    )
    logger.info(f"è¯»å–ç›®å½•: {latest_run}")
    logger.info(
        f"æˆåŠŸè¯»å– Top {len(latest_top_df)} ä¸ªç»„åˆï¼ˆæ’åºæ–¹å¼ï¼š{latest_sort_method}ï¼‰"
    )
    logger.info("")

    # è‹¥æŒ‡å®šç™½åå•ï¼Œåˆ™åº”ç”¨åˆ°æœ€æ–°TopN
    def _load_whitelist(path: str):
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(f"ç™½åå•æ–‡ä»¶ä¸å­˜åœ¨: {p}")
        combos = []
        try:
            if p.suffix.lower() in [".csv", ".parquet", ".feather", ".pkl", ".pickle", ".tsv", ".txt"]:
                if p.suffix.lower() == ".csv":
                    df_wl = pd.read_csv(p)
                elif p.suffix.lower() == ".tsv":
                    df_wl = pd.read_csv(p, sep='\t')
                elif p.suffix.lower() == ".parquet":
                    df_wl = pd.read_parquet(p)
                elif p.suffix.lower() in [".feather"]:
                    df_wl = pd.read_feather(p)
                elif p.suffix.lower() in [".pkl", ".pickle"]:
                    import pickle
                    with open(p, "rb") as f:
                        obj = pickle.load(f)
                    if isinstance(obj, (list, tuple)):
                        return [str(x) for x in obj]
                    elif isinstance(obj, pd.DataFrame):
                        df_wl = obj
                    else:
                        return [str(obj)]
                else:
                    # .txt å°è¯•é€è¡Œè¯»å–
                    with open(p, "r", encoding="utf-8") as f:
                        return [line.strip() for line in f if line.strip()]

                # DataFrame æƒ…å†µï¼šä¼˜å…ˆå– 'combo' åˆ—ï¼›å¦åˆ™å–ç¬¬ä¸€åˆ—
                if isinstance(df_wl, pd.DataFrame):
                    if 'combo' in df_wl.columns:
                        combos = df_wl['combo'].astype(str).tolist()
                    else:
                        combos = df_wl.iloc[:,0].astype(str).tolist()
                else:
                    combos = []
            else:
                with open(p, "r", encoding="utf-8") as f:
                    combos = [line.strip() for line in f if line.strip()]
        except Exception as e:
            raise RuntimeError(f"ç™½åå•æ–‡ä»¶è§£æå¤±è´¥: {p}, åŸå› : {e}")
        return combos

    # ğŸ”¥ ç™½åå•æœºåˆ¶å·²å®Œå…¨ç§»é™¤
    # åŸä»£ç ï¼šwhitelist_combos ç­›é€‰é€»è¾‘
    # æ–°é€»è¾‘ï¼šç›´æ¥ä½¿ç”¨ latest_top_df (WFO TopK)ï¼Œæ— ä»»ä½•çº¦æŸ
    logger.info("âš ï¸  ç™½åå•æœºåˆ¶å·²ç¦ç”¨ï¼Œç›´æ¥ä½¿ç”¨WFOæ’åºç»“æœ")

    # å¦‚æœ‰"ä¸Šä¸€æ¬¡"runï¼Œè¯»å–ä»¥ä¾¿å¯¹æ¯”
    prev_top_df = None
    if prev_run is not None:
        logger.info("=" * 100)
        logger.info("è¯»å–WFO Top 100ç»„åˆï¼ˆä¸Šä¸€è½® runï¼‰...")
        logger.info("=" * 100)
        try:
            prev_top_df, prev_sort_method = load_top_combos_from_run(
                prev_run, top_n=config["combo_wfo"]["top_n"]
            )
            logger.info(f"è¯»å–ç›®å½•: {prev_run}")
            logger.info(
                f"æˆåŠŸè¯»å– Top {len(prev_top_df)} ä¸ªç»„åˆï¼ˆæ’åºæ–¹å¼ï¼š{prev_sort_method}ï¼‰"
            )
            # ğŸ”¥ ç§»é™¤ä¸Šä¸€è½®ç™½åå•ç­›é€‰é€»è¾‘ï¼ˆå·²æ— ç™½åå•æœºåˆ¶ï¼‰
        except Exception as e:
            logger.warning(f"è¯»å–ä¸Šä¸€è½® run å¤±è´¥ï¼Œå°†ä»…å›æµ‹æœ€æ–°ä¸€è½®ã€‚åŸå› : {e}")
        logger.info("")

    # ========== æ‰¹é‡å›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰ ==========
    def _backtest_single_combo(
        idx,
        row,
        factors_data_shared,
        returns_shared,
        etf_names,
        factor_names,
        run_tag,
        test_freq=None,
        test_position_size=None,
        progress_meta=None,
        progress_counter=None,
        total_tasks: int | None = None,
        progress_step: int = 10,
    ):
        """
        å•ä¸ªç»„åˆå›æµ‹ï¼ˆç”¨äºå¹¶è¡ŒåŒ–ï¼‰

        å‚æ•°:
            test_freq: int or None, å¦‚æœæŒ‡å®šåˆ™è¦†ç›–WFOæ¨èé¢‘ç‡è¿›è¡Œæµ‹è¯•
            test_position_size: int or None, å¦‚æœæŒ‡å®šåˆ™è¦†ç›–é»˜è®¤æŒä»“æ•°è¿›è¡Œæµ‹è¯•
        """
        # å¯é€‰ï¼šè¿›ç¨‹çº§ numba é¢„çƒ­ï¼ˆä»…é¦–æ¬¡ï¼‰
        if os.environ.get("RB_NUMBA_WARMUP", "1").strip().lower() in ("1", "true", "yes"):
            _numba_warmup()

        combo_name = row["combo"]
        wfo_freq = int(row["best_rebalance_freq"])
        combo_size = int(row["combo_size"])
        wfo_ic = row["mean_oos_ic"]
        wfo_score = row["stability_score"]

        # ä½¿ç”¨æµ‹è¯•é¢‘ç‡æˆ–WFOæ¨èé¢‘ç‡
        rebalance_freq = test_freq if test_freq is not None else wfo_freq
        # ä½¿ç”¨æµ‹è¯•æŒä»“æ•°æˆ–é»˜è®¤æŒä»“æ•°5
        position_size = test_position_size if test_position_size is not None else 5

        # è§£æå› å­åç§°
        factor_list = [f.strip() for f in combo_name.split("+")]

        # æ£€æŸ¥å› å­æ˜¯å¦å­˜åœ¨
        missing_factors = [f for f in factor_list if f not in factor_names]
        if missing_factors:
            return None

        # æå–å› å­æ•°æ®
        factor_indices = [factor_names.index(f) for f in factor_list]
        factors_selected = factors_data_shared[:, :, factor_indices]

        progress_enabled = bool(progress_meta is not None and progress_counter is not None and total_tasks is not None)
        if progress_enabled:
            try:
                logger.info(
                    f"[START {idx+1}/{total_tasks}] combo={combo_name[:60]} freq={rebalance_freq} size={combo_size}"
                )
            except Exception:
                pass

        # å›æµ‹
        try:
            result = backtest_no_lookahead(
                factors_data=factors_selected,
                returns=returns_shared,
                etf_names=etf_names,
                rebalance_freq=rebalance_freq,
                lookback_window=252,
                position_size=position_size,
                commission_rate=commission_rate_cfg,
                initial_capital=1_000_000.0,
                factors_data_full=factors_data_shared,
                factor_indices_for_cache=factor_indices,
            )

            # æ·»åŠ ç»„åˆä¿¡æ¯
            result["combo"] = combo_name
            result["combo_size"] = combo_size
            result["wfo_ic"] = wfo_ic
            result["wfo_score"] = wfo_score
            result["wfo_freq"] = wfo_freq  # WFOæ¨èçš„é¢‘ç‡
            result["test_freq"] = rebalance_freq  # å®é™…æµ‹è¯•çš„é¢‘ç‡
            result["test_position_size"] = position_size  # å®é™…æµ‹è¯•çš„æŒä»“æ•°
            result["rank"] = idx + 1
            result["run_tag"] = run_tag
            # è¿›åº¦æ›´æ–°ä¸å®Œæˆæ—¥å¿—
            if progress_enabled:
                try:
                    # æ›´æ–°è®¡æ•°
                    with progress_counter.get_lock():
                        progress_counter.value += 1
                        done = progress_counter.value
                    # å•ç»„åˆè€—æ—¶ï¼ˆè‹¥profileå¼€å¯ï¼‰
                    combo_time_ms = (
                        result.get("profile", {}).get("time_total", 0.0) * 1000.0
                    )
                    logger.info(
                        f"[DONE  {idx+1}/{total_tasks}] combo={combo_name[:50]} time={combo_time_ms:.1f}ms annual={result['annual_ret']:.2%} sharpe={result['sharpe']:.3f}"
                    )
                    if done % progress_step == 0 or done == total_tasks:
                        start_ts = progress_meta.get("start_ts", None)
                        if start_ts is not None:
                            elapsed = time.perf_counter() - start_ts
                            avg = elapsed / done
                            eta = avg * (total_tasks - done)
                            logger.info(
                                f"[PROGRESS] {done}/{total_tasks} ({done/total_tasks:.1%}) avg={avg:.3f}s ETA={eta/60:.1f}m elapsed={elapsed:.1f}s"
                            )
                except Exception:
                    pass
            return result

        except Exception as e:
            try:
                logger.warning(f"å›æµ‹å¤±è´¥: combo={combo_name[:60]} freq={rebalance_freq} err={e}")
            except Exception:
                pass
            return None

    def run_batch_backtest(
        top_df: pd.DataFrame,
        run_tag: str,
        n_jobs=8,  # âœ… æå‡é»˜è®¤å¹¶è¡Œåº¦åˆ°8æ ¸ï¼ˆä»4æ ¸ï¼‰
        test_all_freqs=False,
        freq_range=range(1, 31),
        test_all_position_sizes=False,
        position_size_range=range(1, 11),
        force_freq: int | None = None,
    ):
        """
        æ‰¹é‡å›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰

        å‚æ•°:
            test_all_freqs: bool, æ˜¯å¦æµ‹è¯•æ‰€æœ‰æ¢ä»“é¢‘ç‡
            freq_range: range, æµ‹è¯•çš„é¢‘ç‡èŒƒå›´(é»˜è®¤1-30å¤©)
            test_all_position_sizes: bool, æ˜¯å¦æµ‹è¯•æ‰€æœ‰æŒä»“æ•°
            position_size_range: range, æµ‹è¯•çš„æŒä»“æ•°èŒƒå›´(é»˜è®¤1-10)
        """
        profile_enabled = os.environ.get("RB_PROFILE_BACKTEST", "0").strip().lower() in ("1", "true", "yes")
        # è¿›åº¦æ§åˆ¶ç¯å¢ƒå˜é‡
        progress_enabled = os.environ.get("RB_ENABLE_PROGRESS", "0").strip().lower() in ("1", "true", "yes")
        progress_step = int(os.environ.get("RB_PROGRESS_STEP", "10") or 10)
        manager = None
        progress_counter = None
        progress_meta = None
        if progress_enabled:
            try:
                manager = Manager()
                progress_counter = manager.Value('i', 0)
                progress_meta = manager.dict()
                progress_meta['start_ts'] = time.perf_counter()
            except Exception as e:
                logger.warning(f"è¿›åº¦ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                progress_enabled = False

        # ================= é¢„åŠ è½½æ‰€æœ‰ TopK å› å­åˆ—æ»šåŠ¨ICï¼ˆé¿å…é¦–åˆ—å¡«å……æ…¢ç‚¹ï¼‰ =================
        preload_ic = os.environ.get("RB_PRELOAD_IC", "0").strip().lower() in ("1", "true", "yes")
        lookback_window = 252
        if preload_ic and not (test_all_freqs or test_all_position_sizes):
            try:
                t_preload_start = time.perf_counter()
                # æ”¶é›†æ‰€æœ‰å› å­ç´¢å¼•
                all_factor_names = set()
                for _, r in top_df.iterrows():
                    for f in str(r["combo"]).split("+"):
                        fn = f.strip()
                        if fn:
                            all_factor_names.add(fn)
                factor_index_set = {factor_names.index(fn) for fn in all_factor_names if fn in factor_names}
                factor_index_arr = np.asarray(sorted(factor_index_set), dtype=np.int64)
                # æ”¶é›†æ‰€æœ‰é¢‘ç‡ï¼ˆæˆ–å¼ºåˆ¶é¢‘ç‡ï¼‰
                freq_candidates = set()
                if force_freq is not None:
                    freq_candidates.add(int(force_freq))
                else:
                    for _, r in top_df.iterrows():
                        try:
                            freq_candidates.add(int(r["best_rebalance_freq"]))
                        except Exception:
                            pass
                # é€é¢‘ç‡å¡«å……ç¼“å­˜
                filled_pairs = 0
                for freq_val in sorted(freq_candidates):
                    rebalance_indices = np.arange(lookback_window + 1, factors_data.shape[0], freq_val, dtype=np.int32)
                    _ = get_ic_weights_matrix_cached(
                        factors_data_full=factors_data,
                        returns=returns,
                        rebalance_indices=rebalance_indices,
                        lookback_window=lookback_window,
                        factor_indices=factor_index_arr,
                    )
                    filled_pairs += 1
                t_preload_elapsed = time.perf_counter() - t_preload_start
                logger.info(
                    f"ğŸ”„ é¢„åŠ è½½ICç¼“å­˜å®Œæˆ: å› å­åˆ—={len(factor_index_arr)} é¢‘ç‡é›†åˆ={sorted(freq_candidates)} æ¬¡æ•°={filled_pairs} ç”¨æ—¶={t_preload_elapsed:.2f}s"
                )
            except Exception as e:
                logger.warning(f"é¢„åŠ è½½ICç¼“å­˜å¤±è´¥: {e}")

        # ================= ä»»åŠ¡æ‰¹å†…åˆå¹¶ï¼ˆå‡å°‘è°ƒåº¦å¼€é”€ï¼‰ =================
        task_batch_size_env = os.environ.get("RB_TASK_BATCH_SIZE", "1")
        try:
            task_batch_size = max(1, int(task_batch_size_env))
        except Exception:
            task_batch_size = 1
        def _group_batches(task_list):
            if task_batch_size <= 1:
                return [[t] for t in task_list]
            return [task_list[i:i+task_batch_size] for i in range(0, len(task_list), task_batch_size)]
        def _run_batch_wrapper(batch, mode_tag, total_tasks):
            # batch: list of tuples representing original tasks arguments
            results_local = []
            for args in batch:
                results_local.append(args())
            return results_local

        if test_all_freqs and test_all_position_sizes:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨å‚æ•°æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(freq_range)} ä¸ªé¢‘ç‡ Ã— {len(position_size_range)} ä¸ªæŒä»“æ•° = {len(top_df) * len(freq_range) * len(position_size_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, é¢‘ç‡, æŒä»“æ•°)ä»»åŠ¡ä¸‰å…ƒç»„
            tasks_raw = []
            for idx, row in top_df.iterrows():
                for freq in freq_range:
                    for pos_size in position_size_range:
                        tasks_raw.append(lambda idx=idx, row=row, freq=freq, pos_size=pos_size: _backtest_single_combo(
                            idx,
                            row,
                            factors_data,
                            returns,
                            etf_names,
                            factor_names,
                            run_tag,
                            test_freq=freq,
                            test_position_size=pos_size,
                            progress_meta=progress_meta if progress_enabled else None,
                            progress_counter=progress_counter if progress_enabled else None,
                            total_tasks=None if not progress_enabled else len(top_df) * len(freq_range) * len(position_size_range),
                            progress_step=progress_step,
                        ))
            batches = _group_batches(tasks_raw)

            total_tasks = len(tasks_raw)
            results_nested = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_run_batch_wrapper)(batch, "all_param", total_tasks) for batch in batches
            )
            results = [r for batch in results_nested for r in batch]

        elif test_all_freqs:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨é¢‘ç‡æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(freq_range)} ä¸ªé¢‘ç‡ = {len(top_df) * len(freq_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, é¢‘ç‡)ä»»åŠ¡å¯¹
            tasks_raw = []
            for idx, row in top_df.iterrows():
                for freq in freq_range:
                    tasks_raw.append(lambda idx=idx, row=row, freq=freq: _backtest_single_combo(
                        idx,
                        row,
                        factors_data,
                        returns,
                        etf_names,
                        factor_names,
                        run_tag,
                        test_freq=freq,
                        progress_meta=progress_meta if progress_enabled else None,
                        progress_counter=progress_counter if progress_enabled else None,
                        total_tasks=None if not progress_enabled else len(top_df) * len(freq_range),
                        progress_step=progress_step,
                    ))
            batches = _group_batches(tasks_raw)
            total_tasks = len(tasks_raw)
            results_nested = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_run_batch_wrapper)(batch, "all_freq", total_tasks) for batch in batches
            )
            results = [r for batch in results_nested for r in batch]

        elif test_all_position_sizes:
            logger.info("=" * 100)
            logger.info(
                f"ğŸš€ å…¨æŒä»“æ•°æ‰«ææ¨¡å¼: Top {len(top_df)} ç»„åˆ Ã— {len(position_size_range)} ä¸ªæŒä»“æ•° = {len(top_df) * len(position_size_range)} ä¸ªç­–ç•¥"
            )
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # ç”Ÿæˆæ‰€æœ‰(ç»„åˆ, æŒä»“æ•°)ä»»åŠ¡å¯¹
            tasks_raw = []
            for idx, row in top_df.iterrows():
                for pos_size in position_size_range:
                    tasks_raw.append(lambda idx=idx, row=row, pos_size=pos_size: _backtest_single_combo(
                        idx,
                        row,
                        factors_data,
                        returns,
                        etf_names,
                        factor_names,
                        run_tag,
                        test_position_size=pos_size,
                        progress_meta=progress_meta if progress_enabled else None,
                        progress_counter=progress_counter if progress_enabled else None,
                        total_tasks=None if not progress_enabled else len(top_df) * len(position_size_range),
                        progress_step=progress_step,
                    ))
            batches = _group_batches(tasks_raw)
            total_tasks = len(tasks_raw)
            results_nested = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_run_batch_wrapper)(batch, "all_pos", total_tasks) for batch in batches
            )
            results = [r for batch in results_nested for r in batch]

        else:
            logger.info("=" * 100)
            logger.info(f"å¼€å§‹æ‰¹é‡å›æµ‹ Top {len(top_df)} ç»„åˆï¼ˆ{run_tag}ï¼Œæ— æœªæ¥å‡½æ•°ï¼‰")
            logger.info(f"å¹¶è¡Œåº¦: {n_jobs} æ ¸å¿ƒ")
            logger.info("=" * 100)
            logger.info("")

            # å¹¶è¡Œå›æµ‹(ä½¿ç”¨WFOæ¨èé¢‘ç‡å’Œé»˜è®¤æŒä»“æ•°)
            tasks_raw = []
            for idx, row in top_df.iterrows():
                tasks_raw.append(lambda idx=idx, row=row: _backtest_single_combo(
                    idx,
                    row,
                    factors_data,
                    returns,
                    etf_names,
                    factor_names,
                    run_tag,
                    test_freq=force_freq if force_freq is not None else None,
                    test_position_size=None,
                    progress_meta=progress_meta if progress_enabled else None,
                    progress_counter=progress_counter if progress_enabled else None,
                    total_tasks=None if not progress_enabled else len(top_df),
                    progress_step=progress_step,
                ))
            batches = _group_batches(tasks_raw)
            total_tasks = len(tasks_raw)
            results_nested = Parallel(n_jobs=n_jobs, verbose=10)(
                delayed(_run_batch_wrapper)(batch, "default", total_tasks) for batch in batches
            )
            results = [r for batch in results_nested for r in batch]

        # è¿‡æ»¤å¤±è´¥çš„å›æµ‹
        all_results_local = [r for r in results if r is not None]

        if not all_results_local:
            logger.error("æ²¡æœ‰æˆåŠŸå®Œæˆçš„å›æµ‹ï¼")
            return None

        # è¾“å‡ºå›æµ‹ç»“æœ(å…¨é¢‘ç‡æ¨¡å¼ä¸‹åªæ˜¾ç¤ºéƒ¨åˆ†)
        logger.info("")
        if test_all_freqs:
            logger.info(f"âœ… å®Œæˆ {len(all_results_local)} ä¸ªç­–ç•¥å›æµ‹")
            logger.info("æ˜¾ç¤ºå‰20ä¸ªç»“æœ:")
            for r in all_results_local[:20]:
                logger.info(f'[#{r["rank"]}] {r["combo"][:50]} | {r["test_freq"]}å¤©')
                logger.info(
                    f'      å›æµ‹ç»“æœ: å¹´åŒ–{r["annual_ret"]:>6.1%} | Sharpe {r["sharpe"]:>5.3f} | å›æ’¤{r["max_dd"]:>6.1%}'
                )
        else:
            for r in all_results_local:
                logger.info(f'[{r["rank"]}/{len(top_df)}] {r["combo"]}')
                logger.info(
                    f'         å›æµ‹ç»“æœ: 100ä¸‡â†’{r["final"]/10000:>8.1f}ä¸‡ | '
                    f'å¹´åŒ–{r["annual_ret"]:>6.1%} | Sharpe {r["sharpe"]:>5.3f} | '
                    f'å›æ’¤{r["max_dd"]:>6.1%} | è°ƒä»“{r["n_rebalance"]:>3d}æ¬¡'
                )

        rows = []
        for r in all_results_local:
            row_dict = {
                "rank": r["rank"],
                "combo": r["combo"],
                "combo_size": r["combo_size"],
                "wfo_freq": r["wfo_freq"],
                "test_freq": r["test_freq"],
                "test_position_size": r.get("test_position_size", 5),  # âœ¨ æ–°å¢ï¼šæµ‹è¯•çš„æŒä»“æ•°
                "freq": r["freq"],  # å®é™…ä½¿ç”¨çš„é¢‘ç‡
                "wfo_ic": r["wfo_ic"],
                "wfo_score": r["wfo_score"],
                "final_value": r["final"],
                "total_ret": r["total_ret"],
                "annual_ret": r["annual_ret"],
                "vol": r["vol"],
                "sharpe": r["sharpe"],
                "max_dd": r["max_dd"],
                "n_rebalance": r["n_rebalance"],
                "avg_turnover": r["avg_turnover"],
                "avg_n_holdings": r["avg_n_holdings"],  # âœ¨ æ–°å¢ï¼šå¹³å‡æŒä»“æ•°
                # æ–°å¢å­—æ®µï¼šèƒœç‡ç›¸å…³
                "win_rate": r["win_rate"],
                "winning_days": r["winning_days"],
                "losing_days": r["losing_days"],
                "avg_win": r["avg_win"],
                "avg_loss": r["avg_loss"],
                "profit_factor": r["profit_factor"],
                # æ–°å¢å­—æ®µï¼šé£é™©è°ƒæ•´æŒ‡æ ‡
                "calmar_ratio": r["calmar_ratio"],
                "sortino_ratio": r["sortino_ratio"],
                "max_consecutive_wins": r["max_consecutive_wins"],
                "max_consecutive_losses": r["max_consecutive_losses"],
                "run_tag": r["run_tag"],
            }
            if profile_enabled:
                profile = r.get("profile") or {}
                row_dict["profile_time_total"] = profile.get("time_total")
                row_dict["profile_time_precompute_ic"] = profile.get("time_precompute_ic")
                row_dict["profile_time_main_loop"] = profile.get("time_main_loop")
                row_dict["profile_rebalance_executed"] = profile.get("rebalance_executed")
                row_dict["profile_loop_iterations"] = profile.get("loop_iterations")
                row_dict["profile_avg_turnover"] = profile.get("avg_turnover")
            rows.append(row_dict)

        df_local = pd.DataFrame(rows)

        if profile_enabled:
            profile_cols = [
                "profile_time_total",
                "profile_time_precompute_ic",
                "profile_time_main_loop",
            ]
            available_cols = [c for c in profile_cols if c in df_local.columns]
            if available_cols and not df_local[available_cols].dropna(how="all").empty:
                logger.info("")
                logger.info("ğŸ•’ Profilingæ‘˜è¦ (ms)")
                profile_stats = df_local[available_cols].dropna()
                for col in available_cols:
                    series_ms = profile_stats[col] * 1000
                    if series_ms.empty:
                        continue
                    logger.info(
                        f"  {col.replace('profile_', '')}: mean {series_ms.mean():7.1f} | median {series_ms.median():7.1f} | max {series_ms.max():7.1f}"
                    )

                if "profile_time_total" in profile_stats.columns:
                    worst_idx = profile_stats["profile_time_total"].idxmax()
                    if worst_idx is not None and not pd.isna(worst_idx):
                        worst_row = df_local.loc[worst_idx]
                        logger.info(
                            f"  æœ€æ…¢ç»„åˆ: rank {worst_row['rank']} | {worst_row['combo'][:60]} | time_total {profile_stats.loc[worst_idx, 'profile_time_total']*1000:7.1f} ms"
                        )
                # ====== Outlierè¯Šæ–­ (å¯é€‰) ======
                if os.environ.get("RB_OUTLIER_REPORT","0").strip().lower() in ("1","true","yes"):
                    try:
                        total_ms = profile_stats["profile_time_total"] * 1000
                        p95 = float(np.percentile(total_ms.values, 95))
                        p99 = float(np.percentile(total_ms.values, 99))
                        outliers = profile_stats[total_ms > p95].index.tolist()
                        logger.info(f"  Outlieré˜ˆå€¼: p95={p95:.1f}ms p99={p99:.1f}ms (count>{len(outliers)})")
                        for oid in outliers:
                            row = df_local.loc[oid]
                            path = row.get("profile_ic_path","?") if "profile_ic_path" in row else row.get("ic_path","?")
                            stable_flag = row.get("profile_stable_rank", row.get("stable_rank", False))
                            loop_ms = row.get("profile_time_main_loop", 0.0) * 1000
                            pre_ms = row.get("profile_time_precompute_ic", 0.0) * 1000
                            total_val = row.get("profile_time_total", 0.0) * 1000
                            loop_ratio = (loop_ms / total_val) if total_val > 0 else 0.0
                            logger.info(
                                f"    [OUTLIER] rank={row['rank']} combo={row['combo'][:50]} total={total_val:.1f}ms pre_ic={pre_ms:.1f}ms loop={loop_ms:.1f}ms loop_ratio={loop_ratio:.2f} ic_path={path} stable={stable_flag}"
                            )
                    except Exception as e:
                        logger.warning(f"  OutlieræŠ¥å‘Šå¤±è´¥: {e}")

        return df_local

    # ========== å…¨é¢‘ç‡æ‰«ææ¨¡å¼(å¯é€‰) ==========
    # åŸé€»è¾‘æ ¹æ®é…ç½®/ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦è§¦å‘ 1-30 å¤©å…¨é¢‘ç‡æ‰«æï¼›å½“å‰å·²éªŒè¯ 8 å¤©é¢‘ç‡æœ€ä¼˜ï¼Œ
    # ä¸ºé¿å…è¯¯è§¦å¯¼è‡´ 30x æ‰©å®¹çš„å·¨é‡ä»»åŠ¡ï¼Œè¿™é‡Œå¼ºåˆ¶å…³é—­å…¨é¢‘ç‡æ‰«æã€‚
    # å¦‚æœåç»­ç¡®éœ€é‡æ–°å¼€å¯ï¼Œè¯·å°†ä¸‹é¢çš„ TEST_ALL_FREQS æ”¹ä¸ºåŸæ¥çš„è¯»å–é…ç½®æ–¹å¼ï¼š
    # TEST_ALL_FREQS = config.get("backtest", {}).get("test_all_frequencies", False)
    TEST_ALL_FREQS = False  # ğŸ”’ å¼ºåˆ¶ç¦ç”¨å…¨é¢‘ç‡æ‰«æ
    # åŒæ­¥å›å†™ï¼ˆé˜²æ­¢åç»­ä»£ç å†æ¬¡è¯»å–é…ç½®è§¦å‘ Trueï¼‰
    if "backtest" in config:
        config["backtest"]["test_all_frequencies"] = False
    TEST_ALL_POSITION_SIZES = config.get("backtest", {}).get(
        "test_all_position_sizes", False
    )
    # é¢‘ç‡èŒƒå›´ï¼šæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥å­é›†
    FREQ_RANGE = range(1, 31) if not freq_subset_list else list(sorted(set(freq_subset_list)))
    POSITION_SIZE_RANGE = range(1, 11)  # 1-10ä¸ªæŒä»“

    # ç»Ÿä¸€çš„ç»“æœè¾“å‡ºç›®å½•ï¼Œéœ€åœ¨å…¨é¢‘ç‡/å¸¸è§„å›æµ‹å‰åˆ›å»º
    output_dir = Path("results_combo_wfo")
    output_dir.mkdir(exist_ok=True)
    # æ¯æ¬¡è„šæœ¬è°ƒç”¨ç”Ÿæˆç‹¬ç«‹è°ƒç”¨æ—¶é—´æˆ³ï¼Œé¿å…è¦†ç›–åŒä¸€ WFO run è¾“å‡º
    invocation_ts = os.environ.get("RB_RESULT_TS") or datetime.now().strftime("%Y%m%d_%H%M%S")
    def _make_run_output_dir(latest_ts: str) -> Path:
        d = output_dir / f"{latest_ts}_{invocation_ts}"
        d.mkdir(parents=True, exist_ok=True)
        return d

    if TEST_ALL_FREQS and TEST_ALL_POSITION_SIZES:
        # å…¨å‚æ•°æ‰«æï¼ˆé¢‘ç‡+æŒä»“æ•°ï¼‰
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨å…¨å‚æ•°æ‰«ææ¨¡å¼: 1-30å¤©æ¢ä»“ Ã— 1-10ä¸ªæŒä»“")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        all_param_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_param:{latest_run.name}",
            n_jobs=8,
            test_all_freqs=True,
            freq_range=FREQ_RANGE,
            test_all_position_sizes=True,
            position_size_range=POSITION_SIZE_RANGE,
        )

        if all_param_results_df is not None:
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = _make_run_output_dir(latest_ts)
            all_param_file = run_output_dir / f"all_param_scan_{latest_ts}_{invocation_ts}.csv"
            all_param_results_df.to_csv(all_param_file, index=False)
            logger.info(f"å…¨å‚æ•°æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_param_file}")

            # æŒ‰æŒä»“æ•°åˆ†ç»„åˆ†æ
            logger.info("")
            logger.info("=" * 100)
            logger.info("æŒ‰æŒä»“æ•°ç»Ÿè®¡æ€§èƒ½")
            logger.info("=" * 100)
            pos_stats = (
                all_param_results_df.groupby("test_position_size")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )
            logger.info(pos_stats.to_string())

            best_pos_by_sharpe = (
                all_param_results_df.groupby("test_position_size")["sharpe"]
                .mean()
                .idxmax()
            )
            logger.info(f"\nğŸ“Š å¹³å‡Sharpeæœ€ä¼˜æŒä»“æ•°: {best_pos_by_sharpe}ä¸ª")
            return

    elif TEST_ALL_POSITION_SIZES:
        # ä»…æŒä»“æ•°æ‰«æ
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨æŒä»“æ•°æ‰«ææ¨¡å¼: 1-10ä¸ªæŒä»“")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        all_pos_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_pos:{latest_run.name}",
            n_jobs=8,
            test_all_position_sizes=True,
            position_size_range=POSITION_SIZE_RANGE,
        )

        if all_pos_results_df is not None:
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = _make_run_output_dir(latest_ts)
            all_pos_file = run_output_dir / f"all_pos_scan_{latest_ts}_{invocation_ts}.csv"
            all_pos_results_df.to_csv(all_pos_file, index=False)
            logger.info(f"æŒä»“æ•°æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_pos_file}")

            # æŒ‰æŒä»“æ•°åˆ†ç»„åˆ†æ
            logger.info("")
            logger.info("=" * 100)
            logger.info("æŒ‰æŒä»“æ•°ç»Ÿè®¡æ€§èƒ½")
            logger.info("=" * 100)
            pos_stats = (
                all_pos_results_df.groupby("test_position_size")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )
            logger.info(pos_stats.to_string())

            best_pos_by_sharpe = (
                all_pos_results_df.groupby("test_position_size")["sharpe"]
                .mean()
                .idxmax()
            )
            best_pos_by_return = (
                all_pos_results_df.groupby("test_position_size")["annual_ret"]
                .mean()
                .idxmax()
            )
            logger.info(f"\nğŸ“Š å¹³å‡Sharpeæœ€ä¼˜æŒä»“æ•°: {best_pos_by_sharpe}ä¸ª")
            logger.info(f"ğŸ“Š å¹³å‡å¹´åŒ–æœ€ä¼˜æŒä»“æ•°: {best_pos_by_return}ä¸ª")
            return

    elif TEST_ALL_FREQS:  # æ­¤åˆ†æ”¯ç°åœ¨ä¸å¯è¾¾ï¼ˆTEST_ALL_FREQS å¼ºåˆ¶ä¸º Falseï¼‰
        logger.info("")
        logger.info("âš¡ï¸" * 50)
        logger.info("å¯åŠ¨å…¨é¢‘ç‡æ‰«ææ¨¡å¼: 1-30å¤©æ¢ä»“é¢‘ç‡å…¨æ‰«æ")
        logger.info("âš¡ï¸" * 50)
        logger.info("")

        # å…¨é¢‘ç‡å›æµ‹
        all_freq_results_df = run_batch_backtest(
            latest_top_df,
            run_tag=f"all_freq:{latest_run.name}",
            n_jobs=8,  # 3000ä¸ªä»»åŠ¡,ç”¨æ›´å¤šæ ¸å¿ƒ
            test_all_freqs=True,
            freq_range=FREQ_RANGE,
        )

        if all_freq_results_df is not None:
            # ä¿å­˜å…¨é¢‘ç‡ç»“æœ
            latest_ts = latest_run.name.replace("run_", "")
            run_output_dir = _make_run_output_dir(latest_ts)
            all_freq_file = run_output_dir / f"all_freq_scan_{latest_ts}_{invocation_ts}.csv"
            all_freq_results_df.to_csv(all_freq_file, index=False)

            logger.info("")
            logger.info("=" * 100)
            logger.info("å…¨é¢‘ç‡æ‰«æç»“æœåˆ†æ")
            logger.info("=" * 100)

            # æŒ‰é¢‘ç‡åˆ†ç»„ç»Ÿè®¡
            freq_stats = (
                all_freq_results_df.groupby("test_freq")
                .agg(
                    {
                        "sharpe": ["mean", "std", "max"],
                        "annual_ret": ["mean", "std", "max"],
                        "max_dd": "mean",
                    }
                )
                .round(3)
            )

            logger.info("\nå„æ¢ä»“é¢‘ç‡è¡¨ç°ç»Ÿè®¡:")
            logger.info(freq_stats.to_string())

            # æ‰¾å‡ºæœ€ä¼˜é¢‘ç‡
            best_freq_by_sharpe = (
                all_freq_results_df.groupby("test_freq")["sharpe"].mean().idxmax()
            )
            best_freq_by_return = (
                all_freq_results_df.groupby("test_freq")["annual_ret"].mean().idxmax()
            )

            logger.info("")
            logger.info(f"ğŸ“Š å¹³å‡Sharpeæœ€ä¼˜é¢‘ç‡: {best_freq_by_sharpe}å¤©")
            logger.info(f"ğŸ“Š å¹³å‡å¹´åŒ–æœ€ä¼˜é¢‘ç‡: {best_freq_by_return}å¤©")

            # Top 10 å…¨å±€æœ€ä¼˜ç­–ç•¥
            logger.info("")
            logger.info("=" * 100)
            logger.info("Top 10 å…¨å±€æœ€ä¼˜ç­–ç•¥ï¼ˆè·¨æ‰€æœ‰é¢‘ç‡ï¼‰")
            logger.info("=" * 100)
            top10_global = all_freq_results_df.nlargest(10, "sharpe")
            for i, row in top10_global.iterrows():
                logger.info(
                    f'{i+1:>2}. [WFO#{row["rank"]:>3}] {row["combo"][:60]} | {row["test_freq"]}å¤©'
                )
                logger.info(
                    f'    å¹´åŒ–{row["annual_ret"]:>6.1%} | Sharpe {row["sharpe"]:>5.3f} | å›æ’¤{row["max_dd"]:>6.1%}'
                )

            logger.info("")
            logger.info(f"å…¨é¢‘ç‡æ‰«æç»“æœå·²ä¿å­˜è‡³: {all_freq_file}")
            logger.info("")

            # ç»§ç»­æ‰§è¡Œåç»­é€»è¾‘å‰å…ˆè¿”å›(å¯é€‰)
            # return
            # é¢‘ç‡ç›¸å…³æ€§ä¸æœ€ä½³é¢‘ç‡åˆ†å¸ƒæ‘˜è¦è¾“å‡º
            try:
                latest_ts = latest_run.name.replace("run_", "")
                run_output_dir = _make_run_output_dir(latest_ts)
                base_freq = 8 if 8 in all_freq_results_df['test_freq'].unique() else None
                summary = {}
                if base_freq is not None:
                    base_df = all_freq_results_df[all_freq_results_df['test_freq']==base_freq][['combo','sharpe']].rename(columns={'sharpe':f'sharpe_{base_freq}'})
                    import math  # ä½¿ç”¨å…¨å±€å·²å¯¼å…¥çš„ numpy as npï¼Œé¿å…åœ¨å‡½æ•°ä½œç”¨åŸŸé‡æ–°ç»‘å®š
                    try:
                        from scipy.stats import spearmanr  # type: ignore
                        _spearman = lambda a,b: spearmanr(a,b).correlation
                    except Exception:
                        def _spearman(a,b):
                            a = np.asarray(a); b = np.asarray(b)
                            ra = np.argsort(np.argsort(a)).astype(float)
                            rb = np.argsort(np.argsort(b)).astype(float)
                            ra -= ra.mean(); rb -= rb.mean()
                            num = (ra*rb).sum(); den = np.sqrt((ra**2).sum()*(rb**2).sum())
                            return float(num/den) if den!=0 else 0.0
                    corrs = {}
                    for f in sorted(all_freq_results_df['test_freq'].unique()):
                        if f == base_freq: continue
                        cur = all_freq_results_df[all_freq_results_df['test_freq']==f][['combo','sharpe']].rename(columns={'sharpe':f'sharpe_{f}'})
                        merged = base_df.merge(cur, on='combo')
                        if len(merged) >= 30:
                            val = _spearman(merged[f'sharpe_{base_freq}'], merged[f'sharpe_{f}'])
                            if val is not None and not math.isnan(val):
                                corrs[int(f)] = float(val)
                    if corrs:
                        summary['base_freq'] = base_freq
                        summary['spearman_vs_base'] = corrs
                        summary['median_spearman'] = float(np.median(list(corrs.values())))
                best_freq_series = all_freq_results_df.sort_values(['combo','sharpe'], ascending=[True,False]).groupby('combo').first()['test_freq']
                summary['best_freq_counts'] = {int(k): int(v) for k,v in best_freq_series.value_counts().to_dict().items()}
                summary['n_combos'] = int(all_freq_results_df['combo'].nunique())
                summary['n_rows'] = int(len(all_freq_results_df))
                summary_path = run_output_dir / f"freq_correlation_summary_{latest_ts}_{invocation_ts}.json"
                import json
                with open(summary_path,'w') as fp:
                    json.dump(summary, fp, ensure_ascii=False, indent=2)
                logger.info(f"é¢‘ç‡ç›¸å…³æ€§æ‘˜è¦å·²ä¿å­˜: {summary_path}")
            except Exception as e:
                logger.warning(f"é¢‘ç‡ç›¸å…³æ€§æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

    # ========== å¸¸è§„å•é¢‘ç‡å›æµ‹ ==========
    # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡ RB_FORCE_FREQ å¼ºåˆ¶è¦†ç›–æ‰€æœ‰ç»„åˆçš„å›æµ‹é¢‘ç‡ï¼ˆä¾‹å¦‚ç»Ÿä¸€ç”¨8å¤©éªŒè¯æ’åºä¸€è‡´æ€§ï¼‰
    force_freq_env = os.environ.get("RB_FORCE_FREQ")
    force_freq = None
    if force_freq_env:
        try:
            force_freq = int(force_freq_env)
            logger.info(f"âš™ï¸ å¯ç”¨å¼ºåˆ¶é¢‘ç‡: æ‰€æœ‰ç»„åˆç»Ÿä¸€ä½¿ç”¨ {force_freq} å¤©æ¢ä»“")
        except Exception:
            logger.warning(f"RB_FORCE_FREQ è§£æå¤±è´¥: {force_freq_env}")
    latest_results_df = run_batch_backtest(
        latest_top_df, run_tag=f"latest:{latest_run.name}", force_freq=force_freq
    )
    if latest_results_df is None:
        return

    # ========== ç»“æœæ±‡æ€»ï¼ˆæœ€æ–°ï¼‰ ==========
    logger.info("=" * 100)
    logger.info("å›æµ‹ç»“æœæ±‡æ€»ï¼ˆæœ€æ–°ï¼‰")
    logger.info("=" * 100)

    # ========== æœ€æ–°ç»“æœï¼šæ’åº/å±•ç¤º/ä¿å­˜ ==========
    results_df_sorted = latest_results_df.sort_values(
        "sharpe", ascending=False
    ).reset_index(drop=True)

    logger.info(f"\næˆåŠŸå®Œæˆ {len(latest_results_df)} ä¸ªç»„åˆçš„å›æµ‹")
    logger.info("")

    # Top 10 by Sharpe
    logger.info("=" * 100)
    logger.info("Top 10 ç»„åˆï¼ˆæŒ‰Sharpeæ’åºï¼‰")
    logger.info("=" * 100)
    top10 = results_df_sorted.head(10)
    for i, row in top10.iterrows():
        logger.info(f'{i+1:>2}. [WFOæ’å#{row["rank"]:>3}] {row["combo"][:80]}')
        logger.info(
            f'    {row["freq"]}å¤©æ¢ä»“ | å¹´åŒ–{row["annual_ret"]:>6.1%} | Sharpe {row["sharpe"]:>5.3f} | '
            f'å›æ’¤{row["max_dd"]:>6.1%} | 100ä¸‡â†’{row["final_value"]/10000:>7.1f}ä¸‡'
        )
        logger.info("")

    # ç»Ÿè®¡åˆ†æ
    logger.info("=" * 100)
    logger.info("ç»Ÿè®¡åˆ†æ")
    logger.info("=" * 100)
    logger.info(f'å¹³å‡å¹´åŒ–æ”¶ç›Š: {latest_results_df["annual_ret"].mean():>6.1%}')
    logger.info(f'å¹³å‡Sharpe:   {latest_results_df["sharpe"].mean():>6.3f}')
    logger.info(f'å¹³å‡æœ€å¤§å›æ’¤: {latest_results_df["max_dd"].mean():>6.1%}')
    logger.info(
        f'å¹´åŒ–>0ç»„åˆ:   {(latest_results_df["annual_ret"] > 0).sum()}/{len(latest_results_df)} ({(latest_results_df["annual_ret"] > 0).mean()*100:.1f}%)'
    )
    logger.info(
        f'Sharpe>0ç»„åˆ: {(latest_results_df["sharpe"] > 0).sum()}/{len(latest_results_df)} ({(latest_results_df["sharpe"] > 0).mean()*100:.1f}%)'
    )

    # WFOæ’å vs å®é™…è¡¨ç°ç›¸å…³æ€§
    from scipy.stats import spearmanr

    corr_sharpe, p_sharpe = spearmanr(
        latest_results_df["rank"], latest_results_df["sharpe"]
    )
    corr_ret, p_ret = spearmanr(
        latest_results_df["rank"], latest_results_df["annual_ret"]
    )

    logger.info("")
    logger.info("WFOæ’åä¸å®é™…è¡¨ç°ç›¸å…³æ€§:")
    logger.info(f"  WFOæ’å vs å®ç›˜Sharpe: {corr_sharpe:>6.3f} (p={p_sharpe:.3f})")
    logger.info(f"  WFOæ’å vs å®ç›˜å¹´åŒ–:   {corr_ret:>6.3f} (p={p_ret:.3f})")
    if corr_sharpe < -0.3 and p_sharpe < 0.05:
        logger.info("  âœ… WFOæ’åä¸å®ç›˜è¡¨ç°æ˜¾è‘—è´Ÿç›¸å…³ â†’ WFOæ’åæœ‰æ•ˆï¼")
    elif abs(corr_sharpe) < 0.1:
        logger.info("  âš ï¸  WFOæ’åä¸å®ç›˜è¡¨ç°ç›¸å…³æ€§è¾ƒå¼±")

    # ä¿å­˜ç»“æœ
    latest_ts = latest_run.name.replace("run_", "")
    run_output_dir = _make_run_output_dir(latest_ts)
    topN = len(latest_top_df)
    output_file = run_output_dir / f"top{topN}_backtest_by_ic_{latest_ts}_{invocation_ts}.csv"
    results_df_sorted.to_csv(output_file, index=False)

    logger.info("")
    logger.info(f"æœ€æ–°ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    logger.info("")

    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆåŒè¡¨å³å¯ï¼‰
    output_file_full = run_output_dir / f"top{topN}_backtest_by_ic_{latest_ts}_{invocation_ts}_full.csv"
    results_df_sorted.to_csv(output_file_full, index=False)
    logger.info(f"æœ€æ–°å®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {output_file_full}")

    # ========== è‹¥å­˜åœ¨ä¸Šä¸€è½® runï¼Œåˆ™è¿›è¡Œå¯¹æ¯”å¹¶ä¿å­˜å¯¹æ¯”æ–‡ä»¶ ==========
    if prev_top_df is not None:
        prev_results_df = run_batch_backtest(
            prev_top_df, run_tag=f"prev:{prev_run.name}"
        )
        if prev_results_df is not None:
            prev_ts = prev_run.name.replace("run_", "")

            # å¯¹æ¯”æ±‡æ€»
            latest_summary = summarize_results(latest_results_df)
            prev_summary = summarize_results(prev_results_df)

            logger.info("")
            logger.info("=" * 100)
            logger.info("ä¸ä¸Šä¸€è½®ç»“æœå¯¹æ¯”ï¼ˆæ±‡æ€»ï¼‰")
            logger.info("=" * 100)
            logger.info(
                f'- æœ€æ–°({latest_ts}) å¹³å‡å¹´åŒ–: {format_pct(latest_summary["mean_annual"])}, å¹³å‡Sharpe: {latest_summary["mean_sharpe"]:>6.3f}, å¹³å‡å›æ’¤: {format_pct(latest_summary["mean_max_dd"]) }'
            )
            logger.info(
                f'- ä¹‹å‰({prev_ts}) å¹³å‡å¹´åŒ–: {format_pct(prev_summary["mean_annual"])}, å¹³å‡Sharpe: {prev_summary["mean_sharpe"]:>6.3f}, å¹³å‡å›æ’¤: {format_pct(prev_summary["mean_max_dd"]) }'
            )
            if (
                "spearman_rank_sharpe" in latest_summary
                and "spearman_rank_sharpe" in prev_summary
            ):
                logger.info(
                    f'- æœ€æ–° Rank~Sharpe: {latest_summary["spearman_rank_sharpe"]:>6.3f} (p={latest_summary["spearman_rank_sharpe_p"]:.3f})'
                )
                logger.info(
                    f'- ä¹‹å‰ Rank~Sharpe: {prev_summary["spearman_rank_sharpe"]:>6.3f} (p={prev_summary["spearman_rank_sharpe_p"]:.3f})'
                )

            # é‡å ç»„åˆå¯¹é½å¯¹æ¯”
            latest_small = latest_results_df[
                ["combo", "rank", "annual_ret", "sharpe"]
            ].rename(
                columns={
                    "rank": "rank_latest",
                    "annual_ret": "annual_latest",
                    "sharpe": "sharpe_latest",
                }
            )
            prev_small = prev_results_df[
                ["combo", "rank", "annual_ret", "sharpe"]
            ].rename(
                columns={
                    "rank": "rank_prev",
                    "annual_ret": "annual_prev",
                    "sharpe": "sharpe_prev",
                }
            )
            merged = latest_small.merge(prev_small, on="combo", how="inner")
            if not merged.empty:
                merged["delta_sharpe"] = merged["sharpe_latest"] - merged["sharpe_prev"]
                merged["delta_annual"] = merged["annual_latest"] - merged["annual_prev"]
                merged["delta_rank"] = merged["rank_latest"] - merged["rank_prev"]

                logger.info("")
                logger.info("é‡å ç»„åˆå¯¹æ¯”ï¼ˆå‡å€¼ï¼‰:")
                logger.info(
                    f"- å¹³å‡ Sharpe å˜åŒ–: {merged['delta_sharpe'].mean():>6.3f}"
                )
                logger.info(f"- å¹³å‡ å¹´åŒ–  å˜åŒ–: {merged['delta_annual'].mean():>6.3%}")
                logger.info(
                    f"- å¹³å‡ æ’å  å˜åŒ–: {merged['delta_rank'].mean():>6.2f} (è´Ÿæ•°=æœ€æ–°æ’åæ›´é å‰)"
                )
                logger.info(
                    f"- æå‡å æ¯”(Sharpe>0): {(merged['delta_sharpe']>0).mean()*100:>5.1f}%  ({(merged['delta_sharpe']>0).sum()}/{len(merged)})"
                )

                compare_file = (
                    run_output_dir / f"compare_top100_{prev_ts}_vs_{latest_ts}.csv"
                )
                merged.sort_values("delta_sharpe", ascending=False).to_csv(
                    compare_file, index=False
                )
                logger.info(f"å¯¹æ¯”æ˜ç»†å·²ä¿å­˜: {compare_file}")
            else:
                logger.info("ä¸¤è½®Top100æ— é‡å ç»„åˆï¼Œè·³è¿‡é€ç»„åˆå¯¹æ¯”ä¿å­˜ã€‚")


if __name__ == "__main__":
    main()
