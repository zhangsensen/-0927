#!/usr/bin/env python3
"""
ä»å†å²å›æµ‹ç»“æœä¸­æå–å¯¹ç…§æ•°æ®

ç›®æ ‡ï¼šéªŒè¯æ ¡å‡†å™¨æ’åº vs IC æ’åºåœ¨çœŸå®å›æµ‹ä¸­çš„è¡¨ç°å·®å¼‚

è¾“å…¥ï¼š
  - å†å²å›æµ‹ç»“æœ: results_combo_wfo/*/top*_full.csv
  - å½“å‰ run çš„ä¸¤ç»„ Top100 æ’åº

è¾“å‡ºï¼š
  - å¯¹æ¯”æŠ¥å‘Šï¼šä¸¤ç»„ç­–ç•¥åœ¨çœŸå®å›æµ‹ä¸­çš„å®é™…è¡¨ç°

ç”¨æ³•ï¼š
  python scripts/analyze_historical_backtest.py \
    --backtest-csv results_combo_wfo/20251109_032515_20251110_001325/top12597_backtest_by_ic_20251109_032515_20251110_001325_full.csv \
    --ic-ranking results/run_20251112_223854/top100_ic_combos.csv \
    --calibrated-ranking results/run_20251112_223854/top100_calibrated_combos.csv \
    --output results/run_20251112_223854/historical_comparison
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

import logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def load_backtest_results(backtest_csv: Path) -> pd.DataFrame:
    """åŠ è½½å†å²å›æµ‹ç»“æœ"""
    logger.info(f"ğŸ“‚ åŠ è½½å›æµ‹ç»“æœ: {backtest_csv}")
    df = pd.read_csv(backtest_csv, low_memory=False)
    logger.info(f"  - æ€»ç­–ç•¥æ•°: {len(df)}")
    logger.info(f"  - åˆ—æ•°: {len(df.columns)}")
    
    # æ˜¾ç¤ºå…³é”®åˆ—
    key_cols = ['combo', 'annual_ret', 'sharpe', 'max_dd', 'vol', 'n_rebalance', 'avg_turnover']
    available_cols = [c for c in key_cols if c in df.columns]
    logger.info(f"  - å¯ç”¨åˆ—: {available_cols}")
    
    return df


def merge_ranking_with_backtest(
    ranking_df: pd.DataFrame,
    backtest_df: pd.DataFrame,
    label: str,
) -> pd.DataFrame:
    """åˆå¹¶æ’åºä¸å›æµ‹ç»“æœ"""
    logger.info(f"ğŸ”— åˆå¹¶ {label} æ’åºä¸å›æµ‹ç»“æœ")
    
    # åˆå¹¶
    merged = ranking_df.merge(
        backtest_df,
        on='combo',
        how='left',
        suffixes=('_rank', '_bt')
    )
    
    # ç»Ÿè®¡è¦†ç›–ç‡
    matched = merged['annual_ret'].notna().sum()
    coverage = matched / len(ranking_df) * 100
    
    logger.info(f"  - æ’åºç­–ç•¥æ•°: {len(ranking_df)}")
    logger.info(f"  - åŒ¹é…åˆ°å›æµ‹: {matched} ({coverage:.1f}%)")
    
    if matched == 0:
        logger.warning(f"âš ï¸  {label} æ²¡æœ‰ç­–ç•¥åŒ¹é…åˆ°å›æµ‹ç»“æœï¼")
    elif coverage < 50:
        logger.warning(f"âš ï¸  {label} è¦†ç›–ç‡ä½äº50%ï¼Œç»“æœå¯èƒ½ä¸å¯é ")
    
    return merged


def compute_stats(df: pd.DataFrame, label: str) -> dict:
    """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
    # åªè®¡ç®—æœ‰å›æµ‹ç»“æœçš„è¡Œ
    valid = df[df['annual_ret'].notna()]
    
    if len(valid) == 0:
        logger.error(f"âŒ {label} æ²¡æœ‰æœ‰æ•ˆçš„å›æµ‹æ•°æ®")
        return {
            "n_valid": 0,
            "annual_ret_mean": None,
            "annual_ret_median": None,
            "sharpe_mean": None,
            "sharpe_median": None,
            "max_dd_mean": None,
            "vol_mean": None,
        }
    
    stats = {
        "n_valid": len(valid),
        "annual_ret_mean": float(valid['annual_ret'].mean()),
        "annual_ret_median": float(valid['annual_ret'].median()),
        "sharpe_mean": float(valid['sharpe'].mean()),
        "sharpe_median": float(valid['sharpe'].median()),
        "max_dd_mean": float(valid['max_dd'].mean()),
        "vol_mean": float(valid['vol'].mean()),
    }
    
    # å¯é€‰åˆ—
    if 'avg_turnover' in valid.columns:
        stats['avg_turnover_mean'] = float(valid['avg_turnover'].mean())
    if 'n_rebalance' in valid.columns:
        stats['n_rebalance_mean'] = float(valid['n_rebalance'].mean())
    
    logger.info(f"ğŸ“Š {label} ç»Ÿè®¡:")
    logger.info(f"  - æœ‰æ•ˆæ ·æœ¬: {stats['n_valid']}")
    logger.info(f"  - å¹´åŒ–æ”¶ç›Š: {stats['annual_ret_mean']:.2%} (ä¸­ä½æ•° {stats['annual_ret_median']:.2%})")
    logger.info(f"  - Sharpe: {stats['sharpe_mean']:.3f} (ä¸­ä½æ•° {stats['sharpe_median']:.3f})")
    logger.info(f"  - æœ€å¤§å›æ’¤: {stats['max_dd_mean']:.2%}")
    
    return stats


def generate_report(
    ic_stats: dict,
    cal_stats: dict,
    output_dir: Path,
    backtest_source: str,
):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
    if ic_stats['n_valid'] == 0 or cal_stats['n_valid'] == 0:
        logger.error("âŒ æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šè‡³å°‘ä¸€ç»„æ•°æ®æ— æ•ˆ")
        
        # ä»ç„¶ç”Ÿæˆä¸€ä¸ªåŸºæœ¬æŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "backtest_source": backtest_source,
            "error": "No valid backtest data for one or both rankings",
            "ic_ranking": ic_stats,
            "calibrated_ranking": cal_stats,
        }
        
        json_path = output_dir / "historical_comparison.json"
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"âœ… é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
        return
    
    # è®¡ç®—å¢é‡
    deltas = {
        "annual_ret_delta": cal_stats['annual_ret_mean'] - ic_stats['annual_ret_mean'],
        "sharpe_delta": cal_stats['sharpe_mean'] - ic_stats['sharpe_mean'],
        "max_dd_delta": cal_stats['max_dd_mean'] - ic_stats['max_dd_mean'],
    }
    
    # åˆ¤å®š
    verdict = {
        "annual_ret_improved": deltas['annual_ret_delta'] > 0,
        "sharpe_improved": deltas['sharpe_delta'] > 0,
        "both_improved": (deltas['annual_ret_delta'] > 0) and (deltas['sharpe_delta'] > 0),
    }
    
    # ä¿å­˜ JSON
    report = {
        "timestamp": datetime.now().isoformat(),
        "backtest_source": backtest_source,
        "ic_ranking": ic_stats,
        "calibrated_ranking": cal_stats,
        "deltas": deltas,
        "verdict": verdict,
    }
    
    json_path = output_dir / "historical_comparison.json"
    with open(json_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"âœ… JSON æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_lines = [
        "# å†å²å›æµ‹å¯¹ç…§åˆ†ææŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}",
        f"**å›æµ‹æ•°æ®æº**: `{backtest_source}`",
        "",
        "## æ•°æ®è¦†ç›–",
        "",
        f"- IC æ’åºæœ‰æ•ˆæ ·æœ¬: {ic_stats['n_valid']}/100",
        f"- æ ¡å‡†æ’åºæœ‰æ•ˆæ ·æœ¬: {cal_stats['n_valid']}/100",
        "",
        "## çœŸå®å›æµ‹ç»“æœå¯¹æ¯”",
        "",
        "| æŒ‡æ ‡ | IC æ’åº | æ ¡å‡†æ’åº | å¢é‡ | æå‡? |",
        "|------|---------|----------|------|-------|",
    ]
    
    # å¹´åŒ–æ”¶ç›Š
    md_lines.append(
        f"| å¹´åŒ–æ”¶ç›Šç‡ (å‡å€¼) | {ic_stats['annual_ret_mean']:.2%} | "
        f"{cal_stats['annual_ret_mean']:.2%} | "
        f"{deltas['annual_ret_delta']:+.2%} | "
        f"{'âœ…' if verdict['annual_ret_improved'] else 'âŒ'} |"
    )
    
    # Sharpe
    md_lines.append(
        f"| Sharpe (å‡å€¼) | {ic_stats['sharpe_mean']:.3f} | "
        f"{cal_stats['sharpe_mean']:.3f} | "
        f"{deltas['sharpe_delta']:+.3f} | "
        f"{'âœ…' if verdict['sharpe_improved'] else 'âŒ'} |"
    )
    
    # æœ€å¤§å›æ’¤
    md_lines.append(
        f"| æœ€å¤§å›æ’¤ (å‡å€¼) | {ic_stats['max_dd_mean']:.2%} | "
        f"{cal_stats['max_dd_mean']:.2%} | "
        f"{deltas['max_dd_delta']:+.2%} | "
        f"{'âœ…' if deltas['max_dd_delta'] > 0 else 'âŒ'} |"
    )
    
    # æ³¢åŠ¨ç‡
    md_lines.append(
        f"| æ³¢åŠ¨ç‡ (å‡å€¼) | {ic_stats['vol_mean']:.2%} | "
        f"{cal_stats['vol_mean']:.2%} | - | - |"
    )
    
    # å¯é€‰æŒ‡æ ‡
    if 'avg_turnover_mean' in ic_stats:
        turnover_delta = cal_stats['avg_turnover_mean'] - ic_stats['avg_turnover_mean']
        md_lines.append(
            f"| å¹³å‡æ¢æ‰‹ç‡ | {ic_stats['avg_turnover_mean']:.2f} | "
            f"{cal_stats['avg_turnover_mean']:.2f} | "
            f"{turnover_delta:+.2f} | - |"
        )
    
    md_lines.extend([
        "",
        "## åˆ¤å®šç»“æœ",
        "",
    ])
    
    if verdict['both_improved']:
        md_lines.extend([
            "âœ… **æ ¡å‡†æ’åºåœ¨çœŸå®å›æµ‹ä¸­åŒæ—¶æå‡äº†å¹´åŒ–æ”¶ç›Šå’Œ Sharpeï¼Œè¯æ˜æ ¡å‡†å™¨æœ‰æ•ˆï¼**",
            "",
            "### å…³é”®å‘ç°",
            f"- å¹´åŒ–æ”¶ç›Šæå‡: **{deltas['annual_ret_delta']:+.2%}**",
            f"- Sharpe æå‡: **{deltas['sharpe_delta']:+.3f}**",
            f"- æœ€å¤§å›æ’¤æ”¹å–„: **{deltas['max_dd_delta']:+.2%}**",
            "",
            "### åç»­è¡ŒåŠ¨",
            "1. âœ… æ ¡å‡†å™¨éªŒè¯é€šè¿‡ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒå¯ç”¨",
            "2. æ‰©å±•éªŒè¯åˆ° Top200/500 å¹¶è¯„ä¼°æˆæœ¬æ•æ„Ÿæ€§",
            "3. æ›´æ–°ä¼˜åŒ–å™¨é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨æ ¡å‡†æ’åº",
            "4. æŒç»­ç›‘æ§æ ¡å‡†å™¨åœ¨æ–°æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›",
        ])
    elif verdict['annual_ret_improved'] or verdict['sharpe_improved']:
        md_lines.extend([
            "âš ï¸  **æ ¡å‡†æ’åºä»…éƒ¨åˆ†æ”¹å–„ï¼Œéœ€æƒè¡¡åˆ©å¼Š**",
            "",
            f"- å¹´åŒ–æ”¶ç›Šæå‡: {'âœ…' if verdict['annual_ret_improved'] else 'âŒ'} ({deltas['annual_ret_delta']:+.2%})",
            f"- Sharpe æå‡: {'âœ…' if verdict['sharpe_improved'] else 'âŒ'} ({deltas['sharpe_delta']:+.3f})",
            "",
            "### åç»­è¡ŒåŠ¨",
            "1. åˆ†ææˆæœ¬ä¾µèš€å’Œæ¢æ‰‹å½±å“",
            "2. æ£€æŸ¥æ ·æœ¬è¦†ç›–ç‡æ˜¯å¦å……åˆ†",
            "3. è€ƒè™‘è°ƒæ•´é—¨æ§é˜ˆå€¼ï¼ˆä¾‹å¦‚è¦æ±‚åŒæŒ‡æ ‡åŒæ—¶æå‡ï¼‰",
            "4. åœ¨æ›´é•¿æ—¶é—´çª—å£æˆ–ä¸åŒå¸‚åœºç¯å¢ƒä¸‹éªŒè¯",
        ])
    else:
        md_lines.extend([
            "âŒ **æ ¡å‡†æ’åºåœ¨çœŸå®å›æµ‹ä¸­æœªäº§ç”Ÿæå‡ï¼Œå»ºè®®æ”¾å¼ƒå½“å‰æ ¡å‡†å™¨**",
            "",
            f"- å¹´åŒ–æ”¶ç›Šä¸‹é™: **{deltas['annual_ret_delta']:.2%}**",
            f"- Sharpe ä¸‹é™: **{deltas['sharpe_delta']:.3f}**",
            "",
            "### è¯Šæ–­ä¸æ”¹è¿›",
            "1. **åˆ†å¸ƒæ¼‚ç§»**: è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„ç‰¹å¾åˆ†å¸ƒå¯èƒ½ä¸ä¸€è‡´",
            "2. **è¿‡æ‹Ÿåˆ**: æ ¡å‡†å™¨å¯èƒ½è®°å¿†äº†è®­ç»ƒæœŸçš„å™ªå£°æ¨¡å¼",
            "3. **ç‰¹å¾å¤±æ•ˆ**: WFO ç»Ÿè®¡ç‰¹å¾åœ¨æ–°æ•°æ®ä¸Šä¸å†æœ‰æ•ˆ",
            "",
            "### åç»­è¡ŒåŠ¨",
            "1. âŒ åœæ­¢ä½¿ç”¨å½“å‰æ ¡å‡†å™¨ï¼Œå›é€€åˆ° IC æ’åº",
            "2. è¯Šæ–­ç‰¹å¾åˆ†å¸ƒï¼šå¯¹æ¯”è®­ç»ƒé›†ä¸å½“å‰ run çš„ç‰¹å¾åˆ†ä½æ•°",
            "3. é‡æ–°è®­ç»ƒï¼šç¼©çŸ­è®­ç»ƒçª—å£æˆ–å¢åŠ æ—¶é—´è¡°å‡æƒé‡",
            "4. ç®€åŒ–æ¨¡å‹ï¼šè€ƒè™‘ç”¨è§„åˆ™æ›¿ä»£ GBDTï¼ˆä¾‹å¦‚ IC>é˜ˆå€¼ ä¸” ç¨³å®šæ€§>é˜ˆå€¼ï¼‰",
        ])
    
    md_path = output_dir / "historical_comparison.md"
    md_path.write_text("\n".join(md_lines))
    
    logger.info(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“Š å†å²å›æµ‹å¯¹ç…§æ‘˜è¦")
    print("="*80)
    print(f"æ•°æ®æº: {backtest_source}")
    print(f"è¦†ç›–ç‡: ICæ’åº {ic_stats['n_valid']}/100, æ ¡å‡†æ’åº {cal_stats['n_valid']}/100")
    print("-"*80)
    print(f"å¹´åŒ–æ”¶ç›Šç‡: IC {ic_stats['annual_ret_mean']:.2%} vs æ ¡å‡† {cal_stats['annual_ret_mean']:.2%} (Î” {deltas['annual_ret_delta']:+.2%})")
    print(f"Sharpe:    IC {ic_stats['sharpe_mean']:.3f} vs æ ¡å‡† {cal_stats['sharpe_mean']:.3f} (Î” {deltas['sharpe_delta']:+.3f})")
    print(f"æœ€å¤§å›æ’¤:  IC {ic_stats['max_dd_mean']:.2%} vs æ ¡å‡† {cal_stats['max_dd_mean']:.2%} (Î” {deltas['max_dd_delta']:+.2%})")
    print("="*80)
    
    if verdict['both_improved']:
        print("âœ… æ ¡å‡†å™¨åœ¨çœŸå®å›æµ‹ä¸­æœ‰æ•ˆï¼Œå»ºè®®é‡‡çº³")
    elif not (verdict['annual_ret_improved'] or verdict['sharpe_improved']):
        print("âŒ æ ¡å‡†å™¨åœ¨çœŸå®å›æµ‹ä¸­æ— æ•ˆï¼Œå»ºè®®æ”¾å¼ƒ")
    else:
        print("âš ï¸  æ•ˆæœä¸æ˜ç¡®ï¼Œéœ€æƒè¡¡åˆ©å¼Š")
    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description="å†å²å›æµ‹å¯¹ç…§åˆ†æ")
    parser.add_argument(
        "--backtest-csv",
        type=str,
        required=True,
        help="å†å²å›æµ‹ç»“æœ CSV æ–‡ä»¶",
    )
    parser.add_argument(
        "--ic-ranking",
        type=str,
        required=True,
        help="IC æ’åºçš„ Top100 ç­–ç•¥æ–‡ä»¶",
    )
    parser.add_argument(
        "--calibrated-ranking",
        type=str,
        required=True,
        help="æ ¡å‡†æ’åºçš„ Top100 ç­–ç•¥æ–‡ä»¶",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºç›®å½•",
    )
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶
    backtest_csv = Path(args.backtest_csv)
    ic_ranking = Path(args.ic_ranking)
    cal_ranking = Path(args.calibrated_ranking)
    
    if not backtest_csv.exists():
        logger.error(f"âŒ å›æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {backtest_csv}")
        sys.exit(1)
    
    if not ic_ranking.exists():
        logger.error(f"âŒ IC æ’åºæ–‡ä»¶ä¸å­˜åœ¨: {ic_ranking}")
        sys.exit(1)
    
    if not cal_ranking.exists():
        logger.error(f"âŒ æ ¡å‡†æ’åºæ–‡ä»¶ä¸å­˜åœ¨: {cal_ranking}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*80)
    logger.info("ğŸš€ å†å²å›æµ‹å¯¹ç…§åˆ†æå¯åŠ¨")
    logger.info("="*80)
    
    # åŠ è½½æ•°æ®
    backtest_df = load_backtest_results(backtest_csv)
    ic_df = pd.read_csv(ic_ranking)
    cal_df = pd.read_csv(cal_ranking)
    
    logger.info(f"âœ… IC æ’åºç­–ç•¥æ•°: {len(ic_df)}")
    logger.info(f"âœ… æ ¡å‡†æ’åºç­–ç•¥æ•°: {len(cal_df)}")
    
    # åˆå¹¶æ’åºä¸å›æµ‹
    ic_merged = merge_ranking_with_backtest(ic_df, backtest_df, "ICæ’åº")
    cal_merged = merge_ranking_with_backtest(cal_df, backtest_df, "æ ¡å‡†æ’åº")
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    ic_merged.to_csv(output_dir / "ic_ranking_with_backtest.csv", index=False)
    cal_merged.to_csv(output_dir / "calibrated_ranking_with_backtest.csv", index=False)
    
    logger.info(f"âœ… åˆå¹¶ç»“æœå·²ä¿å­˜åˆ° {output_dir}")
    
    # è®¡ç®—ç»Ÿè®¡
    ic_stats = compute_stats(ic_merged, "ICæ’åº")
    cal_stats = compute_stats(cal_merged, "æ ¡å‡†æ’åº")
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(ic_stats, cal_stats, output_dir, str(backtest_csv))


if __name__ == "__main__":
    main()
