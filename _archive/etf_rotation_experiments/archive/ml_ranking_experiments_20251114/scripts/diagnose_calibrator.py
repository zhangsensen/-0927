#!/usr/bin/env python3
"""
Calibratorè¯Šæ–­åˆ†æ - æ­ç¤ºTop1ç›¸åŒä½†ä¸­ä½æ•°æå‡çš„çœŸç›¸
"""
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from pathlib import Path
from scipy.stats import spearmanr, kendalltau

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è·¯å¾„é…ç½®
exp_root = Path("/Users/zhangshenshen/æ·±åº¦é‡åŒ–0927/etf_rotation_experiments")
calibrator_path = exp_root.parent / "etf_rotation_experiments/results/calibrator_gbdt_full.joblib"
ranking_ic = exp_root / "results/run_20251113_145102/ranking_blends/ranking_baseline.parquet"
ranking_cal = exp_root / "results/run_20251113_145102/ranking_blends/ranking_lightgbm.parquet"
output_dir = exp_root / "results/run_20251113_145102/calibrator_diagnosis"
output_dir.mkdir(exist_ok=True)

print("=" * 100)
print("ğŸ”¬ Calibrator è¯Šæ–­åˆ†æ")
print("=" * 100)

# 1. ç‰¹å¾é‡è¦æ€§åˆ†æ
print("\nã€1ã€‘ç‰¹å¾é‡è¦æ€§åˆ†æ")
print("-" * 100)

calibrator = joblib.load(calibrator_path)

# calibratoræ˜¯dict, åŒ…å«modelå’Œmetadata
if isinstance(calibrator, dict):
    model = calibrator.get('model')
    feature_names = calibrator.get('feature_names', [])
else:
    model = calibrator
    feature_names = []

if model and hasattr(model, 'feature_importances_'):
    importances = model.feature_importances_
    if not feature_names:
        feature_names = [f"feature_{i}" for i in range(len(importances))]
    
    feat_imp = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10 é‡è¦ç‰¹å¾:")
    print(feat_imp.head(10).to_string(index=False))
    
    # æ£€æŸ¥mean_oos_icçš„é‡è¦æ€§
    ic_features = feat_imp[feat_imp['feature'].str.contains('ic', case=False)]
    total_ic_importance = ic_features['importance'].sum()
    print(f"\nâš ï¸  æ‰€æœ‰ICç›¸å…³ç‰¹å¾çš„æ€»é‡è¦æ€§: {total_ic_importance:.1%}")
    
    if total_ic_importance > 0.7:
        print("âŒ è­¦å‘Š: ICç‰¹å¾å ä¸»å¯¼åœ°ä½(>70%),calibratorå¯èƒ½åªæ˜¯ICçš„å˜ç§!")
    elif total_ic_importance > 0.5:
        print("âš ï¸  æ³¨æ„: ICç‰¹å¾å æ¯”è¾ƒé«˜(>50%),éœ€è¦å¢åŠ å…¶ä»–ç‰¹å¾æƒé‡")
    else:
        print("âœ… ICç‰¹å¾æƒé‡åˆç†(<50%),calibratorå­¦åˆ°äº†ICä¹‹å¤–çš„æ¨¡å¼")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾
    plt.figure(figsize=(10, 6))
    feat_imp.head(15).plot(x='feature', y='importance', kind='barh', figsize=(10, 8))
    plt.xlabel('é‡è¦æ€§')
    plt.title('Calibratorç‰¹å¾é‡è¦æ€§ Top15')
    plt.tight_layout()
    plt.savefig(output_dir / "feature_importance.png", dpi=150)
    print(f"\nâœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {output_dir / 'feature_importance.png'}")

# 2. æ’åºç›¸å…³æ€§åˆ†æ
print("\nã€2ã€‘æ’åºç›¸å…³æ€§åˆ†æ")
print("-" * 100)

df_ic = pd.read_parquet(ranking_ic)
df_cal = pd.read_parquet(ranking_cal)

# ç¡®ä¿comboåˆ—å¯¹é½
merged = df_ic[['combo', 'rank_score']].rename(columns={'rank_score': 'ic_score'}).merge(
    df_cal[['combo', 'rank_score']].rename(columns={'rank_score': 'cal_score'}),
    on='combo'
)

# è®¡ç®—ç›¸å…³æ€§
spearman_corr, _ = spearmanr(merged['ic_score'], merged['cal_score'])
kendall_corr, _ = kendalltau(merged['ic_score'], merged['cal_score'])

print(f"\nå…¨å±€æ’åºç›¸å…³æ€§:")
print(f"  Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f}")
print(f"  Kendallç›¸å…³ç³»æ•°:  {kendall_corr:.4f}")

if spearman_corr > 0.95:
    print("âŒ è­¦å‘Š: æ’åºé«˜åº¦ç›¸å…³(>0.95),calibratorå‡ ä¹ç­‰åŒäºICæ’åº!")
elif spearman_corr > 0.85:
    print("âš ï¸  æ³¨æ„: æ’åºç›¸å…³æ€§è¾ƒé«˜(>0.85),calibratoræ”¹å˜æœ‰é™")
else:
    print("âœ… æ’åºæœ‰æ˜¾è‘—å·®å¼‚(<0.85),calibratoræä¾›äº†æ–°è§†è§’")

# ä¸åŒTopKçš„overlapåˆ†æ
topk_list = [10, 50, 100, 500, 1000, 3000]
overlap_results = []

for topk in topk_list:
    ic_top = set(df_ic.head(topk)['combo'])
    cal_top = set(df_cal.head(topk)['combo'])
    overlap = len(ic_top & cal_top)
    overlap_rate = overlap / topk
    
    overlap_results.append({
        'topk': topk,
        'overlap_count': overlap,
        'overlap_rate': overlap_rate,
        'unique_to_ic': len(ic_top - cal_top),
        'unique_to_cal': len(cal_top - ic_top)
    })

overlap_df = pd.DataFrame(overlap_results)
print(f"\nä¸åŒTopKçš„ç»„åˆoverlap:")
print(overlap_df.to_string(index=False))

# 3. Top1è¯¦ç»†åˆ†æ
print("\nã€3ã€‘Top1ç»„åˆåˆ†æ")
print("-" * 100)

top1_ic = df_ic.iloc[0]
top1_cal = df_cal.iloc[0]

print(f"\nICæ’åºTop1:")
print(f"  ç»„åˆ: {top1_ic['combo']}")
print(f"  mean_oos_ic: {top1_ic.get('mean_oos_ic', 'N/A')}")
print(f"  stability_score: {top1_ic.get('stability_score', 'N/A')}")

print(f"\næ ¡å‡†æ’åºTop1:")
print(f"  ç»„åˆ: {top1_cal['combo']}")
print(f"  calibrated_sharpe_pred: {top1_cal.get('calibrated_sharpe_pred', 'N/A')}")
print(f"  mean_oos_ic: {top1_cal.get('mean_oos_ic', 'N/A')}")

if top1_ic['combo'] == top1_cal['combo']:
    print(f"\nâœ… Top1ç»„åˆå®Œå…¨ç›¸åŒ: {top1_ic['combo']}")
    print(f"   è¿™è¯´æ˜ä¸¤ç§æ–¹æ³•å¯¹æœ€ä¼˜ç»„åˆè¾¾æˆå…±è¯†")
else:
    print(f"\nâŒ Top1ç»„åˆä¸åŒ!")
    print(f"   ICé€‰æ‹©: {top1_ic['combo']}")
    print(f"   æ ¡å‡†é€‰æ‹©: {top1_cal['combo']}")

# 4. æ•£ç‚¹å›¾: IC score vs Calibrated score
print("\nã€4ã€‘ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
print("-" * 100)

fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# 4.1 å…¨å±€scatter
ax = axes[0, 0]
ax.scatter(merged['ic_score'], merged['cal_score'], alpha=0.3, s=10)
ax.plot([merged['ic_score'].min(), merged['ic_score'].max()], 
        [merged['ic_score'].min(), merged['ic_score'].max()], 
        'r--', label='y=x')
ax.set_xlabel('IC Score')
ax.set_ylabel('Calibrated Score')
ax.set_title(f'å…¨å±€æ’åºå¯¹æ¯” (Spearman={spearman_corr:.3f})')
ax.legend()
ax.grid(alpha=0.3)

# 4.2 Top1000 scatter
ax = axes[0, 1]
top1000_merged = merged.sort_values('ic_score', ascending=False).head(1000)
ax.scatter(top1000_merged['ic_score'], top1000_merged['cal_score'], alpha=0.5, s=20)
ax.set_xlabel('IC Score')
ax.set_ylabel('Calibrated Score')
ax.set_title('Top1000æ’åºå¯¹æ¯”')
ax.grid(alpha=0.3)

# 4.3 Overlap rate vs TopK
ax = axes[1, 0]
ax.plot(overlap_df['topk'], overlap_df['overlap_rate'], marker='o', linewidth=2)
ax.set_xlabel('TopK')
ax.set_ylabel('Overlap Rate')
ax.set_title('ç»„åˆOverlapç‡ vs TopKè§„æ¨¡')
ax.axhline(y=0.8, color='r', linestyle='--', label='80%é˜ˆå€¼')
ax.legend()
ax.grid(alpha=0.3)

# 4.4 Rank difference histogram
ax = axes[1, 1]
merged['ic_rank'] = merged['ic_score'].rank(ascending=False)
merged['cal_rank'] = merged['cal_score'].rank(ascending=False)
merged['rank_diff'] = merged['cal_rank'] - merged['ic_rank']
ax.hist(merged['rank_diff'], bins=100, alpha=0.7, edgecolor='black')
ax.set_xlabel('æ’åå˜åŒ– (Calibrated - IC)')
ax.set_ylabel('ç»„åˆæ•°')
ax.set_title('æ’åå˜åŒ–åˆ†å¸ƒ')
ax.axvline(x=0, color='r', linestyle='--', linewidth=2)
ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / "ranking_comparison.png", dpi=150)
print(f"âœ… æ’åºå¯¹æ¯”å›¾å·²ä¿å­˜: {output_dir / 'ranking_comparison.png'}")

# 5. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
print("\nã€5ã€‘ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š")
print("-" * 100)

report_path = output_dir / "diagnosis_report.md"
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("# Calibratorè¯Šæ–­æŠ¥å‘Š\n\n")
    f.write(f"**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now()}\n\n")
    
    f.write("## 1. ç‰¹å¾é‡è¦æ€§\n\n")
    f.write(feat_imp.head(15).to_markdown(index=False))
    f.write(f"\n\n**ICç›¸å…³ç‰¹å¾æ€»é‡è¦æ€§**: {total_ic_importance:.1%}\n\n")
    
    f.write("## 2. æ’åºç›¸å…³æ€§\n\n")
    f.write(f"- Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.4f}\n")
    f.write(f"- Kendallç›¸å…³ç³»æ•°: {kendall_corr:.4f}\n\n")
    
    f.write("## 3. TopK Overlapåˆ†æ\n\n")
    f.write(overlap_df.to_markdown(index=False))
    f.write("\n\n")
    
    f.write("## 4. æ ¸å¿ƒç»“è®º\n\n")
    
    if total_ic_importance > 0.7 and spearman_corr > 0.95:
        f.write("âŒ **Calibratorè¿‡åº¦ä¾èµ–ICç‰¹å¾,æ’åºé«˜åº¦ç›¸å…³**\n\n")
        f.write("**å»ºè®®è¡ŒåŠ¨**:\n")
        f.write("1. é‡æ–°è®­ç»ƒcalibrator,ç§»é™¤mean_oos_icç‰¹å¾\n")
        f.write("2. æ·»åŠ æ–°ç‰¹å¾: æ¢æ‰‹ç‡ã€æœ€å¤§å›æ’¤ã€æŒä»“é›†ä¸­åº¦ç­‰\n")
        f.write("3. æˆ–è€…ç›´æ¥ä½¿ç”¨ICæ’åº,ä¸ä½¿ç”¨calibrator\n")
    elif overlap_df[overlap_df['topk']==3000]['overlap_rate'].values[0] > 0.95:
        f.write("âš ï¸  **Calibratorä¸ICæ’åºé«˜åº¦é‡å ,ä½†å¯èƒ½åœ¨ç»†èŠ‚ä¸Šæœ‰å·®å¼‚**\n\n")
        f.write("**å»ºè®®è¡ŒåŠ¨**:\n")
        f.write("1. åˆ†æTop100-1000ä¹‹é—´çš„æ’åºå·®å¼‚\n")
        f.write("2. å¦‚æœä¸­ä½æ•°æå‡æ˜¾è‘—,calibratorä»æœ‰ä»·å€¼\n")
        f.write("3. è€ƒè™‘å°†calibratorç”¨äºç»„åˆæ± ç­›é€‰è€Œéå•ä¸€é€‰æ‹©\n")
    else:
        f.write("âœ… **Calibratoræä¾›äº†ä¸ICä¸åŒçš„æ’åºè§†è§’**\n\n")
        f.write("**å»ºè®®è¡ŒåŠ¨**:\n")
        f.write("1. ç»§ç»­ä½¿ç”¨calibratorè¿›è¡Œç»„åˆç­›é€‰\n")
        f.write("2. å®šæœŸé‡æ–°è®­ç»ƒä»¥é€‚åº”å¸‚åœºå˜åŒ–\n")
        f.write("3. å»ºç«‹ensembleç­–ç•¥ç»¼åˆICå’Œcalibratoræ’åº\n")

print(f"âœ… è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

print("\n" + "=" * 100)
print("ğŸ¯ è¯Šæ–­å®Œæˆ!")
print("=" * 100)
print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
print(f"  - feature_importance.png")
print(f"  - ranking_comparison.png")
print(f"  - diagnosis_report.md")
print("\nè¯·æŸ¥çœ‹æŠ¥å‘Šå¹¶æ ¹æ®å»ºè®®å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚")
