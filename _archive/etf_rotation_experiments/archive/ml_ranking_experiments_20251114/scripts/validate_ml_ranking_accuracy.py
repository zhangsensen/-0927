#!/usr/bin/env python3
"""
ML æ’åºå‡†ç¡®åº¦éªŒè¯è„šæœ¬

ç›®æ ‡ï¼š
    åœ¨ Top3000 èŒƒå›´å†…éªŒè¯ ML æ ¡å‡†å™¨çš„æ’åºå‡†ç¡®åº¦

æ ¸å¿ƒæŒ‡æ ‡ï¼š
    1. Spearman æ’åºç›¸å…³æ€§ï¼ˆMLé¢„æµ‹ vs çœŸå®Sharpeï¼‰
    2. Top-K å‘½ä¸­ç‡ï¼ˆMLé€‰å‡ºçš„TopKï¼Œæœ‰å¤šå°‘åœ¨çœŸå®TopKé‡Œï¼‰
    3. åˆ†å±‚å‡†ç¡®åº¦ï¼ˆTop100/500/1000/3000çš„æ’åºè´¨é‡ï¼‰
    4. æ’åºæå‡å¹…åº¦ï¼ˆML vs ICæ’åºçš„ç›¸å¯¹æå‡ï¼‰

ç”¨æ³•ï¼š
    python scripts/validate_ml_ranking_accuracy.py \
        --run-dir results/run_20251112_223854 \
        --topk 3000 \
        --slippage-bps 2.0
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
from scipy.stats import spearmanr


def compute_topk_precision(
    ml_ranking: pd.DataFrame,
    true_ranking: pd.DataFrame,
    topk_list: List[int],
) -> Dict[int, float]:
    """
    è®¡ç®— Top-K å‘½ä¸­ç‡
    
    Args:
        ml_ranking: ML æ’åºç»“æœ (æŒ‰ calibrated_sharpe_pred é™åº)
        true_ranking: çœŸå®æ’åºç»“æœ (æŒ‰ sharpe é™åº)
        topk_list: è¦è¯„ä¼°çš„ K å€¼åˆ—è¡¨
    
    Returns:
        {K: precision} - å„ K å€¼çš„ç²¾ç¡®ç‡
    """
    precisions = {}
    
    for k in topk_list:
        if k > len(ml_ranking) or k > len(true_ranking):
            continue
        
        ml_topk = set(ml_ranking.head(k)['combo'].values)
        true_topk = set(true_ranking.head(k)['combo'].values)
        
        intersection = ml_topk & true_topk
        precision = len(intersection) / k
        precisions[k] = precision
    
    return precisions


def compute_stratified_correlation(
    merged: pd.DataFrame,
    strata: List[int],
) -> Dict[str, float]:
    """
    è®¡ç®—åˆ†å±‚çš„ Spearman ç›¸å…³æ€§
    
    Args:
        merged: åˆå¹¶åçš„æ•°æ® (åŒ…å« calibrated_sharpe_pred å’Œ sharpe)
        strata: åˆ†å±‚è¾¹ç•Œï¼Œå¦‚ [100, 500, 1000, 3000]
    
    Returns:
        {f"top{k}": corr} - å„å±‚çº§çš„ç›¸å…³ç³»æ•°
    """
    correlations = {}
    
    for k in strata:
        if k > len(merged):
            k = len(merged)
        
        subset = merged.head(k)
        if len(subset) < 10:  # è‡³å°‘10ä¸ªæ ·æœ¬æ‰è®¡ç®—
            continue
        
        corr, pval = spearmanr(
            subset['calibrated_sharpe_pred'],
            subset['sharpe']
        )
        
        correlations[f'top{k}'] = {
            'spearman': float(corr),
            'pvalue': float(pval),
            'n_samples': len(subset),
        }
    
    return correlations


def analyze_ranking_improvement(
    ic_ranking: pd.DataFrame,
    ml_ranking: pd.DataFrame,
    backtest_results: pd.DataFrame,
) -> Dict:
    """
    å¯¹æ¯” IC æ’åº vs ML æ’åºçš„å®é™…æ•ˆæœæå‡
    
    Returns:
        æ”¹è¿›åˆ†æç»“æœ
    """
    # åˆå¹¶å›æµ‹ç»“æœ
    ic_merged = ic_ranking.merge(
        backtest_results[['combo', 'annual_ret', 'sharpe', 'max_dd']],
        on='combo',
        how='inner'
    )
    
    ml_merged = ml_ranking.merge(
        backtest_results[['combo', 'annual_ret', 'sharpe', 'max_dd']],
        on='combo',
        how='inner'
    )
    
    improvements = {}
    
    # å„ TopK å±‚çº§çš„æ•ˆæœå¯¹æ¯”
    for k in [10, 50, 100, 500, 1000]:
        if k > len(ic_merged) or k > len(ml_merged):
            continue
        
        ic_topk = ic_merged.head(k)
        ml_topk = ml_merged.head(k)
        
        improvements[f'top{k}'] = {
            'ic_sorting': {
                'annual_ret': float(ic_topk['annual_ret'].mean()),
                'sharpe': float(ic_topk['sharpe'].mean()),
                'max_dd': float(ic_topk['max_dd'].mean()),
            },
            'ml_sorting': {
                'annual_ret': float(ml_topk['annual_ret'].mean()),
                'sharpe': float(ml_topk['sharpe'].mean()),
                'max_dd': float(ml_topk['max_dd'].mean()),
            },
            'delta': {
                'annual_ret': float(ml_topk['annual_ret'].mean() - ic_topk['annual_ret'].mean()),
                'sharpe': float(ml_topk['sharpe'].mean() - ic_topk['sharpe'].mean()),
                'max_dd': float(ml_topk['max_dd'].mean() - ic_topk['max_dd'].mean()),
            }
        }
    
    return improvements


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯ ML æ’åºå‡†ç¡®åº¦')
    parser.add_argument('--run-dir', type=str, required=True,
                        help='WFO run ç›®å½•')
    parser.add_argument('--topk', type=int, default=3000,
                        help='éªŒè¯èŒƒå›´ (é»˜è®¤ 3000)')
    parser.add_argument('--slippage-bps', type=float, default=2.0,
                        help='æ»‘ç‚¹ (åŸºç‚¹)')
    
    args = parser.parse_args()
    
    run_dir = Path(args.run_dir)
    topk = args.topk
    
    print("=" * 80)
    print("ğŸ” ML æ’åºå‡†ç¡®åº¦éªŒè¯")
    print("=" * 80)
    print(f"\nRun ç›®å½•: {run_dir}")
    print(f"éªŒè¯èŒƒå›´: Top{topk}")
    print(f"æ»‘ç‚¹è®¾ç½®: {args.slippage_bps} bps")
    
    # 1. åŠ è½½æ’åæ–‡ä»¶
    print("\n" + "=" * 80)
    print("ğŸ“‚ åŠ è½½æ’åæ•°æ®")
    print("=" * 80)
    
    ic_ranking_file = run_dir / 'ranking_blends/ranking_baseline.parquet'
    ml_ranking_file = run_dir / 'ranking_blends/ranking_lightgbm.parquet'
    
    if not ic_ranking_file.exists() or not ml_ranking_file.exists():
        print(f"âŒ æ’åæ–‡ä»¶ä¸å­˜åœ¨")
        print(f"   IC: {ic_ranking_file}")
        print(f"   ML: {ml_ranking_file}")
        return
    
    ic_ranking = pd.read_parquet(ic_ranking_file)
    ml_ranking = pd.read_parquet(ml_ranking_file)
    
    # æŒ‰æ’åºå­—æ®µæ’åºå¹¶æˆªå– TopK
    ic_ranking = ic_ranking.nlargest(topk, 'mean_oos_ic').copy()
    ml_ranking = ml_ranking.nlargest(topk, 'calibrated_sharpe_pred').copy()
    
    print(f"âœ… IC æ’åº: {len(ic_ranking)} ä¸ªç­–ç•¥")
    print(f"âœ… ML æ’åº: {len(ml_ranking)} ä¸ªç­–ç•¥")
    
    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œå›æµ‹
    print("\n" + "=" * 80)
    print("ğŸ” æ£€æŸ¥å›æµ‹ç»“æœ")
    print("=" * 80)
    
    # æŸ¥æ‰¾å·²æœ‰çš„å›æµ‹ç»“æœ
    results_dir = Path('results_combo_wfo')
    run_id = run_dir.name.replace('run_', '')
    
    # æŸ¥æ‰¾åŒ…å«è¿™ä¸ª run_id çš„å›æµ‹ç»“æœç›®å½•
    backtest_dirs = list(results_dir.glob(f'{run_id}_*'))
    
    if backtest_dirs:
        print(f"âœ… æ‰¾åˆ° {len(backtest_dirs)} ä¸ªå›æµ‹ç»“æœç›®å½•")
        for d in backtest_dirs:
            csv_files = list(d.glob('*.csv'))
            if csv_files:
                print(f"   - {d.name}: {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å›æµ‹æ•°æ®
    all_backtest_results = []
    for d in backtest_dirs:
        for csv_file in d.glob('*.csv'):
            df = pd.read_csv(csv_file)
            if 'combo' in df.columns and 'sharpe' in df.columns:
                all_backtest_results.append(df)
    
    if all_backtest_results:
        backtest_results = pd.concat(all_backtest_results, ignore_index=True)
        # å»é‡ï¼ˆå¯èƒ½æœ‰é‡å¤çš„å›æµ‹ï¼‰
        backtest_results = backtest_results.drop_duplicates(subset=['combo'], keep='last')
        print(f"\nâœ… å·²æœ‰å›æµ‹ç»“æœ: {len(backtest_results)} ä¸ªç­–ç•¥")
    else:
        backtest_results = None
        print(f"\nâš ï¸  æœªæ‰¾åˆ°å›æµ‹ç»“æœ")
    
    # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è¿è¡Œæ–°çš„å›æµ‹
    if backtest_results is None or len(backtest_results) < topk * 0.8:
        print("\n" + "=" * 80)
        print("ğŸš€ å‡†å¤‡è¿è¡Œå›æµ‹")
        print("=" * 80)
        
        # å‡†å¤‡ Top3000 çš„æ’åæ–‡ä»¶
        output_dir = run_dir / 'backtest_comparison'
        output_dir.mkdir(parents=True, exist_ok=True)
        
        ic_topk_file = output_dir / f'ranking_ic_top{topk}.parquet'
        ml_topk_file = output_dir / f'ranking_ml_top{topk}.parquet'
        
        # æ·»åŠ  rank_score åˆ—ï¼ˆå›æµ‹è„šæœ¬éœ€è¦ï¼‰
        ic_ranking['rank_score'] = ic_ranking['mean_oos_ic']
        ml_ranking['rank_score'] = ml_ranking['calibrated_sharpe_pred']
        
        ic_ranking.to_parquet(ic_topk_file, index=False)
        ml_ranking.to_parquet(ml_topk_file, index=False)
        
        print(f"âœ… å·²ç”Ÿæˆæ’åæ–‡ä»¶:")
        print(f"   IC: {ic_topk_file}")
        print(f"   ML: {ml_topk_file}")
        
        print(f"\n" + "=" * 80)
        print(f"â³ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨å›æµ‹ (é¢„è®¡è€—æ—¶ {topk * 1.5 / 60:.0f}-{topk * 3 / 60:.0f} åˆ†é’Ÿ):")
        print(f"=" * 80)
        
        print(f"\n# IC æ’åºå›æµ‹")
        print(f"nohup python real_backtest/run_profit_backtest.py \\")
        print(f"  --topk {topk} \\")
        print(f"  --ranking-file {ic_topk_file} \\")
        print(f"  --slippage-bps {args.slippage_bps} \\")
        print(f"  > /tmp/ic_top{topk}_backtest.log 2>&1 &")
        
        print(f"\n# ML æ’åºå›æµ‹")
        print(f"nohup python real_backtest/run_profit_backtest.py \\")
        print(f"  --topk {topk} \\")
        print(f"  --ranking-file {ml_topk_file} \\")
        print(f"  --slippage-bps {args.slippage_bps} \\")
        print(f"  > /tmp/ml_top{topk}_backtest.log 2>&1 &")
        
        print(f"\nå›æµ‹å®Œæˆåï¼Œé‡æ–°è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œåˆ†æã€‚")
        return
    
    # 4. æ‰§è¡Œæ’åºå‡†ç¡®åº¦åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ’åºå‡†ç¡®åº¦åˆ†æ")
    print("=" * 80)
    
    # åˆå¹¶ ML é¢„æµ‹å’ŒçœŸå®ç»“æœ
    ml_with_truth = ml_ranking.merge(
        backtest_results[['combo', 'sharpe', 'annual_ret', 'max_dd']],
        on='combo',
        how='inner'
    )
    
    print(f"\næ•°æ®è¦†ç›–: {len(ml_with_truth)} / {topk} ({len(ml_with_truth)/topk*100:.1f}%)")
    
    # 4.1 æ•´ä½“ Spearman ç›¸å…³æ€§
    overall_corr, overall_p = spearmanr(
        ml_with_truth['calibrated_sharpe_pred'],
        ml_with_truth['sharpe']
    )
    
    print(f"\nã€æ•´ä½“æ’åºç›¸å…³æ€§ã€‘")
    print(f"  Spearman ç›¸å…³ç³»æ•°: {overall_corr:.4f} (p={overall_p:.4e})")
    
    # 4.2 åˆ†å±‚ç›¸å…³æ€§
    strata = [100, 500, 1000, 2000, 3000]
    strata = [s for s in strata if s <= topk]
    
    stratified_corr = compute_stratified_correlation(
        ml_with_truth.sort_values('calibrated_sharpe_pred', ascending=False),
        strata
    )
    
    print(f"\nã€åˆ†å±‚æ’åºç›¸å…³æ€§ã€‘")
    for layer, stats in stratified_corr.items():
        print(f"  {layer:8s}: Spearman {stats['spearman']:+.4f} (p={stats['pvalue']:.4e}, n={stats['n_samples']})")
    
    # 4.3 Top-K å‘½ä¸­ç‡
    true_ranking = backtest_results.sort_values('sharpe', ascending=False)
    ml_ranking_sorted = ml_ranking.sort_values('calibrated_sharpe_pred', ascending=False)
    
    topk_list = [10, 50, 100, 500, 1000, 2000, 3000]
    topk_list = [k for k in topk_list if k <= topk]
    
    precisions = compute_topk_precision(ml_ranking_sorted, true_ranking, topk_list)
    
    print(f"\nã€Top-K å‘½ä¸­ç‡ã€‘")
    print(f"  (ML é€‰å‡ºçš„ TopK ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„åœ¨çœŸå® TopK é‡Œ)")
    for k, prec in precisions.items():
        print(f"  Top{k:4d}: {prec*100:5.1f}%")
    
    # 4.4 å¯¹æ¯” IC æ’åºçš„æ”¹è¿›
    ic_with_truth = ic_ranking.merge(
        backtest_results[['combo', 'sharpe', 'annual_ret', 'max_dd']],
        on='combo',
        how='inner'
    )
    
    ic_corr, ic_p = spearmanr(
        ic_with_truth['mean_oos_ic'],
        ic_with_truth['sharpe']
    )
    
    print(f"\nã€IC æ’åº vs ML æ’åºã€‘")
    print(f"  IC æ’åº Spearman: {ic_corr:.4f}")
    print(f"  ML æ’åº Spearman: {overall_corr:.4f}")
    print(f"  ç›¸å…³æ€§æå‡: {(overall_corr - ic_corr):+.4f} ({(overall_corr - ic_corr)/abs(ic_corr)*100:+.1f}%)")
    
    # 4.5 å®é™…æ•ˆæœæå‡
    improvements = analyze_ranking_improvement(
        ic_ranking.sort_values('mean_oos_ic', ascending=False),
        ml_ranking_sorted,
        backtest_results
    )
    
    print(f"\nã€å®é™…æ•ˆæœæå‡ã€‘")
    for layer, stats in improvements.items():
        print(f"\n  {layer}:")
        print(f"    å¹´åŒ–æ”¶ç›Š: IC {stats['ic_sorting']['annual_ret']:6.2%} â†’ ML {stats['ml_sorting']['annual_ret']:6.2%} (Î” {stats['delta']['annual_ret']:+.2%})")
        print(f"    Sharpe:   IC {stats['ic_sorting']['sharpe']:6.3f} â†’ ML {stats['ml_sorting']['sharpe']:6.3f} (Î” {stats['delta']['sharpe']:+.3f})")
    
    # 5. ä¿å­˜ç»“æœ
    print(f"\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ")
    print("=" * 80)
    
    result = {
        'run_dir': str(run_dir),
        'topk': topk,
        'coverage': {
            'total': topk,
            'validated': len(ml_with_truth),
            'coverage_rate': len(ml_with_truth) / topk,
        },
        'overall_correlation': {
            'ic_sorting_spearman': float(ic_corr),
            'ml_sorting_spearman': float(overall_corr),
            'improvement': float(overall_corr - ic_corr),
        },
        'stratified_correlation': stratified_corr,
        'topk_precision': {f'top{k}': prec for k, prec in precisions.items()},
        'performance_improvement': improvements,
    }
    
    output_file = run_dir / f'ml_ranking_validation_top{topk}.json'
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    # 6. ç”Ÿæˆå†³ç­–å»ºè®®
    print(f"\n" + "=" * 80)
    print("ğŸ¯ å†³ç­–å»ºè®®")
    print("=" * 80)
    
    # åˆ¤æ–­æ ‡å‡†
    is_good_correlation = overall_corr > 0.3
    is_better_than_ic = overall_corr > ic_corr + 0.05
    has_positive_improvement = all(
        improvements[k]['delta']['sharpe'] > 0 
        for k in ['top100', 'top500'] if k in improvements
    )
    
    if is_good_correlation and is_better_than_ic and has_positive_improvement:
        print("âœ… ML æ’åºæ˜¾è‘—ä¼˜äº IC æ’åºï¼Œå»ºè®®é‡‡çº³")
        print(f"   - æ’åºç›¸å…³æ€§è¾¾åˆ° {overall_corr:.3f}ï¼ˆæå‡ {(overall_corr-ic_corr)*100:.1f}%ï¼‰")
        print(f"   - å„å±‚çº§æ•ˆæœå‡æœ‰æ”¹å–„")
    elif is_better_than_ic:
        print("âš ï¸  ML æ’åºæœ‰æ”¹å–„ï¼Œä½†æå‡æœ‰é™")
        print(f"   - æ’åºç›¸å…³æ€§ {overall_corr:.3f}ï¼ˆä»…æå‡ {(overall_corr-ic_corr)*100:.1f}%ï¼‰")
        print(f"   - å»ºè®®ç»§ç»­ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹æˆ–æ¨¡å‹")
    else:
        print("âŒ ML æ’åºæœªè¾¾é¢„æœŸï¼Œéœ€é‡æ–°å®¡è§†æ–¹æ¡ˆ")
        print(f"   - æ’åºç›¸å…³æ€§ä»… {overall_corr:.3f}")
        print(f"   - å¯èƒ½é—®é¢˜: ç‰¹å¾é€‰æ‹©ä¸å½“ã€æ¨¡å‹è¿‡æ‹Ÿåˆã€è®­ç»ƒæ•°æ®åå·®")
    
    print("=" * 80)


if __name__ == '__main__':
    main()
