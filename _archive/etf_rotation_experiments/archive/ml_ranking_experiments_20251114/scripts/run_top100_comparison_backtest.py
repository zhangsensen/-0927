#!/usr/bin/env python3
"""
å¯¹æœ€æ–° run çš„ IC æ’åºå’Œæ ¡å‡†æ’åº Top100 è¿è¡ŒçœŸå®å›æµ‹

ç”¨æ³•:
  cd /Users/zhangshenshen/æ·±åº¦é‡åŒ–0927/etf_rotation_experiments
  python scripts/run_top100_comparison_backtest.py --run-dir results/run_20251112_223854
"""

import argparse
import subprocess
import sys
from pathlib import Path
from datetime import datetime

import pandas as pd


def prepare_ranking_file(combos_csv: Path, output_parquet: Path, sort_column: str, run_dir: Path):
    """å°† CSV æ ¼å¼çš„ç­–ç•¥åˆ—è¡¨è½¬æ¢ä¸ºå›æµ‹è„šæœ¬éœ€è¦çš„ parquet æ ¼å¼"""
    df = pd.read_csv(combos_csv)
    
    # ç¡®ä¿æœ‰ combo åˆ—
    if 'combo' not in df.columns:
        raise ValueError(f"CSV æ–‡ä»¶ç¼ºå°‘ 'combo' åˆ—: {combos_csv}")
    
    # æŒ‰æ’åºåˆ—é™åºæ’åˆ—
    if sort_column in df.columns:
        df = df.sort_values(sort_column, ascending=False)
    
    # æ·»åŠ  rank_scoreï¼ˆå›æµ‹è„šæœ¬å¯èƒ½éœ€è¦ï¼‰
    if 'rank_score' not in df.columns:
        if sort_column in df.columns:
            df['rank_score'] = df[sort_column]
        else:
            df['rank_score'] = range(len(df), 0, -1)
    
    # åˆå¹¶å¿…è¦çš„å…ƒä¿¡æ¯ï¼ˆå¦‚ best_rebalance_freqï¼‰
    all_path = run_dir / "all_combos.parquet"
    if all_path.exists():
        try:
            all_df = pd.read_parquet(all_path)[["combo", "best_rebalance_freq"]]
            df = df.merge(all_df, on="combo", how="left")
        except Exception:
            pass

    # ä¿å­˜ä¸º parquet
    output_parquet.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_parquet, index=False)
    
    print(f"âœ… å·²ç”Ÿæˆæ’åºæ–‡ä»¶: {output_parquet}")
    print(f"   - ç­–ç•¥æ•°: {len(df)}")
    print(f"   - æ’åºåˆ—: {sort_column}")
    if sort_column in df.columns:
        print(f"   - {sort_column} èŒƒå›´: {df[sort_column].min():.4f} ~ {df[sort_column].max():.4f}")
    

def run_backtest(
    ranking_file: Path,
    topk: int,
    slippage_bps: float,
    label: str,
    python_bin: str = "python",
):
    """è¿è¡ŒçœŸå®å›æµ‹"""
    print("\n" + "="*80)
    print(f"ğŸš€ å¼€å§‹ {label} å›æµ‹")
    print("="*80)
    print(f"æ’åºæ–‡ä»¶: {ranking_file}")
    print(f"TopK: {topk}")
    print(f"æ»‘ç‚¹: {slippage_bps} bps")
    print()
    
    # æ„å»ºå›æµ‹å‘½ä»¤
    cmd = [
        python_bin,
        "real_backtest/run_profit_backtest.py",
        "--topk", str(topk),
        "--ranking-file", str(ranking_file),
        "--slippage-bps", str(slippage_bps),
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print()
    
    # è¿è¡Œå›æµ‹
    try:
        result = subprocess.run(cmd, check=True, capture_output=False, text=True)
        print(f"\nâœ… {label} å›æµ‹å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ {label} å›æµ‹å¤±è´¥: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Top100 å¯¹ç…§å›æµ‹")
    parser.add_argument(
        "--run-dir",
        type=str,
        required=True,
        help="WFO run ç›®å½•ï¼Œä¾‹å¦‚ results/run_20251112_223854",
    )
    parser.add_argument(
        "--topk",
        type=int,
        default=100,
        help="å›æµ‹ TopKï¼Œé»˜è®¤ 100",
    )
    parser.add_argument(
        "--slippage-bps",
        type=float,
        default=2.0,
        help="æ»‘ç‚¹ï¼ˆbpsï¼‰ï¼Œé»˜è®¤ 2.0",
    )
    parser.add_argument(
        "--python-bin",
        type=str,
        default="python",
        help="Python è§£é‡Šå™¨è·¯å¾„",
    )
    parser.add_argument(
        "--skip-ic",
        action="store_true",
        help="è·³è¿‡ IC æ’åºå›æµ‹",
    )
    parser.add_argument(
        "--skip-calibrated",
        action="store_true",
        help="è·³è¿‡æ ¡å‡†æ’åºå›æµ‹",
    )
    args = parser.parse_args()
    
    run_dir = Path(args.run_dir)
    if not run_dir.exists():
        print(f"âŒ Run ç›®å½•ä¸å­˜åœ¨: {run_dir}")
        sys.exit(1)
    
    print("="*80)
    print("ğŸ“Š Top100 å¯¹ç…§å›æµ‹")
    print("="*80)
    print(f"Run ç›®å½•: {run_dir}")
    print(f"TopK: {args.topk}")
    print(f"æ»‘ç‚¹: {args.slippage_bps} bps")
    print()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    ic_csv = run_dir / "top100_ic_combos.csv"
    cal_csv = run_dir / "top100_calibrated_combos.csv"
    
    if not ic_csv.exists():
        print(f"âŒ IC æ’åºæ–‡ä»¶ä¸å­˜åœ¨: {ic_csv}")
        sys.exit(1)
    
    if not cal_csv.exists():
        print(f"âŒ æ ¡å‡†æ’åºæ–‡ä»¶ä¸å­˜åœ¨: {cal_csv}")
        sys.exit(1)
    
    # åˆ›å»ºå›æµ‹ä¸´æ—¶ç›®å½•
    backtest_dir = run_dir / "backtest_comparison"
    backtest_dir.mkdir(exist_ok=True)
    
    # å‡†å¤‡æ’åºæ–‡ä»¶
    ic_ranking = backtest_dir / "ranking_ic_top100.parquet"
    cal_ranking = backtest_dir / "ranking_calibrated_top100.parquet"
    
    print("ğŸ“ å‡†å¤‡æ’åºæ–‡ä»¶...")
    prepare_ranking_file(ic_csv, ic_ranking, "mean_oos_ic", run_dir)
    prepare_ranking_file(cal_csv, cal_ranking, "calibrated_sharpe_pred", run_dir)
    print()
    
    # è¿è¡Œå›æµ‹
    results = {}
    
    if not args.skip_ic:
        results['ic'] = run_backtest(
            ranking_file=ic_ranking,
            topk=args.topk,
            slippage_bps=args.slippage_bps,
            label="ICæ’åºTop100",
            python_bin=args.python_bin,
        )
    
    if not args.skip_calibrated:
        results['calibrated'] = run_backtest(
            ranking_file=cal_ranking,
            topk=args.topk,
            slippage_bps=args.slippage_bps,
            label="æ ¡å‡†æ’åºTop100",
            python_bin=args.python_bin,
        )
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š å›æµ‹å®Œæˆæ€»ç»“")
    print("="*80)
    for label, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"{label}: {status}")
    print()
    
    if all(results.values()):
        print("âœ… æ‰€æœ‰å›æµ‹å‡æˆåŠŸå®Œæˆ")
        print()
        print("ä¸‹ä¸€æ­¥:")
        print("  1. æŸ¥çœ‹å›æµ‹ç»“æœ CSV æ–‡ä»¶ï¼ˆåœ¨ results_combo_wfo/ ç›®å½•ä¸‹ï¼‰")
        print("  2. è¿è¡Œå¯¹ç…§åˆ†æè„šæœ¬:")
        print(f"     python scripts/analyze_historical_backtest.py \\")
        print(f"       --backtest-csv <å›æµ‹ç»“æœè·¯å¾„> \\")
        print(f"       --ic-ranking {ic_csv} \\")
        print(f"       --calibrated-ranking {cal_csv} \\")
        print(f"       --output {run_dir}/latest_backtest_comparison")
    else:
        print("âš ï¸  éƒ¨åˆ†å›æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    
    print("="*80)


if __name__ == "__main__":
    main()
