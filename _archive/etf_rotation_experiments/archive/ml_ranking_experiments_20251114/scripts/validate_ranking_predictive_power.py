#!/usr/bin/env python3
"""
éªŒè¯WFOæ’åºçš„é¢„æµ‹èƒ½åŠ›

æ ¸å¿ƒé—®é¢˜ï¼š
1. WFOæ’åºèƒ½å¦é¢„æµ‹çœŸå®å›æµ‹æ”¶ç›Šï¼Ÿ(Rank Correlation)
2. Calibratorç›¸æ¯”IC baselineæå‡äº†å¤šå°‘ï¼Ÿ
3. æ’åºæ˜¯å¦æœ‰ç»æµä»·å€¼ï¼Ÿ(Top-K Precision, Decile Analysis)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from scipy.stats import spearmanr, kendalltau
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import json

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class RankingValidator:
    def __init__(self, run_dir: Path, backtest_dir: Path):
        self.run_dir = run_dir
        self.backtest_dir = backtest_dir
        self.data = None
        
    def load_data(self):
        """åŠ è½½WFOæ’åºå’ŒçœŸå®å›æµ‹æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        
        # 1. åŠ è½½all_combosï¼ˆcombo_id â†’ combo_stræ˜ å°„ï¼‰
        all_combos = pd.read_parquet(self.run_dir / "all_combos.parquet")
        print(f"   - all_combos: {len(all_combos)} ç»„åˆ")
        
        # 2. åŠ è½½ICæ’åº
        ranking_ic = pd.read_parquet(self.run_dir / "ranking_blends" / "ranking_baseline.parquet")
        ranking_ic['ic_rank'] = range(1, len(ranking_ic) + 1)
        ranking_ic['combo_str'] = ranking_ic['combo'].astype(str)
        print(f"   - ICæ’åº: {len(ranking_ic)} ç»„åˆ")
        
        # 3. åŠ è½½Calibratoræ’åº
        ranking_cal = pd.read_parquet(self.run_dir / "ranking_blends" / "ranking_lightgbm.parquet")
        ranking_cal['cal_rank'] = range(1, len(ranking_cal) + 1)
        ranking_cal['combo_str'] = ranking_cal['combo'].astype(str)
        print(f"   - Calibratoræ’åº: {len(ranking_cal)} ç»„åˆ")
        
        # 4. åŠ è½½çœŸå®å›æµ‹ç»“æœï¼ˆæŸ¥æ‰¾IC baselineçš„Top1000å›æµ‹ï¼‰
        # çœŸå®å›æµ‹ç»“æœåœ¨å­ç›®å½•ä¸­ï¼Œæ ¼å¼ï¼š{run_ts}_{backtest_ts}/top{K}_profit_backtest_*.csv
        backtest_subdirs = sorted([d for d in self.backtest_dir.iterdir() if d.is_dir()], reverse=True)
        
        backtest_csv = None
        for subdir in backtest_subdirs:
            # æŸ¥æ‰¾åŒ…å«run_tsçš„å­ç›®å½•
            if self.run_dir.name.replace('run_', '') in subdir.name:
                csv_files = list(subdir.glob("top*_profit_backtest_*.csv"))
                if csv_files:
                    # æ£€æŸ¥run_tagï¼Œæ‰¾IC baselineï¼ˆranking_baseline.parquetï¼‰
                    for csv in csv_files:
                        df_sample = pd.read_csv(csv, nrows=1)
                        if 'ranking_baseline.parquet' in df_sample['run_tag'].iloc[0]:
                            if 'top1000' in csv.name or 'top3000' in csv.name:
                                backtest_csv = csv
                                break
                    if backtest_csv:
                        break
        
        if not backtest_csv:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°IC baselineçš„çœŸå®å›æµ‹CSVæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå›æµ‹")
        
        backtest = pd.read_csv(backtest_csv)
        print(f"   - çœŸå®å›æµ‹: {len(backtest)} ç»„åˆ (from {backtest_csv.name})")
        
        # 5. åˆå¹¶æ•°æ®
        # æ‰€æœ‰æ•°æ®éƒ½ä½¿ç”¨comboå­—ç¬¦ä¸²ä½œä¸ºå…³è”é”®
        
        # é¦–å…ˆï¼Œä¸ºall_combosæ·»åŠ combo_idï¼ˆè¡Œç´¢å¼•ï¼‰å¹¶æ ‡å‡†åŒ–combo_str
        all_combos = all_combos.reset_index().rename(columns={'index': 'combo_id'})
        all_combos['combo_str'] = all_combos['combo'].astype(str)
        backtest['combo_str'] = backtest['combo'].astype(str)
        
        # å…³è”all_comboså’Œrankingï¼ˆä½¿ç”¨combo_strï¼‰
        data = all_combos[['combo_id', 'combo_str', 'mean_oos_ic']].copy()
        data = data.merge(
            ranking_ic[['combo_str', 'ic_rank']], 
            on='combo_str', 
            how='left'
        )
        data = data.merge(
            ranking_cal[['combo_str', 'cal_rank']], 
            on='combo_str', 
            how='left'
        )
        
        # å…³è”çœŸå®å›æµ‹ç»“æœ
        data = data.merge(
            backtest[['combo_str', 'sharpe_net', 'annual_ret_net', 'max_dd_net', 'win_rate']],
            on='combo_str',
            how='inner'
        )
        
        # è®¡ç®—çœŸå®å›æµ‹æ’åï¼ˆæŒ‰Sharpeï¼‰
        data['backtest_rank'] = data['sharpe_net'].rank(ascending=False, method='min').astype(int)
        
        # åªä¿ç•™æœ‰å®Œæ•´æ’åºä¿¡æ¯çš„ç»„åˆ
        data = data.dropna(subset=['ic_rank', 'cal_rank', 'backtest_rank'])
        
        print(f"   - åˆå¹¶å: {len(data)} ç»„åˆ")
        print(f"   - ICæ’åºèŒƒå›´: {data['ic_rank'].min():.0f} - {data['ic_rank'].max():.0f}")
        print(f"   - Calibratoræ’åºèŒƒå›´: {data['cal_rank'].min():.0f} - {data['cal_rank'].max():.0f}")
        
        self.data = data
        return data
    
    def compute_rank_correlation(self):
        """è®¡ç®—æ’åºç›¸å…³æ€§"""
        print("\nğŸ“Š è®¡ç®—æ’åºç›¸å…³æ€§...")
        
        # ICæ’åº vs çœŸå®å›æµ‹æ’åº
        ic_spearman, ic_p = spearmanr(self.data['ic_rank'], self.data['backtest_rank'])
        ic_kendall, ic_kp = kendalltau(self.data['ic_rank'], self.data['backtest_rank'])
        
        # Calibratoræ’åº vs çœŸå®å›æµ‹æ’åº
        cal_spearman, cal_p = spearmanr(self.data['cal_rank'], self.data['backtest_rank'])
        cal_kendall, cal_kp = kendalltau(self.data['cal_rank'], self.data['backtest_rank'])
        
        results = {
            'IC_Baseline': {
                'spearman': ic_spearman,
                'spearman_p': ic_p,
                'kendall': ic_kendall,
                'kendall_p': ic_kp
            },
            'Calibrator': {
                'spearman': cal_spearman,
                'spearman_p': cal_p,
                'kendall': cal_kendall,
                'kendall_p': cal_kp
            }
        }
        
        print(f"\n{'æ’åºæ–¹æ³•':<15} {'Spearman':<10} {'på€¼':<10} {'Kendall':<10} {'på€¼':<10}")
        print("=" * 55)
        print(f"{'IC Baseline':<15} {ic_spearman:>9.4f} {ic_p:>9.4e} {ic_kendall:>9.4f} {ic_kp:>9.4e}")
        print(f"{'Calibrator':<15} {cal_spearman:>9.4f} {cal_p:>9.4e} {cal_kendall:>9.4f} {cal_kp:>9.4e}")
        
        # åˆ¤æ–­ç›¸å…³æ€§å¼ºåº¦
        def judge_correlation(r):
            if abs(r) > 0.7: return "Excellent"
            if abs(r) > 0.5: return "Good"
            if abs(r) > 0.3: return "Moderate"
            return "Poor"
        
        print(f"\nè¯„ä»·ï¼š")
        print(f"  IC Baseline: {judge_correlation(ic_spearman)}")
        print(f"  Calibrator: {judge_correlation(cal_spearman)}")
        
        return results
    
    def compute_topk_precision(self, k_values=[10, 20, 50, 100]):
        """è®¡ç®—Top-Kç²¾åº¦"""
        print(f"\nğŸ¯ è®¡ç®—Top-K Precision...")
        
        results = {}
        
        for k in k_values:
            # WFO Top-K vs çœŸå®å›æµ‹ Top-Kçš„é‡å 
            ic_topk = set(self.data.nsmallest(k, 'ic_rank')['combo_id'])
            cal_topk = set(self.data.nsmallest(k, 'cal_rank')['combo_id'])
            backtest_topk = set(self.data.nsmallest(k, 'backtest_rank')['combo_id'])
            
            ic_precision = len(ic_topk & backtest_topk) / k
            cal_precision = len(cal_topk & backtest_topk) / k
            
            ic_recall = len(ic_topk & backtest_topk) / k
            cal_recall = len(cal_topk & backtest_topk) / k
            
            results[f'Top{k}'] = {
                'IC_precision': ic_precision,
                'IC_recall': ic_recall,
                'Calibrator_precision': cal_precision,
                'Calibrator_recall': cal_recall
            }
            
            print(f"\nTop-{k}:")
            print(f"  IC Baseline: Precision={ic_precision:.2%}, Recall={ic_recall:.2%}")
            print(f"  Calibrator:  Precision={cal_precision:.2%}, Recall={cal_recall:.2%}")
        
        return results
    
    def decile_analysis(self, n_deciles=10):
        """Decileæ€§èƒ½åˆ†æ"""
        print(f"\nğŸ“‰ Decileåˆ†æ (åˆ†æˆ{n_deciles}ç»„)...")
        
        # æŒ‰ICæ’åºåˆ†ç»„
        self.data['ic_decile'] = pd.qcut(self.data['ic_rank'], n_deciles, labels=False, duplicates='drop') + 1
        # æŒ‰Calibratoræ’åºåˆ†ç»„
        self.data['cal_decile'] = pd.qcut(self.data['cal_rank'], n_deciles, labels=False, duplicates='drop') + 1
        
        # è®¡ç®—æ¯ä¸ªdecileçš„å¹³å‡Sharpe
        ic_decile_perf = self.data.groupby('ic_decile')['sharpe_net'].agg(['mean', 'median', 'count'])
        cal_decile_perf = self.data.groupby('cal_decile')['sharpe_net'].agg(['mean', 'median', 'count'])
        
        print(f"\nIC Baseline - Decileå¹³å‡Sharpe:")
        print(ic_decile_perf)
        
        print(f"\nCalibrator - Decileå¹³å‡Sharpe:")
        print(cal_decile_perf)
        
        # æ£€æŸ¥å•è°ƒæ€§
        ic_monotonic = all(ic_decile_perf['mean'].iloc[i] >= ic_decile_perf['mean'].iloc[i+1] 
                          for i in range(len(ic_decile_perf)-1))
        cal_monotonic = all(cal_decile_perf['mean'].iloc[i] >= cal_decile_perf['mean'].iloc[i+1] 
                           for i in range(len(cal_decile_perf)-1))
        
        print(f"\nå•è°ƒæ€§æ£€éªŒ:")
        print(f"  IC Baseline: {'âœ“ å•è°ƒé€’å‡' if ic_monotonic else 'âœ— éå•è°ƒ'}")
        print(f"  Calibrator: {'âœ“ å•è°ƒé€’å‡' if cal_monotonic else 'âœ— éå•è°ƒ'}")
        
        return {
            'IC_decile': ic_decile_perf.to_dict(),
            'Calibrator_decile': cal_decile_perf.to_dict(),
            'IC_monotonic': ic_monotonic,
            'Calibrator_monotonic': cal_monotonic
        }
    
    def statistical_tests(self):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        print("\nğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
        
        # æå–Top100çš„Sharpe
        ic_top100 = self.data.nsmallest(100, 'ic_rank')['sharpe_net']
        cal_top100 = self.data.nsmallest(100, 'cal_rank')['sharpe_net']
        
        # Mann-Whitney Uæ£€éªŒï¼ˆéå‚æ•°æ£€éªŒï¼‰
        u_stat, p_value = stats.mannwhitneyu(cal_top100, ic_top100, alternative='greater')
        
        # Tæ£€éªŒï¼ˆå‚æ•°æ£€éªŒï¼‰
        t_stat, t_p = stats.ttest_ind(cal_top100, ic_top100)
        
        print(f"\nTop100 Sharpeå¯¹æ¯”:")
        print(f"  IC Baseline: å‡å€¼={ic_top100.mean():.4f}, ä¸­ä½æ•°={ic_top100.median():.4f}")
        print(f"  Calibrator:  å‡å€¼={cal_top100.mean():.4f}, ä¸­ä½æ•°={cal_top100.median():.4f}")
        print(f"  æå‡: {(cal_top100.mean() / ic_top100.mean() - 1) * 100:+.2f}%")
        print(f"\nMann-Whitney Uæ£€éªŒ: U={u_stat:.1f}, p={p_value:.4e}")
        print(f"  ç»“è®º: {'âœ“ Calibratoræ˜¾è‘—ä¼˜äºIC (p<0.05)' if p_value < 0.05 else 'âœ— æ— æ˜¾è‘—å·®å¼‚'}")
        
        return {
            'IC_mean': ic_top100.mean(),
            'IC_median': ic_top100.median(),
            'Calibrator_mean': cal_top100.mean(),
            'Calibrator_median': cal_top100.median(),
            'improvement_pct': (cal_top100.mean() / ic_top100.mean() - 1) * 100,
            'mann_whitney_u': u_stat,
            'mann_whitney_p': p_value,
            't_stat': t_stat,
            't_p': t_p
        }
    
    def visualize(self, output_dir: Path):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        output_dir.mkdir(exist_ok=True, parents=True)
        
        # å›¾1: Rank Correlation Scatter
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # IC Baseline
        axes[0].scatter(self.data['ic_rank'], self.data['sharpe_net'], alpha=0.3, s=10)
        axes[0].set_xlabel('WFO ICæ’å', fontsize=12)
        axes[0].set_ylabel('çœŸå®å›æµ‹ Sharpe', fontsize=12)
        axes[0].set_title('IC Baseline: WFOæ’åº vs çœŸå®è¡¨ç°', fontsize=14)
        rho, _ = spearmanr(self.data['ic_rank'], self.data['sharpe_net'])
        axes[0].text(0.05, 0.95, f'Spearman Ï = {rho:.3f}', 
                    transform=axes[0].transAxes, fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # Calibrator
        axes[1].scatter(self.data['cal_rank'], self.data['sharpe_net'], alpha=0.3, s=10, color='orange')
        axes[1].set_xlabel('WFO Calibratoræ’å', fontsize=12)
        axes[1].set_ylabel('çœŸå®å›æµ‹ Sharpe', fontsize=12)
        axes[1].set_title('Calibrator: WFOæ’åº vs çœŸå®è¡¨ç°', fontsize=14)
        rho, _ = spearmanr(self.data['cal_rank'], self.data['sharpe_net'])
        axes[1].text(0.05, 0.95, f'Spearman Ï = {rho:.3f}', 
                    transform=axes[1].transAxes, fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(output_dir / 'rank_correlation_scatter.png', dpi=300, bbox_inches='tight')
        print(f"   - ä¿å­˜: rank_correlation_scatter.png")
        
        # å›¾2: Decile Performance
        fig, ax = plt.subplots(figsize=(12, 6))
        
        ic_decile_perf = self.data.groupby('ic_decile')['sharpe_net'].mean()
        cal_decile_perf = self.data.groupby('cal_decile')['sharpe_net'].mean()
        
        x = np.arange(len(ic_decile_perf))
        width = 0.35
        
        ax.bar(x - width/2, ic_decile_perf.values, width, label='IC Baseline', alpha=0.8)
        ax.bar(x + width/2, cal_decile_perf.values, width, label='Calibrator', alpha=0.8, color='orange')
        
        ax.set_xlabel('Decile (1=æœ€ä¼˜, 10=æœ€å·®)', fontsize=12)
        ax.set_ylabel('å¹³å‡Sharpe', fontsize=12)
        ax.set_title('Decileæ€§èƒ½å¯¹æ¯”ï¼šæ’åè¶Šé«˜æ˜¯å¦çœŸçš„è¶Šå¥½ï¼Ÿ', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels([f'D{i+1}' for i in range(len(ic_decile_perf))])
        ax.legend(fontsize=11)
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'decile_performance.png', dpi=300, bbox_inches='tight')
        print(f"   - ä¿å­˜: decile_performance.png")
        
        # å›¾3: Cumulative Performance Curve
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # æŒ‰æ’åæ’åºåè®¡ç®—ç´¯è®¡å¹³å‡
        ic_sorted = self.data.sort_values('ic_rank')
        cal_sorted = self.data.sort_values('cal_rank')
        
        topk_range = range(10, len(self.data), 10)
        ic_cumulative = [ic_sorted.head(k)['sharpe_net'].mean() for k in topk_range]
        cal_cumulative = [cal_sorted.head(k)['sharpe_net'].mean() for k in topk_range]
        
        ax.plot(topk_range, ic_cumulative, label='IC Baseline', linewidth=2)
        ax.plot(topk_range, cal_cumulative, label='Calibrator', linewidth=2, color='orange')
        
        ax.set_xlabel('Top-Kç»„åˆæ•°é‡', fontsize=12)
        ax.set_ylabel('å¹³å‡Sharpe', fontsize=12)
        ax.set_title('Top-Kç´¯è®¡å¹³å‡æ€§èƒ½ï¼šé€‰æ‹©æ›´å¤šç»„åˆå¦‚ä½•å½±å“è´¨é‡', fontsize=14)
        ax.legend(fontsize=11)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'cumulative_performance.png', dpi=300, bbox_inches='tight')
        print(f"   - ä¿å­˜: cumulative_performance.png")
        
        # å›¾4: Correlation Heatmap
        fig, ax = plt.subplots(figsize=(10, 8))
        
        corr_data = self.data[['ic_rank', 'cal_rank', 'backtest_rank', 
                               'mean_oos_ic', 'sharpe_net', 'annual_ret_net']].corr()
        
        sns.heatmap(corr_data, annot=True, fmt='.3f', cmap='coolwarm', center=0,
                   square=True, linewidths=1, cbar_kws={"shrink": 0.8}, ax=ax)
        ax.set_title('æ’åºæŒ‡æ ‡ç›¸å…³æ€§çŸ©é˜µ', fontsize=14)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')
        print(f"   - ä¿å­˜: correlation_heatmap.png")
        
        plt.close('all')
    
    def generate_report(self, metrics: dict, output_file: Path):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        print("\nğŸ“ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        
        report = f"""# WFOæ’åºé¢„æµ‹èƒ½åŠ›éªŒè¯æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘ŠéªŒè¯äº†WFOæ’åºï¼ˆIC Baselineå’ŒCalibratorï¼‰å¯¹çœŸå®å›æµ‹æ”¶ç›Šçš„**é¢„æµ‹èƒ½åŠ›**ã€‚

### æ ¸å¿ƒå‘ç°

1. **æ’åºä¸€è‡´æ€§**ï¼š
   - IC Baseline Spearmanç›¸å…³æ€§: **{metrics['rank_correlation']['IC_Baseline']['spearman']:.3f}**
   - Calibrator Spearmanç›¸å…³æ€§: **{metrics['rank_correlation']['Calibrator']['spearman']:.3f}**
   
2. **Top-10ç²¾åº¦**ï¼š
   - IC Baseline: {metrics['topk_precision']['Top10']['IC_precision']:.1%}
   - Calibrator: {metrics['topk_precision']['Top10']['Calibrator_precision']:.1%}

3. **æ€§èƒ½æå‡**ï¼š
   - Calibrator vs IC (Top100å¹³å‡): **{metrics['statistical_tests']['improvement_pct']:+.2f}%**
   - ç»Ÿè®¡æ˜¾è‘—æ€§: p = {metrics['statistical_tests']['mann_whitney_p']:.4e}

---

## 1. æ’åºç›¸å…³æ€§åˆ†æ

### 1.1 Spearmanç§©ç›¸å…³ç³»æ•°

| æ’åºæ–¹æ³• | Spearman Ï | på€¼ | Kendall Ï„ | på€¼ | è¯„ä»· |
|---------|-----------|-----|-----------|-----|------|
| IC Baseline | {metrics['rank_correlation']['IC_Baseline']['spearman']:.4f} | {metrics['rank_correlation']['IC_Baseline']['spearman_p']:.4e} | {metrics['rank_correlation']['IC_Baseline']['kendall']:.4f} | {metrics['rank_correlation']['IC_Baseline']['kendall_p']:.4e} | {self._judge_correlation(metrics['rank_correlation']['IC_Baseline']['spearman'])} |
| Calibrator | {metrics['rank_correlation']['Calibrator']['spearman']:.4f} | {metrics['rank_correlation']['Calibrator']['spearman_p']:.4e} | {metrics['rank_correlation']['Calibrator']['kendall']:.4f} | {metrics['rank_correlation']['Calibrator']['kendall_p']:.4e} | {self._judge_correlation(metrics['rank_correlation']['Calibrator']['spearman'])} |

**è§£è¯»**ï¼š
- Spearmanç›¸å…³ç³»æ•°è¡¡é‡WFOæ’åºå’ŒçœŸå®å›æµ‹æ’åºçš„ä¸€è‡´æ€§
- Ï > 0.7: Excellent, 0.5-0.7: Good, 0.3-0.5: Moderate, <0.3: Poor
- på€¼ < 0.05è¡¨ç¤ºç»Ÿè®¡æ˜¾è‘—

### 1.2 å¯è§†åŒ–

å‚è§å›¾è¡¨ï¼š
- `rank_correlation_scatter.png`: WFOæ’åº vs çœŸå®Sharpeæ•£ç‚¹å›¾
- `correlation_heatmap.png`: æ‰€æœ‰æŒ‡æ ‡çš„ç›¸å…³æ€§çŸ©é˜µ

---

## 2. Top-Kç²¾åº¦åˆ†æ

### 2.1 Precision@Kï¼ˆé¢„æµ‹çš„Top-Kæœ‰å¤šå°‘çœŸçš„åœ¨çœŸå®Top-Kä¸­ï¼‰

| Top-K | IC Baseline | Calibrator | æå‡ |
|-------|------------|-----------|------|
| Top-10 | {metrics['topk_precision']['Top10']['IC_precision']:.1%} | {metrics['topk_precision']['Top10']['Calibrator_precision']:.1%} | {(metrics['topk_precision']['Top10']['Calibrator_precision'] - metrics['topk_precision']['Top10']['IC_precision']) * 100:+.1f}pp |
| Top-20 | {metrics['topk_precision']['Top20']['IC_precision']:.1%} | {metrics['topk_precision']['Top20']['Calibrator_precision']:.1%} | {(metrics['topk_precision']['Top20']['Calibrator_precision'] - metrics['topk_precision']['Top20']['IC_precision']) * 100:+.1f}pp |
| Top-50 | {metrics['topk_precision']['Top50']['IC_precision']:.1%} | {metrics['topk_precision']['Top50']['Calibrator_precision']:.1%} | {(metrics['topk_precision']['Top50']['Calibrator_precision'] - metrics['topk_precision']['Top50']['IC_precision']) * 100:+.1f}pp |
| Top-100 | {metrics['topk_precision']['Top100']['IC_precision']:.1%} | {metrics['topk_precision']['Top100']['Calibrator_precision']:.1%} | {(metrics['topk_precision']['Top100']['Calibrator_precision'] - metrics['topk_precision']['Top100']['IC_precision']) * 100:+.1f}pp |

**è§£è¯»**ï¼š
- Precision > 70%: Excellent, 50-70%: Good, 30-50%: Moderate, <30%: Poor
- è¿™æ˜¯æ’åºç³»ç»Ÿæœ€ç›´æ¥çš„ä»·å€¼ä½“ç°

---

## 3. Decileæ€§èƒ½åˆ†æ

### 3.1 å•è°ƒæ€§æ£€éªŒ

- IC Baseline: {'âœ“ å•è°ƒé€’å‡' if metrics['decile_analysis']['IC_monotonic'] else 'âœ— éå•è°ƒ'}
- Calibrator: {'âœ“ å•è°ƒé€’å‡' if metrics['decile_analysis']['Calibrator_monotonic'] else 'âœ— éå•è°ƒ'}

### 3.2 å¯è§†åŒ–

å‚è§å›¾è¡¨ï¼š
- `decile_performance.png`: å„Decileçš„å¹³å‡Sharpeå¯¹æ¯”
- `cumulative_performance.png`: Top-Kç´¯è®¡å¹³å‡æ€§èƒ½æ›²çº¿

**è§£è¯»**ï¼š
- å•è°ƒé€’å‡è¡¨ç¤ºæ’åºå…·æœ‰å…¨å±€æœ‰æ•ˆæ€§ï¼ˆä¸åªæ˜¯Topå‡ ä¸ªå¥½ï¼‰
- Decile 1ï¼ˆæœ€ä¼˜ï¼‰åº”è¯¥æ˜¾è‘—é«˜äºDecile 10ï¼ˆæœ€å·®ï¼‰

---

## 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

### 4.1 Top100æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | IC Baseline | Calibrator | æå‡ |
|------|------------|-----------|------|
| å¹³å‡Sharpe | {metrics['statistical_tests']['IC_mean']:.4f} | {metrics['statistical_tests']['Calibrator_mean']:.4f} | {metrics['statistical_tests']['improvement_pct']:+.2f}% |
| ä¸­ä½æ•°Sharpe | {metrics['statistical_tests']['IC_median']:.4f} | {metrics['statistical_tests']['Calibrator_median']:.4f} | {(metrics['statistical_tests']['Calibrator_median'] / metrics['statistical_tests']['IC_median'] - 1) * 100:+.2f}% |

### 4.2 Mann-Whitney Uæ£€éªŒ

- Uç»Ÿè®¡é‡: {metrics['statistical_tests']['mann_whitney_u']:.1f}
- på€¼: {metrics['statistical_tests']['mann_whitney_p']:.4e}
- ç»“è®º: **{'Calibratoræ˜¾è‘—ä¼˜äºIC Baseline (p<0.05)' if metrics['statistical_tests']['mann_whitney_p'] < 0.05 else 'æ— æ˜¾è‘—å·®å¼‚'}**

---

## 5. æœ€ç»ˆç»“è®º

### 5.1 æ’åºæ˜¯å¦æœ‰ä»·å€¼ï¼Ÿ

åŸºäºSpearmanç›¸å…³æ€§ï¼š
- IC Baseline: {self._judge_correlation(metrics['rank_correlation']['IC_Baseline']['spearman'])}
- Calibrator: {self._judge_correlation(metrics['rank_correlation']['Calibrator']['spearman'])}

**ç»“è®º**: {'âœ… æ’åºå…·æœ‰é¢„æµ‹ä»·å€¼' if metrics['rank_correlation']['Calibrator']['spearman'] > 0.3 else 'âŒ æ’åºé¢„æµ‹èƒ½åŠ›ä¸è¶³ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆé£é™©'}

### 5.2 å¯¹æ¯”åŸºå‡†æå‡äº†å¤šå°‘ï¼Ÿ

- Top100å¹³å‡Sharpeæå‡: **{metrics['statistical_tests']['improvement_pct']:+.2f}%**
- ç»Ÿè®¡æ˜¾è‘—æ€§: **{'æ˜¾è‘—' if metrics['statistical_tests']['mann_whitney_p'] < 0.05 else 'ä¸æ˜¾è‘—'}** (p={metrics['statistical_tests']['mann_whitney_p']:.4e})

### 5.3 ç»æµä»·å€¼è¯„ä¼°

å‡è®¾ï¼š
- åŸºå‡†ç­–ç•¥ï¼ˆIC Top10ï¼‰ï¼šSharpe = {metrics['statistical_tests']['IC_mean']:.3f}
- Calibratorç­–ç•¥ï¼ˆCal Top10ï¼‰ï¼šSharpe = {metrics['statistical_tests']['Calibrator_mean']:.3f}
- å¹´åŒ–æ”¶ç›Šæå‡ï¼šçº¦{metrics['statistical_tests']['improvement_pct']:.1f}%

**ç»æµä»·å€¼**: {'âœ… å€¼å¾—éƒ¨ç½²' if metrics['statistical_tests']['improvement_pct'] > 10 else 'âš ï¸ æå‡æœ‰é™ï¼Œéœ€è°¨æ…è¯„ä¼°'}

---

## 6. å»ºè®®

"""
        
        # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
        if metrics['rank_correlation']['Calibrator']['spearman'] > 0.5:
            report += """
### âœ… å¼ºçƒˆæ¨èï¼šCalibratoræ’åºç³»ç»Ÿ

1. **ç«‹å³éƒ¨ç½²**ï¼šCalibratorå…·æœ‰è‰¯å¥½çš„é¢„æµ‹èƒ½åŠ›å’Œæ˜¾è‘—çš„æ€§èƒ½æå‡
2. **å»ºè®®é…ç½®**ï¼šé€‰æ‹©Calibrator Top10-50ä½œä¸ºå®ç›˜ç»„åˆæ± 
3. **ç›‘æ§æŒ‡æ ‡**ï¼š
   - å®ç›˜Sharpe vs å›æµ‹Sharpeçš„åå·®
   - Topç»„åˆçš„çœŸå®æ”¶ç›Šæ’å
   - å®šæœŸï¼ˆæ¯å­£åº¦ï¼‰é‡æ–°è®­ç»ƒCalibrator
"""
        elif metrics['rank_correlation']['Calibrator']['spearman'] > 0.3:
            report += """
### âš ï¸ è°¨æ…ä½¿ç”¨ï¼šCalibratoræ’åºç³»ç»Ÿ

1. **å°è§„æ¨¡è¯•ç‚¹**ï¼šå…ˆç”¨å°‘é‡èµ„é‡‘éªŒè¯
2. **åŠ å¼ºç›‘æ§**ï¼šå¯†åˆ‡è·Ÿè¸ªå®ç›˜vså›æµ‹çš„åå·®
3. **ä¼˜åŒ–æ–¹å‘**ï¼š
   - å¢åŠ æ›´å¤šç¨³å®šæ€§ç‰¹å¾
   - ä½¿ç”¨Ensembleæ–¹æ³•ï¼ˆIC + CalibratoråŠ æƒï¼‰
   - ç¼©çŸ­WFOè®­ç»ƒçª—å£ï¼Œæé«˜æ—¶æ•ˆæ€§
"""
        else:
            report += """
### âŒ ä¸å»ºè®®éƒ¨ç½²ï¼šæ’åºç³»ç»Ÿé¢„æµ‹èƒ½åŠ›ä¸è¶³

1. **æ ¹æœ¬é—®é¢˜**ï¼šWFOæ’åºå’ŒçœŸå®å›æµ‹ç›¸å…³æ€§ä½ï¼Œå­˜åœ¨ä¸¥é‡è¿‡æ‹Ÿåˆ
2. **æ”¹è¿›æ–¹å‘**ï¼š
   - é‡æ–°å®¡è§†WFOçª—å£è®¾ç½®ï¼ˆå¯èƒ½çª—å£å¤ªé•¿æˆ–å¤ªçŸ­ï¼‰
   - å¢åŠ æ ·æœ¬å¤–éªŒè¯ï¼ˆä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æ—¶é—´æ®µï¼‰
   - ç®€åŒ–æ¨¡å‹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
   - è€ƒè™‘ä½¿ç”¨æ›´ç¨³å¥çš„æ’åºæŒ‡æ ‡
"""
        
        report += """
---

## é™„å½•ï¼šæ•°æ®ç»Ÿè®¡

- WFO Run: {run_dir}
- æ ·æœ¬æ•°é‡: {sample_size}
- æ’åºèŒƒå›´: {rank_range}
- åˆ†ææ—¥æœŸ: {analysis_date}
""".format(
            run_dir=self.run_dir.name,
            sample_size=len(self.data),
            rank_range=f"1 - {len(self.data)}",
            analysis_date=pd.Timestamp.now().strftime('%Y-%m-%d')
        )
        
        output_file.write_text(report, encoding='utf-8')
        print(f"   - ä¿å­˜æŠ¥å‘Š: {output_file}")
    
    def _judge_correlation(self, r):
        """åˆ¤æ–­ç›¸å…³æ€§å¼ºåº¦"""
        if abs(r) > 0.7: return "Excellent (ä¼˜ç§€)"
        if abs(r) > 0.5: return "Good (è‰¯å¥½)"
        if abs(r) > 0.3: return "Moderate (ä¸­ç­‰)"
        return "Poor (è¾ƒå·®)"
    
    def run_full_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹"""
        print("="*60)
        print("ğŸš€ WFOæ’åºé¢„æµ‹èƒ½åŠ›éªŒè¯")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. è®¡ç®—å„é¡¹æŒ‡æ ‡
        rank_corr = self.compute_rank_correlation()
        topk_prec = self.compute_topk_precision()
        decile_anal = self.decile_analysis()
        stat_tests = self.statistical_tests()
        
        # 3. æ±‡æ€»æŒ‡æ ‡
        metrics = {
            'rank_correlation': rank_corr,
            'topk_precision': topk_prec,
            'decile_analysis': decile_anal,
            'statistical_tests': stat_tests
        }
        
        # 4. ç”Ÿæˆå¯è§†åŒ–
        output_dir = self.run_dir / "ranking_validation"
        self.visualize(output_dir)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(metrics, output_dir / "RANKING_VALIDATION_REPORT.md")
        
        # 6. ä¿å­˜æ•°å€¼ç»“æœ
        with open(output_dir / "validation_metrics.json", 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            def convert(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert(item) for item in obj]
                return obj
            
            json.dump(convert(metrics), f, indent=2)
        
        print("\n" + "="*60)
        print("âœ… éªŒè¯å®Œæˆï¼")
        print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print("="*60)
        
        return metrics


def main():
    # è·¯å¾„é…ç½®
    run_dir = Path("etf_rotation_experiments/results/run_20251113_145102")
    backtest_dir = Path("etf_rotation_experiments/results_combo_wfo")
    
    # è¿è¡ŒéªŒè¯
    validator = RankingValidator(run_dir, backtest_dir)
    metrics = validator.run_full_validation()
    
    # æ‰“å°å…³é”®ç»“è®º
    print("\n" + "="*60)
    print("ğŸ¯ æ ¸å¿ƒç»“è®º")
    print("="*60)
    
    ic_rho = metrics['rank_correlation']['IC_Baseline']['spearman']
    cal_rho = metrics['rank_correlation']['Calibrator']['spearman']
    improvement = metrics['statistical_tests']['improvement_pct']
    p_value = metrics['statistical_tests']['mann_whitney_p']
    
    print(f"\n1. æ’åºé¢„æµ‹èƒ½åŠ›:")
    print(f"   - IC Baseline: Spearman = {ic_rho:.3f}")
    print(f"   - Calibrator: Spearman = {cal_rho:.3f}")
    print(f"   - è¯„ä»·: {validator._judge_correlation(cal_rho)}")
    
    print(f"\n2. æ€§èƒ½æå‡:")
    print(f"   - Top100å¹³å‡Sharpeæå‡: {improvement:+.2f}%")
    print(f"   - ç»Ÿè®¡æ˜¾è‘—æ€§: {'âœ“ æ˜¾è‘— (p<0.05)' if p_value < 0.05 else 'âœ— ä¸æ˜¾è‘—'}")
    
    print(f"\n3. æœ€ç»ˆå»ºè®®:")
    if cal_rho > 0.5 and improvement > 10 and p_value < 0.05:
        print("   âœ… å¼ºçƒˆæ¨èéƒ¨ç½²Calibratoræ’åºç³»ç»Ÿ")
    elif cal_rho > 0.3:
        print("   âš ï¸ å¯ä»¥è¯•ç‚¹ï¼Œä½†éœ€åŠ å¼ºç›‘æ§")
    else:
        print("   âŒ ä¸å»ºè®®éƒ¨ç½²ï¼Œéœ€é‡æ–°ä¼˜åŒ–")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    main()
