#!/usr/bin/env python3
"""
å¿«é€Ÿ LambdaMART vs GBDT å¯¹æ¯”å®éªŒ
ä½¿ç”¨å·²æœ‰çš„çœŸå®å›æµ‹æ•°æ®è®­ç»ƒå’ŒéªŒè¯
"""

import glob
import json
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from lightgbm import LGBMRanker
from scipy.stats import spearmanr
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import ndcg_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def load_backtest_results(run_id: str):
    """åŠ è½½çœŸå®å›æµ‹ç»“æœ"""
    pattern = f'results_combo_wfo/{run_id}_*/*.csv'
    backtest_files = glob.glob(pattern)
    
    all_results = []
    for f in backtest_files:
        try:
            df = pd.read_csv(f)
            if 'combo' in df.columns and 'sharpe' in df.columns:
                all_results.append(df)
        except:
            pass
    
    if not all_results:
        raise ValueError(f"æœªæ‰¾åˆ°å›æµ‹ç»“æœæ–‡ä»¶: {pattern}")
    
    combined = pd.concat(all_results, ignore_index=True)
    combined = combined.drop_duplicates(subset=['combo'], keep='last')
    return combined


def merge_wfo_features(backtest_df: pd.DataFrame, wfo_file: Path):
    """åˆå¹¶ WFO ç‰¹å¾"""
    wfo_df = pd.read_parquet(wfo_file)
    
    # é€‰æ‹©åŸºç¡€ç‰¹å¾ï¼ˆä¸GBDTæ¨¡å‹ä¸€è‡´ï¼‰
    feature_cols = ['combo', 'mean_oos_ic', 'oos_ic_std', 'positive_rate', 'stability_score', 'combo_size']
    wfo_features = wfo_df[feature_cols].copy()
    
    # åˆå¹¶
    merged = backtest_df.merge(wfo_features, on='combo', how='inner')
    return merged


def to_relevance(y: np.ndarray, n_bins: int = 32) -> np.ndarray:
    """è½¬æ¢ä¸ºæ’åºæ ‡ç­¾"""
    quantiles = np.linspace(0, 1, n_bins + 1)
    bins = np.quantile(y, quantiles)
    bins = np.unique(bins)
    if bins.size <= 2:
        ranks = pd.Series(y).rank(method="dense").astype(int) - 1
        return np.maximum(ranks, 0)
    thresholds = bins[1:-1]
    labels = np.digitize(y, thresholds, right=False)
    return labels.astype(int)


def calc_ranking_metrics(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
    """è®¡ç®—æ’åºæŒ‡æ ‡"""
    # Spearmanç›¸å…³æ€§
    spearman, p_value = spearmanr(y_pred, y_true)
    
    # Top-Ké‡å ç‡
    metrics = {
        'model': model_name,
        'spearman': float(spearman),
        'spearman_pvalue': float(p_value),
    }
    
    for k in [10, 50, 100, 500, 1000]:
        if k > len(y_true):
            continue
        idx_true = set(np.argsort(-y_true)[:k])
        idx_pred = set(np.argsort(-y_pred)[:k])
        overlap = len(idx_true & idx_pred) / k
        metrics[f'top{k}_overlap'] = float(overlap)
    
    # NDCG
    ndcg_k = min(len(y_true), 1000)
    gains = y_true - y_true.min()
    ndcg = float(ndcg_score([gains], [y_pred], k=ndcg_k))
    metrics['ndcg@1000'] = ndcg
    
    return metrics


def main():
    print("=" * 80)
    print("ğŸ”¬ LambdaMART vs GBDT å¿«é€Ÿå¯¹æ¯”å®éªŒ")
    print("=" * 80)
    
    run_id = '20251112_223854'
    run_dir = Path(f'results/run_{run_id}')
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    backtest_df = load_backtest_results(run_id)
    print(f"âœ… çœŸå®å›æµ‹ç»“æœ: {len(backtest_df)} ä¸ªç­–ç•¥")
    
    wfo_file = run_dir / 'ranking_blends/ranking_baseline.parquet'
    merged_df = merge_wfo_features(backtest_df, wfo_file)
    print(f"âœ… åˆå¹¶ WFO ç‰¹å¾å: {len(merged_df)} ä¸ªç­–ç•¥")
    
    # 2. å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
    # å¤„ç†åˆ—åå†²çªï¼ˆcombo_size å¯èƒ½å˜æˆ combo_size_x æˆ– combo_size_yï¼‰
    if 'combo_size' not in merged_df.columns:
        if 'combo_size_y' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_y']
        elif 'combo_size_x' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_x']
    
    feature_cols = ['mean_oos_ic', 'oos_ic_std', 'positive_rate', 'stability_score', 'combo_size']
    X = merged_df[feature_cols].values
    y = merged_df['sharpe'].values  # ä½¿ç”¨çœŸå® Sharpe ä½œä¸ºç›®æ ‡
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    print(f"  ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"  ç‰¹å¾: {feature_cols}")
    
    # 3. è®­ç»ƒ GBDT å›å½’æ¨¡å‹
    print("\n" + "-" * 80)
    print("ğŸŒ² è®­ç»ƒ GBDT å›å½’æ¨¡å‹...")
    print("-" * 80)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    gbdt_model = GradientBoostingRegressor(
        n_estimators=400,
        max_depth=5,
        learning_rate=0.05,
        subsample=0.8,
        random_state=42,
        verbose=0
    )
    gbdt_model.fit(X_train_scaled, y_train)
    
    gbdt_pred_train = gbdt_model.predict(X_train_scaled)
    gbdt_pred_test = gbdt_model.predict(X_test_scaled)
    
    gbdt_metrics_train = calc_ranking_metrics(y_train, gbdt_pred_train, 'GBDT (è®­ç»ƒé›†)')
    gbdt_metrics_test = calc_ranking_metrics(y_test, gbdt_pred_test, 'GBDT (æµ‹è¯•é›†)')
    
    print(f"è®­ç»ƒé›† Spearman: {gbdt_metrics_train['spearman']:.4f}")
    print(f"æµ‹è¯•é›† Spearman: {gbdt_metrics_test['spearman']:.4f}")
    print(f"æµ‹è¯•é›† NDCG@1000: {gbdt_metrics_test['ndcg@1000']:.4f}")
    print(f"æµ‹è¯•é›† Top100é‡å : {gbdt_metrics_test.get('top100_overlap', 0):.2%}")
    
    # 4. è®­ç»ƒ LambdaMART æ’åºæ¨¡å‹
    print("\n" + "-" * 80)
    print("ğŸ¯ è®­ç»ƒ LambdaMART æ’åºæ¨¡å‹...")
    print("-" * 80)
    
    y_train_rank = to_relevance(y_train)
    y_test_rank = to_relevance(y_test)
    
    lambdarank_model = LGBMRanker(
        objective="lambdarank",
        metric="ndcg",
        learning_rate=0.05,
        n_estimators=400,
        num_leaves=31,
        min_data_in_leaf=20,
        feature_fraction=0.8,
        bagging_fraction=0.8,
        bagging_freq=3,
        lambda_l1=0.1,
        lambda_l2=0.1,
        random_state=42,
        n_jobs=-1,
        label_gain=list(range(32)),
        verbose=-1
    )
    
    lambdarank_model.fit(
        X_train,
        y_train_rank,
        group=[len(X_train)],
        eval_set=[(X_test, y_test_rank)],
        eval_group=[[len(X_test)]],
        callbacks=[]
    )
    
    rank_pred_train = lambdarank_model.predict(X_train)
    rank_pred_test = lambdarank_model.predict(X_test)
    
    rank_metrics_train = calc_ranking_metrics(y_train, rank_pred_train, 'LambdaMART (è®­ç»ƒé›†)')
    rank_metrics_test = calc_ranking_metrics(y_test, rank_pred_test, 'LambdaMART (æµ‹è¯•é›†)')
    
    print(f"è®­ç»ƒé›† Spearman: {rank_metrics_train['spearman']:.4f}")
    print(f"æµ‹è¯•é›† Spearman: {rank_metrics_test['spearman']:.4f}")
    print(f"æµ‹è¯•é›† NDCG@1000: {rank_metrics_test['ndcg@1000']:.4f}")
    print(f"æµ‹è¯•é›† Top100é‡å : {rank_metrics_test.get('top100_overlap', 0):.2%}")
    
    # 5. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š å¯¹æ¯”åˆ†æç»“æœ")
    print("=" * 80)
    
    print("\nã€æµ‹è¯•é›† Spearman ç›¸å…³æ€§å¯¹æ¯”ã€‘")
    print(f"  GBDT å›å½’:     {gbdt_metrics_test['spearman']:+.4f}")
    print(f"  LambdaMART:    {rank_metrics_test['spearman']:+.4f}")
    improvement = rank_metrics_test['spearman'] - gbdt_metrics_test['spearman']
    print(f"  æ”¹è¿›å¹…åº¦:      {improvement:+.4f} ({improvement/abs(gbdt_metrics_test['spearman'])*100:+.1f}%)")
    
    print("\nã€æµ‹è¯•é›† NDCG@1000 å¯¹æ¯”ã€‘")
    print(f"  GBDT å›å½’:     {gbdt_metrics_test['ndcg@1000']:.4f}")
    print(f"  LambdaMART:    {rank_metrics_test['ndcg@1000']:.4f}")
    ndcg_improvement = rank_metrics_test['ndcg@1000'] - gbdt_metrics_test['ndcg@1000']
    print(f"  æ”¹è¿›å¹…åº¦:      {ndcg_improvement:+.4f}")
    
    print("\nã€Top-K é‡å ç‡å¯¹æ¯”ã€‘")
    for k in [10, 50, 100, 500, 1000]:
        key = f'top{k}_overlap'
        if key in gbdt_metrics_test and key in rank_metrics_test:
            gbdt_val = gbdt_metrics_test[key]
            rank_val = rank_metrics_test[key]
            diff = rank_val - gbdt_val
            print(f"  Top{k:4d}:  GBDT {gbdt_val:.2%}  â†’  LambdaMART {rank_val:.2%}  (Î” {diff:+.2%})")
    
    # 6. ä¿å­˜ç»“æœ
    results = {
        'experiment': 'LambdaMART_vs_GBDT_quick_comparison',
        'run_id': run_id,
        'n_samples': len(merged_df),
        'n_train': len(X_train),
        'n_test': len(X_test),
        'features': feature_cols,
        'gbdt_regressor': {
            'train': gbdt_metrics_train,
            'test': gbdt_metrics_test,
        },
        'lambdarank': {
            'train': rank_metrics_train,
            'test': rank_metrics_test,
        },
        'improvement': {
            'spearman_delta': improvement,
            'spearman_pct': improvement / abs(gbdt_metrics_test['spearman']) * 100,
            'ndcg_delta': ndcg_improvement,
        }
    }
    
    output_file = run_dir / 'lambdarank_vs_gbdt_comparison.json'
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    # 7. ç»“è®º
    print("\n" + "=" * 80)
    print("ğŸ¯ å®éªŒç»“è®º")
    print("=" * 80)
    
    if rank_metrics_test['spearman'] > gbdt_metrics_test['spearman'] + 0.05:
        print("âœ… LambdaMART æ˜¾è‘—ä¼˜äº GBDT å›å½’")
        print(f"   æ’åºç›¸å…³æ€§æå‡ {improvement:.4f} (ç›¸å¯¹æå‡ {improvement/abs(gbdt_metrics_test['spearman'])*100:.1f}%)")
        print("   å»ºè®®ï¼šé‡‡ç”¨ LambdaMART æ›¿ä»£å½“å‰ GBDT å›å½’æ¨¡å‹")
    elif rank_metrics_test['spearman'] > gbdt_metrics_test['spearman']:
        print("âš ï¸  LambdaMART ç•¥ä¼˜äº GBDT å›å½’")
        print(f"   æ’åºç›¸å…³æ€§æå‡ {improvement:.4f}")
        print("   å»ºè®®ï¼šå¯ä»¥å°è¯•é‡‡ç”¨ï¼Œä½†æå‡æœ‰é™")
    else:
        print("âŒ LambdaMART æœªæ˜¾ç¤ºå‡ºä¼˜åŠ¿")
        print(f"   æ’åºç›¸å…³æ€§å˜åŒ– {improvement:.4f}")
        print("   å»ºè®®ï¼šç»§ç»­ä½¿ç”¨ GBDT å›å½’æ¨¡å‹ï¼Œæˆ–å°è¯•å…¶ä»–ä¼˜åŒ–æ–¹å‘")
    
    print("=" * 80)


if __name__ == '__main__':
    main()
