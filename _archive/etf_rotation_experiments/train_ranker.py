#!/usr/bin/env python3
"""
è®­ç»ƒLTRæ’åºæ¨¡å‹ (å•æ•°æ®æºè®­ç»ƒ)

âš ï¸  æ³¨æ„: æœ¬è„šæœ¬ä¿ç•™ç”¨äºå‘åå…¼å®¹
æ¨èä½¿ç”¨æ–°çš„ç»Ÿä¸€Pipelineå…¥å£: run_ranking_pipeline.py

æ–°å…¥å£æ”¯æŒ:
- å¤šæ•°æ®æº/å¤šæ¢ä»“å‘¨æœŸè®­ç»ƒ
- è‡ªåŠ¨ç¨³å¥æ€§è¯„ä¼°
- ç»Ÿä¸€é…ç½®ç®¡ç†

ä½¿ç”¨æ–¹å¼:
  # ä¼ ç»Ÿå•æ•°æ®æºè®­ç»ƒ (æœ¬è„šæœ¬)
  python train_ranker.py --wfo-dir results/run_xxx --backtest-dir results_combo_wfo/xxx
  
  # æ–°çš„ç»Ÿä¸€Pipeline (æ¨è)
  python run_ranking_pipeline.py --config configs/ranking_datasets.yaml
"""
import argparse
from pathlib import Path
import json
import warnings
import pandas as pd

warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥æ–°çš„pipelineæ¨¡å—
try:
    from ml_ranker.config import DatasetConfig
    from ml_ranker.pipeline import run_training_pipeline
    PIPELINE_AVAILABLE = True
except ImportError:
    PIPELINE_AVAILABLE = False

from ml_ranker.data_loader import build_training_dataset, find_latest_wfo_run, find_latest_backtest_run
from ml_ranker.feature_engineer import build_feature_matrix
from ml_ranker.ltr_model import LTRRanker
from ml_ranker.evaluator import generate_evaluation_report, create_ranking_comparison_df


def parse_args():
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒLTRæ’åºæ¨¡å‹ (å•æ•°æ®æº)",
        epilog="æ¨èä½¿ç”¨: python run_ranking_pipeline.py --config configs/ranking_datasets.yaml"
    )
    parser.add_argument(
        "--wfo-dir",
        type=str,
        default=None,
        help="WFOç»“æœç›®å½• (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)"
    )
    parser.add_argument(
        "--backtest-dir",
        type=str,
        default=None,
        help="å›æµ‹ç»“æœç›®å½• (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)"
    )
    parser.add_argument(
        "--model-dir",
        type=str,
        default="ml_ranker/models",
        help="æ¨¡å‹ä¿å­˜ç›®å½•"
    )
    parser.add_argument(
        "--eval-dir",
        type=str,
        default="ml_ranker/evaluation",
        help="è¯„ä¼°æŠ¥å‘Šä¿å­˜ç›®å½•"
    )
    parser.add_argument(
        "--n-folds",
        type=int,
        default=5,
        help="äº¤å‰éªŒè¯æŠ˜æ•°"
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=500,
        help="LightGBMæ ‘æ•°é‡"
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=0.05,
        help="å­¦ä¹ ç‡"
    )
    parser.add_argument(
        "--use-pipeline",
        action="store_true",
        help="ä½¿ç”¨æ–°çš„pipelineæ¨¡å—(æ¨è)"
    )
    parser.add_argument(
        "--use-gpu",
        action="store_true",
        help="å¯ç”¨ LightGBM GPU æ¨¡å¼ï¼ˆéœ€å·²ç¼–è¯‘/å®‰è£… GPU ç‰ˆ LightGBMï¼‰"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    
    print(f"\n{'='*80}")
    print("ğŸš€ å¼€å§‹è®­ç»ƒLTRæ’åºæ¨¡å‹ (å•æ•°æ®æºæ¨¡å¼)")
    print(f"{'='*80}\n")
    
    # å¦‚æœå¯ç”¨pipelineæ¨¡å¼ä¸”å¯ç”¨,è½¬è€Œä½¿ç”¨æ–°pipeline
    if args.use_pipeline and PIPELINE_AVAILABLE:
        print("â„¹ï¸  ä½¿ç”¨æ–°çš„Pipelineæ¨¡å¼...")
        
        # ç¡®å®šæ•°æ®è·¯å¾„
        if args.wfo_dir is None:
            wfo_dir = find_latest_wfo_run()
        else:
            wfo_dir = Path(args.wfo_dir)
        
        if args.backtest_dir is None:
            backtest_dir = find_latest_backtest_run()
        else:
            backtest_dir = Path(args.backtest_dir)
        
        # åˆ›å»ºå•æ•°æ®æºé…ç½®
        config = DatasetConfig.from_single_source(
            wfo_dir=str(wfo_dir),
            real_dir=str(backtest_dir),
            rebalance_days=8  # é»˜è®¤8å¤©
        )
        
        # è°ƒç”¨pipeline
        result = run_training_pipeline(
            config=config,
            model_params={
                'n_estimators': args.n_estimators,
                'learning_rate': args.learning_rate
            },
            enable_robustness=False,  # å•æ•°æ®æºæ¨¡å¼é»˜è®¤ä¸åšç¨³å¥æ€§è¯„ä¼°
            save_model=True,
            output_dir=Path("ml_ranker"),
            verbose=True
        )
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ")
        print(f"ğŸ’¡ æç¤º: è‹¥è¦ä½¿ç”¨å¤šæ•°æ®æºè®­ç»ƒ,è¯·è¿è¡Œ:")
        print(f"   python run_ranking_pipeline.py --config configs/ranking_datasets.yaml\n")
        
        return
    
    # åŸæœ‰çš„è®­ç»ƒé€»è¾‘(ä¿æŒä¸å˜)
    # 1. ç¡®å®šæ•°æ®è·¯å¾„
    if args.wfo_dir is None:
        print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°WFOç»“æœ...")
        wfo_dir = find_latest_wfo_run()
    else:
        wfo_dir = Path(args.wfo_dir)
    
    if args.backtest_dir is None:
        print("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°å›æµ‹ç»“æœ...")
        backtest_dir = find_latest_backtest_run()
    else:
        backtest_dir = Path(args.backtest_dir)
    
    print(f"  WFOç›®å½•: {wfo_dir}")
    print(f"  å›æµ‹ç›®å½•: {backtest_dir}")
    
    # 2. åŠ è½½è®­ç»ƒæ•°æ®
    print(f"\n{'='*80}")
    print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®")
    print(f"{'='*80}")
    
    from ml_ranker.data_loader import load_wfo_features, load_real_backtest_results
    
    wfo_df = load_wfo_features(wfo_dir)
    real_df = load_real_backtest_results(backtest_dir)
    
    merged_df, y, metadata = build_training_dataset(
        wfo_df=wfo_df,
        real_df=real_df
    )
    
    print(f"  æ ·æœ¬æ•°: {len(merged_df)}")
    print(f"  ç›®æ ‡å˜é‡: {metadata['target_col']}")
    print(f"  å‡å€¼: {y.mean():.4f}, æ ‡å‡†å·®: {y.std():.4f}")
    
    # 3. æ„å»ºç‰¹å¾çŸ©é˜µ
    print(f"\n{'='*80}")
    print("ğŸ› ï¸ æ„å»ºç‰¹å¾çŸ©é˜µ")
    print(f"{'='*80}")
    
    X_df = build_feature_matrix(merged_df)
    X = X_df.values
    feature_names = list(X_df.columns)
    
    print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"  æ ·æœ¬ç¤ºä¾‹: {feature_names[:5]}")
    
    # 4. è®­ç»ƒæ¨¡å‹
    print(f"\n{'='*80}")
    print("ğŸ”¥ è®­ç»ƒLTRæ¨¡å‹")
    print(f"{'='*80}")
    
    model = LTRRanker(
        objective="regression",  # ä½¿ç”¨å›å½’æ¨¡å¼é¿å…query sizeé™åˆ¶
        metric="rmse",
        n_estimators=args.n_estimators,
        learning_rate=args.learning_rate,
        max_depth=6,
        num_leaves=31,
        min_data_in_leaf=20,
        verbose=-1
    )
    # å¦‚æœä¼ å…¥äº† --use-gpu æ ‡å¿—ï¼Œåˆ™å°† GPU è¯·æ±‚ä¼ é€’ç»™æ¨¡å‹
    if hasattr(args, 'use_gpu') and args.use_gpu:
        model = LTRRanker(
            objective="regression",
            metric="rmse",
            n_estimators=args.n_estimators,
            learning_rate=args.learning_rate,
            max_depth=6,
            num_leaves=31,
            min_data_in_leaf=20,
            verbose=-1,
            use_gpu=True,
        )
    
    model.train(
        X=pd.DataFrame(X, columns=feature_names),
        y=y,
        cv_folds=args.n_folds
    )
    
    print(f"\n  âœ“ è®­ç»ƒå®Œæˆ")
    print(f"  CV Spearman: {model.cv_results[-1]['spearman_corr']:.4f}")
    
    # 5. é¢„æµ‹
    print(f"\n{'='*80}")
    print("ğŸ¯ é¢„æµ‹æ’åº")
    print(f"{'='*80}")
    
    scores, ranks = model.predict(X)
    
    print(f"  é¢„æµ‹åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
    print(f"  æ’åèŒƒå›´: [1, {ranks.max()}]")
    
    # 6. ç‰¹å¾é‡è¦æ€§
    print(f"\n{'='*80}")
    print("ğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print(f"{'='*80}")
    
    importance_df = model.get_feature_importance()
    print(f"\n  Top-15 é‡è¦ç‰¹å¾:")
    for idx, row in importance_df.head(15).iterrows():
        print(f"    {row['feature']:35s}: {row['importance']:10.0f}")
    
    # 7. è¯„ä¼°
    print(f"\n{'='*80}")
    print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°")
    print(f"{'='*80}")
    
    # ä½¿ç”¨WFOåŸå§‹mean_oos_icä½œä¸ºbaseline
    baseline_scores = merged_df["mean_oos_ic"].values
    
    eval_report = generate_evaluation_report(
        y_true=y.values,
        y_pred=scores,
        baseline_scores=baseline_scores,
        metadata=metadata,
        feature_importance=importance_df,
        output_path=Path(args.eval_dir) / "evaluation_report.json"
    )
    
    # 8. ä¿å­˜æ¨¡å‹
    print(f"\n{'='*80}")
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹")
    print(f"{'='*80}")
    
    model_dir = Path(args.model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = model_dir / "ltr_ranker"
    model.save(str(model_path))
    
    print(f"  âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}.txt")
    
    # 9. ä¿å­˜æ’åºå¯¹æ¯”è¡¨
    print(f"\n{'='*80}")
    print("ğŸ“‹ ç”Ÿæˆæ’åºå¯¹æ¯”è¡¨")
    print(f"{'='*80}")
    
    comparison_df = create_ranking_comparison_df(
        y_true=y.values,
        y_pred=scores,
        baseline_scores=baseline_scores,
        combos=merged_df["combo"].values,
        top_n=100
    )
    
    comparison_path = Path(args.eval_dir) / "ranking_comparison_top100.csv"
    comparison_path.parent.mkdir(parents=True, exist_ok=True)
    comparison_df.to_csv(comparison_path, index=False, encoding="utf-8-sig")
    
    print(f"  âœ“ Top-100å¯¹æ¯”è¡¨å·²ä¿å­˜: {comparison_path}")
    
    # 10. æ€»ç»“
    print(f"\n{'='*80}")
    print("âœ… è®­ç»ƒå®Œæˆ")
    print(f"{'='*80}")
    
    print(f"\n  æ¨¡å‹æ€§èƒ½:")
    print(f"    Spearmanç›¸å…³æ€§: {eval_report['model_metrics']['spearman_corr']:.4f}")
    print(f"    NDCG@10: {eval_report['model_metrics'].get('ndcg@10', 0):.4f}")
    print(f"    Top-10å‘½ä¸­ç‡: {eval_report['top10_analysis']['model_hits']}/10")
    print(f"    Top-10å¹³å‡æ”¶ç›Š: {eval_report['top10_analysis']['model_pred_mean']:.4f}")
    
    print(f"\n  è¾“å‡ºæ–‡ä»¶:")
    print(f"    æ¨¡å‹: {model_path}.txt")
    print(f"    å…ƒæ•°æ®: {model_path}.meta.pkl")
    print(f"    è¯„ä¼°æŠ¥å‘Š: {Path(args.eval_dir) / 'evaluation_report.json'}")
    print(f"    å¯¹æ¯”è¡¨: {comparison_path}")
    
    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    main()
