#!/usr/bin/env python3
"""
LightGBMä½¿ç”¨ç¤ºä¾‹ - é€‚ç”¨äºApple Silicon Mac
åŒ…å«åˆ†ç±»ã€å›å½’å’Œå‚æ•°è°ƒä¼˜ç¤ºä¾‹
"""

import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, mean_squared_error
import time

# ================================
# 1. åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹
# ================================

def binary_classification_example():
    """äºŒåˆ†ç±»ä»»åŠ¡ç¤ºä¾‹"""
    print("ğŸ¯ äºŒåˆ†ç±»ä»»åŠ¡ç¤ºä¾‹")
    print("-" * 30)

    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=5000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        random_state=42
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Apple Siliconä¼˜åŒ–çš„å‚æ•°
    params = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'num_threads': 4,  # MèŠ¯ç‰‡ä¼˜åŒ–
        'seed': 42
    }

    # è®­ç»ƒ
    train_data = lgb.Dataset(X_train, label=y_train)
    model = lgb.train(
        params,
        train_data,
        num_boost_round=100,
        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
    )

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_binary = (y_pred > 0.5).astype(int)
    accuracy = accuracy_score(y_test, y_pred_binary)

    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"è®­ç»ƒè½®æ•°: {model.num_trees()}")

    return model

# ================================
# 2. å›å½’ä»»åŠ¡ç¤ºä¾‹
# ================================

def regression_example():
    """å›å½’ä»»åŠ¡ç¤ºä¾‹"""
    print("\nğŸ“ˆ å›å½’ä»»åŠ¡ç¤ºä¾‹")
    print("-" * 30)

    # ç”Ÿæˆæ•°æ®
    X, y = make_regression(
        n_samples=3000,
        n_features=15,
        n_informative=10,
        noise=0.1,
        random_state=42
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # å›å½’å‚æ•°
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'verbose': -1,
        'num_threads': 4,
        'seed': 42
    }

    # è®­ç»ƒ
    train_data = lgb.Dataset(X_train, label=y_train)
    model = lgb.train(
        params,
        train_data,
        num_boost_round=100,
        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]
    )

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"RMSE: {rmse:.4f}")
    print(f"è®­ç»ƒè½®æ•°: {model.num_trees()}")

    return model

# ================================
# 3. Scikit-learnæ¥å£ç¤ºä¾‹
# ================================

def sklearn_interface_example():
    """ä½¿ç”¨Scikit-learnæ¥å£"""
    print("\nğŸ”§ Scikit-learnæ¥å£ç¤ºä¾‹")
    print("-" * 30)

    # ç”Ÿæˆæ•°æ®
    X, y = make_classification(
        n_samples=2000,
        n_features=10,
        random_state=42
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # ä½¿ç”¨LGBMClassifier
    clf = lgb.LGBMClassifier(
        num_leaves=31,
        learning_rate=0.05,
        n_estimators=100,
        n_jobs=4,  # å¹¶è¡Œå¤„ç†
        random_state=42,
        verbose=-1
    )

    # è®­ç»ƒ
    start_time = time.time()
    clf.fit(X_train, y_train)
    training_time = time.time() - start_time

    # é¢„æµ‹
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.3f}ç§’")

    return clf

# ================================
# 4. å‚æ•°è°ƒä¼˜ç¤ºä¾‹
# ================================

def hyperparameter_tuning():
    """å‚æ•°è°ƒä¼˜ç¤ºä¾‹"""
    print("\nâš™ï¸ å‚æ•°è°ƒä¼˜ç¤ºä¾‹")
    print("-" * 30)

    # ç”Ÿæˆå°æ•°æ®é›†ç”¨äºè°ƒä¼˜
    X, y = make_classification(
        n_samples=1000,
        n_features=15,
        random_state=42
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'num_leaves': [15, 31, 63],
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [50, 100, 200],
        'min_child_samples': [10, 20, 30]
    }

    # ä½¿ç”¨GridSearchCV
    lgb_clf = lgb.LGBMClassifier(
        n_jobs=4,
        random_state=42,
        verbose=-1
    )

    grid_search = GridSearchCV(
        lgb_clf,
        param_grid,
        cv=3,
        scoring='accuracy',
        n_jobs=1,
        verbose=0
    )

    print("æ­£åœ¨è¿›è¡Œå‚æ•°è°ƒä¼˜...")
    start_time = time.time()
    grid_search.fit(X_train, y_train)
    tuning_time = time.time() - start_time

    print(f"è°ƒä¼˜å®Œæˆï¼ç”¨æ—¶: {tuning_time:.2f}ç§’")
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")

    # ä½¿ç”¨æœ€ä½³æ¨¡å‹
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)

    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")

# ================================
# 5. å¤„ç†Pandas DataFrame
# ================================

def pandas_dataframe_example():
    """å¤„ç†Pandas DataFrame"""
    print("\nğŸ“Š Pandas DataFrameç¤ºä¾‹")
    print("-" * 30)

    # åˆ›å»ºç¤ºä¾‹DataFrame
    np.random.seed(42)
    data = {
        'feature1': np.random.randn(1000),
        'feature2': np.random.randn(1000),
        'feature3': np.random.choice(['A', 'B', 'C'], 1000),
        'feature4': np.random.rand(1000),
        'target': np.random.choice([0, 1], 1000)
    }

    df = pd.DataFrame(data)

    # å¤„ç†åˆ†ç±»ç‰¹å¾
    df_encoded = pd.get_dummies(df, columns=['feature3'])

    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df_encoded.drop('target', axis=1)
    y = df_encoded['target']

    # ä½¿ç”¨feature_nameæŒ‡å®šç‰¹å¾å
    feature_names = X.columns.tolist()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # è®­ç»ƒæ¨¡å‹
    train_data = lgb.Dataset(
        X_train,
        label=y_train,
        feature_name=feature_names,
        categorical_feature=['feature3_A', 'feature3_B', 'feature3_C']
    )

    model = lgb.train(
        {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'verbose': -1,
            'num_threads': 4
        },
        train_data,
        num_boost_round=50
    )

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_binary = (y_pred > 0.5).astype(int)
    accuracy = accuracy_score(y_test, y_pred_binary)

    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(feature_names, model.feature_importance()))
    top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:3]

    print("Top 3 é‡è¦ç‰¹å¾:")
    for feature, importance in top_features:
        print(f"  {feature}: {importance:.2f}")

# ================================
# ä¸»å‡½æ•°
# ================================

if __name__ == "__main__":
    print("ğŸ LightGBMåœ¨Apple Silicon Macä¸Šçš„ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    binary_classification_example()
    regression_example()
    sklearn_interface_example()
    hyperparameter_tuning()
    pandas_dataframe_example()

    print("\nğŸ’¡ Apple Silicon Macä¼˜åŒ–å»ºè®®:")
    print("1. ä½¿ç”¨ num_threads=4 (æˆ–ä½ çš„MèŠ¯ç‰‡æ ¸å¿ƒæ•°)")
    print("2. ä¼˜å…ˆä½¿ç”¨CPUè€ŒéGPU")
    print("3. è®¾ç½® verbose=-1 å‡å°‘è¾“å‡ºå¼€é”€")
    print("4. ä½¿ç”¨ early_stopping é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("5. å¯¹äºå¤§æ•°æ®é›†ï¼Œä½¿ç”¨ feature_fraction å’Œ bagging_fraction")

    print("\nğŸš€ å¼€å§‹ä½ çš„LightGBMä¹‹æ—…å§ï¼")