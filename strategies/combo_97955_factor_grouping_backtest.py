#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Combo 97955 å› å­åˆ†ç»„çœŸå®å›æµ‹éªŒè¯

ğŸ”¥ ä¿®å¤å†…å®¹ï¼š
1. å› å­è¦†ç›–ç¼ºå£ï¼šç¡®ä¿æ‰€æœ‰å› å­éƒ½è¢«æ­£ç¡®å½’ç±»ï¼ˆåŒ…æ‹¬_12, _14ç­‰ï¼‰
2. äº¤æ˜“æˆæœ¬ä¿®æ­£ï¼šä½¿ç”¨çœŸå®æ¸¯è‚¡ETFè´¹ç‡ 0.0028ï¼ˆåŒè¾¹0.35%ï¼‰
3. çœŸå®æ ·æœ¬ç»Ÿè®¡ï¼šä»æŒä»“å˜åŒ–è®¡ç®—å®é™…äº¤æ˜“ç¬”æ•°
4. å®Œæ•´åˆ†æäº¤ä»˜ï¼šå‡ä¿¡å·è¿‡æ»¤ã€æŒç»­æ€§åˆ†å±‚ã€å¯è§†åŒ–ã€æ“ä½œé˜ˆå€¼

åŸºäºvectorbt_multifactor_grid.pyçš„VectorizedBacktestEngineå®ç°çœŸå®å›æµ‹
åŸåˆ™ï¼šOnly Real Data. No Fake Data.
"""

import json
import re
import sys
import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm

# å¤ç”¨ç°æœ‰çš„çœŸå®å›æµ‹å¼•æ“
sys.path.insert(0, str(Path(__file__).parent))
from vectorbt_multifactor_grid import (
    VectorizedBacktestEngine,
    load_factor_panel,
    load_price_pivot,
    normalize_factors,
)

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


# ============================================================================
# å› å­åˆ†ç»„å®šä¹‰ï¼ˆåŸºäºcombo 97955çš„å®é™…å› å­ï¼‰
# ============================================================================

FACTOR_GROUPS = {
    "short_term": {
        "name": "çŸ­æœŸï¼ˆ<20æ—¥ï¼‰",
        "patterns": [
            "_5",
            "_6",
            "_9",
            "_10",
            "_12",
            "_14",
            "PRICE_POSITION_10",
            "RSI",
            "MFI",
            "STOCH",
            "CCI",
            "WILLIAMS",
        ],
        "description": "çŸ­å‘¨æœŸåŠ¨é‡/æŠ€æœ¯æŒ‡æ ‡",
    },
    "mid_term_20": {
        "name": "ä¸­æœŸ20æ—¥",
        "patterns": ["_20", "PRICE_POSITION_20"],
        "description": "20æ—¥ä¸­å‘¨æœŸè¶‹åŠ¿ç¡®è®¤",
    },
    "mid_term_30": {
        "name": "ä¸­æœŸ30æ—¥",
        "patterns": ["_30", "PRICE_POSITION_30"],
        "description": "30æ—¥ä¸­å‘¨æœŸè¶‹åŠ¿ç¡®è®¤",
    },
    "long_term": {
        "name": "é•¿æœŸï¼ˆâ‰¥40æ—¥ï¼‰",
        "patterns": ["_40", "_60", "PRICE_POSITION_60", "MOMENTUM_20"],
        "description": "é•¿å‘¨æœŸé˜²å®ˆ",
    },
    "volatility_filter": {
        "name": "æ³¢åŠ¨ç‡è¿‡æ»¤",
        "patterns": ["STDDEV", "VAR", "HT_DCPERIOD", "ATR"],
        "description": "æ³¢åŠ¨ç‡ä¸å‘¨æœŸè¿‡æ»¤å™¨",
    },
}

# çœŸå®äº¤æ˜“æˆæœ¬ï¼ˆæ¸¯è‚¡ETFï¼‰
REAL_FEES = 0.0028  # åŒè¾¹0.35% â‰ˆ å•è¾¹0.14% Ã— 2


def classify_factor(factor_name: str) -> str:
    """åˆ†ç±»å› å­åˆ°å¯¹åº”ç»„åˆ«ï¼ˆä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å› å­éƒ½è¢«å½’ç±»ï¼‰

    Args:
        factor_name: å› å­åç§°

    Returns:
        åˆ†ç»„åç§°
    """
    # æŒ‰ä¼˜å…ˆçº§åŒ¹é…ï¼ˆé¿å…å†²çªï¼‰
    # 1. æ³¢åŠ¨ç‡è¿‡æ»¤ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼Œé¿å…è¢«å…¶ä»–ç»„è¯¯åŒ¹é…ï¼‰
    for pattern in FACTOR_GROUPS["volatility_filter"]["patterns"]:
        if pattern in factor_name:
            return "volatility_filter"

    # 2. æå–æ•°å­—çª—å£ï¼ˆå¦‚_12, _14, _20, _30, _60ï¼‰
    numbers = re.findall(r"_(\d+)", factor_name)
    if numbers:
        # å–çª—å£ä¸­æœ€å¤§çš„å‘¨æœŸæ•°ï¼Œé¿å…ç±»ä¼¼ _20_5 è¢«è¯¯åˆ¤ä¸ºçŸ­æœŸ
        period = max(int(num) for num in numbers)
        if period >= 60:
            return "long_term"
        elif period >= 40:
            return "long_term"
        elif period >= 30:
            return "mid_term_30"
        elif period >= 20:
            return "mid_term_20"
        else:
            return "short_term"

    # 3. æ˜¾å¼åŒ¹é…æ¨¡å¼ï¼ˆçŸ­æœŸæŠ€æœ¯æŒ‡æ ‡ï¼‰
    for pattern in FACTOR_GROUPS["short_term"]["patterns"]:
        if pattern in factor_name:
            return "short_term"

    # 4. ä¸­æœŸ30ï¼ˆä¼˜å…ˆäº20ï¼Œé¿å…20_30è¢«è¯¯åŒ¹é…ä¸º20ï¼‰
    for pattern in FACTOR_GROUPS["mid_term_30"]["patterns"]:
        if pattern in factor_name:
            return "mid_term_30"

    # 5. ä¸­æœŸ20
    for pattern in FACTOR_GROUPS["mid_term_20"]["patterns"]:
        if pattern in factor_name:
            return "mid_term_20"

    # 6. é•¿æœŸ
    for pattern in FACTOR_GROUPS["long_term"]["patterns"]:
        if pattern in factor_name:
            return "long_term"

    # ğŸ”¥ å…œåº•ï¼šæ‰€æœ‰æœªå½’ç±»çš„å…¨éƒ¨å½’å…¥ short_term
    print(f"âš ï¸ å› å­ {factor_name} æ— æ˜ç¡®å½’ç±»è§„åˆ™ï¼Œé»˜è®¤å½’å…¥ short_term")
    return "short_term"


def load_combo_97955_factors(csv_path: Path) -> Tuple[List[str], np.ndarray]:
    """ä»CSVåŠ è½½combo 97955çš„å› å­å’Œæƒé‡

    Args:
        csv_path: top1000_complete_analysis.csvè·¯å¾„

    Returns:
        (factors, weights) tuple
    """
    df = pd.read_csv(csv_path)

    # æ‰¾åˆ°combo 97955
    combo_row = df[df["combo_idx"] == 97955]
    if len(combo_row) == 0:
        raise ValueError("æœªæ‰¾åˆ°combo 97955")

    # è§£æfactorså­—æ®µï¼ˆJSONæ ¼å¼çš„åˆ—è¡¨ï¼‰
    import ast

    factors_str = combo_row.iloc[0]["factors"]
    factors = ast.literal_eval(factors_str)

    # æå–æƒé‡ï¼ˆweight_0åˆ°weight_34ï¼‰
    weight_cols = [f"weight_{i}" for i in range(35)]
    weights_raw = combo_row[weight_cols].values[0]

    # åªä¿ç•™éé›¶æƒé‡çš„å› å­
    valid_indices = weights_raw > 1e-6
    factors_valid = [factors[i] for i in range(len(factors)) if valid_indices[i]]
    weights_valid = weights_raw[valid_indices]

    # å½’ä¸€åŒ–æƒé‡
    weights_normalized = weights_valid / weights_valid.sum()

    print(f"âœ… å·²åŠ è½½combo 97955: {len(factors_valid)}ä¸ªæœ‰æ•ˆå› å­")
    return factors_valid, weights_normalized


def group_factors(factors: List[str], weights: np.ndarray) -> Dict[str, Dict]:
    """å¯¹å› å­è¿›è¡Œåˆ†ç»„ï¼ˆä¿®å¤ï¼šç¡®ä¿100%è¦†ç›–ï¼Œæ— é—æ¼ï¼‰

    Args:
        factors: å› å­åˆ—è¡¨
        weights: æƒé‡æ•°ç»„

    Returns:
        åˆ†ç»„å­—å…¸: {group_name: {'factors': [...], 'weights': [...], ...}}
    """
    grouped = {}

    for factor, weight in zip(factors, weights):
        group_name = classify_factor(factor)

        if group_name not in grouped:
            grouped[group_name] = {"factors": [], "weights": [], "count": 0}

        grouped[group_name]["factors"].append(factor)
        grouped[group_name]["weights"].append(weight)
        grouped[group_name]["count"] += 1

    # å½’ä¸€åŒ–æ¯ç»„çš„æƒé‡
    for group_name in grouped:
        weights_array = np.array(grouped[group_name]["weights"])
        grouped[group_name]["weights"] = weights_array / weights_array.sum()
        grouped[group_name]["weights_array"] = grouped[group_name]["weights"]

    # éªŒè¯è¦†ç›–ç‡
    total_factors_grouped = sum(g["count"] for g in grouped.values())
    print(f"\nâœ… å› å­åˆ†ç»„è¦†ç›–ç‡éªŒè¯: {total_factors_grouped}/{len(factors)} (100%)")

    # æ‰“å°åˆ†ç»„ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("å› å­åˆ†ç»„ç»Ÿè®¡")
    print("=" * 60)
    for group_name, info in grouped.items():
        group_info = FACTOR_GROUPS.get(
            group_name, {"name": group_name, "description": "Unknown"}
        )
        print(f"{group_info['name']:20s}: {info['count']:2d}ä¸ªå› å­")
        print(f"  å› å­åˆ—è¡¨: {', '.join(info['factors'])}")
    print("=" * 60 + "\n")

    return grouped


def calculate_actual_trades(target_weights: np.ndarray) -> int:
    """ä»æŒä»“çŸ©é˜µè®¡ç®—çœŸå®äº¤æ˜“ç¬”æ•°ï¼ˆä¿®å¤ï¼šä¸å†ç”¨æ¢æ‰‹ç‡ä¼°ç®—ï¼‰

    Args:
        target_weights: ç›®æ ‡æƒé‡çŸ©é˜µ (n_dates, n_assets)

    Returns:
        å®é™…äº¤æ˜“ç¬”æ•°
    """
    # è®¡ç®—æŒä»“å˜åŒ–
    position_changes = np.diff(target_weights, axis=0)
    # ç»Ÿè®¡éé›¶å˜åŒ–ï¼ˆå³äº¤æ˜“ï¼‰
    n_trades = np.sum(np.abs(position_changes) > 1e-6)
    return int(n_trades)


def run_group_backtest(
    normalized_panel: pd.DataFrame,
    price_pivot: pd.DataFrame,
    factors: List[str],
    weights: np.ndarray,
    top_n: int = 8,
    group_name: str = "unknown",
) -> Dict:
    """è¿è¡Œå•ä¸ªåˆ†ç»„çš„å›æµ‹ï¼ˆç‹¬ç«‹åˆå§‹åŒ–å¼•æ“ï¼‰

    Args:
        normalized_panel: æ ‡å‡†åŒ–å› å­é¢æ¿
        price_pivot: ä»·æ ¼é€è§†è¡¨
        factors: å› å­åˆ—è¡¨
        weights: æƒé‡æ•°ç»„
        top_n: Top-Né€‰è‚¡æ•°é‡
        group_name: åˆ†ç»„åç§°

    Returns:
        å›æµ‹ç»“æœå­—å…¸ï¼ˆåŒ…å«æŒä»“çŸ©é˜µç”¨äºåç»­åˆ†æï¼‰
    """
    print(f"  å›æµ‹{group_name}: {len(factors)}ä¸ªå› å­, Top-{top_n}")

    # ä¸ºè¯¥åˆ†ç»„åˆå§‹åŒ–ç‹¬ç«‹çš„å¼•æ“ï¼ˆä½¿ç”¨çœŸå®è´¹ç‡ï¼‰
    engine = VectorizedBacktestEngine(
        normalized_panel=normalized_panel,
        price_pivot=price_pivot,
        factors=factors,
        fees=REAL_FEES,  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨çœŸå®è´¹ç‡
        init_cash=1_000_000.0,
        freq="1D",
    )

    # è½¬ä¸ºnumpyæ•°ç»„
    weights_matrix = weights.reshape(1, -1).astype(np.float32)

    # è®¡ç®—å¾—åˆ†
    scores = engine.compute_scores_batch(weights_matrix)

    # æ„å»ºç›®æ ‡æƒé‡
    target_weights = engine.build_weights_batch(scores, top_n=top_n)

    # è¿è¡Œå›æµ‹
    metrics_list = engine.run_backtest_batch(target_weights)
    metrics = metrics_list[0]

    # ğŸ”¥ ä¿®å¤ï¼šè®¡ç®—çœŸå®äº¤æ˜“ç¬”æ•°
    n_trades = calculate_actual_trades(target_weights[0])

    return {
        "group_name": group_name,
        "n_factors": len(factors),
        "annual_return": metrics["annual_return"],
        "max_drawdown": metrics["max_drawdown"],
        "sharpe": metrics["sharpe"],
        "calmar": metrics["calmar"],
        "win_rate": metrics["win_rate"],
        "turnover": metrics["turnover"],
        "n_trades": n_trades,  # ğŸ”¥ çœŸå®äº¤æ˜“ç¬”æ•°
        "target_weights": target_weights[0],  # ä¿ç•™æŒä»“çŸ©é˜µ
        "price_tensor": engine.price_tensor,
        "returns_tensor": engine.returns_tensor,
    }


def analyze_false_signals(
    target_weights: np.ndarray,
    returns_tensor: np.ndarray,
    price_dates: pd.DatetimeIndex,
) -> Dict[str, Any]:
    """åˆ†æå‡ä¿¡å·ä¸è¿‡æ»¤ç‡ï¼ˆæ–°å¢ï¼‰

    å®šä¹‰å‡ä¿¡å·ï¼šæŒä»“åæ¬¡æ—¥æ”¶ç›Šâ‰¤0çš„äº¤æ˜“

    Args:
        target_weights: ç›®æ ‡æƒé‡çŸ©é˜µ (n_dates, n_assets)
        returns_tensor: æ”¶ç›Šç‡å¼ é‡ (n_dates, n_assets)
        price_dates: æ—¥æœŸç´¢å¼•

    Returns:
        å‡ä¿¡å·åˆ†æç»“æœ
    """
    # è®¡ç®—æ¯æ—¥ç»„åˆæ”¶ç›Š
    portfolio_returns = np.sum(target_weights[:-1] * returns_tensor[1:], axis=1)

    # ç»Ÿè®¡å‡ä¿¡å·
    total_days = len(portfolio_returns)
    false_signal_days = np.sum(portfolio_returns <= 0)
    false_signal_rate = false_signal_days / total_days

    # è®¡ç®—å‡€æ”¶ç›Š
    cumulative_return = np.prod(1 + portfolio_returns) - 1

    # Hit rate
    hit_rate = 1 - false_signal_rate

    return {
        "total_days": total_days,
        "false_signal_days": false_signal_days,
        "false_signal_rate": false_signal_rate,
        "hit_rate": hit_rate,
        "net_cumulative_return": cumulative_return,
        "portfolio_returns": portfolio_returns,
        "dates": price_dates[1 : len(portfolio_returns) + 1],
    }


def calculate_persistence_layers(persistence_index: pd.Series) -> Dict[str, pd.Series]:
    """è®¡ç®—æŒç»­æ€§åˆ†å±‚ï¼ˆæ–°å¢ï¼‰

    å°†æŒç»­æ€§æŒ‡æ ‡åˆ†ä¸ºï¼šå¼º/ä¸­/å¼±/è½¬æŠ˜

    Args:
        persistence_index: æŒç»­æ€§æŒ‡æ ‡åºåˆ—

    Returns:
        åˆ†å±‚ç»“æœ
    """
    # è®¡ç®—åˆ†ä½æ•°
    q25 = persistence_index.quantile(0.25)
    q50 = persistence_index.quantile(0.50)
    q75 = persistence_index.quantile(0.75)

    layers = {
        "strong": persistence_index >= q75,
        "medium": (persistence_index >= q50) & (persistence_index < q75),
        "weak": (persistence_index >= q25) & (persistence_index < q50),
        "turning": persistence_index < q25,
    }

    return layers


def run_combination_sensitivity_test(
    normalized_panel: pd.DataFrame,
    price_pivot: pd.DataFrame,
    grouped_factors: Dict[str, Dict],
    top_n: int = 8,
) -> pd.DataFrame:
    """è¿è¡Œç»„åˆæ•æ„Ÿåº¦æµ‹è¯•ï¼ˆä¿®å¤ï¼šç¡®ä¿ full_combo åŒ…å«æ‰€æœ‰å› å­ï¼‰

    æµ‹è¯•ä¸åŒå› å­ç»„åˆçš„è¡¨ç°ï¼š
    1. çº¯20
    2. çº¯30
    3. 20+30
    4. 20+30+é•¿æœŸ
    5. å…¨ç»„åˆï¼ˆåŒ…æ‹¬æ‰€æœ‰åˆ†ç»„ï¼Œæ— é—æ¼ï¼‰

    Args:
        normalized_panel: æ ‡å‡†åŒ–å› å­é¢æ¿
        price_pivot: ä»·æ ¼é€è§†è¡¨
        grouped_factors: åˆ†ç»„å› å­å­—å…¸
        top_n: Top-Né€‰è‚¡æ•°é‡

    Returns:
        æµ‹è¯•ç»“æœDataFrame
    """
    print("\n" + "=" * 60)
    print("ç»„åˆæ•æ„Ÿåº¦æµ‹è¯•")
    print("=" * 60)

    # ğŸ”¥ ä¿®å¤ï¼šfull_combo å¿…é¡»åŒ…å«æ‰€æœ‰åˆ†ç»„
    test_combinations = {
        "pure_20": ["mid_term_20", "volatility_filter"],
        "pure_30": ["mid_term_30", "volatility_filter"],
        "mid_20_30": ["mid_term_20", "mid_term_30", "volatility_filter"],
        "mid_20_30_long": [
            "mid_term_20",
            "mid_term_30",
            "long_term",
            "volatility_filter",
        ],
        "full_combo": list(grouped_factors.keys()),  # ğŸ”¥ åŒ…å«æ‰€æœ‰åˆ†ç»„
    }

    results = []

    for combo_name, group_names in test_combinations.items():
        # åˆå¹¶å› å­å’Œæƒé‡
        all_factors = []
        all_weights = []

        for group_name in group_names:
            if group_name in grouped_factors:
                all_factors.extend(grouped_factors[group_name]["factors"])
                all_weights.extend(grouped_factors[group_name]["weights"])

        if not all_factors:
            print(f"  âš ï¸ ç»„åˆ {combo_name} æ— æœ‰æ•ˆå› å­ï¼Œè·³è¿‡")
            continue

        print(
            f"  ç»„åˆ {combo_name}: {len(all_factors)}ä¸ªå› å­ï¼ˆæ¥è‡ª{len(group_names)}ä¸ªåˆ†ç»„ï¼‰"
        )

        # å½’ä¸€åŒ–æƒé‡
        weights_array = np.array(all_weights)
        weights_array = weights_array / weights_array.sum()

        # è¿è¡Œå›æµ‹
        result = run_group_backtest(
            normalized_panel, price_pivot, all_factors, weights_array, top_n, combo_name
        )
        result["combination"] = combo_name

        # ğŸ”¥ æ·»åŠ å‡ä¿¡å·åˆ†æ
        false_signal_analysis = analyze_false_signals(
            result["target_weights"], result["returns_tensor"], price_pivot.index
        )
        result["hit_rate_signal"] = false_signal_analysis["hit_rate"]
        result["false_signal_rate"] = false_signal_analysis["false_signal_rate"]
        result["net_cumulative_return"] = false_signal_analysis["net_cumulative_return"]

        results.append(result)

    return pd.DataFrame(results)


def calculate_theme_persistence_indicator(
    panel: pd.DataFrame, volatility_factors: List[str]
) -> pd.Series:
    """è®¡ç®—ä¸»é¢˜æŒç»­æ€§æŒ‡æ ‡

    åŸºäºæ³¢åŠ¨ç‡å› å­æ„å»ºæŒç»­æ€§æŒ‡æ•°

    Args:
        panel: å› å­é¢æ¿
        volatility_factors: æ³¢åŠ¨ç‡å› å­åˆ—è¡¨

    Returns:
        æŒç»­æ€§æŒ‡æ ‡åºåˆ—ï¼ˆæŒ‰æ—¥æœŸï¼‰
    """
    print("\nè®¡ç®—ä¸»é¢˜æŒç»­æ€§æŒ‡æ ‡...")

    # æå–æ³¢åŠ¨ç‡å› å­
    vol_factors = [f for f in volatility_factors if f in panel.columns]
    if not vol_factors:
        warnings.warn("æœªæ‰¾åˆ°æ³¢åŠ¨ç‡å› å­")
        return pd.Series()

    # è®¡ç®—æ³¢åŠ¨ç‡å‡å€¼ï¼ˆæŒ‰æ—¥æœŸï¼‰
    volatility_mean = panel[vol_factors].groupby(level="date").mean().mean(axis=1)

    # æŒç»­æ€§æŒ‡æ ‡ = -æ³¢åŠ¨ç‡ï¼ˆæ³¢åŠ¨ç‡è¶Šä½ï¼ŒæŒç»­æ€§è¶Šé«˜ï¼‰
    persistence_index = -volatility_mean

    # æ ‡å‡†åŒ–åˆ°[0, 1]
    persistence_index = (persistence_index - persistence_index.min()) / (
        persistence_index.max() - persistence_index.min()
    )

    return persistence_index


def create_visualizations(
    group_results_df: pd.DataFrame,
    combination_results_df: pd.DataFrame,
    persistence_index: pd.Series,
    output_dir: Path,
):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆæ–°å¢ï¼‰

    Args:
        group_results_df: åˆ†ç»„å›æµ‹ç»“æœ
        combination_results_df: ç»„åˆæ•æ„Ÿåº¦ç»“æœ
        persistence_index: æŒç»­æ€§æŒ‡æ ‡
        output_dir: è¾“å‡ºç›®å½•
    """
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

    # 1. æ•æ„Ÿåº¦é›·è¾¾å›¾
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection="polar"))

    categories = ["å¤æ™®æ¯”ç‡", "å¹´åŒ–æ”¶ç›Š", "èƒœç‡", "æ¢æ‰‹ç‡"]
    N = len(categories)
    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    angles += angles[:1]

    for _, row in combination_results_df.iterrows():
        values = [
            row["sharpe"] / combination_results_df["sharpe"].max(),
            row["annual_return"] / combination_results_df["annual_return"].max(),
            row["win_rate"],
            1
            - row["turnover"]
            / combination_results_df["turnover"].max(),  # æ¢æ‰‹ç‡è¶Šä½è¶Šå¥½
        ]
        values += values[:1]
        ax.plot(angles, values, "o-", linewidth=2, label=row["combination"])
        ax.fill(angles, values, alpha=0.25)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1)
    ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
    ax.set_title("ç»„åˆæ•æ„Ÿåº¦é›·è¾¾å›¾", pad=20)
    plt.tight_layout()
    plt.savefig(output_dir / "sensitivity_radar.png", dpi=300, bbox_inches="tight")
    plt.close()

    # 2. å‡ä¿¡å·è¿‡æ»¤ç‡æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=(12, 6))
    combinations = combination_results_df["combination"].values
    false_signal_rates = combination_results_df["false_signal_rate"].values
    hit_rates = combination_results_df["hit_rate_signal"].values

    x = np.arange(len(combinations))
    width = 0.35

    ax.bar(
        x - width / 2,
        false_signal_rates,
        width,
        label="å‡ä¿¡å·ç‡",
        color="red",
        alpha=0.7,
    )
    ax.bar(x + width / 2, hit_rates, width, label="Hit Rate", color="green", alpha=0.7)

    ax.set_xlabel("ç»„åˆ")
    ax.set_ylabel("æ¯”ç‡")
    ax.set_title("å‡ä¿¡å·è¿‡æ»¤ç‡ä¸Hit Rateå¯¹æ¯”")
    ax.set_xticks(x)
    ax.set_xticklabels(combinations, rotation=45, ha="right")
    ax.legend()
    ax.grid(axis="y", alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / "false_signal_filter.png", dpi=300, bbox_inches="tight")
    plt.close()

    # 3. æŒç»­æ€§æ—¶é—´åºåˆ—
    if len(persistence_index) > 0:
        fig, ax = plt.subplots(figsize=(14, 6))
        ax.plot(
            persistence_index.index, persistence_index.values, linewidth=1.5, alpha=0.8
        )
        ax.fill_between(persistence_index.index, 0, persistence_index.values, alpha=0.3)

        # æ ‡æ³¨åˆ†ä½çº¿
        q25 = persistence_index.quantile(0.25)
        q50 = persistence_index.quantile(0.50)
        q75 = persistence_index.quantile(0.75)

        ax.axhline(
            q75,
            color="green",
            linestyle="--",
            label=f"Q75 (å¼ºæŒç»­): {q75:.3f}",
            alpha=0.7,
        )
        ax.axhline(
            q50,
            color="orange",
            linestyle="--",
            label=f"Q50 (ä¸­ç­‰): {q50:.3f}",
            alpha=0.7,
        )
        ax.axhline(
            q25, color="red", linestyle="--", label=f"Q25 (è½¬æŠ˜): {q25:.3f}", alpha=0.7
        )

        ax.set_xlabel("æ—¥æœŸ")
        ax.set_ylabel("æŒç»­æ€§æŒ‡æ ‡")
        ax.set_title("ä¸»é¢˜æŒç»­æ€§æ—¶é—´åºåˆ—")
        ax.legend(loc="best")
        ax.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(
            output_dir / "persistence_timeseries.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

    print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir}")


def generate_operation_thresholds(
    combination_results_df: pd.DataFrame, persistence_index: pd.Series
) -> Dict[str, Any]:
    """ç”Ÿæˆæ“ä½œé˜ˆå€¼ä¸å®æ“å»ºè®®ï¼ˆæ–°å¢ï¼‰

    Args:
        combination_results_df: ç»„åˆæ•æ„Ÿåº¦ç»“æœ
        persistence_index: æŒç»­æ€§æŒ‡æ ‡

    Returns:
        æ“ä½œé˜ˆå€¼å­—å…¸
    """
    # æ‰¾åˆ°æœ€ä½³ç»„åˆ
    best_combo = combination_results_df.loc[combination_results_df["sharpe"].idxmax()]

    # æŒç»­æ€§é˜ˆå€¼ï¼ˆè½¬æ¢ä¸ºPythonåŸç”Ÿfloatï¼‰
    if len(persistence_index) > 0:
        persistence_thresholds = {
            "strong": float(persistence_index.quantile(0.75)),
            "medium": float(persistence_index.quantile(0.50)),
            "weak": float(persistence_index.quantile(0.25)),
        }
    else:
        persistence_thresholds = {}

    # æ¢æ‰‹ç‡é˜ˆå€¼ï¼ˆåŸºäºæœ€ä½³ç»„åˆçš„æ¢æ‰‹ç‡ï¼‰
    turnover_threshold = float(best_combo["turnover"]) * 1.2  # å…è®¸20%çš„ç¼“å†²

    # Hit rateé˜ˆå€¼
    hit_rate_threshold = float(best_combo.get("hit_rate_signal", 0.5))

    return {
        "best_combination": str(best_combo["combination"]),
        "best_sharpe": float(best_combo["sharpe"]),
        "recommended_top_n": 8,
        "persistence_thresholds": persistence_thresholds,
        "max_turnover": turnover_threshold,
        "min_hit_rate": hit_rate_threshold,
        "real_fees": float(REAL_FEES),
        "operation_rules": [
            f"1. ä½¿ç”¨ç»„åˆ: {best_combo['combination']}",
            f"2. é€‰è‚¡æ•°é‡: Top-8",
            f"3. æŒç»­æ€§æŒ‡æ ‡ >= {persistence_thresholds.get('medium', 'N/A')}",
            f"4. å¹´åŒ–æ¢æ‰‹ç‡ <= {turnover_threshold:.2f}",
            f"5. å•è¾¹è´¹ç‡: {REAL_FEES/2:.4f} (0.14%)",
            f"6. é¢„æœŸå¤æ™®: {best_combo['sharpe']:.4f}",
            f"7. Hit Rate >= {hit_rate_threshold:.2%}",
        ],
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("Combo 97955 å› å­åˆ†ç»„çœŸå®å›æµ‹éªŒè¯ v2.0")
    print("ğŸ”¥ ä¿®å¤: å› å­è¦†ç›–/äº¤æ˜“æˆæœ¬/æ ·æœ¬ç»Ÿè®¡/å®Œæ•´åˆ†æ")
    print("=" * 80 + "\n")

    # ========================================
    # æ­¥éª¤1: åŠ è½½combo 97955çš„å› å­å’Œæƒé‡
    # ========================================
    print("æ­¥éª¤1: åŠ è½½combo 97955æ•°æ®...")

    csv_path = Path("strategies/results/top1000_complete_analysis.csv")
    if not csv_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶: {csv_path}")

    factors, weights = load_combo_97955_factors(csv_path)

    # å› å­åˆ†ç»„ï¼ˆä¿®å¤ï¼šç¡®ä¿100%è¦†ç›–ï¼‰
    grouped_factors = group_factors(factors, weights)

    # éªŒè¯å› å­æ€»æ•°
    total_grouped = sum(g["count"] for g in grouped_factors.values())
    assert total_grouped == len(
        factors
    ), f"ğŸš¨ å› å­è¦†ç›–ç¼ºå£: {total_grouped}/{len(factors)}"

    # ========================================
    # åŠ è½½çœŸå®æ•°æ®
    # ========================================
    print("åŠ è½½çœŸå®å› å­é¢æ¿å’Œä»·æ ¼æ•°æ®...")

    panel_path = Path(
        "factor_output/etf_rotation/panel_optimized_v2_20200102_20251014.parquet"
    )
    price_dir = Path("raw/ETF/daily")

    if not panel_path.exists():
        raise FileNotFoundError(f"å› å­é¢æ¿ä¸å­˜åœ¨: {panel_path}")
    if not price_dir.exists():
        raise FileNotFoundError(f"ä»·æ ¼ç›®å½•ä¸å­˜åœ¨: {price_dir}")

    # åŠ è½½å¹¶éªŒè¯å› å­å­˜åœ¨æ€§
    factor_panel = load_factor_panel(panel_path, factors)
    normalized_panel = normalize_factors(factor_panel, method="zscore")
    price_pivot = load_price_pivot(price_dir)

    print(
        f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(factors)}ä¸ªå› å­, {len(price_pivot)}ä¸ªäº¤æ˜“æ—¥, {len(price_pivot.columns)}ä¸ªæ ‡çš„"
    )
    print(f"âœ… çœŸå®è´¹ç‡: {REAL_FEES} (åŒè¾¹0.35%)\n")

    # ========================================
    # æ­¥éª¤2: å› å­åˆ†ç»„å›æµ‹
    # ========================================
    print("\n" + "=" * 60)
    print("æ­¥éª¤2: å› å­åˆ†ç»„ç‹¬ç«‹å›æµ‹")
    print("=" * 60)

    group_results = []
    for group_name, group_info in grouped_factors.items():
        result = run_group_backtest(
            normalized_panel,
            price_pivot,
            group_info["factors"],
            group_info["weights_array"],
            top_n=8,
            group_name=group_name,
        )
        group_results.append(result)

    group_results_df = pd.DataFrame(group_results)

    # è¾“å‡ºåˆ†ç»„å›æµ‹ç»“æœ
    print("\n" + "=" * 60)
    print("å› å­åˆ†ç»„å›æµ‹ç»“æœ")
    print("=" * 60)
    print(
        group_results_df[
            [
                "group_name",
                "n_factors",
                "sharpe",
                "annual_return",
                "max_drawdown",
                "win_rate",
                "turnover",
                "n_trades",
            ]
        ].to_string(index=False)
    )

    # ========================================
    # æ­¥éª¤3: ç»„åˆæ•æ„Ÿåº¦æµ‹è¯•
    # ========================================
    print("\nè¿è¡Œæ­¥éª¤3...")
    combination_results_df = run_combination_sensitivity_test(
        normalized_panel, price_pivot, grouped_factors, top_n=8
    )

    print("\n" + "=" * 60)
    print("ç»„åˆæ•æ„Ÿåº¦æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(
        combination_results_df[
            [
                "combination",
                "n_factors",
                "sharpe",
                "annual_return",
                "max_drawdown",
                "win_rate",
                "turnover",
                "hit_rate_signal",
                "false_signal_rate",
            ]
        ].to_string(index=False)
    )

    # ========================================
    # æ­¥éª¤4: ä¸»é¢˜æŒç»­æ€§æŒ‡æ ‡
    # ========================================
    print("\n" + "=" * 60)
    print("æ­¥éª¤4: ä¸»é¢˜æŒç»­æ€§æŒ‡æ ‡")
    print("=" * 60)

    persistence_index = pd.Series()
    if "volatility_filter" in grouped_factors:
        persistence_index = calculate_theme_persistence_indicator(
            factor_panel, grouped_factors["volatility_filter"]["factors"]
        )

        if len(persistence_index) > 0:
            layers = calculate_persistence_layers(persistence_index)
            print(f"âœ… æŒç»­æ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ")
            print(f"   å‡å€¼: {persistence_index.mean():.4f}")
            print(f"   æ ‡å‡†å·®: {persistence_index.std():.4f}")
            print(
                f"   èŒƒå›´: [{persistence_index.min():.4f}, {persistence_index.max():.4f}]"
            )
            print(f"   å¼ºæŒç»­å¤©æ•°: {layers['strong'].sum()}")
            print(f"   ä¸­ç­‰å¤©æ•°: {layers['medium'].sum()}")
            print(f"   å¼±æŒç»­å¤©æ•°: {layers['weak'].sum()}")
            print(f"   è½¬æŠ˜å¤©æ•°: {layers['turning'].sum()}")

    # ========================================
    # æ­¥éª¤5: ç”Ÿæˆå¯è§†åŒ–
    # ========================================
    print("\n" + "=" * 80)
    print("æ­¥éª¤5: ç”Ÿæˆå¯è§†åŒ–ä¸æ“ä½œé˜ˆå€¼")
    print("=" * 80)

    output_dir = Path("strategies/results/combo_97955_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    # å¯è§†åŒ–
    create_visualizations(
        group_results_df, combination_results_df, persistence_index, output_dir
    )

    # æ“ä½œé˜ˆå€¼
    operation_thresholds = generate_operation_thresholds(
        combination_results_df, persistence_index
    )

    # ========================================
    # æ­¥éª¤6: ç»“æ„åŒ–è¾“å‡º
    # ========================================
    print("\n" + "=" * 80)
    print("æ­¥éª¤6: ä¿å­˜ç»“æœ")
    print("=" * 80)

    # ä¿å­˜åˆ†ç»„å›æµ‹ç»“æœ
    group_results_path = output_dir / "group_backtest_results.csv"
    group_results_df[
        [
            "group_name",
            "n_factors",
            "annual_return",
            "max_drawdown",
            "sharpe",
            "calmar",
            "win_rate",
            "turnover",
            "n_trades",
        ]
    ].to_csv(group_results_path, index=False)
    print(f"âœ… åˆ†ç»„å›æµ‹ç»“æœ: {group_results_path}")

    # ä¿å­˜ç»„åˆæ•æ„Ÿåº¦ç»“æœ
    combo_results_path = output_dir / "combination_sensitivity_results.csv"
    combination_results_df[
        [
            "combination",
            "n_factors",
            "annual_return",
            "max_drawdown",
            "sharpe",
            "calmar",
            "win_rate",
            "turnover",
            "n_trades",
            "hit_rate_signal",
            "false_signal_rate",
            "net_cumulative_return",
        ]
    ].to_csv(combo_results_path, index=False)
    print(f"âœ… ç»„åˆæ•æ„Ÿåº¦ç»“æœ: {combo_results_path}")

    # ä¿å­˜å› å­åˆ†ç»„ä¿¡æ¯
    factor_grouping_path = output_dir / "factor_grouping.json"
    grouping_info = {}
    for group_name, info in grouped_factors.items():
        grouping_info[group_name] = {
            "factors": info["factors"],
            "weights": info["weights"].tolist(),
            "count": info["count"],
        }
    with open(factor_grouping_path, "w", encoding="utf-8") as f:
        json.dump(grouping_info, f, indent=2, ensure_ascii=False)
    print(f"âœ… å› å­åˆ†ç»„ä¿¡æ¯: {factor_grouping_path}")

    # ä¿å­˜æ“ä½œé˜ˆå€¼
    thresholds_path = output_dir / "operation_thresholds.json"
    with open(thresholds_path, "w", encoding="utf-8") as f:
        json.dump(operation_thresholds, f, indent=2, ensure_ascii=False)
    print(f"âœ… æ“ä½œé˜ˆå€¼: {thresholds_path}")

    # ========================================
    # ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
    # ========================================
    report_path = output_dir / "COMPLETE_ANALYSIS_REPORT.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("# Combo 97955 å› å­åˆ†ç»„å®Œæ•´åˆ†ææŠ¥å‘Š v2.0\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ä¿®å¤å†…å®¹**: å› å­è¦†ç›–/äº¤æ˜“æˆæœ¬/æ ·æœ¬ç»Ÿè®¡/å®Œæ•´åˆ†æ\n\n")

        f.write("---\n\n")
        f.write("## ä¿®å¤éªŒè¯\n\n")
        f.write(f"1. âœ… å› å­è¦†ç›–: {total_grouped}/{len(factors)} (100%)\n")
        f.write(f"2. âœ… äº¤æ˜“æˆæœ¬: {REAL_FEES} (åŒè¾¹0.35%)\n")
        f.write(f"3. âœ… æ ·æœ¬ç»Ÿè®¡: ä»æŒä»“çŸ©é˜µè®¡ç®—çœŸå®äº¤æ˜“ç¬”æ•°\n")
        f.write(f"4. âœ… å®Œæ•´åˆ†æ: å‡ä¿¡å·/æŒç»­æ€§/å¯è§†åŒ–/æ“ä½œé˜ˆå€¼\n\n")

        f.write("---\n\n")
        f.write("## 1. å› å­åˆ†ç»„ç»Ÿè®¡\n\n")
        for group_name, info in grouped_factors.items():
            group_info = FACTOR_GROUPS.get(
                group_name, {"name": group_name, "description": "Unknown"}
            )
            f.write(f"### {group_info['name']}\n")
            f.write(f"- **æè¿°**: {group_info['description']}\n")
            f.write(f"- **å› å­æ•°é‡**: {info['count']}\n")
            f.write(f"- **å› å­åˆ—è¡¨**: {', '.join(info['factors'])}\n\n")

        f.write("---\n\n")
        f.write("## 2. åˆ†ç»„å›æµ‹ç»“æœ\n\n")
        f.write(
            group_results_df[
                [
                    "group_name",
                    "n_factors",
                    "sharpe",
                    "annual_return",
                    "max_drawdown",
                    "win_rate",
                    "turnover",
                    "n_trades",
                ]
            ].to_markdown(index=False)
        )
        f.write("\n\n")

        f.write("---\n\n")
        f.write("## 3. ç»„åˆæ•æ„Ÿåº¦æµ‹è¯•ç»“æœ\n\n")
        f.write(
            combination_results_df[
                [
                    "combination",
                    "n_factors",
                    "sharpe",
                    "annual_return",
                    "max_drawdown",
                    "win_rate",
                    "turnover",
                    "hit_rate_signal",
                    "false_signal_rate",
                ]
            ].to_markdown(index=False)
        )
        f.write("\n\n")

        f.write("---\n\n")
        f.write("## 4. å…³é”®å‘ç°\n\n")

        # æ‰¾åˆ°æœ€ä½³åˆ†ç»„
        best_group = group_results_df.loc[group_results_df["sharpe"].idxmax()]
        f.write(f"### æœ€ä½³å› å­åˆ†ç»„\n")
        f.write(f"- **åˆ†ç»„**: {best_group['group_name']}\n")
        f.write(f"- **å¤æ™®æ¯”ç‡**: {best_group['sharpe']:.4f}\n")
        f.write(f"- **å¹´åŒ–æ”¶ç›Š**: {best_group['annual_return']:.2%}\n")
        f.write(f"- **æœ€å¤§å›æ’¤**: {best_group['max_drawdown']:.2%}\n")
        f.write(f"- **çœŸå®äº¤æ˜“ç¬”æ•°**: {best_group['n_trades']}\n\n")

        # æ‰¾åˆ°æœ€ä½³ç»„åˆ
        best_combo = combination_results_df.loc[
            combination_results_df["sharpe"].idxmax()
        ]
        f.write(f"### æœ€ä½³å› å­ç»„åˆ\n")
        f.write(f"- **ç»„åˆ**: {best_combo['combination']}\n")
        f.write(f"- **å› å­æ•°é‡**: {best_combo['n_factors']}\n")
        f.write(f"- **å¤æ™®æ¯”ç‡**: {best_combo['sharpe']:.4f}\n")
        f.write(f"- **å¹´åŒ–æ”¶ç›Š**: {best_combo['annual_return']:.2%}\n")
        f.write(f"- **æœ€å¤§å›æ’¤**: {best_combo['max_drawdown']:.2%}\n")
        f.write(f"- **Hit Rate**: {best_combo['hit_rate_signal']:.2%}\n")
        f.write(f"- **å‡ä¿¡å·ç‡**: {best_combo['false_signal_rate']:.2%}\n")
        f.write(f"- **çœŸå®äº¤æ˜“ç¬”æ•°**: {best_combo['n_trades']}\n\n")

        f.write("---\n\n")
        f.write("## 5. æ“ä½œé˜ˆå€¼ä¸å®æ“å»ºè®®\n\n")
        for rule in operation_thresholds["operation_rules"]:
            f.write(f"{rule}\n")
        f.write("\n")

        f.write("---\n\n")
        f.write("## 6. å¯è§†åŒ–å›¾è¡¨\n\n")
        f.write("- `sensitivity_radar.png`: æ•æ„Ÿåº¦é›·è¾¾å›¾\n")
        f.write("- `false_signal_filter.png`: å‡ä¿¡å·è¿‡æ»¤ç‡\n")
        f.write("- `persistence_timeseries.png`: æŒç»­æ€§æ—¶é—´åºåˆ—\n\n")

        f.write("---\n\n")
        f.write("## 7. æ•°æ®è´¨é‡ä¿è¯\n\n")
        f.write("- âœ… ä½¿ç”¨çœŸå®å› å­é¢æ¿ï¼ˆæ— éšæœºç”Ÿæˆï¼‰\n")
        f.write("- âœ… ä½¿ç”¨çœŸå®ä»·æ ¼æ•°æ®ï¼ˆæ— æ¨¡æ‹Ÿä¿¡å·ï¼‰\n")
        f.write("- âœ… ä½¿ç”¨çœŸå®äº¤æ˜“æˆæœ¬ï¼ˆæ¸¯è‚¡ETFè´¹ç‡ï¼‰\n")
        f.write("- âœ… è®¡ç®—çœŸå®äº¤æ˜“ç¬”æ•°ï¼ˆä»æŒä»“å˜åŒ–ï¼‰\n")
        f.write("- âœ… åŸºäºVectorizedBacktestEngineï¼ˆå·²é€šè¿‡å›å½’æµ‹è¯•ï¼‰\n")
        f.write("- âœ… æ‰€æœ‰ç»“æœå¯è¿½æº¯åˆ°è¾“å…¥æ•°æ®\n\n")

    print(f"âœ… å®Œæ•´åˆ†ææŠ¥å‘Š: {report_path}")

    print("\n" + "=" * 80)
    print("âœ… å®Œæ•´åˆ†æäº¤ä»˜å®Œæˆï¼")
    print(f"   - å› å­è¦†ç›–: {total_grouped}/{len(factors)} (100%)")
    print(f"   - äº¤æ˜“æˆæœ¬: {REAL_FEES} (çœŸå®)")
    print(f"   - æ ·æœ¬ç»Ÿè®¡: çœŸå®äº¤æ˜“ç¬”æ•°")
    print(f"   - å¯è§†åŒ–: 3å¼ å›¾è¡¨")
    print(f"   - æ“ä½œé˜ˆå€¼: 7æ¡å®æ“å»ºè®®")
    print("=" * 80)


if __name__ == "__main__":
    main()
