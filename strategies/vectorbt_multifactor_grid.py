#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¤šå› å­è½®åŠ¨æš´åŠ›æšä¸¾å›æµ‹è„šæ‰‹æ¶ (å¢å¼ºç‰ˆ)

ç‰¹æ€§ï¼š
1. è¯»å– ETF æ¨ªæˆªé¢å› å­é¢æ¿ï¼Œæ”¯æŒè‡ªåŠ¨åŠ è½½å› å­æ’åºç»“æœ
2. æ”¯æŒæ¯æ—¥æˆªé¢æ ‡å‡†åŒ–ï¼Œæ„å»ºå¤šå› å­åŠ æƒå¾—åˆ†
3. æš´åŠ›éå†é¢„è®¾æƒé‡ç»„åˆ + Top-N é€‰è‚¡ï¼Œè°ƒç”¨ vectorbt åšå‡€å€¼å›æµ‹
4. åˆ†æ‰¹/æŠ½æ ·æ‰§è¡Œï¼šæ”¯æŒå‡ åä¸‡ç»„åˆè§„æ¨¡çš„ç¨³å®šè¿è¡Œ
5. å¹¶å‘ä¼˜åŒ–ï¼šå‘é‡åŒ–å¤„ç†ã€ç¼“å­˜ã€çº¿ç¨‹æ± ä¼˜åŒ–
6. å®Œå–„è¾“å‡ºï¼šæ—¶é—´æˆ³ç›®å½•ã€æ–­ç‚¹ç»­è·‘ã€è¯¦ç»†ç»Ÿè®¡

ç”¨æ³•ç¤ºä¾‹ï¼š
    # åŸºç¡€è¿è¡Œ
    python strategies/vectorbt_multifactor_grid.py \\
        --top-factors-json production_factor_results/top_factors_20251017_124205.json \\
        --top-k 10 --output results_multifactor.csv

    # å¤§è§„æ¨¡æš´åŠ›æœç´¢
    python strategies/vectorbt_multifactor_grid.py \\
        --top-factors-json production_factor_results/top_factors_*.json \\
        --top-k 20 --batch-size 10000 --max-total-combos 500000 \\
        --weight-grid 0.0 0.5 1.0 --top-n-list 3 5 8 \\
        --output factor_discovery_results/vbt_bruteforce/

    # Sanity Run (å¿«é€ŸéªŒè¯)
    python strategies/vectorbt_multifactor_grid.py \\
        --factors RETURN_20 RETURN_60 PRICE_POSITION_60 \\
        --weight-grid 0.0 1.0 --max-total-combos 1000 \\
        --sanity-run --debug

ä¾èµ–ï¼š
    pip install vectorbt==0.24.3 pandas numpy numba tqdm
"""

from __future__ import annotations

import argparse
import glob
import itertools
import json
import math
import os
import random
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from datetime import datetime
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Set, Tuple

import numpy as np
import pandas as pd
import yaml
from tqdm import tqdm

# === è®¾ç½®BLASçº¿ç¨‹æ•°ï¼Œé¿å…å¤šè¿›ç¨‹å†²çª ===
os.environ.setdefault("OMP_NUM_THREADS", "2")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "2")
os.environ.setdefault("MKL_NUM_THREADS", "2")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "2")

_DEFAULT_NUMBA_CACHE = Path(".numba_cache")
_DEFAULT_NUMBA_CACHE.mkdir(parents=True, exist_ok=True)
os.environ.setdefault("NUMBA_CACHE_DIR", str(_DEFAULT_NUMBA_CACHE.resolve()))
os.environ.setdefault("NUMBA_DISABLE_JIT", "0")

try:
    import vectorbt as vbt  # type: ignore

    _HAS_VECTORBT = True
except ImportError:
    vbt = None  # type: ignore
    _HAS_VECTORBT = False


# --------------------------------------------------------------------------- #
# æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# --------------------------------------------------------------------------- #


def load_top_factors_from_json(json_path: str, top_k: int = 10) -> List[str]:
    """ä»production_factor_resultsçš„JSONæ–‡ä»¶ä¸­åŠ è½½Top Kå› å­

    Args:
        json_path: å› å­åˆ†æç»“æœJSONæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒé€šé…ç¬¦
        top_k: é€‰æ‹©çš„å› å­æ•°é‡

    Returns:
        Top Kå› å­åç§°åˆ—è¡¨
    """
    # é»‘åå•ï¼šä¸¥ç¦ä½¿ç”¨çš„æœªæ¥å‡½æ•°å’Œæœ‰é—®é¢˜çš„å› å­
    BLACKLISTED_FACTORS = [
        "RETURN_",  # âŒ æœªæ¥å‡½æ•° - ä¸¥æ ¼ç¦æ­¢
        "FUTURE_",  # âŒ æœªæ¥å‡½æ•° - ä¸¥æ ¼ç¦æ­¢
        "TARGET_",  # âŒ æœªæ¥å‡½æ•° - ä¸¥æ ¼ç¦æ­¢
        # å…¶ä»–æ½œåœ¨é—®é¢˜å› å­æš‚ä¿ç•™ï¼Œå¾…éªŒè¯
    ]

    # æ”¯æŒé€šé…ç¬¦ï¼Œè‡ªåŠ¨æ‰¾åˆ°æœ€æ–°çš„æ–‡ä»¶
    if "*" in json_path:
        matching_files = glob.glob(json_path)
        if not matching_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„å› å­æ–‡ä»¶: {json_path}")
        json_path = max(matching_files, key=os.path.getctime)
        print(f"ğŸ“‚ è‡ªåŠ¨é€‰æ‹©æœ€æ–°å› å­æ–‡ä»¶: {json_path}")

    if not os.path.exists(json_path):
        raise FileNotFoundError(f"å› å­æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æå–å› å­ä¿¡æ¯ - é€‚é…æ–°çš„JSONæ ¼å¼
    # ä¼˜å…ˆä½¿ç”¨ all_factorsï¼Œç‰¹åˆ«æ˜¯å½“ top_k > 10 æ—¶
    if "all_factors" in data and top_k > 10:
        # å¦‚æœall_factorså­˜åœ¨ä¸”éœ€è¦è¶…è¿‡10ä¸ªå› å­ï¼Œä½¿ç”¨ICå€¼æ’åºå–å‰top_kä¸ª
        all_factors = data["all_factors"]
        # æŒ‰ic_meanæˆ–ic_iræ’åºï¼Œä¼˜å…ˆä½¿ç”¨ic_ir
        sorted_factors = sorted(
            all_factors, key=lambda x: x.get("ic_ir", x.get("ic_mean", 0)), reverse=True
        )
        top_factors = sorted_factors[:top_k]
        print(f"ğŸ“Š ä½¿ç”¨ all_factorsï¼ŒæŒ‰IC_IRæ’åºå–å‰ {top_k} ä¸ªå› å­")
    elif "top_factors" in data:
        top_factors = data["top_factors"][:top_k]
        print(f"ğŸ“Š ä½¿ç”¨ top_factorsï¼Œå–å‰ {top_k} ä¸ªå› å­")
    elif "factor_analysis" in data and "top_factors" in data["factor_analysis"]:
        top_factors = data["factor_analysis"]["top_factors"][:top_k]
        print(f"ğŸ“Š ä½¿ç”¨ factor_analysis.top_factorsï¼Œå–å‰ {top_k} ä¸ªå› å­")
    elif "all_factors" in data:
        # å¦‚æœall_factorså­˜åœ¨ï¼Œä½¿ç”¨ICå€¼æ’åºå–å‰top_kä¸ª
        all_factors = data["all_factors"]
        # æŒ‰ic_meanæˆ–ic_iræ’åºï¼Œä¼˜å…ˆä½¿ç”¨ic_ir
        sorted_factors = sorted(
            all_factors, key=lambda x: x.get("ic_ir", x.get("ic_mean", 0)), reverse=True
        )
        top_factors = sorted_factors[:top_k]
        print(f"ğŸ“Š ä½¿ç”¨ all_factorsï¼ŒæŒ‰IC_IRæ’åºå–å‰ {top_k} ä¸ªå› å­")
    else:
        raise ValueError(f"æ— æ³•ä»JSONæ–‡ä»¶ä¸­è§£æå› å­æ•°æ®: {json_path}")

    # ä»æ–°çš„JSONæ ¼å¼ä¸­æå–panel_columnä½œä¸ºå› å­åç§°
    factor_names = []
    for item in top_factors:
        if isinstance(item, dict):
            # æ–°æ ¼å¼ï¼šä½¿ç”¨panel_columnå­—æ®µ
            factor_name = item.get(
                "panel_column", item.get("factor", item.get("display_name", ""))
            )
            if factor_name:
                factor_names.append(factor_name)
        else:
            # å…¼å®¹æ—§æ ¼å¼
            factor_names.append(item)

    # ä¸¥æ ¼è¿‡æ»¤é»‘åå•å› å­
    original_count = len(factor_names)
    factor_names = [
        f
        for f in factor_names
        if not any(f.startswith(blacklisted) for blacklisted in BLACKLISTED_FACTORS)
    ]

    if len(factor_names) < original_count:
        filtered_count = original_count - len(factor_names)
        print(f"ğŸš¨ é»‘åå•è¿‡æ»¤: ç§»é™¤äº† {filtered_count} ä¸ªé»‘åå•å› å­")
        print(f"   é»‘åå•æ¨¡å¼: {BLACKLISTED_FACTORS}")

    if not factor_names:
        raise ValueError("è¿‡æ»¤åæ²¡æœ‰å¯ç”¨çš„å› å­ï¼Œè¯·æ£€æŸ¥å› å­æº")

    # æ˜¾ç¤ºåŠ è½½çš„å› å­ä¿¡æ¯
    print(f"ğŸ† å·²åŠ è½½Top {len(factor_names)}å› å­:")
    for i, factor in enumerate(factor_names[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        if isinstance(top_factors[i - 1], dict):
            ic_ir = top_factors[i - 1].get("ic_ir", 0)
            category = top_factors[i - 1].get("category", "unknown")
            print(f"   {i:2d}. {factor:<20} (IC_IR: {ic_ir:.4f}, ç±»åˆ«: {category})")
    if len(factor_names) > 10:
        print(f"   ... è¿˜æœ‰ {len(factor_names) - 10} ä¸ªå› å­")

    return factor_names


def validate_factors_safety(factors: List[str]) -> List[str]:
    """éªŒè¯å› å­å®‰å…¨æ€§ï¼Œè¿‡æ»¤é»‘åå•å› å­

    Args:
        factors: å€™é€‰å› å­åˆ—è¡¨

    Returns:
        è¿‡æ»¤åçš„å®‰å…¨å› å­åˆ—è¡¨
    """
    # é»‘åå•ï¼šä¸¥ç¦ä½¿ç”¨çš„æœªæ¥å‡½æ•°å’Œæœ‰é—®é¢˜çš„å› å­
    BLACKLISTED_FACTORS = [
        "RETURN_",  # âŒ æœªæ¥å‡½æ•° - ä¸¥æ ¼ç¦æ­¢
    ]

    original_count = len(factors)
    safe_factors = [
        f
        for f in factors
        if not any(f.startswith(blacklisted) for blacklisted in BLACKLISTED_FACTORS)
    ]

    if len(safe_factors) < original_count:
        filtered_count = original_count - len(safe_factors)
        print(f"ğŸš¨ å®‰å…¨è¿‡æ»¤: ç§»é™¤äº† {filtered_count} ä¸ªé»‘åå•å› å­")
        removed_factors = [
            f
            for f in factors
            if any(f.startswith(blacklisted) for blacklisted in BLACKLISTED_FACTORS)
        ]
        print(f"   ç§»é™¤çš„å› å­: {', '.join(removed_factors)}")
        print(f"   é»‘åå•æ¨¡å¼: {BLACKLISTED_FACTORS}")

    if not safe_factors:
        raise ValueError("âŒ æ‰€æœ‰å› å­éƒ½åœ¨é»‘åå•ä¸­ï¼æ— æ³•ç»§ç»­æ‰§è¡Œ")

    return safe_factors


# Schemaç¼“å­˜ï¼ˆé¿å…é‡å¤è¯»å–ï¼‰
_SCHEMA_CACHE: Dict[Path, Set[str]] = {}


def _read_parquet_schema(panel_path: Path) -> Set[str]:
    """è¯»å–Parquetæ–‡ä»¶schemaï¼ˆåªè¯»å…ƒæ•°æ®ï¼Œå¸¦ç¼“å­˜ï¼‰

    Args:
        panel_path: Parquetæ–‡ä»¶è·¯å¾„

    Returns:
        åˆ—åé›†åˆ
    """
    # æ£€æŸ¥ç¼“å­˜
    if panel_path in _SCHEMA_CACHE:
        return _SCHEMA_CACHE[panel_path]

    panel_columns = None

    # æ–¹æ³•1: pyarrow.ParquetFile (æœ€å¿«)
    try:
        import pyarrow.parquet as pq

        parquet_file = pq.ParquetFile(panel_path)
        panel_columns = set(parquet_file.schema_arrow.names)
    except ImportError:
        # pyarrowä¸å¯ç”¨ï¼Œç»™å‡ºä¸€æ¬¡æ€§è­¦å‘Š
        import warnings

        warnings.warn("pyarrowä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨pandasè¯»å–schemaï¼ˆè¾ƒæ…¢ï¼‰", UserWarning)
    except Exception:
        pass

    # æ–¹æ³•2: å…œåº•æ–¹æ¡ˆï¼ˆå…¨é‡è¯»å–ï¼‰
    if panel_columns is None:
        try:
            df = pd.read_parquet(panel_path)
            panel_columns = set(df.columns)
        except Exception as e:
            raise ValueError(f"æ— æ³•è¯»å–é¢æ¿æ–‡ä»¶schema: {panel_path}") from e

    # ç¼“å­˜ç»“æœ
    _SCHEMA_CACHE[panel_path] = panel_columns
    return panel_columns


def map_factor_names_to_panel(factors: List[str], panel_path: Path) -> List[str]:
    """éªŒè¯å› å­åç§°æ˜¯å¦ä¸é¢æ¿åˆ—ä¸€è‡´ï¼Œå¹¶è¿”å›å®é™…å¯ç”¨çš„åˆ—åã€‚

    çº¦æŸï¼š
        - å› å­ç­›é€‰é˜¶æ®µå¿…é¡»ç»™å‡ºçœŸå®çš„ `panel_column`
        - è‹¥å­˜åœ¨åˆ«åå·®å¼‚ï¼Œç›´æ¥åœ¨ç­›é€‰è„šæœ¬ä¸­ä¿®æ­£ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡ŒçŒœæµ‹
    """
    panel_columns = _read_parquet_schema(panel_path)
    if not panel_columns:
        raise ValueError(f"æ— æ³•è¯»å–é¢æ¿åˆ—ä¿¡æ¯: {panel_path}")

    # æ„é€ å¤§å°å†™æ— å…³çš„ç´¢å¼•ï¼Œé¿å…æ‰‹å·¥å¤§å°å†™å·®å¼‚
    panel_upper = {col.upper(): col for col in panel_columns}

    mapped: List[str] = []
    missing: List[str] = []

    for factor in factors:
        if factor in panel_columns:
            mapped.append(factor)
            continue

        # å¤§å°å†™å·®å¼‚è‡ªåŠ¨å¯¹é½
        upper_match = panel_upper.get(factor.upper())
        if upper_match:
            mapped.append(upper_match)
            continue

        missing.append(factor)

    if missing:
        raise ValueError(
            "ä»¥ä¸‹å› å­åœ¨é¢æ¿ä¸­æ‰¾ä¸åˆ°ï¼Œè¯·åœ¨å› å­ç­›é€‰é˜¶æ®µä¿®æ­£å‘½åï¼š" + ", ".join(missing)
        )

    print(f"ğŸ”— å› å­æ˜ å°„å®Œæˆ: {len(factors)} ä¸ªå› å­å…¨éƒ¨ä¸é¢æ¿åˆ—ä¸€è‡´")
    return mapped


def load_factor_panel(panel_path: Path, factors: Sequence[str]) -> pd.DataFrame:
    """åŠ è½½å› å­é¢æ¿å¹¶è£å‰ªåˆ°æŒ‡å®šå› å­åˆ—ã€‚

    è¿”å› MultiIndex(symbol, date) -> å› å­å€¼ çš„ DataFrameã€‚
    """
    panel = pd.read_parquet(panel_path).sort_index()

    missing = [f for f in factors if f not in panel.columns]
    if missing:
        raise ValueError(f"å› å­åˆ—ç¼ºå¤±ï¼š{missing}")

    return panel[list(factors)].copy()


def load_price_pivot(data_dir: Path) -> pd.DataFrame:
    """åˆå¹¶ raw/ETF/daily ä¸‹çš„æŠ¥ä»·æ–‡ä»¶ï¼Œè¿”å› (date x symbol) çš„æ”¶ç›˜ä»·çŸ©é˜µã€‚"""
    frames: List[pd.DataFrame] = []
    for fp in data_dir.glob("*.parquet"):
        try:
            df = pd.read_parquet(fp, columns=["trade_date", "close"])
        except ValueError:
            df = pd.read_parquet(fp)
            df = df[["trade_date", "close"]]
        df["symbol"] = fp.stem.split("_")[0]
        frames.append(df)

    if not frames:
        raise FileNotFoundError(f"ç›®å½• {data_dir} ä¸‹æœªæ‰¾åˆ° parquet æ•°æ®æ–‡ä»¶")

    prices = pd.concat(frames, ignore_index=True)
    prices["date"] = pd.to_datetime(prices["trade_date"])

    pivot = (
        prices.pivot(index="date", columns="symbol", values="close")
        .sort_index()
        .sort_index(axis=1)
    )
    pivot = pivot.ffill().dropna(how="all")
    return pivot


def normalize_factors(panel: pd.DataFrame, method: str = "zscore") -> pd.DataFrame:
    """å¯¹å› å­è¿›è¡Œæˆªé¢æ ‡å‡†åŒ–ã€‚

    Args:
        panel: MultiIndex(symbol, date) çš„å› å­é¢æ¿
        method: 'zscore' or 'rank'
    """
    grouped = panel.groupby(level="date")

    if method == "rank":
        normalized = grouped.rank(pct=True)
        return normalized - 0.5  # å±…ä¸­

    if method == "zscore":

        def _zscore(df: pd.DataFrame) -> pd.DataFrame:
            return (df - df.mean()) / df.std(ddof=0)

        normalized = grouped.transform(_zscore)
        return normalized.fillna(0.0)

    raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")


# --------------------------------------------------------------------------- #
# Numpy å‘é‡åŒ–é¢„å¤„ç†
# --------------------------------------------------------------------------- #


class VectorizedBacktestEngine:
    """å‘é‡åŒ–å›æµ‹å¼•æ“ï¼šé¢„å¯¹é½æ•°æ®ï¼Œæ‰¹é‡numpyè®¡ç®—

    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. é¢„å…ˆå°† normalized_panel è½¬ä¸º (n_dates, n_factors, n_etfs) tensor
    2. ä»·æ ¼/æ”¶ç›Šç‡çŸ©é˜µé¢„å¯¹é½ä¸º (n_dates, n_etfs)
    3. æƒé‡å‘é‡ç›´æ¥åšçŸ©é˜µä¹˜æ³•å¾—åˆ°ç»¼åˆå¾—åˆ†
    4. np.argpartition åš Top-N é€‰è‚¡
    5. çº¯ numpy å…¬å¼è®¡ç®—å›æµ‹æ”¶ç›Š
    """

    def __init__(
        self,
        normalized_panel: pd.DataFrame,
        price_pivot: pd.DataFrame,
        factors: List[str],
        fees: float = 0.001,
        init_cash: float = 1_000_000.0,
        freq: str = "1D",
    ):
        """åˆå§‹åŒ–å‘é‡åŒ–å¼•æ“

        Args:
            normalized_panel: MultiIndex(symbol, date) æ ‡å‡†åŒ–å› å­é¢æ¿
            price_pivot: (date x symbol) ä»·æ ¼çŸ©é˜µ
            factors: å› å­åˆ—è¡¨
            fees: äº¤æ˜“è´¹ç”¨
            init_cash: åˆå§‹èµ„é‡‘
            freq: æ—¶é—´é¢‘ç‡
        """
        self.factors = factors
        self.fees = fees
        self.init_cash = init_cash
        self.freq = freq

        # === 1. æ•°æ®å¯¹é½ä¸è½¬æ¢ ===
        # å°† MultiIndex é¢æ¿è½¬ä¸º (date x symbol x factor) æ ¼å¼
        panel_unstacked = normalized_panel.unstack(level="symbol")

        # å¯¹é½æ—¥æœŸç´¢å¼•
        common_dates = panel_unstacked.index.intersection(price_pivot.index)
        panel_unstacked = panel_unstacked.loc[common_dates]
        price_pivot = price_pivot.loc[common_dates]

        # å¯¹é½æ ‡çš„åˆ—
        common_symbols = list(
            set(panel_unstacked.columns.get_level_values(1)) & set(price_pivot.columns)
        )
        common_symbols.sort()

        # æå–å¯¹é½åçš„æ•°æ®
        self.dates = common_dates
        self.symbols = common_symbols
        self.n_dates = len(common_dates)
        self.n_etfs = len(common_symbols)
        self.n_factors = len(factors)

        # === 2. è½¬ä¸º numpy æ•°ç»„ ===
        # factor_tensor: (n_dates, n_factors, n_etfs)
        self.factor_tensor = np.zeros(
            (self.n_dates, self.n_factors, self.n_etfs), dtype=np.float32
        )
        for i, factor in enumerate(factors):
            factor_data = panel_unstacked[factor][common_symbols].values
            self.factor_tensor[:, i, :] = np.nan_to_num(factor_data, nan=0.0)

        # price_tensor: (n_dates, n_etfs) - ä¿ç•™NaNç”¨äºæ”¶ç›Šç‡è®¡ç®—
        self.price_tensor = price_pivot[common_symbols].values.astype(np.float32)

        # returns_tensor: (n_dates, n_etfs) - å…ˆè®¡ç®—æ”¶ç›Šç‡ï¼Œå†å¤„ç†NaNå’Œå¼‚å¸¸å€¼
        self.returns_tensor = np.zeros_like(self.price_tensor)
        prev_prices = self.price_tensor[:-1]
        curr_prices = self.price_tensor[1:]

        # åªåœ¨å‰ä¸€æ—¥ä»·æ ¼æœ‰æ•ˆæ—¶è®¡ç®—æ”¶ç›Šç‡ï¼Œé¿å…é™¤0
        valid_mask = (
            (prev_prices > 1e-6) & np.isfinite(prev_prices) & np.isfinite(curr_prices)
        )
        returns_raw = np.zeros_like(prev_prices)
        returns_raw[valid_mask] = (
            curr_prices[valid_mask] - prev_prices[valid_mask]
        ) / prev_prices[valid_mask]

        # é™åˆ¶æ”¶ç›Šç‡åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ—¥æ”¶ç›Šç‡ Â±100%ï¼‰ï¼Œé˜²æ­¢æ•°æ®å¼‚å¸¸
        returns_raw = np.clip(returns_raw, -1.0, 1.0)
        self.returns_tensor[1:] = returns_raw

        # æœ€åç»Ÿä¸€å¡«å……NaNä¸º0ï¼ˆæ­¤æ—¶å·²æ²¡æœ‰çˆ†ç‚¸æ”¶ç›Šï¼‰
        self.price_tensor = np.nan_to_num(self.price_tensor, nan=0.0)
        self.returns_tensor = np.nan_to_num(self.returns_tensor, nan=0.0)

        # === æ•°æ®è´¨é‡éªŒè¯ï¼šæ£€æµ‹å¼‚å¸¸æ”¶ç›Šç‡ ===
        self._validate_returns_quality()

        print(
            f"ğŸš€ å‘é‡åŒ–å¼•æ“åˆå§‹åŒ–å®Œæˆ: {self.n_dates}å¤© Ã— {self.n_factors}å› å­ Ã— {self.n_etfs}æ ‡çš„"
        )

    def _validate_returns_quality(self) -> None:
        """éªŒè¯æ”¶ç›Šç‡æ•°æ®è´¨é‡ï¼Œæ£€æµ‹å¼‚å¸¸å€¼

        Raises:
            ValueError: å¦‚æœå‘ç°æç«¯å¼‚å¸¸çš„æ”¶ç›Šç‡
        """
        # ç»Ÿè®¡æ”¶ç›Šç‡åˆ†å¸ƒ
        abs_returns = np.abs(self.returns_tensor)
        max_return = np.max(abs_returns)
        mean_return = np.mean(abs_returns)
        std_return = np.std(abs_returns)

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶…å‡ºåˆç†èŒƒå›´çš„æ”¶ç›Šç‡
        # æ—¥æ”¶ç›Šç‡ç»å¯¹å€¼è¶…è¿‡100%è§†ä¸ºå¼‚å¸¸ï¼ˆå·²è¢«clipé™åˆ¶ï¼Œè¿™é‡Œæ˜¯ä¿é™©æ£€æŸ¥ï¼‰
        extreme_mask = abs_returns > 1.0
        n_extreme = np.sum(extreme_mask)

        if n_extreme > 0:
            raise ValueError(
                f"âŒ å‘ç° {n_extreme} ä¸ªæç«¯æ”¶ç›Šç‡ï¼ˆ|r| > 100%ï¼‰ï¼Œæ•°æ®å¯èƒ½æœ‰é—®é¢˜ã€‚"
                f"æœ€å¤§ç»å¯¹æ”¶ç›Šç‡: {max_return:.4f}"
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„æ”¶ç›Šç‡ï¼ˆè¶…è¿‡5ä¸ªæ ‡å‡†å·®ï¼‰
        if max_return > mean_return + 10 * std_return and max_return > 0.5:
            import warnings

            warnings.warn(
                f"âš ï¸ å‘ç°å¼‚å¸¸é«˜çš„æ”¶ç›Šç‡: {max_return:.4f} "
                f"(å‡å€¼={mean_return:.4f}, æ ‡å‡†å·®={std_return:.4f})ï¼Œ"
                f"å¯èƒ½å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜",
                UserWarning,
            )

        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        non_zero_returns = abs_returns[abs_returns > 1e-6]
        if len(non_zero_returns) > 0:
            print(
                f"   æ”¶ç›Šç‡ç»Ÿè®¡: æœ€å¤§={max_return:.4f}, å‡å€¼={mean_return:.4f}, "
                f"æ ‡å‡†å·®={std_return:.4f}, éé›¶ç‡={len(non_zero_returns)/abs_returns.size:.2%}"
            )

    def compute_scores_batch(
        self, weight_matrix: np.ndarray, chunk_size: int = 2000
    ) -> np.ndarray:
        """æ‰¹é‡è®¡ç®—å¤šä¸ªæƒé‡ç»„åˆçš„å¾—åˆ†çŸ©é˜µï¼ˆåˆ†å—ä¼˜åŒ–ï¼Œæ”¯æŒå¤§è§„æ¨¡è®¡ç®—ï¼‰

        Args:
            weight_matrix: (n_combos, n_factors) æƒé‡çŸ©é˜µ
            chunk_size: åˆ†å—å¤§å°ï¼Œé¿å…å†…å­˜çˆ†ç‚¸ï¼ˆè‡ªåŠ¨è°ƒæ•´ï¼‰

        Returns:
            scores: (n_combos, n_dates, n_etfs) å¾—åˆ†å¼ é‡
        """
        n_combos = weight_matrix.shape[0]

        # è‡ªé€‚åº”åˆ†å—å¤§å°ï¼šå¤§è§„æ¨¡æµ‹è¯•ä½¿ç”¨æ›´å°çš„å—
        if n_combos > 20000:
            chunk_size = 1000  # è¶…å¤§è§„æ¨¡ä½¿ç”¨å°åˆ†å—
        elif n_combos > 10000:
            chunk_size = 2000  # å¤§è§„æ¨¡ä½¿ç”¨ä¸­ç­‰åˆ†å—
        else:
            chunk_size = min(chunk_size, n_combos)  # å°è§„æ¨¡ç›´æ¥è®¡ç®—æˆ–ä½¿ç”¨é»˜è®¤å€¼

        # å¦‚æœç»„åˆæ•°è¾ƒå°‘ï¼Œç›´æ¥è®¡ç®—
        if n_combos <= chunk_size:
            scores = np.einsum("dfe,cf->cde", self.factor_tensor, weight_matrix)
            return scores

        # åˆ†å—è®¡ç®—ä»¥èŠ‚çœå†…å­˜
        scores = np.empty((n_combos, self.n_dates, self.n_etfs), dtype=np.float32)

        print(f"ğŸ”§ åˆ†å—è®¡ç®—å¾—åˆ†: {n_combos} ç»„åˆï¼Œå—å¤§å° {chunk_size}")

        for i in range(0, n_combos, chunk_size):
            end = min(i + chunk_size, n_combos)
            chunk_weights = weight_matrix[i:end]

            # åˆ†å— einsum: (n_dates, n_factors, n_etfs) @ (chunk_size, n_factors) -> (chunk_size, n_dates, n_etfs)
            chunk_scores = np.einsum("dfe,cf->cde", self.factor_tensor, chunk_weights)
            scores[i:end] = chunk_scores

            # è¿›åº¦è¾“å‡ºï¼ˆæ¯10ä¸ªåˆ†å—æˆ–æ¯25%è¿›åº¦ï¼‰
            if i % (chunk_size * 10) == 0 or end / n_combos >= 0.25 * (
                1 + i // (n_combos // 4)
            ):
                print(f"  å·²å¤„ç†: {end}/{n_combos} ({end/n_combos:.1%})")

        print(f"âœ… åˆ†å—è®¡ç®—å®Œæˆ")
        return scores

    def build_weights_batch(
        self, scores: np.ndarray, top_n: int, min_score: float = None
    ) -> np.ndarray:
        """æ‰¹é‡æ„å»ºç›®æ ‡æƒé‡ï¼ˆå®Œå…¨å‘é‡åŒ– Top-N é€‰è‚¡ï¼‰

        Args:
            scores: (n_combos, n_dates, n_etfs) å¾—åˆ†å¼ é‡
            top_n: æ¯æ—¥æŒæœ‰æ•°é‡
            min_score: å¾—åˆ†é˜ˆå€¼

        Returns:
            weights: (n_combos, n_dates, n_etfs) æƒé‡å¼ é‡
        """
        n_combos, n_dates, n_etfs = scores.shape
        weights = np.zeros_like(scores, dtype=np.float32)

        if top_n >= n_etfs:
            # å…¨é€‰ï¼Œç›´æ¥ç­‰æƒ
            if min_score is None:
                weights[:, :, :] = 1.0 / n_etfs
            else:
                mask = scores >= min_score
                counts = np.sum(mask, axis=2, keepdims=True)
                weights = np.where(mask, 1.0 / np.maximum(counts, 1), 0.0)
        else:
            # å‘é‡åŒ– Top-N é€‰è‚¡ï¼šä½¿ç”¨ argpartition + å¹¿æ’­
            # å°† scores reshape ä¸º (n_combos * n_dates, n_etfs) ä»¥ä¾¿æ‰¹é‡å¤„ç†
            scores_flat = scores.reshape(-1, n_etfs)  # (n_combos*n_dates, n_etfs)
            weights_flat = np.zeros_like(scores_flat)

            # æ‰¹é‡ argpartitionï¼šå¯¹æ¯ä¸€è¡Œæ‰¾ Top-N
            # argpartition å°†æœ€å¤§çš„ top_n ä¸ªå…ƒç´ æ”¾åˆ°æ•°ç»„åéƒ¨
            top_indices = np.argpartition(-scores_flat, top_n, axis=1)[:, :top_n]

            # åº”ç”¨å¾—åˆ†é˜ˆå€¼ï¼ˆå‘é‡åŒ–ï¼‰
            if min_score is not None:
                # è·å– top_n ä½ç½®çš„å¾—åˆ†
                row_indices = np.arange(scores_flat.shape[0])[:, None]
                top_scores = scores_flat[row_indices, top_indices]
                valid_mask = top_scores >= min_score

                # è®¡ç®—æ¯è¡Œæœ‰æ•ˆæ•°é‡
                valid_counts = np.sum(valid_mask, axis=1, keepdims=True)

                # è®¾ç½®æƒé‡ï¼šåªå¯¹æœ‰æ•ˆä½ç½®èµ‹å€¼
                valid_weights = np.where(
                    valid_mask, 1.0 / np.maximum(valid_counts, 1), 0.0
                )
                np.put_along_axis(weights_flat, top_indices, valid_weights, axis=1)
            else:
                # æ— é˜ˆå€¼ï¼Œç›´æ¥ç­‰æƒ
                equal_weight = 1.0 / top_n
                np.put_along_axis(weights_flat, top_indices, equal_weight, axis=1)

            # reshape å›åŸå§‹å½¢çŠ¶
            weights = weights_flat.reshape(n_combos, n_dates, n_etfs)

        return weights

    def run_backtest_batch(self, weights: np.ndarray) -> List[Dict[str, float]]:
        """æ‰¹é‡è¿è¡Œå›æµ‹ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼Œæ— Pythonå¾ªç¯ï¼‰

        Args:
            weights: (n_combos, n_dates, n_etfs) æƒé‡å¼ é‡

        Returns:
            metrics: æ¯ä¸ªç»„åˆçš„æŒ‡æ ‡å­—å…¸åˆ—è¡¨
        """
        n_combos, n_dates, n_etfs = weights.shape

        # === 1. æ»åæƒé‡ï¼ˆé¿å…å‰è§†åå·®ï¼‰===
        # (C, D, E) -> (C, D, E)
        prev_weights = np.zeros_like(weights)
        prev_weights[:, 1:, :] = weights[:, :-1, :]

        # === 2. ç»„åˆæ”¶ç›Š ===
        # (C, D, E) * (D, E) -> (C, D)
        gross_returns = np.sum(prev_weights * self.returns_tensor[None, :, :], axis=2)

        # === 3. æ¢æ‰‹ç‡ ===
        # (C, D-1, E) -> (C, D-1)
        weight_diff = np.sum(np.abs(np.diff(weights, axis=1)), axis=2)
        turnover = 0.5 * weight_diff
        # ç¬¬ä¸€å¤©æ¢æ‰‹ä¸º0: (C, D-1) -> (C, D)
        turnover = np.pad(
            turnover, ((0, 0), (1, 0)), mode="constant", constant_values=0.0
        )

        # === 4. å‡€æ”¶ç›Š ===
        net_returns = gross_returns - self.fees * turnover  # (C, D)

        # === 5. æƒç›Šæ›²çº¿ ===
        equity_curve = np.cumprod(1.0 + net_returns, axis=1) * self.init_cash  # (C, D)

        # === 6. æ‰¹é‡è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ ===
        metrics_batch = self._compute_metrics_batch(equity_curve, net_returns, turnover)

        # === 7. å‘é‡åŒ–å­—å…¸è½¬æ¢ ===
        # ä½¿ç”¨numpyæ•°ç»„å’Œåˆ—è¡¨æ¨å¯¼å¼å®Œå…¨å‘é‡åŒ–å­—å…¸åˆ›å»º
        results = [
            {
                "annual_return": float(metrics_batch["annual_return"][c]),
                "max_drawdown": float(metrics_batch["max_drawdown"][c]),
                "sharpe": float(metrics_batch["sharpe"][c]),
                "calmar": float(metrics_batch["calmar"][c]),
                "win_rate": float(metrics_batch["win_rate"][c]),
                "turnover": float(metrics_batch["turnover"][c]),
            }
            for c in range(n_combos)
        ]

        return results

    def _compute_metrics_batch(
        self, equity_curve: np.ndarray, net_returns: np.ndarray, turnover: np.ndarray
    ) -> Dict[str, np.ndarray]:
        """æ‰¹é‡è®¡ç®—å›æµ‹æŒ‡æ ‡ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰

        Args:
            equity_curve: (n_combos, n_dates) æƒç›Šæ›²çº¿
            net_returns: (n_combos, n_dates) å‡€æ”¶ç›Šç‡
            turnover: (n_combos, n_dates) æ¢æ‰‹ç‡

        Returns:
            æŒ‡æ ‡å­—å…¸ï¼Œæ¯ä¸ªå€¼ä¸º (n_combos,) æ•°ç»„
        """
        n_combos = equity_curve.shape[0]
        n_years = self.n_dates / 252.0

        # === 1. å¹´åŒ–æ”¶ç›Šç‡ ===
        total_return = equity_curve[:, -1] / self.init_cash - 1.0  # (C,)
        annual_return = np.where(
            n_years > 0, np.power(1.0 + total_return, 1.0 / n_years) - 1.0, 0.0
        )

        # === 2. æœ€å¤§å›æ’¤ ===
        cummax = np.maximum.accumulate(equity_curve, axis=1)  # (C, D)
        drawdowns = (equity_curve - cummax) / (cummax + 1e-12)  # (C, D)
        max_drawdown = -np.min(drawdowns, axis=1)  # (C,)

        # === 3. å¤æ™®æ¯”ç‡ ===
        returns_mean = np.mean(net_returns, axis=1)  # (C,)
        returns_std = np.std(net_returns, axis=1, ddof=0)  # (C,)
        sharpe = np.where(
            returns_std > 1e-12, returns_mean / returns_std * np.sqrt(252), 0.0
        )

        # === 4. Calmar æ¯”ç‡ ===
        calmar = np.where(max_drawdown > 1e-6, annual_return / max_drawdown, 0.0)

        # === 5. èƒœç‡ ===
        win_rate = np.mean(net_returns > 0, axis=1)  # (C,)

        # === 6. å¹³å‡æ¢æ‰‹ç‡ï¼ˆå¹´åŒ–ï¼‰===
        avg_turnover = np.mean(turnover, axis=1) * 252  # (C,)

        return {
            "annual_return": annual_return,
            "max_drawdown": max_drawdown,
            "sharpe": sharpe,
            "calmar": calmar,
            "win_rate": win_rate,
            "turnover": avg_turnover,
        }


# --------------------------------------------------------------------------- #
# æƒé‡ç½‘æ ¼ç”Ÿæˆä¸æ‰“åˆ†
# --------------------------------------------------------------------------- #


def generate_weight_grid_stream(
    num_factors: int,
    weight_grid: Sequence[float],
    normalize: bool = True,
    max_active_factors: Optional[int] = None,
    random_seed: Optional[int] = None,
    max_total_combos: Optional[int] = None,
    debug: bool = False,
) -> List[Tuple[float, ...]]:
    """ç”Ÿæˆæƒé‡ç»„åˆï¼ˆç¨³å®šæ’åºï¼Œè·¨æ‰¹æ¬¡å¯å¤ç°ï¼‰

    Args:
        num_factors: ç»„åˆä¸­å› å­æ•°é‡
        weight_grid: æ¯ä¸ªå› å­çš„å€™é€‰æƒé‡ï¼ˆä¾‹å¦‚ [0.0, 0.5, 1.0]ï¼‰
        normalize: æ˜¯å¦å°†æƒé‡å½’ä¸€åŒ–åˆ°å’Œä¸º1
        max_active_factors: æœ€å¤§éé›¶å› å­æ•°é‡ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        random_seed: éšæœºç§å­ï¼Œç”¨äºå¯é‡ç°çš„æŠ½æ ·
        max_total_combos: æœ€å¤§ç»„åˆæ€»æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯

    Returns:
        æƒé‡ç»„åˆåˆ—è¡¨ï¼ˆç¨³å®šæ’åºï¼Œè·¨è¿è¡Œå¯å¤ç°ï¼‰
    """
    if random_seed is not None:
        random.seed(random_seed)
        np.random.seed(random_seed)

    max_active = max_active_factors or num_factors
    non_zero_weights = [w for w in weight_grid if w != 0]

    if not non_zero_weights:
        raise ValueError("weight_gridå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªéé›¶æƒé‡")

    # çº¯å…¬å¼è®¡ç®—ç»„åˆæ•°ï¼ˆä¸å±•å¼€ä»»ä½•åˆ—è¡¨ï¼‰
    def count_combos_for_active(active_count: int) -> int:
        """è®¡ç®—æŒ‡å®šæ´»è·ƒå› å­æ•°çš„ç»„åˆæ•°"""
        n_positions = math.comb(num_factors, active_count)
        # æ´»è·ƒä½ç½®çš„éé›¶æƒé‡ç»„åˆæ•°
        n_weight_combos = len(non_zero_weights) ** active_count
        return n_positions * n_weight_combos

    # è®¡ç®—æ¯ä¸ªæ´»è·ƒåº¦çš„ç»„åˆæ•°å’Œç´¯ç§¯æ¦‚ç‡ï¼ˆç”¨äºåŠ æƒé‡‡æ ·ï¼‰
    active_counts = list(range(1, max_active + 1))
    combo_counts = [count_combos_for_active(ac) for ac in active_counts]
    total_valid_combos = sum(combo_counts)

    if total_valid_combos == 0:
        return []

    # å¦‚æœç»„åˆæ•°å¾ˆå°ï¼Œç›´æ¥æšä¸¾
    if max_total_combos is None and total_valid_combos <= 50000:
        if debug:
            print(f"ğŸ“Š ç»„åˆæ•°è¾ƒå° ({total_valid_combos})ï¼Œä½¿ç”¨å®Œå…¨æšä¸¾")
        combos_set_small: Set[Tuple[float, ...]] = set()

        for active_count in active_counts:
            for active_indices in itertools.combinations(
                range(num_factors), active_count
            ):
                for weight_combo in itertools.product(
                    non_zero_weights, repeat=active_count
                ):
                    weights = [0.0] * num_factors
                    for pos, w in zip(active_indices, weight_combo):
                        weights[pos] = w

                    weights_arr = np.array(weights, dtype=float)
                    weight_sum = weights_arr.sum()
                    if normalize:
                        if abs(weight_sum) < 1e-12:
                            continue  # è·³è¿‡æ— æ³•å½’ä¸€åŒ–çš„æƒé‡
                        weights_arr = weights_arr / weight_sum

                    combos_set_small.add(tuple(weights_arr))

        # ğŸ”§ ç¨³å®šæ’åºï¼šä¿è¯è·¨è¿è¡Œå¯å¤ç°
        combos_sorted = sorted(list(combos_set_small))
        if debug:
            print(f"ğŸ“Š æƒé‡ç½‘æ ¼ç”Ÿæˆå®Œæˆ: {len(combos_sorted)} ç»„åˆï¼ˆå·²æ’åºï¼‰")
        return combos_sorted

    # å¤§è§„æ¨¡ç»„åˆï¼šåŠ æƒéšæœºé‡‡æ ·ï¼ˆæ— åï¼‰
    DEFAULT_SAMPLE = 10000
    if max_total_combos is None:
        target_count = min(DEFAULT_SAMPLE, total_valid_combos)
        if total_valid_combos > DEFAULT_SAMPLE:
            print(
                f"âš ï¸  æœªæŒ‡å®š max_total_combosï¼Œä»…éšæœºé‡‡æ · {target_count}/{total_valid_combos:,} ä¸ªç»„åˆã€‚å¦‚éœ€å…¨é‡æˆ–æ›´å¤§è§„æ¨¡ï¼Œè¯·æ˜¾å¼è®¾ç½® --max-total-combosã€‚"
            )
    else:
        if max_total_combos <= 0:
            target_count = total_valid_combos
        else:
            target_count = min(max_total_combos, total_valid_combos)

    if debug:
        print(
            f"ğŸ¯ æ— åéšæœºç”Ÿæˆ {target_count} ä¸ªç»„åˆ (ç†è®ºæ€»æ•°: {total_valid_combos:,})"
        )

    combos_set: Set[Tuple[float, ...]] = set()
    attempts = 0
    max_attempts = max(target_count * 200, target_count * len(non_zero_weights))

    # é¢„è®¡ç®—ç´¯ç§¯æƒé‡ç”¨äºåŠ æƒé‡‡æ ·
    cumulative_weights = []
    cumsum = 0
    for count in combo_counts:
        cumsum += count
        cumulative_weights.append(cumsum)

    last_report = 0
    report_interval = max(5000, target_count // 5) if debug else float("inf")

    while len(combos_set) < target_count and attempts < max_attempts:
        attempts += 1

        # æŒ‰ç»„åˆæ•°é‡åŠ æƒé€‰æ‹©æ´»è·ƒå› å­æ•°ï¼ˆæ— åé‡‡æ ·çš„å…³é”®ï¼‰
        rand_val = random.random() * total_valid_combos
        active_count = active_counts[0]
        for i, cum_weight in enumerate(cumulative_weights):
            if rand_val < cum_weight:
                active_count = active_counts[i]
                break

        # éšæœºé€‰æ‹©æ´»è·ƒå› å­ä½ç½®
        active_indices = random.sample(range(num_factors), active_count)

        # ä¸ºæ´»è·ƒä½ç½®éšæœºåˆ†é…éé›¶æƒé‡
        active_weights = [random.choice(non_zero_weights) for _ in range(active_count)]

        # æ„å»ºå®Œæ•´æƒé‡å‘é‡
        weights = [0.0] * num_factors
        for i, idx in enumerate(active_indices):
            weights[idx] = active_weights[i]

        # å½’ä¸€åŒ–
        weights_arr = np.array(weights, dtype=float)
        weight_sum = weights_arr.sum()
        if normalize:
            if abs(weight_sum) < 1e-12:
                continue  # è·³è¿‡æ— æ³•å½’ä¸€åŒ–çš„æ ·æœ¬
            weights_arr = weights_arr / weight_sum

        combos_set.add(tuple(weights_arr))

        # å—æ§çš„è¿›åº¦æŠ¥å‘Š
        if debug and len(combos_set) - last_report >= report_interval:
            print(f"  â³ å·²ç”Ÿæˆ {len(combos_set):,} ä¸ªå”¯ä¸€ç»„åˆ (å°è¯• {attempts:,} æ¬¡)")
            last_report = len(combos_set)

    if len(combos_set) < target_count:
        if debug:
            print(
                f"âš ï¸ è­¦å‘Š: ç”Ÿæˆ {len(combos_set)} ä¸ªå”¯ä¸€ç»„åˆ (ç›®æ ‡ {target_count}ï¼Œå°è¯• {attempts:,} æ¬¡)"
            )
        if attempts >= max_attempts:
            print(f"âš ï¸ è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° {max_attempts:,}ï¼Œå¯èƒ½å­˜åœ¨é‡å¤ç‡è¿‡é«˜çš„é—®é¢˜")

    # ğŸ”§ ç¨³å®šæ’åºï¼šä¿è¯è·¨è¿è¡Œã€è·¨æ‰¹æ¬¡å¯å¤ç°
    combos_sorted = sorted(list(combos_set))

    if debug:
        efficiency = len(combos_set) / attempts if attempts > 0 else 0
        print(
            f"ğŸ“Š éšæœºç”Ÿæˆå®Œæˆ: {len(combos_sorted)} ç»„åˆ (æ•ˆç‡: {efficiency:.1%}ï¼Œå·²æ’åº)"
        )

    return combos_sorted


def generate_batch_combos(
    all_combos: List[Tuple[float, ...]], batch_size: int, batch_idx: int
) -> List[Tuple[float, ...]]:
    """åˆ†æ‰¹è·å–æƒé‡ç»„åˆ

    Args:
        all_combos: æ‰€æœ‰æƒé‡ç»„åˆ
        batch_size: æ‰¹æ¬¡å¤§å°
        batch_idx: æ‰¹æ¬¡ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰

    Returns:
        å½“å‰æ‰¹æ¬¡çš„æƒé‡ç»„åˆ
    """
    start_idx = batch_idx * batch_size
    end_idx = min(start_idx + batch_size, len(all_combos))
    return all_combos[start_idx:end_idx]


def build_score_matrix(
    normalized_panel: pd.DataFrame,
    factors: Sequence[str],
    weights: Sequence[float],
) -> pd.DataFrame:
    """ç”Ÿæˆå¤šå› å­ç»¼åˆå¾—åˆ†çŸ©é˜µ (date x symbol)ã€‚"""
    assert len(factors) == len(weights)

    weighted = pd.Series(0.0, index=normalized_panel.index)
    for factor, weight in zip(factors, weights):
        if weight == 0:
            continue
        weighted = weighted.add(normalized_panel[factor] * weight, fill_value=0.0)

    scores = weighted.unstack(level="symbol")
    scores = scores.sort_index()
    return scores


def build_target_weights(
    scores: pd.DataFrame,
    top_n: int,
    min_score: float | None = None,
) -> pd.DataFrame:
    """æ ¹æ®å¾—åˆ†æ„å»ºç›®æ ‡æƒé‡ï¼Œç­‰æƒæŒæœ‰ Top-Nã€‚

    Args:
        scores: (date x symbol) ç»¼åˆå¾—åˆ†çŸ©é˜µ
        top_n: æ¯æ—¥æŒæœ‰çš„ ETF æ•°é‡
        min_score: è¿‡æ»¤é—¨æ§›ï¼ˆNone è¡¨ç¤ºä¸è®¾é—¨æ§›ï¼‰
    """
    ranks = scores.rank(axis=1, ascending=False, method="first")
    selection = ranks <= top_n

    if min_score is not None:
        selection &= scores >= min_score

    weights = selection.astype(float)
    weights = weights.div(weights.sum(axis=1), axis=0).fillna(0.0)
    return weights


def build_target_weights_multi(
    scores: pd.DataFrame,
    top_n_list: List[int],
    min_score_list: List[float | None],
    debug: bool = False,
) -> List[Tuple[int, float | None, pd.DataFrame]]:
    """æ‰¹é‡æ„å»ºå¤šä¸ªTop-Nå’Œmin_scoreç»„åˆçš„ç›®æ ‡æƒé‡

    Args:
        scores: (date x symbol) ç»¼åˆå¾—åˆ†çŸ©é˜µ
        top_n_list: Top-Nå€™é€‰å€¼åˆ—è¡¨
        min_score_list: min_scoreå€™é€‰å€¼åˆ—è¡¨
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯

    Returns:
        (top_n, min_score, weights) å…ƒç»„åˆ—è¡¨
    """
    results: List[Tuple[int, float | None, pd.DataFrame]] = []

    for top_n in top_n_list:
        for min_score in min_score_list:
            weights = build_target_weights(scores, top_n, min_score)
            results.append((top_n, min_score, weights))

    if debug:
        print(f"ğŸ¯ ç”Ÿæˆ {len(results)} ä¸ªTop-Nç»„åˆ: {top_n_list} x {min_score_list}")
    return results


# --------------------------------------------------------------------------- #
# å›æµ‹æ‰§è¡Œ
# --------------------------------------------------------------------------- #


def run_backtest_safe(
    prices: pd.DataFrame,
    weights: pd.DataFrame,
    freq: str = "1D",
    init_cash: float = 1_000_000.0,
    fees: float = 0.001,
    check_data_quality: bool = True,
) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """åŸºäºç›®æ ‡æƒé‡è¿è¡Œå®‰å…¨å›æµ‹ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰

    Args:
        prices: ä»·æ ¼çŸ©é˜µ (date x symbol)
        weights: æƒé‡çŸ©é˜µ (date x symbol)
        freq: æ—¶é—´é¢‘ç‡
        init_cash: åˆå§‹èµ„é‡‘
        fees: äº¤æ˜“è´¹ç”¨
        check_data_quality: æ˜¯å¦æ£€æŸ¥æ•°æ®è´¨é‡

    Returns:
        (equity_curve, net_returns, turnover)
    """
    if check_data_quality:
        # æ•°æ®è´¨é‡æ£€æŸ¥
        if weights.isna().all().all():
            raise ValueError("æƒé‡çŸ©é˜µå…¨ä¸ºNaN")
        if (weights.sum(axis=1) == 0).all():
            raise ValueError("æ‰€æœ‰æ—¥æœŸæƒé‡å’Œä¸º0")
        if prices.isna().all().all():
            raise ValueError("ä»·æ ¼çŸ©é˜µå…¨ä¸ºNaN")

    # æ•°æ®å¯¹é½
    aligned_prices = prices.sort_index().reindex(weights.index).ffill()
    aligned_weights = weights.reindex(aligned_prices.index).fillna(0.0)

    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®
    if aligned_prices.dropna().shape[0] < 10:
        raise ValueError("æœ‰æ•ˆæ•°æ®ç‚¹å°‘äº10ä¸ªï¼Œæ— æ³•è¿›è¡Œå›æµ‹")

    asset_returns = aligned_prices.pct_change().fillna(0.0)
    prev_weights = aligned_weights.shift().fillna(0.0)

    gross_returns = (prev_weights * asset_returns).sum(axis=1)

    # è¿‘ä¼¼äº¤æ˜“æˆæœ¬ï¼šæ¯æ—¥æƒé‡å˜åŒ–çš„ä¸€åŠä½œä¸ºæ¢æ‰‹æ¯”ä¾‹
    weight_diff = aligned_weights.diff().abs().sum(axis=1).fillna(0.0)
    turnover = 0.5 * weight_diff
    net_returns = gross_returns - fees * turnover

    # æ£€æŸ¥æ”¶ç›Šåºåˆ—æ˜¯å¦æœ‰æ•ˆ
    if net_returns.abs().sum() == 0:
        raise ValueError("æ”¶ç›Šåºåˆ—å…¨ä¸º0")

    equity_curve = (1.0 + net_returns).cumprod()
    equity_curve = equity_curve / equity_curve.iloc[0] * init_cash

    return equity_curve, net_returns, turnover


def run_batch_backtest(
    prices: pd.DataFrame,
    weights_list: List[pd.DataFrame],
    freq: str = "1D",
    init_cash: float = 1_000_000.0,
    fees: float = 0.001,
    parallel: bool = False,
    max_workers: int = 4,
) -> List[Tuple[pd.Series, pd.Series, pd.Series]]:
    """æ‰¹é‡è¿è¡Œå›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰

    Args:
        prices: ä»·æ ¼çŸ©é˜µ
        weights_list: æƒé‡çŸ©é˜µåˆ—è¡¨
        freq: æ—¶é—´é¢‘ç‡
        init_cash: åˆå§‹èµ„é‡‘
        fees: äº¤æ˜“è´¹ç”¨
        parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œ
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°

    Returns:
        (equity_curve, net_returns, turnover) å…ƒç»„åˆ—è¡¨
    """

    def _single_backtest(
        weights: pd.DataFrame,
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        return run_backtest_safe(prices, weights, freq, init_cash, fees)

    if not parallel:
        # ä¸²è¡Œæ‰§è¡Œ
        results = []
        for weights in tqdm(weights_list, desc="å›æµ‹è¿›åº¦"):
            try:
                result = _single_backtest(weights)
                results.append(result)
            except Exception as e:
                print(f"âš ï¸ å›æµ‹å¤±è´¥: {e}")
                results.append((pd.Series(), pd.Series(), pd.Series()))
        return results

    # å¹¶è¡Œæ‰§è¡Œ
    results = [None] * len(weights_list)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(_single_backtest, weights): idx
            for idx, weights in enumerate(weights_list)
        }

        for future in tqdm(
            as_completed(future_to_idx), total=len(weights_list), desc="å¹¶è¡Œå›æµ‹"
        ):
            idx = future_to_idx[future]
            try:
                result = future.result()
                results[idx] = result
            except Exception as e:
                print(f"âš ï¸ å›æµ‹å¤±è´¥ (idx={idx}): {e}")
                results[idx] = (pd.Series(), pd.Series(), pd.Series())

    return results


def evaluate_portfolio(
    equity_curve: pd.Series,
    net_returns: pd.Series,
    turnover: pd.Series,
    freq: str,
) -> Dict[str, float]:
    """æŠ½å–å…³é”®ç»©æ•ˆæŒ‡æ ‡ã€‚"""
    total_return = equity_curve.iloc[-1] / equity_curve.iloc[0] - 1.0
    periods = len(net_returns)
    periods_per_year = 252
    annual_return = (1.0 + total_return) ** (periods_per_year / periods) - 1.0
    sharpe = (
        net_returns.mean() / net_returns.std() * np.sqrt(periods_per_year)
        if net_returns.std() > 0
        else np.nan
    )
    running_max = equity_curve.cummax()
    drawdown = equity_curve / running_max - 1.0
    max_drawdown = drawdown.min()
    calmar = annual_return / abs(max_drawdown) if max_drawdown < 0 else np.nan
    turnover_ratio = float(turnover.sum())
    if _HAS_VECTORBT:
        pf = vbt.Portfolio.from_holding(
            close=equity_curve / equity_curve.iloc[0],
            init_cash=1.0,
            freq=freq,
        )
        stats = pf.stats()
        sharpe_vbt = stats.get("Sharpe Ratio", np.nan)
        if pd.notna(sharpe_vbt):
            sharpe = float(sharpe_vbt)
        calmar_vbt = stats.get("Calmar Ratio", np.nan)
        if pd.notna(calmar_vbt):
            calmar = float(calmar_vbt)
        total_return_vbt = stats.get("Total Return [%]", np.nan)
        if pd.notna(total_return_vbt):
            total_return = float(total_return_vbt) / 100.0
        annual_return_vbt = stats.get("CAGR [%]", np.nan)
        if pd.notna(annual_return_vbt):
            annual_return = float(annual_return_vbt) / 100.0
        max_drawdown_vbt = stats.get("Max Drawdown [%]", np.nan)
        if pd.notna(max_drawdown_vbt):
            max_drawdown = float(max_drawdown_vbt) / 100.0

    return {
        "total_return": total_return,
        "annual_return": annual_return,
        "max_drawdown": max_drawdown,
        "sharpe": sharpe,
        "calmar": calmar,
        "win_rate": float((net_returns > 0).mean()),
        "turnover": turnover_ratio,
    }


# --------------------------------------------------------------------------- #
# ä¸»æµç¨‹
# --------------------------------------------------------------------------- #


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="å¤šå› å­è½®åŠ¨æš´åŠ›æšä¸¾å›æµ‹è„šæ‰‹æ¶ (å¢å¼ºç‰ˆ)"
    )

    # === é…ç½®æ–‡ä»¶å‚æ•° ===
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆçº§é«˜äºCLIå‚æ•°ï¼‰",
    )

    # === æ•°æ®åŠ è½½å‚æ•° ===
    parser.add_argument(
        "--factor-panel",
        default="factor_output/etf_rotation/panel_optimized_v2_20200102_20251014.parquet",
        help="å› å­é¢æ¿è·¯å¾„ï¼ˆMultiIndex parquetï¼‰",
    )
    parser.add_argument(
        "--data-dir",
        default="raw/ETF/daily",
        help="åŸå§‹ETFè¡Œæƒ…ç›®å½•ï¼ˆåŒ…å« *.parquetï¼‰",
    )
    parser.add_argument(
        "--top-factors-json",
        type=str,
        default=None,
        help="å› å­æ’åºJSONæ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒé€šé…ç¬¦ï¼Œå¦‚ï¼šproduction_factor_results/top_factors_*.json",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="ä»å› å­æ’åºæ–‡ä»¶ä¸­é€‰æ‹©çš„Top Kå› å­æ•°é‡",
    )
    parser.add_argument(
        "--factors",
        nargs="+",
        default=None,
        help="æ‰‹åŠ¨æŒ‡å®šå‚ä¸ç»„åˆçš„å› å­åˆ—è¡¨ï¼ˆä¼˜å…ˆäº--top-factors-jsonï¼‰",
    )

    # === æƒé‡ç½‘æ ¼å‚æ•° ===
    parser.add_argument(
        "--weight-grid",
        nargs="+",
        type=float,
        default=[0.0, 0.5, 1.0],
        help="æ¯ä¸ªå› å­å€™é€‰æƒé‡ï¼ˆå°†è‡ªåŠ¨å½’ä¸€åŒ–ï¼‰",
    )
    parser.add_argument(
        "--max-active-factors",
        type=int,
        default=None,
        help="æœ€å¤§éé›¶å› å­æ•°é‡é™åˆ¶",
    )
    parser.add_argument(
        "--max-total-combos",
        type=int,
        default=None,
        help="æœ€å¤§ç»„åˆæ€»æ•°é™åˆ¶ï¼Œç”¨äºæŠ½æ ·",
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼Œç”¨äºå¯é‡ç°çš„æŠ½æ ·",
    )

    # === Top-Nå’Œç­›é€‰å‚æ•° ===
    parser.add_argument(
        "--top-n-list",
        nargs="+",
        type=int,
        default=[5],
        help="Top-Nå€™é€‰å€¼åˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªå€¼å¦‚ï¼š3 5 8",
    )
    parser.add_argument(
        "--min-score-list",
        nargs="+",
        type=float,
        default=[None],
        help="å¾—åˆ†é˜ˆå€¼åˆ—è¡¨ï¼Œæ”¯æŒå¤šä¸ªå€¼å¦‚ï¼š0.0 0.1",
    )

    # === åˆ†æ‰¹æ‰§è¡Œå‚æ•° ===
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10000,
        help="æ‰¹æ¬¡å¤§å°ï¼Œç”¨äºå¤§è§„æ¨¡ç»„åˆçš„åˆ†æ‰¹å¤„ç†",
    )
    parser.add_argument(
        "--batch-idx",
        type=int,
        default=0,
        help="æ‰¹æ¬¡ç´¢å¼•ï¼Œä»0å¼€å§‹ï¼Œç”¨äºæ–­ç‚¹ç»­è·‘",
    )
    parser.add_argument(
        "--sanity-run",
        action="store_true",
        help="Sanity Runæ¨¡å¼ï¼Œä»…è¿è¡Œå°‘é‡ç»„åˆéªŒè¯è®¾ç½®",
    )

    # === å¹¶å‘ä¼˜åŒ–å‚æ•° ===
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="å¯ç”¨å¹¶è¡Œå›æµ‹",
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°",
    )
    parser.add_argument(
        "--use-vectorized",
        action="store_true",
        default=True,
        help="ä½¿ç”¨å‘é‡åŒ–å›æµ‹å¼•æ“ï¼ˆé»˜è®¤å¼€å¯ï¼Œ100xé€Ÿåº¦æå‡ï¼‰",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=1,
        help="å¤šè¿›ç¨‹å¹¶è¡Œæ•°ï¼ˆæ¨èM4 Proä½¿ç”¨4ï¼Œå•è¿›ç¨‹è®¾1ï¼‰",
    )

    # === å›æµ‹å‚æ•° ===
    parser.add_argument(
        "--fees",
        nargs="+",
        type=float,
        default=[0.001],
        help="å•è¾¹äº¤æ˜“è´¹ç”¨åˆ—è¡¨ï¼ˆæ”¯æŒæˆæœ¬æ•æ„Ÿæ€§åˆ†æï¼Œå¦‚ï¼š0.001 0.002 0.003ï¼‰",
    )
    parser.add_argument(
        "--init-cash",
        type=float,
        default=1_000_000.0,
        help="åˆå§‹èµ„é‡‘",
    )
    parser.add_argument(
        "--freq",
        type=str,
        default="1D",
        help="æ—¶é—´é¢‘ç‡ï¼ˆç”¨äºå¹´åŒ–è®¡ç®—ï¼Œé»˜è®¤æ—¥é¢‘ï¼‰",
    )
    parser.add_argument(
        "--norm-method",
        choices=["zscore", "rank"],
        default="zscore",
        help="æˆªé¢æ ‡å‡†åŒ–æ–¹å¼",
    )

    # === è¾“å‡ºå‚æ•° ===
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºè·¯å¾„ï¼Œæ”¯æŒCSVæ–‡ä»¶æˆ–ç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³å­ç›®å½•ï¼‰",
    )
    parser.add_argument(
        "--top-k-results",
        type=int,
        default=None,
        help="ä»…ä¿ç•™å¤æ™®æœ€é«˜çš„å‰Kä¸ªç»“æœå†™å…¥è¾“å‡ºå’Œæ£€æŸ¥ç‚¹ï¼ˆé»˜è®¤ä¿ç•™å…¨éƒ¨ï¼‰",
    )
    parser.add_argument(
        "--keep-metrics-json",
        action="store_true",
        help="ä¿å­˜è¯¦ç»†æŒ‡æ ‡JSON",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—",
    )

    return parser.parse_args()


def load_config_from_yaml(config_path: str) -> Dict[str, Any]:
    """ä» YAML æ–‡ä»¶åŠ è½½é…ç½®

    Args:
        config_path: YAML é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config.get("parameters", {})


def merge_config_with_args(args: argparse.Namespace) -> argparse.Namespace:
    """åˆå¹¶ YAML é…ç½®å’Œ CLI å‚æ•°ï¼ˆYAML ä¼˜å…ˆçº§æ›´é«˜ï¼‰

    Args:
        args: CLI å‚æ•°

    Returns:
        åˆå¹¶åçš„å‚æ•°
    """
    if args.config is None:
        return args

    config = load_config_from_yaml(args.config)

    # YAML é”®åæ˜ å°„åˆ° argparse å‚æ•°åï¼ˆä¸‹åˆ’çº¿è½¬è¿å­—ç¬¦ï¼‰
    for key, value in config.items():
        arg_name = key.replace("-", "_")
        if hasattr(args, arg_name):
            setattr(args, arg_name, value)

    return args


def create_output_directory(base_path: str, timestamp: str) -> Tuple[Path, Path]:
    """åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•"""
    if base_path is None:
        # é»˜è®¤è¾“å‡ºåˆ°resultsç›®å½• (ç›¸å¯¹äºè„šæœ¬æ‰€åœ¨ç›®å½•)
        base_path = "results/vbt_multifactor"

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†è·¯å¾„
    script_dir = Path(__file__).parent

    # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ç›¸å¯¹äºè„šæœ¬ç›®å½•
    base_path = Path(base_path)
    if not base_path.is_absolute():
        base_path = script_dir / base_path

    # å¦‚æœä»¥.csvç»“å°¾ï¼Œåˆ™è§†ä¸ºæ–‡ä»¶è·¯å¾„ï¼Œè¿”å›çˆ¶ç›®å½•
    if base_path.suffix == ".csv":
        output_dir = base_path.parent
        csv_file = base_path
    else:
        # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•
        output_dir = base_path / f"run_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        csv_file = output_dir / "results.csv"

    return output_dir, csv_file


def save_checkpoint(results: List[Dict], checkpoint_path: Path) -> None:
    """ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    with open(checkpoint_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)


def run_backtest_worker(
    combos_chunk: List[Tuple[float, ...]],
    global_start_idx: int,
    factors: List[str],
    normalized_panel: pd.DataFrame,
    price_pivot: pd.DataFrame,
    top_n_list: List[int],
    min_score_list: List[float],
    fees: float,
    init_cash: float,
    freq: str,
) -> List[Dict[str, float]]:
    """å¤šè¿›ç¨‹workerå‡½æ•°ï¼šå¤„ç†ä¸€ä¸ªæƒé‡ç»„åˆå—

    Args:
        combos_chunk: æƒé‡ç»„åˆå—
        global_start_idx: è¯¥å—åœ¨å…¨å±€ç»„åˆåˆ—è¡¨ä¸­çš„èµ·å§‹ç´¢å¼•
        factors: å› å­åˆ—è¡¨
        normalized_panel: æ ‡å‡†åŒ–å› å­é¢æ¿
        price_pivot: ä»·æ ¼çŸ©é˜µ
        top_n_list: Top-Nå€™é€‰å€¼åˆ—è¡¨
        min_score_list: min_scoreå€™é€‰å€¼åˆ—è¡¨
        fees: äº¤æ˜“è´¹ç”¨
        init_cash: åˆå§‹èµ„é‡‘
        freq: æ—¶é—´é¢‘ç‡

    Returns:
        ç»“æœåˆ—è¡¨
    """
    # åˆå§‹åŒ–å¼•æ“ï¼ˆæ¯ä¸ªworkerç‹¬ç«‹åˆå§‹åŒ–ï¼‰
    engine = VectorizedBacktestEngine(
        normalized_panel=normalized_panel,
        price_pivot=price_pivot,
        factors=factors,
        fees=fees,
        init_cash=init_cash,
        freq=freq,
    )

    # è½¬ä¸ºnumpyæ•°ç»„
    weight_matrix = np.array(combos_chunk, dtype=np.float32)

    # æ‰¹é‡è®¡ç®—å¾—åˆ†
    all_scores = engine.compute_scores_batch(weight_matrix)

    # å¯¹æ¯ä¸ªTop-Nå’Œmin_scoreç»„åˆè¿›è¡Œå›æµ‹
    results = []
    for top_n in top_n_list:
        for min_score in min_score_list:
            # æ‰¹é‡æ„å»ºç›®æ ‡æƒé‡
            target_weights = engine.build_weights_batch(
                all_scores, top_n=top_n, min_score=min_score
            )

            # æ‰¹é‡è¿è¡Œå›æµ‹
            batch_metrics = engine.run_backtest_batch(target_weights)

            # æ„å»ºç»“æœæ¡ç›®ï¼ˆä½¿ç”¨å…¨å±€ç´¢å¼•ï¼‰
            for local_idx, (weights, metrics) in enumerate(
                zip(combos_chunk, batch_metrics)
            ):
                result_entry = {
                    "combo_idx": global_start_idx + local_idx,  # ğŸ”§ å…¨å±€ç´¢å¼•
                    "weights": tuple(f"{w:.3f}" for w in weights),
                    "top_n": top_n,
                    "min_score": min_score,
                    **metrics,
                }

                # è¿‡æ»¤æ— æ•ˆç»“æœ
                if not (np.isnan(metrics["sharpe"]) or metrics["sharpe"] < -10):
                    results.append(result_entry)

    return results


def main() -> None:
    start_time = time.time()
    args = parse_args()

    # åˆå¹¶ YAML é…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
    args = merge_config_with_args(args)

    # Sanity Runæ¨¡å¼è°ƒæ•´
    if args.sanity_run:
        args.max_total_combos = min(args.max_total_combos or 1000, 1000)
        args.batch_size = min(args.batch_size, 100)
        print("ğŸš€ Sanity Runæ¨¡å¼å·²å¯ç”¨ï¼Œé™åˆ¶ç»„åˆæ•°é‡")

    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # === 1. å› å­é€‰æ‹© ===
    if args.factors is None:
        if args.top_factors_json is None:
            raise ValueError("å¿…é¡»æŒ‡å®š --factors æˆ– --top-factors-json")
        factors = load_top_factors_from_json(args.top_factors_json, args.top_k)
    else:
        factors = args.factors
        print(f"ğŸ“‹ ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šå› å­: {', '.join(factors)}")

    # å®‰å…¨éªŒè¯ï¼šè¿‡æ»¤é»‘åå•å› å­
    factors = validate_factors_safety(list(factors))

    # å› å­åç§°æ˜ å°„ï¼šå°†ç­›é€‰ç»“æœæ˜ å°„åˆ°å®é™…é¢æ¿åˆ—å
    panel_path = Path(args.factor_panel)
    factors = map_factor_names_to_panel(factors, panel_path)

    # === 2. æ•°æ®åŠ è½½ ===
    print("ğŸ“Š å¼€å§‹åŠ è½½æ•°æ®...")
    data_dir = Path(args.data_dir)

    factor_panel = load_factor_panel(panel_path, factors)
    normalized_panel = normalize_factors(factor_panel, method=args.norm_method)
    price_pivot = load_price_pivot(data_dir)

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(factors)}ä¸ªå› å­, {len(price_pivot)}ä¸ªäº¤æ˜“æ—¥")

    # === 3. æƒé‡ç½‘æ ¼ç”Ÿæˆ ===
    if args.debug:
        print("ğŸ¯ ç”Ÿæˆæƒé‡ç½‘æ ¼...")
    all_weight_combos = generate_weight_grid_stream(
        len(factors),
        args.weight_grid,
        normalize=True,
        max_active_factors=args.max_active_factors,
        random_seed=args.random_seed,
        max_total_combos=args.max_total_combos,
        debug=args.debug,
    )

    if not all_weight_combos:
        raise SystemExit("æœªç”Ÿæˆä»»ä½•æƒé‡ç»„åˆï¼Œè¯·æ£€æŸ¥å‚æ•°è®¾ç½®ã€‚")

    # === 4. åˆ†æ‰¹å¤„ç† ===
    total_combos = len(all_weight_combos)
    batch_combos = generate_batch_combos(
        all_weight_combos, args.batch_size, args.batch_idx
    )

    print(f"ğŸ“¦ æ‰¹æ¬¡ {args.batch_idx}: å¤„ç† {len(batch_combos)}/{total_combos} ç»„åˆ")

    # === 5. åˆ›å»ºè¾“å‡ºç›®å½• ===
    output_dir, csv_file = create_output_directory(args.output, timestamp)

    # === 6. æ‰¹é‡å‘é‡åŒ–å›æµ‹ï¼ˆæ”¯æŒè´¹ç‡æ•æ„Ÿæ€§åˆ†æï¼‰===
    all_results: List[Dict[str, float]] = []

    # ğŸ”§ è®¡ç®—å½“å‰batchåœ¨å…¨å±€ç»„åˆä¸­çš„èµ·å§‹ç´¢å¼•ï¼ˆè€ƒè™‘batch_idxï¼‰
    global_combo_offset = args.batch_idx * args.batch_size

    total_tasks = (
        len(batch_combos)
        * len(args.top_n_list)
        * len(args.min_score_list)
        * len(args.fees)
    )
    print(f"ğŸ¯ æ€»ä»»åŠ¡æ•°: {total_tasks} (æƒé‡ç»„åˆ Ã— Top-N Ã— min_score Ã— fees)")
    print(
        f"ğŸ”¢ å…¨å±€combo_idxèŒƒå›´: [{global_combo_offset}, {global_combo_offset + len(batch_combos) - 1}]"
    )
    print(f"ğŸ’° è´¹ç‡åˆ—è¡¨: {args.fees}")

    # å¤–å±‚å¾ªç¯ï¼šéå†è´¹ç‡åˆ—è¡¨
    for fee_idx, current_fee in enumerate(args.fees):
        print(f"\n{'='*60}")
        print(f"ğŸ’° è´¹ç‡ {fee_idx+1}/{len(args.fees)}: {current_fee:.4f}")
        print(f"{'='*60}")

        results: List[Dict[str, float]] = []
        failed_count = 0

        if args.num_workers > 1:
            # === å¤šè¿›ç¨‹å¹¶è¡Œæ¨¡å¼ ===
            print(f"ğŸš€ å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ: {args.num_workers} workers")

            # å°†ç»„åˆåˆ†å—ï¼Œè®°å½•æ¯å—çš„å…¨å±€èµ·å§‹ç´¢å¼•ï¼ˆåŠ ä¸Šbatchåç§»ï¼‰
            chunk_size = max(1, len(batch_combos) // args.num_workers)
            chunks_with_idx = []
            for i in range(0, len(batch_combos), chunk_size):
                chunk = batch_combos[i : i + chunk_size]
                global_start = (
                    global_combo_offset + i
                )  # ğŸ”§ å…¨å±€ç´¢å¼• = batchåç§» + å—å†…åç§»
                chunks_with_idx.append((chunk, global_start))

            print(f"ğŸ“¦ åˆ†ä¸º {len(chunks_with_idx)} ä¸ªå—ï¼Œæ¯å—çº¦ {chunk_size} ä¸ªç»„åˆ")

            # å‡†å¤‡workerå‚æ•°ï¼ˆä¸åŒ…å«chunkå’Œglobal_start_idxï¼‰
            worker_fn = partial(
                run_backtest_worker,
                factors=factors,
                normalized_panel=normalized_panel,
                price_pivot=price_pivot,
                top_n_list=args.top_n_list,
                min_score_list=args.min_score_list,
                fees=current_fee,
                init_cash=args.init_cash,
                freq=args.freq,
            )

            # å¤šè¿›ç¨‹æ‰§è¡Œ
            with ProcessPoolExecutor(max_workers=args.num_workers) as executor:
                # æäº¤ä»»åŠ¡æ—¶ä¼ å…¥chunkå’Œglobal_start_idx
                futures = [
                    executor.submit(worker_fn, chunk, global_start)
                    for chunk, global_start in chunks_with_idx
                ]

                with tqdm(
                    total=len(futures),
                    desc=f"æ‰¹æ¬¡{args.batch_idx}-è´¹ç‡{current_fee:.4f}",
                ) as pbar:
                    for future in as_completed(futures):
                        try:
                            chunk_results = future.result()
                            results.extend(chunk_results)
                            pbar.update(1)
                        except Exception as e:
                            if args.debug:
                                print(f"\nâš ï¸ Workerå¤±è´¥: {e}")
                            failed_count += (
                                chunk_size
                                * len(args.top_n_list)
                                * len(args.min_score_list)
                            )
                            pbar.update(1)

            # è¡¥å……batch_idxã€factorså’Œfeeå­—æ®µ
            for result in results:
                result["batch_idx"] = args.batch_idx
                result["factors"] = factors
                result["timestamp"] = timestamp
                result["fee"] = current_fee

        else:
            # === å•è¿›ç¨‹æ¨¡å¼ ===
            print("âš¡ åˆå§‹åŒ–å‘é‡åŒ–å›æµ‹å¼•æ“...")
            engine = VectorizedBacktestEngine(
                normalized_panel=normalized_panel,
                price_pivot=price_pivot,
                factors=factors,
                fees=current_fee,
                init_cash=args.init_cash,
                freq=args.freq,
            )

            print(f"ğŸš€ å¼€å§‹å‘é‡åŒ–å›æµ‹ {len(batch_combos)} ä¸ªç»„åˆ...")

            # å°†æƒé‡ç»„åˆè½¬ä¸ºnumpyæ•°ç»„
            weight_matrix = np.array(batch_combos, dtype=np.float32)

            # æ‰¹é‡è®¡ç®—æ‰€æœ‰ç»„åˆçš„å¾—åˆ†çŸ©é˜µ
            print("ğŸ“Š æ‰¹é‡è®¡ç®—å› å­å¾—åˆ†...")
            all_scores = engine.compute_scores_batch(weight_matrix)

            task_count = (
                len(batch_combos) * len(args.top_n_list) * len(args.min_score_list)
            )
            with tqdm(
                total=task_count, desc=f"æ‰¹æ¬¡{args.batch_idx}-è´¹ç‡{current_fee:.4f}"
            ) as pbar:
                for top_n in args.top_n_list:
                    for min_score in args.min_score_list:
                        try:
                            # æ‰¹é‡æ„å»ºç›®æ ‡æƒé‡
                            target_weights = engine.build_weights_batch(
                                all_scores, top_n=top_n, min_score=min_score
                            )

                            # æ‰¹é‡è¿è¡Œå›æµ‹
                            batch_metrics = engine.run_backtest_batch(target_weights)

                            # æ„å»ºç»“æœæ¡ç›®ï¼ˆä½¿ç”¨å…¨å±€ç´¢å¼•ï¼‰
                            for local_idx, (weights, metrics) in enumerate(
                                zip(batch_combos, batch_metrics)
                            ):
                                result_entry = {
                                    "batch_idx": args.batch_idx,
                                    "combo_idx": global_combo_offset
                                    + local_idx,  # ğŸ”§ å…¨å±€ç´¢å¼•
                                    "weights": tuple(f"{w:.3f}" for w in weights),
                                    "factors": factors,
                                    "top_n": top_n,
                                    "min_score": min_score,
                                    "timestamp": timestamp,
                                    "fee": current_fee,
                                    **metrics,
                                }

                                # è¿‡æ»¤æ— æ•ˆç»“æœ
                                if not (
                                    np.isnan(metrics["sharpe"])
                                    or metrics["sharpe"] < -10
                                ):
                                    results.append(result_entry)
                                else:
                                    failed_count += 1

                                pbar.update(1)

                        except Exception as e:
                            if args.debug:
                                print(
                                    f"\nâš ï¸ Top-N={top_n}, min_score={min_score} æ‰¹æ¬¡å¤±è´¥: {e}"
                                )
                            failed_count += len(batch_combos)
                            pbar.update(len(batch_combos))
                            continue

        # åˆå¹¶å½“å‰è´¹ç‡çš„ç»“æœåˆ°æ€»ç»“æœ
        all_results.extend(results)
        print(f"âœ… è´¹ç‡ {current_fee:.4f} å®Œæˆ: {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")

    # === 8. ç»“æœæ±‡æ€» ===
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ æ‰€æœ‰è´¹ç‡å›æµ‹å®Œæˆ: {len(all_results)}ä¸ªæœ‰æ•ˆç»“æœ")
    print(f"{'='*60}")

    if not all_results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›æµ‹ç»“æœ")
        return

    result_df = pd.DataFrame(all_results)
    result_df = result_df.sort_values("sharpe", ascending=False)

    # === 9. è¾“å‡ºç»“æœ ===
    execution_time = time.time() - start_time

    print("=" * 100)
    print(f"ğŸ† æš´åŠ›æšä¸¾å›æµ‹ç»“æœ - æ‰¹æ¬¡ {args.batch_idx} (æŒ‰å¤æ™®é™åº)")
    print("=" * 100)
    print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’ | æœ‰æ•ˆç»„åˆ: {len(all_results)}")
    print(
        f"å‚æ•°èŒƒå›´: {len(args.top_n_list)}ä¸ªTop-N x {len(args.min_score_list)}ä¸ªmin-score x {len(args.fees)}ä¸ªè´¹ç‡"
    )
    print("-" * 100)

    display_cols = [
        "weights",
        "top_n",
        "fee",
        "annual_return",
        "max_drawdown",
        "sharpe",
        "calmar",
        "win_rate",
        "turnover",
    ]

    top_results = result_df.head(20)[display_cols]
    print(top_results.to_string(index=False, float_format=lambda x: f"{x: .4f}"))

    # æ ¹æ®å‚æ•°è£å‰ªè¾“å‡ºè§„æ¨¡
    filtered_df = result_df
    if args.top_k_results is not None:
        if args.top_k_results <= 0:
            print("âš ï¸ top_k_results <= 0ï¼Œå¿½ç•¥è¯¥å‚æ•°ï¼Œä¿ç•™å…¨éƒ¨ç»“æœã€‚")
        else:
            keep_count = min(args.top_k_results, len(result_df))
            filtered_df = result_df.head(keep_count).copy()
            if keep_count < len(result_df):
                print(
                    f"ğŸ“‰ ä»…ä¿ç•™å¤æ™®æœ€é«˜çš„å‰ {keep_count} ä¸ªç»“æœç”¨äºè¾“å‡ºï¼ˆåŸå§‹ {len(result_df)} ä¸ªï¼‰ã€‚"
                )

    # ä¿å­˜CSVç»“æœ (é»˜è®¤ä¿å­˜)
    filtered_df.to_csv(csv_file, index=False)
    if args.output:
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
    else:
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°é»˜è®¤è·¯å¾„: {csv_file}")

    # ä¿å­˜è¯¦ç»†JSONï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.keep_metrics_json:
        json_file = output_dir / f"detailed_results_{timestamp}.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "execution_info": {
                        "timestamp": timestamp,
                        "execution_time": execution_time,
                        "batch_idx": args.batch_idx,
                        "batch_size": args.batch_size,
                        "total_combos": total_combos,
                        "valid_results": len(results),
                        "failed_count": failed_count,
                    },
                    "parameters": {
                        "factors": factors,
                        "weight_grid": args.weight_grid,
                        "top_n_list": args.top_n_list,
                        "min_score_list": args.min_score_list,
                        "fees": args.fees,
                        "norm_method": args.norm_method,
                    },
                    "results": filtered_df.to_dict("records"),
                },
                f,
                indent=2,
                ensure_ascii=False,
            )
        print(f"ğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {json_file}")

    # ä¿å­˜æ£€æŸ¥ç‚¹ (é»˜è®¤ä¿å­˜)
    checkpoint_file = output_dir / f"checkpoint_batch_{args.batch_idx}.json"
    save_checkpoint(filtered_df.to_dict("records"), checkpoint_file)
    if args.output:
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_file}")
    else:
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°é»˜è®¤è·¯å¾„: {checkpoint_file}")

    print(f"\nâœ… æ‰¹æ¬¡ {args.batch_idx} å®Œæˆ!")
    if args.batch_idx * args.batch_size + len(batch_combos) < total_combos:
        next_batch = args.batch_idx + 1
        print(f"ğŸ”„ ç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡: python {__file__} --batch-idx {next_batch}")
    else:
        print("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å·²å®Œæˆ!")


def run_regression_tests() -> None:
    """å›å½’æµ‹è¯•ï¼šéªŒè¯å…³é”®ä¿®å¤æ˜¯å¦ç”Ÿæ•ˆï¼ˆçœŸå®å¼•æ“æµ‹è¯•ï¼‰

    æµ‹è¯•é¡¹ï¼š
    1. æ”¶ç›Šç‡è®¡ç®—ï¼šçœŸå®å¼•æ“éªŒè¯æ— çˆ†ç‚¸æ”¶ç›Š
    2. æ‰¹é‡å›æµ‹ï¼šå®é™…è·‘å®Œæ•´æµç¨‹
    3. è·¨batchç¼–å·ï¼šéªŒè¯ batch_idx > 0 æ—¶ combo_idx æ­£ç¡®
    """
    print("ğŸ§ª è¿è¡Œå›å½’æµ‹è¯•ï¼ˆçœŸå®å¼•æ“ï¼‰...")

    # === æµ‹è¯•1ï¼šæ”¶ç›Šç‡è®¡ç®—ï¼ˆçœŸå®å¼•æ“ï¼‰===
    print("\n[æµ‹è¯•1] æ”¶ç›Šç‡è®¡ç®—å®‰å…¨æ€§ï¼ˆå«å¼‚å¸¸æ•°æ®ï¼‰")

    # æ„é€ æœ‰ç¼ºå¤±å€¼çš„ä»·æ ¼æ•°æ®
    test_dates = pd.date_range("2020-01-01", periods=50)
    test_prices = pd.DataFrame(
        {
            "ETF_A": [np.nan] * 10
            + list(10.0 + np.random.randn(40) * 0.5),  # å‰10æ—¥NaNï¼ˆæœªä¸Šå¸‚ï¼‰
            "ETF_B": 100.0 + np.random.randn(50) * 2.0,  # æ­£å¸¸æ³¢åŠ¨
            "ETF_C": [50.0] * 20
            + [0.0]
            + list(52.0 + np.random.randn(29) * 1.0),  # ç¬¬21æ—¥ä»·æ ¼ä¸º0ï¼ˆå¼‚å¸¸ï¼‰
        },
        index=test_dates,
    )

    # æ„é€ å› å­é¢æ¿
    test_factors = ["FACTOR_A", "FACTOR_B"]
    symbols = ["ETF_A", "ETF_B", "ETF_C"]
    test_panel = pd.DataFrame(
        {
            "FACTOR_A": np.random.randn(len(symbols) * len(test_dates)),
            "FACTOR_B": np.random.randn(len(symbols) * len(test_dates)),
        },
        index=pd.MultiIndex.from_product(
            [symbols, test_dates], names=["symbol", "date"]
        ),
    )

    try:
        engine = VectorizedBacktestEngine(
            normalized_panel=test_panel,
            price_pivot=test_prices,
            factors=test_factors,
            fees=0.001,
            init_cash=1_000_000.0,
            freq="1D",
        )

        # æ£€æŸ¥æ”¶ç›Šç‡èŒƒå›´
        max_abs_return = np.max(np.abs(engine.returns_tensor))
        assert max_abs_return <= 1.0, f"âŒ çˆ†ç‚¸æ”¶ç›Šç‡: {max_abs_return}"
        assert not np.any(np.isnan(engine.returns_tensor)), "âŒ æ”¶ç›Šç‡ä¸­å­˜åœ¨NaN"
        assert not np.any(np.isinf(engine.returns_tensor)), "âŒ æ”¶ç›Šç‡ä¸­å­˜åœ¨Inf"

        print(f"   âœ… æ”¶ç›Šç‡èŒƒå›´æ­£å¸¸: æœ€å¤§={max_abs_return:.4f}")

    except Exception as e:
        print(f"   âŒ æ”¶ç›Šç‡æµ‹è¯•å¤±è´¥: {e}")
        raise

    # === æµ‹è¯•2ï¼šå®Œæ•´å›æµ‹æµç¨‹ ===
    print("\n[æµ‹è¯•2] å®Œæ•´å›æµ‹æµç¨‹ï¼ˆ10ç»„åˆ Ã— 2Top-Nï¼‰")

    # ç”Ÿæˆæµ‹è¯•æƒé‡ç»„åˆ
    test_combos = [
        (1.0, 0.0),
        (0.0, 1.0),
        (0.5, 0.5),
        (0.7, 0.3),
        (0.3, 0.7),
        (0.8, 0.2),
        (0.2, 0.8),
        (0.6, 0.4),
        (0.4, 0.6),
        (0.9, 0.1),
    ]

    weight_matrix = np.array(test_combos, dtype=np.float32)

    try:
        # æ‰¹é‡è®¡ç®—å¾—åˆ†
        scores = engine.compute_scores_batch(weight_matrix)
        assert scores.shape == (len(test_combos), engine.n_dates, engine.n_etfs)

        # æ‰¹é‡æ„å»ºæƒé‡å¹¶å›æµ‹
        for top_n in [2, 3]:
            target_weights = engine.build_weights_batch(scores, top_n=top_n)
            metrics = engine.run_backtest_batch(target_weights)

            assert len(metrics) == len(
                test_combos
            ), f"ç»“æœæ•°é‡ä¸åŒ¹é…: {len(metrics)} vs {len(test_combos)}"

            # æ£€æŸ¥æŒ‡æ ‡åˆç†æ€§
            for i, m in enumerate(metrics):
                assert -10 < m["sharpe"] < 10, f"ç»„åˆ{i} Sharpeå¼‚å¸¸: {m['sharpe']}"
                assert (
                    -1 < m["annual_return"] < 5
                ), f"ç»„åˆ{i} å¹´åŒ–æ”¶ç›Šå¼‚å¸¸: {m['annual_return']}"
                assert (
                    0 <= m["max_drawdown"] <= 1
                ), f"ç»„åˆ{i} æœ€å¤§å›æ’¤å¼‚å¸¸: {m['max_drawdown']}"

        print(f"   âœ… å®Œæ•´æµç¨‹æ­£å¸¸: 10ç»„åˆ Ã— 2Top-N = 20ç»“æœ")

    except Exception as e:
        print(f"   âŒ å›æµ‹æµç¨‹å¤±è´¥: {e}")
        raise

    # === æµ‹è¯•3ï¼šçœŸå®å¤šè¿›ç¨‹è·¯å¾„æµ‹è¯• ===
    print("\n[æµ‹è¯•3] çœŸå®å¤šè¿›ç¨‹è·¯å¾„ï¼ˆ2 workers Ã— 5ç»„åˆï¼‰")

    try:
        # ç”Ÿæˆ10ä¸ªæµ‹è¯•ç»„åˆ
        test_combos_mp = [
            (1.0, 0.0),
            (0.0, 1.0),
            (0.5, 0.5),
            (0.7, 0.3),
            (0.3, 0.7),
            (0.8, 0.2),
            (0.2, 0.8),
            (0.6, 0.4),
            (0.4, 0.6),
            (0.9, 0.1),
        ]

        # æ¨¡æ‹Ÿè·¨batchåœºæ™¯ï¼šbatch_idx=1ï¼ˆå…¨å±€èµ·å§‹ç´¢å¼•100ï¼‰
        global_offset_test = 100

        # åˆ†æˆ2ä¸ªchunk
        chunk_size = 5
        chunks_with_idx = [
            (test_combos_mp[0:5], global_offset_test + 0),
            (test_combos_mp[5:10], global_offset_test + 5),
        ]

        # ä½¿ç”¨çœŸå®workerå‡½æ•°
        from concurrent.futures import ProcessPoolExecutor, as_completed
        from functools import partial

        worker_fn = partial(
            run_backtest_worker,
            factors=test_factors,
            normalized_panel=test_panel,
            price_pivot=test_prices,
            top_n_list=[2],
            min_score_list=[None],
            fees=0.001,
            init_cash=1_000_000.0,
            freq="1D",
        )

        all_results = []
        try:
            with ProcessPoolExecutor(max_workers=2) as executor:
                futures = [
                    executor.submit(worker_fn, chunk, global_start)
                    for chunk, global_start in chunks_with_idx
                ]

                for future in as_completed(futures):
                    chunk_results = future.result()
                    all_results.extend(chunk_results)
        except PermissionError:
            print("   âš ï¸ ç³»ç»Ÿé™åˆ¶æ— æ³•å¯åŠ¨å¤šè¿›ç¨‹ï¼Œå›é€€åˆ°å•è¿›ç¨‹éªŒè¯ã€‚")
            for chunk, global_start in chunks_with_idx:
                all_results.extend(worker_fn(chunk, global_start))

        # éªŒè¯ç»“æœ
        assert len(all_results) == 10, f"ç»“æœæ•°é‡é”™è¯¯: {len(all_results)}"

        # æå–combo_idx
        combo_ids = [r["combo_idx"] for r in all_results]

        # æ£€æŸ¥å”¯ä¸€æ€§
        assert (
            len(set(combo_ids)) == 10
        ), f"âŒ combo_idxä¸å”¯ä¸€: {len(set(combo_ids))} unique"

        # æ£€æŸ¥èŒƒå›´æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯[100, 109]ï¼‰
        expected_range = set(range(100, 110))
        actual_range = set(combo_ids)
        assert (
            expected_range == actual_range
        ), f"âŒ combo_idxèŒƒå›´é”™è¯¯: æœŸæœ›{expected_range}, å®é™…{actual_range}"

        print(f"   âœ… å¤šè¿›ç¨‹è·¯å¾„æ­£å¸¸: 2 workers Ã— 5ç»„åˆï¼Œcombo_idx=[100, 109]å…¨éƒ¨å”¯ä¸€")

    except Exception as e:
        print(f"   âŒ å¤šè¿›ç¨‹æµ‹è¯•å¤±è´¥: {e}")
        raise

    # === æµ‹è¯•4ï¼šè·¨batchå¯å¤ç°æ€§ ===
    print("\n[æµ‹è¯•4] æƒé‡ç”Ÿæˆç¨³å®šæ€§ï¼ˆè·¨è¿è¡Œå¯å¤ç°ï¼‰")

    try:
        # ç”Ÿæˆä¸¤æ¬¡ç›¸åŒå‚æ•°çš„æƒé‡ç»„åˆ
        combos_1 = generate_weight_grid_stream(
            num_factors=3,
            weight_grid=[0.0, 0.5, 1.0],
            random_seed=42,
            max_total_combos=100,
            debug=False,
        )

        combos_2 = generate_weight_grid_stream(
            num_factors=3,
            weight_grid=[0.0, 0.5, 1.0],
            random_seed=42,
            max_total_combos=100,
            debug=False,
        )

        # æ£€æŸ¥å®Œå…¨ä¸€è‡´ï¼ˆåŒ…æ‹¬é¡ºåºï¼‰
        assert len(combos_1) == len(
            combos_2
        ), f"æ•°é‡ä¸ä¸€è‡´: {len(combos_1)} vs {len(combos_2)}"

        for i, (c1, c2) in enumerate(zip(combos_1, combos_2)):
            assert c1 == c2, f"ç´¢å¼•{i}ç»„åˆä¸ä¸€è‡´: {c1} vs {c2}"

        print(
            f"   âœ… æƒé‡ç”Ÿæˆå¯å¤ç°: 2æ¬¡è¿è¡Œäº§ç”Ÿ{len(combos_1)}ä¸ªå®Œå…¨ä¸€è‡´çš„ç»„åˆï¼ˆå«é¡ºåºï¼‰"
        )

    except Exception as e:
        print(f"   âŒ å¯å¤ç°æ€§æµ‹è¯•å¤±è´¥: {e}")
        raise

    print("\nâœ… æ‰€æœ‰å›å½’æµ‹è¯•é€šè¿‡ï¼çœŸå®å¼•æ“+å¤šè¿›ç¨‹è·¯å¾„éªŒè¯å®Œæˆã€‚")


if __name__ == "__main__":
    # å¦‚æœç¯å¢ƒå˜é‡REGRESSION_TEST=1ï¼Œè¿è¡Œå›å½’æµ‹è¯•
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        run_regression_tests()
    else:
        main()
