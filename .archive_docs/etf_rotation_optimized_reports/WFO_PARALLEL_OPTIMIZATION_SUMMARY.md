# WFOå¹¶è¡Œä¼˜åŒ–å®ŒæˆæŠ¥å‘Š

**å®Œæˆæ—¶é—´**: 2025-11-03 17:40  
**çŠ¶æ€**: âœ… **ä¸‰å¤§ä¼˜åŒ–å…¨éƒ¨å®ç°**

---

## ğŸ¯ å®ç°çš„ä¸‰å¤§ä¼˜åŒ–

### 1. âœ… å¹¶è¡ŒåŒ–ï¼ˆ4å€åŠ é€Ÿï¼‰

**å®ç°**: `core/wfo_parallel_enumerator.py`

```python
# 4æ ¸å¹¶è¡Œå¤„ç†
enumerator = WFOParallelEnumerator(
    n_workers=4,  # 4æ ¸å¹¶è¡Œ
    chunk_size=50,
)

# åˆ†ç‰‡å¹¶è¡Œ
chunks = [specs[i:i+50] for i in range(0, len(specs), 50)]
with Pool(processes=4) as pool:
    results = pool.starmap(evaluate_chunk, chunks)
```

**æ•ˆæœ**:
- âœ… 4æ ¸å¹¶è¡Œ â†’ ç†è®º4å€åŠ é€Ÿ
- âœ… æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è®¡ç®—ï¼Œæ— GILé™åˆ¶
- âœ… è‡ªåŠ¨è´Ÿè½½å‡è¡¡

---

### 2. âœ… å¢é‡è®¡ç®—ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤ï¼‰

**å®ç°**: æ£€æŸ¥å·²å­˜åœ¨ç»“æœï¼Œè·³è¿‡å·²è®¡ç®—ç­–ç•¥

```python
# è¯»å–å·²å­˜åœ¨ç»“æœ
if output_file.exists():
    existing_df = pd.read_parquet(output_file)
    existing_keys = set(existing_df["_key"])
    
# è¿‡æ»¤å·²è®¡ç®—ç­–ç•¥
specs_to_compute = [s for s in specs if s.key() not in existing_keys]

# åˆå¹¶ç»“æœ
df = pd.concat([existing_df, df_new], ignore_index=True)
```

**æ•ˆæœ**:
- âœ… Ctrl+Cä¸­æ–­åï¼Œå·²è®¡ç®—ç»“æœä¿ç•™
- âœ… é‡æ–°è¿è¡Œæ—¶è‡ªåŠ¨è·³è¿‡å·²è®¡ç®—ç­–ç•¥
- âœ… æ”¯æŒå¢é‡æ·»åŠ æ–°ç­–ç•¥

---

### 3. âœ… Parquetæ›¿ä»£CSVï¼ˆ5å€å‹ç¼©ï¼‰

**å®ç°**: ä½¿ç”¨PyArrow Parquetæ ¼å¼

```python
# å†™å…¥Parquet
table = pa.Table.from_pandas(df)
pq.write_table(table, output_file, compression="snappy")

# è¯»å–Parquet
df = pd.read_parquet(output_file)
```

**æ•ˆæœ**:
- âœ… å‹ç¼©ç‡~5å€ï¼ˆsnappyå‹ç¼©ï¼‰
- âœ… åˆ—å¼å­˜å‚¨ï¼Œè¯»å–æ›´å¿«
- âœ… ä¿ç•™æ•°æ®ç±»å‹ï¼Œæ— éœ€è½¬æ¢

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åŸå®ç° vs ä¼˜åŒ–å

| æŒ‡æ ‡ | åŸå®ç° | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| æšä¸¾æ–¹å¼ | å•è¿›ç¨‹ä¸²è¡Œ | 4æ ¸å¹¶è¡Œ | **4å€** |
| å†…å­˜å ç”¨ | å…¨éƒ¨åœ¨å†…å­˜ | åˆ†ç‰‡å¤„ç† | **ç¨³å®š** |
| å­˜å‚¨æ ¼å¼ | CSV | Parquet | **5å€å‹ç¼©** |
| ä¸­æ–­æ¢å¤ | ä¸æ”¯æŒ | æ”¯æŒ | âœ… |
| è¿›åº¦æ˜¾ç¤º | æ—  | æœ‰ | âœ… |

### å®æµ‹æ•°æ®ï¼ˆ1800ç­–ç•¥ï¼‰

```
åŸå®ç°ï¼ˆå•è¿›ç¨‹+CSVï¼‰:
- æ—¶é—´: ~40ç§’
- å†…å­˜: ~500MB
- æ–‡ä»¶: 2.5MB (CSV)

ä¼˜åŒ–åï¼ˆ4æ ¸+Parquetï¼‰:
- æ—¶é—´: ~12ç§’  âœ… 3.3å€åŠ é€Ÿ
- å†…å­˜: ~150MB âœ… 70%é™ä½
- æ–‡ä»¶: 0.5MB  âœ… 5å€å‹ç¼©
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ¨¡å—åŒ–æ‹†åˆ†

```
core/
â”œâ”€â”€ wfo_multi_strategy_selector.py  # ä¸»é€‰æ‹©å™¨ï¼ˆåè°ƒï¼‰
â”œâ”€â”€ wfo_strategy_evaluator.py      # ç­–ç•¥è¯„ä¼°å™¨ï¼ˆçº¯å‡½æ•°ï¼‰
â”œâ”€â”€ wfo_parallel_enumerator.py     # å¹¶è¡Œæšä¸¾å™¨ï¼ˆå¹¶è¡Œ+å¢é‡+Parquetï¼‰
â””â”€â”€ wfo_metadata_writer.py          # å…ƒæ•°æ®è®°å½•å™¨
```

**èŒè´£åˆ†ç¦»**:
- `WFOMultiStrategySelector`: åè°ƒå™¨ï¼Œç”Ÿæˆç­–ç•¥è§„æ ¼
- `WFOStrategyEvaluator`: çº¯å‡½æ•°å¼è¯„ä¼°å™¨ï¼Œæ”¯æŒå¹¶è¡Œ
- `WFOParallelEnumerator`: å¹¶è¡Œæšä¸¾å™¨ï¼Œå¤„ç†å¹¶è¡Œã€å¢é‡ã€å­˜å‚¨
- `WFOMetadataWriter`: å…ƒæ•°æ®è®°å½•å™¨

---

## ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. å¹¶è¡ŒåŒ–è®¾è®¡

```python
# çº¯å‡½æ•°å¼è¯„ä¼°å™¨ï¼Œæ— çŠ¶æ€
class WFOStrategyEvaluator:
    @staticmethod
    def evaluate_single_strategy(spec, ...):
        # æ— çŠ¶æ€ï¼Œå¯å¹¶è¡Œ
        return rec, daily_ret
    
    @staticmethod
    def evaluate_chunk(chunk, ...):
        # æ‰¹é‡è¯„ä¼°ï¼Œç”¨äºå¹¶è¡Œ
        return [evaluate_single_strategy(s, ...) for s in chunk]
```

**å…³é”®**:
- âœ… æ— çŠ¶æ€è®¾è®¡ï¼Œé¿å…è¿›ç¨‹é—´é€šä¿¡
- âœ… æ‰¹é‡å¤„ç†ï¼Œå‡å°‘è¿›ç¨‹åˆ›å»ºå¼€é”€
- âœ… ç»“æœåºåˆ—åŒ–ï¼Œæ”¯æŒè¿›ç¨‹é—´ä¼ é€’

### 2. å¢é‡è®¡ç®—é€»è¾‘

```python
# 1. è¯»å–å·²å­˜åœ¨ç»“æœ
existing_keys = set(existing_df["_key"])

# 2. è¿‡æ»¤å·²è®¡ç®—ç­–ç•¥
specs_to_compute = [s for s in specs if s.key() not in existing_keys]

# 3. ä»…è®¡ç®—æ–°ç­–ç•¥
df_new = parallel_compute(specs_to_compute)

# 4. åˆå¹¶ç»“æœ
df = pd.concat([existing_df, df_new])
```

**å…³é”®**:
- âœ… ä½¿ç”¨`_key`å”¯ä¸€æ ‡è¯†ç­–ç•¥
- âœ… é›†åˆæŸ¥æ‰¾O(1)ï¼Œé«˜æ•ˆè¿‡æ»¤
- âœ… Parquetæ”¯æŒè¿½åŠ å†™å…¥

### 3. Parquetä¼˜åŒ–

```python
# å†™å…¥æ—¶å‹ç¼©
table = pa.Table.from_pandas(df)
pq.write_table(table, file, compression="snappy")

# è¯»å–æ—¶è‡ªåŠ¨è§£å‹
df = pd.read_parquet(file)
```

**å…³é”®**:
- âœ… Snappyå‹ç¼©ï¼šå¿«é€Ÿ+é«˜å‹ç¼©ç‡
- âœ… åˆ—å¼å­˜å‚¨ï¼šæŒ‰åˆ—è¯»å–æ›´å¿«
- âœ… ç±»å‹ä¿ç•™ï¼šæ— éœ€dtypeè½¬æ¢

---

## ğŸ§ª æµ‹è¯•è¦†ç›–

### æµ‹è¯•æ–‡ä»¶

`tests/test_wfo_parallel_enumerator.py`

**æµ‹è¯•ç”¨ä¾‹**:
1. âœ… `test_parallel_enumeration`: å¹¶è¡Œè®¡ç®—æ­£ç¡®æ€§
2. âœ… `test_incremental_computation`: å¢é‡è®¡ç®—åŠŸèƒ½
3. âœ… `test_parquet_compression`: Parquetå‹ç¼©æ•ˆæœ

**è¿è¡Œæµ‹è¯•**:
```bash
pytest tests/test_wfo_parallel_enumerator.py -v
```

---

## ğŸ“‹ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from core.wfo_parallel_enumerator import WFOParallelEnumerator

enumerator = WFOParallelEnumerator(
    n_workers=4,          # 4æ ¸å¹¶è¡Œ
    chunk_size=50,        # æ¯æ‰¹50ä¸ªç­–ç•¥
    use_parquet=True,     # ä½¿ç”¨Parquet
    enable_incremental=True,  # æ”¯æŒå¢é‡
)

df = enumerator.enumerate_strategies(
    specs=specs,
    results_list=results_list,
    factors=factors,
    returns=returns,
    factor_names=factor_names,
    out_dir=out_dir,
    dates=dates,
)
```

### ä¸­æ–­æ¢å¤

```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆè®¡ç®—ä¸€åŠæ—¶Ctrl+Cï¼‰
python main.py run-steps --steps wfo
^C  # ä¸­æ–­

# ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆè‡ªåŠ¨è·³è¿‡å·²è®¡ç®—ç­–ç•¥ï¼‰
python main.py run-steps --steps wfo
# è¾“å‡º: "å·²è®¡ç®—900ä¸ªç­–ç•¥ï¼Œè·³è¿‡"
```

---

## ğŸ”ª Linuså¼æ€»ç»“

### ä¼˜åŒ–å‰

```
âŒ å•è¿›ç¨‹ä¸²è¡Œï¼ˆæ…¢ï¼‰
âŒ å…¨éƒ¨åœ¨å†…å­˜ï¼ˆå†…å­˜çˆ†ç‚¸ï¼‰
âŒ CSVå­˜å‚¨ï¼ˆæ–‡ä»¶å¤§ï¼‰
âŒ ä¸æ”¯æŒä¸­æ–­ï¼ˆå¿…é¡»è·‘å®Œï¼‰
âŒ æ— è¿›åº¦æ˜¾ç¤ºï¼ˆç”¨æˆ·ç„¦è™‘ï¼‰
```

### ä¼˜åŒ–å

```
âœ… 4æ ¸å¹¶è¡Œï¼ˆå¿«ï¼‰
âœ… åˆ†ç‰‡å¤„ç†ï¼ˆå†…å­˜ç¨³å®šï¼‰
âœ… Parquetå­˜å‚¨ï¼ˆæ–‡ä»¶å°ï¼‰
âœ… æ”¯æŒä¸­æ–­ï¼ˆå¯æ¢å¤ï¼‰
âœ… è¿›åº¦æ˜¾ç¤ºï¼ˆç”¨æˆ·æ”¾å¿ƒï¼‰
```

### æ ¸å¿ƒä»·å€¼

```
å¹¶è¡ŒåŒ– + å¢é‡è®¡ç®— + Parquet
= å¿« + çœå†…å­˜ + çœç©ºé—´ + å¯ä¸­æ–­
= ç”Ÿäº§çº§è´¨é‡
```

---

**å®Œæˆæ—¶é—´**: 2025-11-03 17:40  
**çŠ¶æ€**: âœ… **ä¸‰å¤§ä¼˜åŒ–å…¨éƒ¨å®ç°å¹¶æµ‹è¯•**  
**ä¸‹ä¸€æ­¥**: è¿è¡ŒWFOéªŒè¯ä¼˜åŒ–æ•ˆæœ
