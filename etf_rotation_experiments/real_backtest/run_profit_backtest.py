#!/usr/bin/env python3
"""
ç›ˆåˆ©ä¼˜å…ˆå›æµ‹ï¼ˆä¸æ”¹ç¨³å®šä»“ï¼‰ï¼šåœ¨ç¨³å®šå›æµ‹åŸºç¡€ä¸Šå åŠ â€œå¸¸æ•°æ»‘ç‚¹â€å¹¶å¯é€‰ç”¨åˆ©æ¶¦æ ¡å‡†å™¨æ’åºã€‚

åŠŸèƒ½è¦ç‚¹
--------
1) è¯»å–æœ€æ–° WFO run_* ç›®å½•çš„ TopK ç»„åˆï¼ˆå¯é€‰ ALLï¼‰
2) å¤ç”¨ç¨³å®šä»“æ•°æ®åŠ è½½/å› å­/æ¨ªæˆªé¢å¤„ç†
3) è°ƒç”¨ç¨³å®šå›æµ‹ backtest_no_lookahead è·å¾—â€œå«ä½£é‡‘â€çš„åŸºçº¿ç»“æœ
4) åŸºäºè°ƒä»“æ¢æ‰‹åºåˆ—ä¸NAVï¼ŒæŒ‰å¸¸æ•°æ»‘ç‚¹(åŸºç‚¹)åšâ€œäº‹åæ‰£å‡æ ¡æ­£â€ï¼Œå¾—åˆ°å‡€æ”¶ç›Šæ›²çº¿ä¸æŒ‡æ ‡
5) è‹¥å­˜åœ¨åˆ©æ¶¦æ ¡å‡†å™¨(results/calibrator_gbdt_profit.joblib)ï¼Œåˆ™å…ˆæŒ‰é¢„æµ‹å¹´åŒ–é™åºé€‰ TopK

çº¦æŸ
----
- ä¸æ”¹ç¨³å®šé¡¹ç›®ä»£ç ï¼Œä»…åœ¨ experiments ä¸­æ–°å¢è„šæœ¬
- æ»‘ç‚¹ä¸ºâ€œè°ƒä»“äº‹ä»¶ä¸Šçš„ç¬æ—¶æˆæœ¬â€ï¼Œä»¥è°ƒä»“å‰ä½™é¢ä¸ºåŸºæ•°ï¼šextra_cost = slippage_rate * turnover * P_before_cost
- P_before_cost = NAV_at_offset + commission_value_at_offsetï¼ˆç”¨ cost_amount_series è¿˜åŸï¼‰
"""
from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple, List, Optional

import numpy as np
import pandas as pd
from tqdm import tqdm

# Suppress verbose logging from run_production_backtest
logging.basicConfig(level=logging.WARNING)
logging.getLogger('real_backtest.run_production_backtest').setLevel(logging.WARNING)


def _ensure_stable_paths():
    """å°†ç¨³å®šä»“åŠ å…¥ sys.pathï¼Œä¿è¯å¯å¯¼å…¥å…¶æ¨¡å—ä¸è„šæœ¬ã€‚"""
    here = Path(__file__).resolve()
    repo_root = here.parents[2]
    stable_root = repo_root / "etf_rotation_optimized"
    stable_rb = stable_root / "real_backtest"
    for p in (stable_root, stable_rb):
        sp = str(p.resolve())
        if sp not in sys.path:
            sys.path.insert(0, sp)


_ensure_stable_paths()

# ä¾èµ–ç¨³å®šä»“çš„æ¨¡å—
from core.cross_section_processor import CrossSectionProcessor  # type: ignore
from core.data_loader import DataLoader  # type: ignore
from core.precise_factor_library_v2 import PreciseFactorLibrary  # type: ignore
from real_backtest.run_production_backtest import (  # type: ignore
    backtest_no_lookahead,
)


def load_config_candidates() -> Tuple[Path, dict]:
    """å¯»æ‰¾ combo_wfo_config.yamlï¼ˆæ”¯æŒ RB_CONFIG_FILE è¦†ç›–ï¼‰ã€‚"""
    candidates: List[Path] = []
    env_cfg = os.environ.get("RB_CONFIG_FILE", "").strip()
    if env_cfg:
        candidates.append(Path(env_cfg).expanduser().resolve())
    here = Path(__file__).resolve()
    # experiments ä¸‹ç›¸å¯¹è·¯å¾„ä¼˜å…ˆ
    candidates.append((here.parent.parent / "configs" / "combo_wfo_config.yaml").resolve())
    # ç¨³å®šä»“å›é€€
    repo_root = here.parents[2]
    candidates.append((repo_root / "etf_rotation_optimized" / "configs" / "combo_wfo_config.yaml").resolve())
    cfg = next((p for p in candidates if p.exists()), None)
    if cfg is None:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œå°è¯•: {candidates}")
    import yaml

    with open(cfg, "r") as f:
        conf = yaml.safe_load(f)
    return cfg, conf


def find_latest_run_dir() -> Path:
    """åœ¨å¤šä¸ªå€™é€‰è·¯å¾„ä¸‹æŸ¥æ‰¾æœ€æ–°çš„ results/run_* ç›®å½•ã€‚"""
    here = Path(__file__).resolve()
    roots: List[Path] = []
    # ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    env_root = os.environ.get("RB_WFO_ROOT", "").strip()
    if env_root:
        p = Path(env_root).expanduser().resolve()
        env_base = p.parent if p.name.startswith("run_") else p
        if env_base.exists():
            env_runs = sorted([d.resolve() for d in env_base.glob("run_*") if d.is_dir()], reverse=True)
            if env_runs:
                return env_runs[0]
        roots.append(env_base)
    # experiments çš„ results
    roots.append((here.parent.parent / "results").resolve())
    roots.append((here.parent.parent.parent / "results").resolve())
    # ç¨³å®šä»“çš„ results
    repo_root = here.parents[2]
    roots.append((repo_root / "etf_rotation_optimized" / "results").resolve())
    uniq = [r for r in roots if r.exists()]
    run_dirs: List[Path] = []
    for r in uniq:
        run_dirs.extend([d for d in r.glob("run_*") if d.is_dir()])
    if not run_dirs:
        raise FileNotFoundError(f"æœªæ‰¾åˆ° run_* ç›®å½•ã€‚å·²æ£€æŸ¥: {uniq}")
    def _extract_ts(path: Path) -> str:
        name = path.name
        return name.replace("run_", "") if name.startswith("run_") else name

    run_dirs = sorted({d.resolve() for d in run_dirs}, key=_extract_ts, reverse=True)
    return run_dirs[0]


def _detect_scoring_strategy(run_dir: Path) -> str:
    summary_path = run_dir / "wfo_summary.json"
    if summary_path.exists():
        try:
            summary = json.loads(summary_path.read_text(encoding="utf-8"))
            strat = summary.get("scoring_strategy")
            if isinstance(strat, str) and strat:
                return strat
        except Exception:
            pass
    for candidate in ("oos_sharpe_true", "oos_sharpe_proxy", "ic"):
        if (run_dir / f"top100_by_{candidate}.parquet").exists():
            return candidate
    return "ic"


def load_top_combos_from_run(run_dir: Path, top_n: Optional[int] = None, load_all: bool = False) -> Tuple[pd.DataFrame, str]:
    """ä¸ç¨³å®šç‰ˆä¿æŒå…¼å®¹ï¼šä¼˜å…ˆç­–ç•¥å¯¹åº”æ–‡ä»¶ â†’ top_combos â†’ all_combos å¹¶æ’åºã€‚"""
    strategy = _detect_scoring_strategy(run_dir)
    top_by_strategy = run_dir / f"top100_by_{strategy}.parquet"
    top_by_ic = run_dir / "top100_by_ic.parquet"
    top_combos = run_dir / "top_combos.parquet"
    all_combos = run_dir / "all_combos.parquet"

    def _sort_df(df: pd.DataFrame, strat: str) -> pd.DataFrame:
        if "calibrated_sharpe_pred" in df.columns:
            return df.sort_values(by=["calibrated_sharpe_pred", "stability_score"], ascending=[False, False])
        if "calibrated_sharpe_full" in df.columns:
            return df.sort_values(by=["calibrated_sharpe_full", "stability_score"], ascending=[False, False])
        if strat == "oos_sharpe_true" and "mean_oos_sharpe" in df.columns:
            cols = ["mean_oos_sharpe", "stability_score", "oos_sharpe_proxy", "mean_oos_ic"]
            existing = [c for c in cols if c in df.columns]
            return df.sort_values(by=existing, ascending=[False] * len(existing))
        if strat != "ic" and "oos_sharpe_proxy" in df.columns:
            cols = ["oos_sharpe_proxy", "stability_score", "mean_oos_ic"]
            existing = [c for c in cols if c in df.columns]
            if existing:
                return df.sort_values(by=existing, ascending=[False] * len(existing))
        return df.sort_values(by=["mean_oos_ic", "stability_score"], ascending=[False, False])

    if load_all or top_n is None:
        if not all_combos.exists():
            raise FileNotFoundError(f"å…¨é‡æ¨¡å¼éœ€è¦ all_combos.parquetï¼Œä½†æœªæ‰¾åˆ°: {all_combos}")
        df = pd.read_parquet(all_combos)
        return _sort_df(df, strategy).reset_index(drop=True), f"ALL ({len(df)})"

    candidate_paths: List[Tuple[Path, str]] = []
    if top_by_strategy.exists():
        candidate_paths.append((top_by_strategy, f"top100_by_{strategy}"))
    if strategy != "ic" and top_by_ic.exists():
        candidate_paths.append((top_by_ic, "top100_by_ic"))
    if top_combos.exists():
        candidate_paths.append((top_combos, "top_combos"))
    if all_combos.exists():
        candidate_paths.append((all_combos, "from_all_combos"))

    for path, label in candidate_paths:
        df = pd.read_parquet(path).reset_index(drop=True)
        df = _sort_df(df, strategy)
        if label.startswith("top100") and len(df) < top_n and all_combos.exists():
            all_df = pd.read_parquet(all_combos)
            return _sort_df(all_df, strategy).head(top_n).reset_index(drop=True), f"{label}(fallback_all)"
        if len(df) >= top_n or not label.startswith("top100"):
            return df.head(top_n).reset_index(drop=True), label

    raise FileNotFoundError(f"æœªæ‰¾åˆ° {run_dir} ä¸‹çš„ top100/top_combos/all_combos æ–‡ä»¶")


def maybe_apply_profit_calibrator(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:
    """è‹¥å­˜åœ¨åˆ©æ¶¦æ ¡å‡†å™¨ï¼Œè¿”å›æŒ‰é¢„æµ‹å¹´åŒ–é™åºçš„ DataFrameã€‚"""
    model_path = Path(__file__).parent.parent / "results" / "calibrator_gbdt_profit.joblib"
    if not model_path.exists():
        return df, "IC_or_calibrated_default"
    try:
        import joblib

        obj = joblib.load(model_path)
        model = obj["model"]
        scaler = obj["scaler"]
        feat_names = obj["feature_names"]
        # ç¼ºå¤±åˆ—è¡¥ NaN/å¸¸æ•°åå†å¡«å……ä¸­ä½æ•°
        data = df.copy()
        for c in feat_names:
            if c not in data.columns:
                data[c] = np.nan
        X = data[feat_names].copy()
        for c in X.columns:
            if X[c].isna().any():
                X[c].fillna(X[c].median(), inplace=True)
        Xs = scaler.transform(X.values)
        y_pred = model.predict(Xs)
        data["calibrated_annual_pred"] = y_pred
        data = data.sort_values(by=["calibrated_annual_pred", "stability_score"], ascending=[False, False]).reset_index(drop=True)
        return data, "profit_calibrated"
    except Exception:
        return df, "IC_or_calibrated_default"


def apply_slippage_to_nav(result: Dict, slippage_rate: float, freq: int) -> Dict:
    """
    åŸºäºâ€œè°ƒä»“æ¢æ‰‹åºåˆ— + NAV + ä½£é‡‘é‡‘é¢â€ï¼ŒæŒ‰å¸¸æ•°æ»‘ç‚¹åšå‡€å€¼æ ¡æ­£ï¼Œè¿”å›åŒ…å« *_net æŒ‡æ ‡çš„æ‹·è´ã€‚
    è¯´æ˜ï¼šåœ¨ offset å¤„çš„é¢å¤–æˆæœ¬ = slippage_rate * turnover * P_before_costï¼›
          P_before_cost = nav[offset] + commission_value_at_offsetï¼ˆç”¨ cost_amount_series æ¢å¤ï¼‰ã€‚
    """
    out = dict(result)  # æµ…æ‹·è´
    if slippage_rate <= 0:
        # é€ä¼ å¹¶è¡¥å…¨ *_net å­—æ®µ
        out.update(
            {
                "final_net": out["final"],
                "total_ret_net": out["total_ret"],
                "annual_ret_net": out["annual_ret"],
                "sharpe_net": out["sharpe"],
                "max_dd_net": out["max_dd"],
            }
        )
        return out

    nav = np.asarray(out["nav"], dtype=float).copy()
    if nav.size < 2:
        # è¾¹ç•Œï¼šæ•°æ®å¤ªçŸ­
        out.update(
            {
                "final_net": out["final"],
                "total_ret_net": out["total_ret"],
                "annual_ret_net": out["annual_ret"],
                "sharpe_net": out["sharpe"],
                "max_dd_net": out["max_dd"],
            }
        )
        return out

    cost_amount = np.asarray(out.get("cost_amount_series", np.zeros(0)), dtype=float)
    turnover = np.asarray(out.get("turnover_series", np.zeros(0)), dtype=float)
    n_rb = len(turnover)
    # æ˜ å°„æ¯æ¬¡è°ƒä»“åˆ° daily offsetï¼š0, freq, 2*freq, ...
    offsets = [i * freq for i in range(n_rb) if i * freq < (nav.size - 1)]
    # é€æ¬¡å¯¹ NAV åšâ€œç‚¹çŠ¶æ‰£å‡ + ä¹‹åå…¨æ®µæŒ‰æ¯”ä¾‹ç¼©æ”¾â€
    nav2 = nav.copy()
    for k, off in enumerate(offsets):
        P_after_commission = nav2[off]
        commission_k = float(cost_amount[k]) if k < len(cost_amount) else 0.0
        P_before = P_after_commission + commission_k
        extra_cost = float(slippage_rate) * float(turnover[k]) * P_before
        if P_after_commission <= 0 or extra_cost <= 0:
            continue
        # å°†å½“å‰ç‚¹çš„ NAV ç›´æ¥æ‰£å‡ extra_costï¼Œå¹¶å°†åç»­ NAV æŒ‰æ¯”ä¾‹ç¼©æ”¾
        new_at_off = max(P_after_commission - extra_cost, 0.0)
        if P_after_commission > 0:
            ratio = new_at_off / P_after_commission
            nav2[off:] = nav2[off:] * ratio

    # åŸºäº nav2 é‡æ–°è®¡ç®—æ—¥æ”¶ç›Šä¸æŒ‡æ ‡
    init_cap = float(nav2[0]) if nav2.size > 0 else 1.0
    daily_ret2 = nav2[1:] / nav2[:-1] - 1.0
    final_net = float(nav2[-1])
    total_ret_net = final_net / init_cap - 1.0
    days = max(len(daily_ret2), 1)
    annual_ret_net = (1 + total_ret_net) ** (252 / days) - 1
    vol = float(np.std(daily_ret2)) * np.sqrt(252)
    sharpe_net = (annual_ret_net / vol) if vol > 0 else 0.0
    cummax = np.maximum.accumulate(nav2)
    dd = (nav2 - cummax) / cummax
    max_dd_net = float(np.min(dd)) if dd.size > 0 else 0.0

    out.update(
        {
            "final_net": final_net,
            "total_ret_net": total_ret_net,
            "annual_ret_net": annual_ret_net,
            "sharpe_net": sharpe_net,
            "max_dd_net": max_dd_net,
            "nav_net": nav2,
        }
    )
    return out


def main():
    parser = argparse.ArgumentParser(description="ç›ˆåˆ©ä¼˜å…ˆå›æµ‹ï¼ˆå åŠ å¸¸æ•°æ»‘ç‚¹ + å¯é€‰åˆ©æ¶¦æ ¡å‡†æ’åºï¼‰")
    # Support None for "run all combos" - only use default 100 if RB_TOPK is explicitly set
    env_topk = os.environ.get("RB_TOPK", "").strip()
    default_topk = int(env_topk) if env_topk else None
    env_commission = os.environ.get("RB_COMMISSION_RATE", "").strip()
    env_stamp = os.environ.get("RB_STAMP_DUTY_RATE", "").strip()
    env_slip_grid = os.environ.get("RB_SLIPPAGE_GRID", "").strip()

    parser.add_argument("--topk", type=int, default=default_topk, help="å›æµ‹TopKï¼ˆRB_TOPKï¼‰,ä¸æŒ‡å®šåˆ™è·‘å…¨éƒ¨")
    parser.add_argument("--all", action="store_true", help="å›æµ‹å…¨é‡ç»„åˆï¼ˆRB_BACKTEST_ALL=1 åŒæ•ˆï¼‰")
    parser.add_argument("--slippage-bps", type=float, default=float(os.environ.get("RB_SLIPPAGE_BPS", "0") or 0), help="æ»‘ç‚¹åŸºç‚¹(åŒè¾¹ç­‰æ•ˆ)ï¼Œå¦‚5è¡¨ç¤º0.05%")
    parser.add_argument("--slippage-grid", type=str, default=env_slip_grid, help="å¯é€‰ï¼šé€—å·åˆ†éš”æ»‘ç‚¹bpsåˆ—è¡¨ï¼Œä¾æ¬¡æ‰§è¡Œå„åœºæ™¯")
    parser.add_argument("--force-freq", type=int, default=int(os.environ.get("RB_FORCE_FREQ", "0") or 0), help="å¼ºåˆ¶é¢‘ç‡è¦†ç›–ï¼ˆ0=ä¸ç”¨ï¼‰")
    parser.add_argument("--n-jobs", type=int, default=int(os.environ.get("RB_N_JOBS", "8") or 8), help="å¹¶è¡Œæ ¸æ•°ï¼ˆå½“å‰ç”¨äºWFOå¤–çš„éƒ¨åˆ†ï¼Œå›æµ‹ä»é€ä¸ªè°ƒç”¨ï¼‰")
    parser.add_argument("--ranking-file", type=str, default=os.environ.get("RB_RANKING_FILE", ""), help="å¯é€‰ï¼šæŒ‡å®šæ’åºç»“æœæ–‡ä»¶ï¼ˆparquetï¼‰ï¼Œä¼˜å…ˆäºé»˜è®¤æ’åº")
    parser.add_argument("--commission-rate", type=float, default=float(env_commission) if env_commission else None, help="è¦†ç›–ä½£é‡‘è´¹ç‡ï¼ˆåŒè¾¹ï¼‰ï¼Œç¤ºä¾‹: 0.002 å³20bp")
    parser.add_argument("--stamp-duty-rate", type=float, default=float(env_stamp) if env_stamp else None, help="è¦†ç›–å°èŠ±ç¨è´¹ç‡ï¼ˆç®€åŒ–ä¸ºåŒè¾¹æ¯”ç‡ï¼‰")
    args = parser.parse_args()

    print("=" * 100)
    print("ç›ˆåˆ©ä¼˜å…ˆå›æµ‹ (å«æ»‘ç‚¹ + åˆ©æ¶¦æ ¡å‡†æ’åº)")
    print("=" * 100)

    # å…ˆåŠ è½½é…ç½®,è·å–ç»Ÿä¸€çš„æ’åºè®¾ç½®
    cfg_path, cfg = load_config_candidates()
    ranking_config = cfg.get("ranking", {})
    ranking_method = ranking_config.get("method", "wfo")  # é»˜è®¤ wfo ä¿æŒå‘åå…¼å®¹
    config_top_n = ranking_config.get("top_n", None)
    
    # ç»Ÿä¸€ TopK é€»è¾‘: 1) args.topk ä¼˜å…ˆ, 2) é…ç½®æ–‡ä»¶ ranking.top_n, 3) None (å…¨éƒ¨)
    final_topk = args.topk if args.topk else config_top_n
    topk_source = "å‚æ•°" if args.topk else ("é…ç½®æ–‡ä»¶" if config_top_n else "é»˜è®¤(å…¨éƒ¨)")
    topk_display = final_topk if final_topk else "å…¨éƒ¨"

    if args.slippage_grid:
        slippage_grid_values = [float(x.strip()) for x in args.slippage_grid.split(",") if x.strip()]
    else:
        slippage_grid_values = [float(args.slippage_bps)]
    if not slippage_grid_values:
        slippage_grid_values = [0.0]
    slip_display = ", ".join(f"{v:g}" for v in slippage_grid_values)

    print(f"å‚æ•°: TopK={topk_display} (æ¥æº: {topk_source}), æ»‘ç‚¹ç½‘æ ¼={slip_display}bps, å¼ºåˆ¶é¢‘ç‡={args.force_freq or 'æ— '}")
    print()

    print(f"âœ“ é…ç½®æ–‡ä»¶: {cfg_path}")
    print()

    # 1) æ•°æ®/å› å­/æ¨ªæˆªé¢ï¼ˆå¤ç”¨ç¨³å®šä»“ï¼‰
    print("åŠ è½½æ•°æ®...")
    loader = DataLoader(data_dir=cfg["data"].get("data_dir"), cache_dir=cfg["data"].get("cache_dir"))
    ohlcv = loader.load_ohlcv(
        etf_codes=cfg["data"]["symbols"],
        start_date=cfg["data"]["start_date"],
        end_date=cfg["data"]["end_date"],
        use_cache=True,
    )
    print(f"âœ“ æ•°æ®: {len(ohlcv['close'])}å¤© Ã— {len(ohlcv['close'].columns)}åªETF")
    
    print("è®¡ç®—å› å­...")
    factor_lib = PreciseFactorLibrary()
    factors_df = factor_lib.compute_all_factors(prices=ohlcv)
    factors_dict = {name: factors_df[name] for name in factor_lib.list_factors()}
    print(f"âœ“ å› å­: {len(factors_dict)}ä¸ª")
    
    print("æ¨ªæˆªé¢æ ‡å‡†åŒ–...")
    processor = CrossSectionProcessor(
        lower_percentile=cfg["cross_section"]["winsorize_lower"] * 100,
        upper_percentile=cfg["cross_section"]["winsorize_upper"] * 100,
        verbose=False,
    )
    standardized = processor.process_all_factors(factors_dict)
    factor_names = sorted(standardized.keys())
    factors_data = np.stack([standardized[n].values for n in factor_names], axis=-1)
    returns = ohlcv["close"].pct_change(fill_method=None).values
    etf_names = list(ohlcv["close"].columns)
    print("âœ“ æ ‡å‡†åŒ–å®Œæˆ")
    print()

    # 2) è¯»å–æœ€æ–° WFO ç»„åˆï¼Œå¹¶æ ¹æ®ç»Ÿä¸€æ’åºé…ç½®é€‰æ‹©æ•°æ®æº
    print("è¯»å– WFO ç»„åˆ...")
    latest_run: Optional[Path] = None
    override_ts = os.environ.get("RB_RUN_TS", "").strip()
    if override_ts:
        override_candidates: List[Path] = []
        override_path = Path(override_ts)
        if override_path.is_absolute():
            override_candidates.append(override_path)
        else:
            if override_ts.startswith("run_"):
                override_candidates.append((Path(__file__).parent.parent / "results" / override_ts).resolve())
            else:
                override_candidates.append((Path(__file__).parent.parent / "results" / f"run_{override_ts}").resolve())
            override_candidates.append((Path.cwd() / override_ts).resolve())
        latest_run = next((p for p in override_candidates if p.exists()), None)
        if latest_run is None:
            print(f"[WARN] RB_RUN_TS={override_ts} æœªæ‰¾åˆ°åŒ¹é…ç›®å½•ï¼Œå›é€€ä¸ºæœ€æ–° run")
    if latest_run is None:
        latest_run = find_latest_run_dir()
    latest_run = latest_run.resolve()
    print(f"âœ“ æœ€æ–° run: {latest_run}")

    ranking_arg = (args.ranking_file or "").strip()
    ranking_path: Optional[Path] = None
    src_label: str
    order_label: str

    # ä¼˜å…ˆçº§: 1) æ˜¾å¼ --ranking-file, 2) æ ¹æ® ranking.method è‡ªåŠ¨é€‰æ‹©
    if ranking_arg:
        # åœºæ™¯1: ç”¨æˆ·æ˜¾å¼æŒ‡å®šæ’åºæ–‡ä»¶,ä¼˜å…ˆä½¿ç”¨
        candidate_paths: List[Path] = []
        rp = Path(ranking_arg)
        if rp.is_absolute():
            candidate_paths.append(rp)
        else:
            candidate_paths.append((Path.cwd() / rp).resolve())
            candidate_paths.append((latest_run / rp).resolve())
            candidate_paths.append((latest_run / "ranking_blends" / rp.name).resolve())
        ranking_path = next((p for p in candidate_paths if p.exists()), None)
        if ranking_path is None:
            raise FileNotFoundError(
                f"æŒ‡å®šçš„ ranking æ–‡ä»¶ä¸å­˜åœ¨: {ranking_arg}. å°è¯•è·¯å¾„: {candidate_paths}"
            )
        ranking_df = pd.read_parquet(ranking_path).reset_index(drop=True)
        top_df_cal = ranking_df
        src_label = f"ranking_file:{ranking_path.name}"
        order_label = src_label
        print(f"âœ“ ä½¿ç”¨æ’åºæ–‡ä»¶: {ranking_path} (æ ·æœ¬={len(top_df_cal)})")
        print(f"  æ¥æº: --ranking-file å‚æ•° (æ˜¾å¼æŒ‡å®š)")
    else:
        # åœºæ™¯2: æ ¹æ®é…ç½®æ–‡ä»¶çš„ ranking.method è‡ªåŠ¨é€‰æ‹©æ’åºæ–¹å¼
        print(f"  æ’åºæ¨¡å¼: {ranking_method.upper()} (æ¥æº: é…ç½®æ–‡ä»¶ ranking.method)")
        
        if ranking_method == "ml":
            # ML æ’åº: æŸ¥æ‰¾ ML æ’åæ–‡ä»¶
            ml_ranking_candidates = [
                latest_run / f"ranking_ml_top{final_topk}.parquet" if final_topk else None,
                latest_run / "ranking_ml_top200.parquet",  # é»˜è®¤ top200
                latest_run / f"ranked_top{final_topk}.parquet" if final_topk else None,
                latest_run / "ranked_combos.parquet",  # å…¨é‡ ML æ’åº
            ]
            ml_ranking_candidates = [p for p in ml_ranking_candidates if p is not None]
            
            ml_ranking_file = next((p for p in ml_ranking_candidates if p.exists()), None)
            
            if ml_ranking_file:
                print(f"âœ“ æ‰¾åˆ° ML æ’åºæ–‡ä»¶: {ml_ranking_file.name}")
                ranking_df = pd.read_parquet(ml_ranking_file).reset_index(drop=True)
                if final_topk and final_topk > 0:
                    ranking_df = ranking_df.head(final_topk).copy()
                top_df_cal = ranking_df
                src_label = f"MLæ’åº:{ml_ranking_file.name}"
                order_label = "ML (LTR æ¨¡å‹)"
                print(f"âœ“ æ’åºæ–¹å¼: {order_label} âœ… ç”Ÿäº§æ¨è")
                print(f"  æ ·æœ¬æ•°: {len(top_df_cal)}")
            else:
                # ML æ’åæ–‡ä»¶ä¸å­˜åœ¨,å›é€€åˆ° WFO
                print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° ML æ’åºæ–‡ä»¶,å°è¯•è·¯å¾„:")
                for p in ml_ranking_candidates:
                    print(f"    - {p}")
                print(f"âš ï¸  è‡ªåŠ¨å›é€€åˆ° WFO æ’åºé€»è¾‘")
                ranking_method = "wfo"  # å¼ºåˆ¶å›é€€
        
        if ranking_method == "wfo":
            # WFO æ’åº: ä½¿ç”¨å†…éƒ¨æ’åºé€»è¾‘
            backtest_all = bool(args.all or (os.environ.get("RB_BACKTEST_ALL", "0").strip().lower() in ("1", "true", "yes")))
            top_df, src_label = load_top_combos_from_run(latest_run, top_n=final_topk, load_all=backtest_all)
            print(f"âœ“ ç»„åˆæ•°: {len(top_df)} (æ¥æº: {src_label})")

            blend_dir = latest_run / "ranking_blends"
            unlimited_path = blend_dir / "ranking_two_stage_unlimited.parquet"
            if unlimited_path.exists():
                print(f"å‘ç° Unlimited æ’å: {unlimited_path}")
                ranking_df = pd.read_parquet(unlimited_path).reset_index(drop=True)
                if final_topk and final_topk > 0:
                    ranking_df = ranking_df.head(final_topk).copy()
                top_df_cal = ranking_df
                order_label = "two_stage_unlimited"
                src_label = f"{src_label}|two_stage_unlimited"
                print(f"âœ“ æ’åºæ–¹å¼: {order_label} (æ ·æœ¬={len(top_df_cal)})")
            else:
                print("åº”ç”¨åˆ©æ¶¦æ ¡å‡†å™¨...")
                top_df_cal, order_label = maybe_apply_profit_calibrator(top_df)
                print(f"âœ“ æ’åºæ–¹å¼: WFO å†…éƒ¨æ’åº âš ï¸ å¤‡ç”¨æ¨¡å¼")
                print(f"  æ’åºæŒ‡æ ‡: {order_label}")
    print()

    # 3) è°ƒç”¨ç¨³å®šå›æµ‹ï¼Œé€ç»„åˆå åŠ å¸¸æ•°æ»‘ç‚¹
    commission_rate_cfg = cfg.get("backtest", {}).get("commission_rate", 0.00005)
    stamp_duty_cfg = cfg.get("backtest", {}).get("stamp_duty_rate", 0.0)
    commission_rate = args.commission_rate if args.commission_rate is not None else commission_rate_cfg
    stamp_duty_rate = args.stamp_duty_rate if args.stamp_duty_rate is not None else stamp_duty_cfg
    effective_commission_rate = commission_rate + stamp_duty_rate
    lookback_window = cfg.get("backtest", {}).get("lookback_window", 252)
    force_freq = int(args.force_freq) if int(args.force_freq) > 0 else None

    invocation_ts = os.environ.get("RB_RESULT_TS") or datetime.now().strftime("%Y%m%d_%H%M%S")
    latest_ts = latest_run.name.replace("run_", "")
    out_dir = Path(__file__).parent.parent / "results_combo_wfo" / f"{latest_ts}_{invocation_ts}"
    out_dir.mkdir(parents=True, exist_ok=True)

    print(f"å¼€å§‹å›æµ‹ {len(top_df_cal)} ä¸ªç»„åˆ...")
    print(f"è¾“å‡ºç›®å½•: {out_dir}")
    print(
        f"ä½£é‡‘å‚æ•°: commission={commission_rate:.4%}, stamp={stamp_duty_rate:.4%}, effective={effective_commission_rate:.4%}"
    )
    print()

    success_any = False
    for slip_bps in slippage_grid_values:
        slippage_rate = max(0.0, float(slip_bps) / 10000.0)
        print("=" * 100)
        print(
            f"ğŸ§® æˆæœ¬åœºæ™¯ -> commission {commission_rate:.4%} + stamp {stamp_duty_rate:.4%} | æ»‘ç‚¹ {slip_bps}bps ({slippage_rate:.4%})"
        )
        results_rows: List[dict] = []

        for idx, row in tqdm(
            top_df_cal.iterrows(),
            total=len(top_df_cal),
            desc=f"å›æµ‹è¿›åº¦(slip={slip_bps}bps)",
        ):
            combo = str(row["combo"])
            wfo_freq = int(row["best_rebalance_freq"])
            freq = force_freq if force_freq is not None else wfo_freq
            factor_list = [s.strip() for s in combo.split("+")]  # æå–å› å­
            if any((f not in factor_names) for f in factor_list):
                continue
            fi = [factor_names.index(f) for f in factor_list]
            factors_sel = factors_data[:, :, fi]
            try:
                base = backtest_no_lookahead(
                    factors_data=factors_sel,
                    returns=returns,
                    etf_names=etf_names,
                    rebalance_freq=freq,
                    lookback_window=lookback_window,
                    position_size=5,
                    commission_rate=effective_commission_rate,
                    initial_capital=1_000_000.0,
                    factors_data_full=factors_data,
                    factor_indices_for_cache=np.asarray(fi, dtype=np.int64),
                )
                enriched = apply_slippage_to_nav(base, slippage_rate=slippage_rate, freq=freq)
            except Exception:
                continue

            rec = {
                "rank": idx + 1,
                "combo": combo,
                "combo_size": int(row.get("combo_size", len(fi))),
                "wfo_freq": wfo_freq,
                "test_freq": freq,
                "test_position_size": 5,
                "freq": freq,
                "wfo_ic": float(row.get("mean_oos_ic", np.nan)),
                "wfo_score": float(row.get("stability_score", np.nan)),
                "final_value": float(enriched["final"]),
                "total_ret": float(enriched["total_ret"]),
                "annual_ret": float(enriched["annual_ret"]),
                "vol": float(enriched["vol"]),
                "sharpe": float(enriched["sharpe"]),
                "max_dd": float(enriched["max_dd"]),
                "n_rebalance": int(enriched["n_rebalance"]),
                "avg_turnover": float(enriched["avg_turnover"]),
                "avg_n_holdings": float(enriched.get("avg_n_holdings", np.nan)),
                "win_rate": float(enriched.get("win_rate", np.nan)),
                "winning_days": int(enriched.get("winning_days", 0)),
                "losing_days": int(enriched.get("losing_days", 0)),
                "avg_win": float(enriched.get("avg_win", np.nan)),
                "avg_loss": float(enriched.get("avg_loss", np.nan)),
                "profit_factor": float(enriched.get("profit_factor", np.nan)),
                "calmar_ratio": float(enriched.get("calmar_ratio", np.nan)),
                "sortino_ratio": float(enriched.get("sortino_ratio", np.nan)),
                "max_consecutive_wins": int(enriched.get("max_consecutive_wins", 0)),
                "max_consecutive_losses": int(enriched.get("max_consecutive_losses", 0)),
                "final_value_net": float(enriched["final_net"]),
                "total_ret_net": float(enriched["total_ret_net"]),
                "annual_ret_net": float(enriched["annual_ret_net"]),
                "sharpe_net": float(enriched["sharpe_net"]),
                "max_dd_net": float(enriched["max_dd_net"]),
                "run_tag": f"{order_label}:{latest_run.name}:slip{slip_bps}bps",
            }
            if "calibrated_annual_pred" in top_df_cal.columns:
                rec["calibrated_annual_pred"] = float(row["calibrated_annual_pred"])
            results_rows.append(rec)

        if not results_rows:
            print(f"âŒ æ— å¯ç”¨å›æµ‹ç»“æœ (slip={slip_bps}bps)")
            continue

        df = pd.DataFrame(results_rows).sort_values("sharpe_net", ascending=False).reset_index(drop=True)
        slip_tag = str(slip_bps).replace(".", "p")
        tag = (
            f"profit_backtest_comm{int(round(commission_rate * 10000))}bp"
            f"_stamp{int(round(stamp_duty_rate * 10000))}bp"
            f"_slip{slip_tag}bps_{latest_ts}_{invocation_ts}"
        )
        out_file = out_dir / f"top{len(df)}_{tag}.csv"
        df.to_csv(out_file, index=False)

        summary = {
            "latest_run": str(latest_run),
            "config_file": str(cfg_path),
            "top_source": src_label,
            "order_label": order_label,
            "commission_rate": float(commission_rate),
            "stamp_duty_rate": float(stamp_duty_rate),
            "effective_commission_rate": float(effective_commission_rate),
            "slippage_bps": float(slip_bps),
            "count": int(len(df)),
            "mean_annual_net": float(df["annual_ret_net"].mean()),
            "median_annual_net": float(df["annual_ret_net"].median()),
            "mean_sharpe_net": float(df["sharpe_net"].mean()),
            "median_sharpe_net": float(df["sharpe_net"].median()),
        }
        with open(out_dir / f"SUMMARY_{tag}.json", "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç›ˆåˆ©ä¼˜å…ˆå›æµ‹å®Œæˆ | æ’åº: {order_label} | æ»‘ç‚¹: {slip_bps} bps")
        print(f"ä¿å­˜æ–‡ä»¶: {out_file.name}")
        print(f"Top1å¹´åŒ–(å‡€): {df.loc[0,'annual_ret_net']:.2%} | Sharpe(å‡€): {df.loc[0,'sharpe_net']:.3f}")
        success_any = True

    if not success_any:
        print("âŒ æ‰€æœ‰æˆæœ¬åœºæ™¯å‡æ— å›æµ‹ç»“æœ")
        return

    print(f"è¾“å‡ºç›®å½•: {out_dir}")
    print("=" * 100)


if __name__ == "__main__":
    main()


