"""
ç»Ÿä¸€è®­ç»ƒPipeline: æ•°æ®åŠ è½½ â†’ è®­ç»ƒ â†’ è¯„ä¼° â†’ ç¨³å¥æ€§éªŒè¯

æœ¬æ¨¡å—å°è£…å®Œæ•´çš„æ’åºæ¨¡å‹è®­ç»ƒæµç¨‹,æ”¯æŒ:
- å•/å¤šæ•°æ®æºè®­ç»ƒ
- è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹
- äº¤å‰éªŒè¯è®­ç»ƒ
- æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”
- ç¨³å¥æ€§éªŒè¯(å¯é€‰)
- æ¨¡å‹ä¿å­˜å’ŒæŠ¥å‘Šç”Ÿæˆ
"""
from __future__ import annotations

from typing import Optional, Dict, Any, Tuple
from pathlib import Path
import json
import warnings

import pandas as pd
import numpy as np

warnings.filterwarnings('ignore')

# å¯¼å…¥ç°æœ‰æ¨¡å—
from .config import DatasetConfig, DataSource
from .data_loader import (
    load_multi_source_data, 
    build_training_dataset,
    load_wfo_features,
    load_real_backtest_results
)
from .feature_engineer import build_feature_matrix
from .ltr_model import LTRRanker
from .evaluator import generate_evaluation_report, create_ranking_comparison_df
from .robustness_eval import (
    evaluate_kfold_cv, 
    evaluate_repeated_holdout,
    generate_robustness_report
)


def run_training_pipeline(
    config: DatasetConfig,
    model_params: Optional[Dict[str, Any]] = None,
    enable_robustness: bool = True,
    robustness_params: Optional[Dict[str, Any]] = None,
    save_model: bool = True,
    output_dir: Path = Path("ml_ranker"),
    verbose: bool = True
) -> Dict[str, Any]:
    """
    ç»Ÿä¸€è®­ç»ƒPipeline: ä¸€é”®å®Œæˆè®­ç»ƒ+è¯„ä¼°+ç¨³å¥æ€§éªŒè¯
    
    Args:
        config: æ•°æ®é›†é…ç½®(å•æˆ–å¤šæ•°æ®æº)
        model_params: æ¨¡å‹å‚æ•°å­—å…¸,Noneåˆ™ä½¿ç”¨é»˜è®¤å€¼
            - n_estimators: æ ‘æ•°é‡(é»˜è®¤500)
            - learning_rate: å­¦ä¹ ç‡(é»˜è®¤0.05)
            - max_depth: æœ€å¤§æ·±åº¦(é»˜è®¤6)
            - å…¶ä»–LightGBMå‚æ•°
        enable_robustness: æ˜¯å¦è¿è¡Œç¨³å¥æ€§è¯„ä¼°
        robustness_params: ç¨³å¥æ€§è¯„ä¼°å‚æ•°
            - n_splits: K-FoldæŠ˜æ•°(é»˜è®¤5)
            - n_repeats: Repeated Holdoutæ¬¡æ•°(é»˜è®¤5)
            - n_estimators: æ¯ä¸ªæ¨¡å‹æ ‘æ•°(é»˜è®¤300,å‡é€Ÿ)
            - random_state: éšæœºç§å­(é»˜è®¤2025)
        save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜
        output_dir: è¾“å‡ºæ ¹ç›®å½•
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        
    Returns:
        å®Œæ•´ç»“æœå­—å…¸åŒ…å«:
        - model: è®­ç»ƒå¥½çš„LTRRankerå¯¹è±¡
        - evaluation: è¯„ä¼°æŠ¥å‘Šdict
        - robustness: ç¨³å¥æ€§æŠ¥å‘Šdict(å¦‚æœå¯ç”¨)
        - metadata: å…ƒä¿¡æ¯dict
        - comparison_df: Top-100æ’åºå¯¹æ¯”è¡¨
        - output_paths: æ‰€æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Example:
        >>> from strategies.ml_ranker.config import DatasetConfig
        >>> config = DatasetConfig.from_yaml("configs/ranking_datasets.yaml")
        >>> result = run_training_pipeline(config, enable_robustness=True)
        >>> print(f"æ¨¡å‹Spearman: {result['evaluation']['model_metrics']['spearman_corr']:.4f}")
    """
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸš€ å¯åŠ¨ç»Ÿä¸€è®­ç»ƒPipeline")
        print(f"{'='*80}\n")
        print(f"é…ç½®: {len(config.datasets)} ä¸ªæ•°æ®æº")
        print(f"ç›®æ ‡åˆ—: {config.target_col}")
        print(f"ç¨³å¥æ€§è¯„ä¼°: {'å¯ç”¨' if enable_robustness else 'ç¦ç”¨'}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # =========================================================================
    # 1. åŠ è½½æ•°æ®
    # =========================================================================
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ“‚ STEP 1: åŠ è½½è®­ç»ƒæ•°æ®")
        print(f"{'='*80}")
    
    if len(config.datasets) == 1:
        # å•æ•°æ®æº: ç›´æ¥åŠ è½½
        ds = config.datasets[0]
        if verbose:
            print(f"  å•æ•°æ®æºæ¨¡å¼: {ds.display_name}")
        wfo_df = load_wfo_features(ds.wfo_dir)
        real_df = load_real_backtest_results(ds.real_dir)
        merged_df, y, metadata = build_training_dataset(wfo_df, real_df, config.target_col)
        
        # æ·»åŠ rebalance_daysåˆ—ä»¥ä¿æŒæ¥å£ä¸€è‡´
        merged_df['rebalance_days'] = ds.rebalance_days
        merged_df['source_label'] = ds.label or "single_source"
        metadata['n_sources'] = 1
        metadata['rebalance_days'] = np.full(len(merged_df), ds.rebalance_days)
    else:
        # å¤šæ•°æ®æº: ä½¿ç”¨multi_sourceåŠ è½½å™¨
        if verbose:
            print(f"  å¤šæ•°æ®æºæ¨¡å¼: {len(config.datasets)} ä¸ªæ•°æ®æº")
        merged_df, y, metadata = load_multi_source_data(config, add_source_id=True, verbose=verbose)
    
    # =========================================================================
    # 2. æ„å»ºç‰¹å¾çŸ©é˜µ
    # =========================================================================
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ› ï¸ STEP 2: æ„å»ºç‰¹å¾çŸ©é˜µ")
        print(f"{'='*80}")
    
    X_df = build_feature_matrix(merged_df)
    X = X_df.values
    feature_names = list(X_df.columns)
    
    if verbose:
        print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"  æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"  ç‰¹å¾ç¤ºä¾‹: {feature_names[:5]}")
    
    # =========================================================================
    # 3. è®­ç»ƒæ¨¡å‹
    # =========================================================================
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ”¥ STEP 3: è®­ç»ƒLTRæ¨¡å‹")
        print(f"{'='*80}")
    
    # é»˜è®¤å‚æ•°
    default_params = {
        "objective": "regression",  # ä½¿ç”¨å›å½’é¿å…lambdarankçš„query sizeé™åˆ¶
        "metric": "rmse",
        "n_estimators": 500,
        "learning_rate": 0.05,
        "max_depth": 6,
        "num_leaves": 31,
        "min_data_in_leaf": 20,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 5,
        "lambda_l1": 0.1,
        "lambda_l2": 0.1,
        "verbose": -1
    }
    
    # åˆå¹¶ç”¨æˆ·å‚æ•°
    if model_params:
        default_params.update(model_params)
    
    if verbose:
        print(f"  æ¨¡å‹å‚æ•°:")
        print(f"    n_estimators: {default_params['n_estimators']}")
        print(f"    learning_rate: {default_params['learning_rate']}")
        print(f"    max_depth: {default_params['max_depth']}")
    
    model = LTRRanker(**default_params)
    model.train(
        X=pd.DataFrame(X, columns=feature_names),
        y=y,
        cv_folds=5
    )
    
    if verbose:
        print(f"\n  âœ“ è®­ç»ƒå®Œæˆ")
        if model.cv_results:
            last_cv = model.cv_results[-1]
            print(f"  CV Spearman: {last_cv.get('spearman_corr', 0):.4f}")
    
    # =========================================================================
    # 4. é¢„æµ‹å¹¶è¯„ä¼°
    # =========================================================================
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ“ˆ STEP 4: æ¨¡å‹è¯„ä¼°")
        print(f"{'='*80}")
    
    scores, ranks = model.predict(X)
    baseline_scores = merged_df["mean_oos_ic"].values
    
    eval_dir = Path(output_dir) / "evaluation"
    eval_dir.mkdir(parents=True, exist_ok=True)
    
    importance_df = model.get_feature_importance()
    
    if verbose:
        print(f"  Top-10é‡è¦ç‰¹å¾:")
        for idx, row in importance_df.head(10).iterrows():
            print(f"    {row['feature']:30s}: {row['importance']:10.0f}")
    
    eval_report = generate_evaluation_report(
        y_true=y.values,
        y_pred=scores,
        baseline_scores=baseline_scores,
        metadata=metadata,
        feature_importance=importance_df,
        output_path=eval_dir / "evaluation_report.json"
    )
    
    if verbose:
        print(f"\n  æ¨¡å‹æ€§èƒ½:")
        print(f"    Spearman: {eval_report['model_metrics']['spearman_corr']:.4f}")
        print(f"    NDCG@10: {eval_report['model_metrics'].get('ndcg@10', 0):.4f}")
        print(f"    NDCG@50: {eval_report['model_metrics'].get('ndcg@50', 0):.4f}")
        print(f"  Top-10åˆ†æ:")
        print(f"    å‘½ä¸­æ•°: {eval_report['top10_analysis']['model_hits']}/10")
        print(f"    å¹³å‡æ”¶ç›Š: {eval_report['top10_analysis']['model_pred_mean']:.4f}")
    
    # =========================================================================
    # 5. ç¨³å¥æ€§è¯„ä¼° (å¯é€‰)
    # =========================================================================
    robustness_report = None
    if enable_robustness:
        if verbose:
            print(f"\n{'='*80}")
            print("ğŸ”¬ STEP 5: ç¨³å¥æ€§è¯„ä¼°")
            print(f"{'='*80}\n")
        
        # é»˜è®¤ç¨³å¥æ€§å‚æ•°
        default_rob_params = {
            'n_splits': 5,
            'n_repeats': 5,
            'n_estimators': 300,  # å‡å°‘æ ‘æ•°ä»¥åŠ é€Ÿ
            'random_state': 2025
        }
        
        if robustness_params:
            default_rob_params.update(robustness_params)
        
        if verbose:
            print(f"  K-Fold CV: {default_rob_params['n_splits']}æŠ˜")
            print(f"  Repeated Holdout: {default_rob_params['n_repeats']}æ¬¡")
            print(f"  æ¯ä¸ªæ¨¡å‹æ ‘æ•°: {default_rob_params['n_estimators']}\n")
        
        try:
            # K-Foldäº¤å‰éªŒè¯
            kfold_results, _ = evaluate_kfold_cv(
                X=X,
                y=y.values,
                feature_names=feature_names,
                baseline_features=merged_df,
                n_splits=default_rob_params['n_splits'],
                n_estimators=default_rob_params['n_estimators'],
                random_state=default_rob_params['random_state']
            )
            
            # Repeated Holdout
            holdout_results, _ = evaluate_repeated_holdout(
                X=X,
                y=y.values,
                feature_names=feature_names,
                baseline_features=merged_df,
                n_repeats=default_rob_params['n_repeats'],
                test_size=0.2,
                n_estimators=default_rob_params['n_estimators'],
                base_random_state=default_rob_params['random_state']
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            robustness_report = generate_robustness_report(
                kfold_results=kfold_results,
                holdout_results=holdout_results,
                output_dir=eval_dir
            )
            
            if verbose:
                kf_spear = robustness_report['kfold_cv']['metrics']['model_spearman']
                rh_spear = robustness_report['repeated_holdout']['metrics']['model_spearman']
                print(f"\n  ç¨³å¥æ€§ç»“æœ:")
                print(f"    K-Fold Spearman: {kf_spear['mean']:.4f} Â± {kf_spear['std']:.4f}")
                print(f"    Holdout Spearman: {rh_spear['mean']:.4f} Â± {rh_spear['std']:.4f}")
                
                avg_std = (kf_spear['std'] + rh_spear['std']) / 2
                if avg_std < 0.03:
                    print(f"    è¯„ä»·: âœ… ç¨³å®šæ€§ä¼˜ç§€ (std={avg_std:.4f} < 0.03)")
                elif avg_std < 0.08:
                    print(f"    è¯„ä»·: âœ… ç¨³å®šæ€§è‰¯å¥½ (std={avg_std:.4f} < 0.08)")
                else:
                    print(f"    è¯„ä»·: âš ï¸  ç¨³å®šæ€§ä¸€èˆ¬ (std={avg_std:.4f} >= 0.08)")
        
        except Exception as e:
            if verbose:
                print(f"  âš ï¸  ç¨³å¥æ€§è¯„ä¼°å¤±è´¥: {e}")
                print(f"  ç»§ç»­æ‰§è¡Œå…¶ä»–æ­¥éª¤...")
    
    # =========================================================================
    # 6. ä¿å­˜æ¨¡å‹
    # =========================================================================
    model_path = None
    if save_model:
        if verbose:
            print(f"\n{'='*80}")
            print("ğŸ’¾ STEP 6: ä¿å­˜æ¨¡å‹")
            print(f"{'='*80}")
        
        model_dir = Path(output_dir) / "models"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / "ltr_ranker"
        model.save(str(model_path))
        
        if verbose:
            print(f"  âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}.txt")
            print(f"  âœ“ å…ƒæ•°æ®å·²ä¿å­˜: {model_path}_meta.pkl")
    
    # =========================================================================
    # 7. ç”Ÿæˆæ’åºå¯¹æ¯”è¡¨
    # =========================================================================
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ“Š STEP 7: ç”Ÿæˆæ’åºå¯¹æ¯”è¡¨")
        print(f"{'='*80}")
    
    comparison_df = create_ranking_comparison_df(
        y_true=y.values,
        y_pred=scores,
        baseline_scores=baseline_scores,
        combos=merged_df["combo"].values,
        top_n=100
    )
    
    comparison_path = eval_dir / "ranking_comparison_top100.csv"
    comparison_df.to_csv(comparison_path, index=False, encoding="utf-8-sig")
    
    if verbose:
        print(f"  âœ“ Top-100å¯¹æ¯”è¡¨å·²ä¿å­˜: {comparison_path}")
    
    # =========================================================================
    # 8. è¿”å›ç»“æœ
    # =========================================================================
    result = {
        'model': model,
        'evaluation': eval_report,
        'robustness': robustness_report,
        'metadata': metadata,
        'comparison_df': comparison_df,
        'X': X,
        'y': y.values,
        'scores': scores,
        'ranks': ranks,
        'output_paths': {
            'model': str(model_path) if model_path else None,
            'evaluation': str(eval_dir / "evaluation_report.json"),
            'robustness': str(eval_dir / "robustness_report.json") if robustness_report else None,
            'robustness_detail': str(eval_dir / "robustness_detail.csv") if robustness_report else None,
            'comparison': str(comparison_path)
        }
    }
    
    if verbose:
        print(f"\n{'='*80}")
        print("âœ… Pipelineæ‰§è¡Œå®Œæˆ")
        print(f"{'='*80}\n")
        print(f"æ€»ç»“:")
        print(f"  æ•°æ®æº: {metadata['n_sources']} ä¸ª")
        print(f"  æ€»æ ·æœ¬: {len(y)}")
        print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
        print(f"  æ¨¡å‹æ€§èƒ½:")
        print(f"    Spearman: {eval_report['model_metrics']['spearman_corr']:.4f}")
        print(f"    NDCG@10: {eval_report['model_metrics'].get('ndcg@10', 0):.4f}")
        print(f"    Top-10å‘½ä¸­: {eval_report['top10_analysis']['model_hits']}/10")
        
        if robustness_report:
            kf_spear = robustness_report['kfold_cv']['metrics']['model_spearman']
            print(f"  ç¨³å¥æ€§:")
            print(f"    K-Fold Spearman: {kf_spear['mean']:.4f} Â± {kf_spear['std']:.4f}")
        
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        for key, path in result['output_paths'].items():
            if path:
                print(f"  {key}: {path}")
        print()
    
    return result
