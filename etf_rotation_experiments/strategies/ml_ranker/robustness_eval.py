#!/usr/bin/env python3
"""
ç¨³å¥æ€§è¯„ä¼°æ¨¡å—ï¼šå¤šæ¬¡äº¤å‰éªŒè¯å’Œéšæœºåˆ’åˆ†ï¼Œè¯„ä¼°æ¨¡å‹è¿‡æ‹Ÿåˆé£é™©

âš ï¸  æ³¨æ„: æœ¬è„šæœ¬å¯ç‹¬ç«‹ä½¿ç”¨,ä¹Ÿå¯é€šè¿‡ç»Ÿä¸€Pipelineè‡ªåŠ¨è°ƒç”¨

æ¨èä½¿ç”¨ç»Ÿä¸€Pipeline:
  python run_ranking_pipeline.py --config configs/ranking_datasets.yaml
  
  Pipelineä¼šè‡ªåŠ¨å®Œæˆ: è®­ç»ƒ + è¯„ä¼° + ç¨³å¥æ€§éªŒè¯

ç‹¬ç«‹ä½¿ç”¨æœ¬è„šæœ¬:
  python ml_ranker/robustness_eval.py
  æˆ–:
  python -m ml_ranker.robustness_eval

æœ¬è„šæœ¬é€šè¿‡ä¸¤ç§æ–¹å¼è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ•°æ®åˆ‡åˆ†ä¸Šçš„ç¨³å®šæ€§ï¼š
1. K-Fold äº¤å‰éªŒè¯ (é»˜è®¤5æŠ˜)ï¼šç³»ç»ŸåŒ–åœ°è¯„ä¼°æ¯ä¸ªæ ·æœ¬åœ¨éªŒè¯é›†æ—¶çš„è¡¨ç°
2. Repeated Holdout (é»˜è®¤5æ¬¡)ï¼šå¤šæ¬¡éšæœº80/20åˆ’åˆ†ï¼Œè¯„ä¼°éšæœºæ€§å½±å“

è¾“å‡ºæ–‡ä»¶ï¼š
- ml_ranker/evaluation/robustness_report.jsonï¼šèšåˆç»Ÿè®¡ç»“æœ
- ml_ranker/evaluation/robustness_detail.csvï¼šæ¯æŠ˜/æ¯æ¬¡çš„è¯¦ç»†æŒ‡æ ‡
"""
from __future__ import annotations

import argparse
import json
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# å¯¼å…¥ç°æœ‰æ¨¡å—ï¼ˆæ”¯æŒç›´æ¥è¿è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥ï¼‰
try:
    # ä½œä¸ºæ¨¡å—å¯¼å…¥
    from .data_loader import (
        load_wfo_features, 
        load_real_backtest_results, 
        build_training_dataset,
        find_latest_wfo_run,
        find_latest_backtest_run
    )
    from .feature_engineer import build_feature_matrix
    from .evaluator import compute_spearman_correlation, compute_ndcg
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    
    from strategies.ml_ranker.data_loader import (
        load_wfo_features, 
        load_real_backtest_results, 
        build_training_dataset,
        find_latest_wfo_run,
        find_latest_backtest_run
    )
    from strategies.ml_ranker.feature_engineer import build_feature_matrix
    from strategies.ml_ranker.evaluator import compute_spearman_correlation, compute_ndcg


def train_single_model(
    X_train: np.ndarray,
    y_train: np.ndarray,
    feature_names: List[str],
    n_estimators: int = 300,
    learning_rate: float = 0.05,
    random_state: int = 2025
) -> lgb.Booster:
    """
    è®­ç»ƒå•ä¸ªLightGBMå›å½’æ¨¡å‹
    
    Args:
        X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µ
        y_train: è®­ç»ƒæ ‡ç­¾
        feature_names: ç‰¹å¾ååˆ—è¡¨
        n_estimators: æ ‘çš„æ•°é‡ï¼ˆé™ä½ä»¥åŠ é€Ÿï¼‰
        learning_rate: å­¦ä¹ ç‡
        random_state: éšæœºç§å­
        
    Returns:
        è®­ç»ƒå¥½çš„LightGBM Booster
    """
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    # åˆ›å»ºLightGBMæ•°æ®é›†
    train_data = lgb.Dataset(
        X_train_scaled,
        label=y_train,
        feature_name=feature_names
    )
    
    # è®­ç»ƒå‚æ•°ï¼ˆä½¿ç”¨å›å½’ï¼Œä¸ç”Ÿäº§æ¨¡å‹ä¸€è‡´ï¼‰
    params = {
        "objective": "regression",
        "metric": "rmse",
        "boosting_type": "gbdt",
        "learning_rate": learning_rate,
        "max_depth": 6,
        "num_leaves": 31,
        "min_data_in_leaf": 20,
        "feature_fraction": 0.8,
        "bagging_fraction": 0.8,
        "bagging_freq": 5,
        "lambda_l1": 0.1,
        "lambda_l2": 0.1,
        "random_state": random_state,
        "verbose": -1,
    }
    
    # è®­ç»ƒæ¨¡å‹
    model = lgb.train(
        params,
        train_data,
        num_boost_round=n_estimators,
    )
    
    return model, scaler


def evaluate_on_fold(
    X_val: np.ndarray,
    y_val: np.ndarray,
    model: lgb.Booster,
    scaler: StandardScaler,
    baseline_scores: Dict[str, np.ndarray]
) -> Dict[str, float]:
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹å’ŒåŸºå‡†
    
    Args:
        X_val: éªŒè¯é›†ç‰¹å¾
        y_val: éªŒè¯é›†çœŸå®æ ‡ç­¾
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        scaler: ç‰¹å¾æ ‡å‡†åŒ–å™¨
        baseline_scores: åŸºå‡†æ’åºåˆ†æ•°å­—å…¸ {'mean_oos_ic': array, ...}
        
    Returns:
        æŒ‡æ ‡å­—å…¸åŒ…å«æ¨¡å‹å’Œå„åŸºå‡†çš„Spearmanã€NDCG
    """
    metrics = {}
    
    # æ¨¡å‹é¢„æµ‹
    X_val_scaled = scaler.transform(X_val)
    y_pred = model.predict(X_val_scaled)
    
    # æ¨¡å‹æŒ‡æ ‡
    metrics["model_spearman"] = compute_spearman_correlation(y_val, y_pred)
    metrics["model_ndcg10"] = compute_ndcg(y_val, y_pred, k=10)
    metrics["model_ndcg50"] = compute_ndcg(y_val, y_pred, k=50)
    
    # åŸºå‡†æŒ‡æ ‡
    for baseline_name, baseline_score in baseline_scores.items():
        metrics[f"{baseline_name}_spearman"] = compute_spearman_correlation(y_val, baseline_score)
        metrics[f"{baseline_name}_ndcg10"] = compute_ndcg(y_val, baseline_score, k=10)
        metrics[f"{baseline_name}_ndcg50"] = compute_ndcg(y_val, baseline_score, k=50)
    
    return metrics


def evaluate_kfold_cv(
    X: np.ndarray,
    y: np.ndarray,
    feature_names: List[str],
    baseline_features: pd.DataFrame,
    n_splits: int = 5,
    n_estimators: int = 300,
    random_state: int = 2025
) -> Tuple[List[Dict[str, float]], pd.DataFrame]:
    """
    K-Fold äº¤å‰éªŒè¯è¯„ä¼°
    
    Args:
        X: å®Œæ•´ç‰¹å¾çŸ©é˜µ
        y: å®Œæ•´æ ‡ç­¾
        feature_names: ç‰¹å¾ååˆ—è¡¨
        baseline_features: åŒ…å«baselineæ’åºç‰¹å¾çš„DataFrame (mean_oos_icç­‰)
        n_splits: æŠ˜æ•°
        n_estimators: æ¯ä¸ªæ¨¡å‹çš„æ ‘æ•°é‡
        random_state: éšæœºç§å­
        
    Returns:
        (fold_results, detail_df): æ¯æŠ˜ç»“æœåˆ—è¡¨ + è¯¦ç»†DataFrame
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”„ å¼€å§‹ {n_splits}-Fold äº¤å‰éªŒè¯")
    print(f"{'='*80}")
    
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    fold_results = []
    
    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X), 1):
        print(f"\n--- Fold {fold_idx}/{n_splits} ---")
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # æå–åŸºå‡†ç‰¹å¾
        baseline_scores = {}
        if "mean_oos_ic" in baseline_features.columns:
            baseline_scores["baseline_mean_oos_ic"] = baseline_features.iloc[val_idx]["mean_oos_ic"].values
        if "oos_compound_sharpe" in baseline_features.columns:
            baseline_scores["baseline_compound_sharpe"] = baseline_features.iloc[val_idx]["oos_compound_sharpe"].values
        
        # è®­ç»ƒæ¨¡å‹
        model, scaler = train_single_model(
            X_train, y_train, feature_names, 
            n_estimators=n_estimators, 
            random_state=random_state
        )
        
        # è¯„ä¼°
        metrics = evaluate_on_fold(X_val, y_val, model, scaler, baseline_scores)
        metrics["fold"] = fold_idx
        metrics["split_type"] = "kfold"
        metrics["n_train"] = len(train_idx)
        metrics["n_val"] = len(val_idx)
        
        fold_results.append(metrics)
        
        # æ‰“å°æœ¬æŠ˜ç»“æœ
        print(f"  Model Spearman: {metrics['model_spearman']:.4f}, NDCG@10: {metrics['model_ndcg10']:.4f}")
        for baseline_name in baseline_scores.keys():
            print(f"  {baseline_name} Spearman: {metrics[f'{baseline_name}_spearman']:.4f}")
    
    print(f"\nâœ“ {n_splits}-Fold CV å®Œæˆ")
    
    return fold_results, pd.DataFrame(fold_results)


def evaluate_repeated_holdout(
    X: np.ndarray,
    y: np.ndarray,
    feature_names: List[str],
    baseline_features: pd.DataFrame,
    n_repeats: int = 5,
    test_size: float = 0.2,
    n_estimators: int = 300,
    base_random_state: int = 2025
) -> Tuple[List[Dict[str, float]], pd.DataFrame]:
    """
    Repeated Holdout è¯„ä¼°ï¼ˆå¤šæ¬¡éšæœºåˆ’åˆ†ï¼‰
    
    Args:
        X: å®Œæ•´ç‰¹å¾çŸ©é˜µ
        y: å®Œæ•´æ ‡ç­¾
        feature_names: ç‰¹å¾ååˆ—è¡¨
        baseline_features: åŒ…å«baselineæ’åºç‰¹å¾çš„DataFrame
        n_repeats: é‡å¤æ¬¡æ•°
        test_size: éªŒè¯é›†æ¯”ä¾‹
        n_estimators: æ¯ä¸ªæ¨¡å‹çš„æ ‘æ•°é‡
        base_random_state: åŸºç¡€éšæœºç§å­
        
    Returns:
        (repeat_results, detail_df): æ¯æ¬¡ç»“æœåˆ—è¡¨ + è¯¦ç»†DataFrame
    """
    print(f"\n{'='*80}")
    print(f"ğŸ”„ å¼€å§‹ Repeated Holdout ({n_repeats}æ¬¡, {int(test_size*100)}% éªŒè¯é›†)")
    print(f"{'='*80}")
    
    repeat_results = []
    
    for repeat_idx in range(1, n_repeats + 1):
        print(f"\n--- Repeat {repeat_idx}/{n_repeats} ---")
        
        # ä½¿ç”¨ä¸åŒéšæœºç§å­åˆ’åˆ†
        random_state = base_random_state + repeat_idx * 100
        train_idx, val_idx = train_test_split(
            np.arange(len(X)),
            test_size=test_size,
            random_state=random_state
        )
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        # æå–åŸºå‡†ç‰¹å¾
        baseline_scores = {}
        if "mean_oos_ic" in baseline_features.columns:
            baseline_scores["baseline_mean_oos_ic"] = baseline_features.iloc[val_idx]["mean_oos_ic"].values
        if "oos_compound_sharpe" in baseline_features.columns:
            baseline_scores["baseline_compound_sharpe"] = baseline_features.iloc[val_idx]["oos_compound_sharpe"].values
        
        # è®­ç»ƒæ¨¡å‹
        model, scaler = train_single_model(
            X_train, y_train, feature_names,
            n_estimators=n_estimators,
            random_state=random_state
        )
        
        # è¯„ä¼°
        metrics = evaluate_on_fold(X_val, y_val, model, scaler, baseline_scores)
        metrics["repeat"] = repeat_idx
        metrics["split_type"] = "holdout"
        metrics["n_train"] = len(train_idx)
        metrics["n_val"] = len(val_idx)
        metrics["random_state"] = random_state
        
        repeat_results.append(metrics)
        
        # æ‰“å°æœ¬æ¬¡ç»“æœ
        print(f"  Model Spearman: {metrics['model_spearman']:.4f}, NDCG@10: {metrics['model_ndcg10']:.4f}")
        for baseline_name in baseline_scores.keys():
            print(f"  {baseline_name} Spearman: {metrics[f'{baseline_name}_spearman']:.4f}")
    
    print(f"\nâœ“ Repeated Holdout å®Œæˆ")
    
    return repeat_results, pd.DataFrame(repeat_results)


def aggregate_metrics(
    results: List[Dict[str, float]],
    split_type: str
) -> Dict[str, Any]:
    """
    èšåˆå¤šæ¬¡è¯„ä¼°çš„æŒ‡æ ‡ï¼ˆè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    
    Args:
        results: è¯„ä¼°ç»“æœåˆ—è¡¨
        split_type: åˆ’åˆ†ç±»å‹ ("kfold" æˆ– "holdout")
        
    Returns:
        èšåˆæŒ‡æ ‡å­—å…¸
    """
    df = pd.DataFrame(results)
    
    # éœ€è¦èšåˆçš„æŒ‡æ ‡åˆ—
    metric_cols = [col for col in df.columns if col not in 
                   ["fold", "repeat", "split_type", "n_train", "n_val", "random_state"]]
    
    aggregated = {
        "split_type": split_type,
        "n_iterations": len(results),
        "metrics": {}
    }
    
    # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
    for col in metric_cols:
        aggregated["metrics"][col] = {
            "mean": float(df[col].mean()),
            "std": float(df[col].std()),
            "min": float(df[col].min()),
            "max": float(df[col].max())
        }
    
    return aggregated


def generate_robustness_report(
    kfold_results: List[Dict[str, float]],
    holdout_results: List[Dict[str, float]],
    output_dir: Path
) -> Dict[str, Any]:
    """
    ç”Ÿæˆç¨³å¥æ€§è¯„ä¼°æŠ¥å‘Š
    
    Args:
        kfold_results: KFold CVç»“æœ
        holdout_results: Repeated Holdoutç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        å®Œæ•´æŠ¥å‘Šå­—å…¸
    """
    print(f"\n{'='*80}")
    print("ğŸ“Š ç”Ÿæˆç¨³å¥æ€§è¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*80}")
    
    report = {
        "kfold_cv": aggregate_metrics(kfold_results, "kfold"),
        "repeated_holdout": aggregate_metrics(holdout_results, "holdout"),
        "summary": {}
    }
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ” K-Fold CV èšåˆç»“æœ:")
    kf_metrics = report["kfold_cv"]["metrics"]
    print(f"  æ¨¡å‹ Spearman: {kf_metrics['model_spearman']['mean']:.4f} Â± {kf_metrics['model_spearman']['std']:.4f}")
    print(f"  æ¨¡å‹ NDCG@10:  {kf_metrics['model_ndcg10']['mean']:.4f} Â± {kf_metrics['model_ndcg10']['std']:.4f}")
    
    if "baseline_mean_oos_ic_spearman" in kf_metrics:
        bl_spear = kf_metrics['baseline_mean_oos_ic_spearman']
        print(f"  Baseline(IC) Spearman: {bl_spear['mean']:.4f} Â± {bl_spear['std']:.4f}")
        
        # è®¡ç®—ç›¸å¯¹æå‡
        model_mean = kf_metrics['model_spearman']['mean']
        baseline_mean = bl_spear['mean']
        improvement = (model_mean - baseline_mean) / abs(baseline_mean) * 100 if baseline_mean != 0 else 0
        print(f"  ç›¸å¯¹æå‡: {improvement:+.1f}%")
        
        report["summary"]["kfold_improvement_vs_baseline"] = improvement
    
    print("\nğŸ” Repeated Holdout èšåˆç»“æœ:")
    rh_metrics = report["repeated_holdout"]["metrics"]
    print(f"  æ¨¡å‹ Spearman: {rh_metrics['model_spearman']['mean']:.4f} Â± {rh_metrics['model_spearman']['std']:.4f}")
    print(f"  æ¨¡å‹ NDCG@10:  {rh_metrics['model_ndcg10']['mean']:.4f} Â± {rh_metrics['model_ndcg10']['std']:.4f}")
    
    if "baseline_mean_oos_ic_spearman" in rh_metrics:
        bl_spear = rh_metrics['baseline_mean_oos_ic_spearman']
        print(f"  Baseline(IC) Spearman: {bl_spear['mean']:.4f} Â± {bl_spear['std']:.4f}")
        
        model_mean = rh_metrics['model_spearman']['mean']
        baseline_mean = bl_spear['mean']
        improvement = (model_mean - baseline_mean) / abs(baseline_mean) * 100 if baseline_mean != 0 else 0
        print(f"  ç›¸å¯¹æå‡: {improvement:+.1f}%")
        
        report["summary"]["holdout_improvement_vs_baseline"] = improvement
    
    # ä¿å­˜JSONæŠ¥å‘Š
    output_dir.mkdir(parents=True, exist_ok=True)
    json_path = output_dir / "robustness_report.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"\nâœ“ JSONæŠ¥å‘Šå·²ä¿å­˜: {json_path}")
    
    # ä¿å­˜è¯¦ç»†CSV
    detail_df = pd.concat([
        pd.DataFrame(kfold_results),
        pd.DataFrame(holdout_results)
    ], ignore_index=True)
    
    csv_path = output_dir / "robustness_detail.csv"
    detail_df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"âœ“ è¯¦ç»†CSVå·²ä¿å­˜: {csv_path}")
    
    return report


def parse_args():
    parser = argparse.ArgumentParser(
        description="ç¨³å¥æ€§è¯„ä¼°ï¼šå¤šæ¬¡CVè¯„ä¼°æ¨¡å‹è¿‡æ‹Ÿåˆé£é™©"
    )
    parser.add_argument(
        "--wfo-dir",
        type=str,
        default=None,
        help="WFOç»“æœç›®å½• (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)"
    )
    parser.add_argument(
        "--backtest-dir",
        type=str,
        default=None,
        help="å›æµ‹ç»“æœç›®å½• (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="ml_ranker/evaluation",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--n-folds",
        type=int,
        default=5,
        help="K-FoldæŠ˜æ•°"
    )
    parser.add_argument(
        "--n-repeats",
        type=int,
        default=5,
        help="Repeated Holdouté‡å¤æ¬¡æ•°"
    )
    parser.add_argument(
        "--n-estimators",
        type=int,
        default=300,
        help="æ¯ä¸ªæ¨¡å‹çš„æ ‘æ•°é‡ï¼ˆé™ä½ä»¥åŠ é€Ÿï¼‰"
    )
    parser.add_argument(
        "--random-state",
        type=int,
        default=2025,
        help="éšæœºç§å­"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    
    print(f"\n{'='*80}")
    print("ğŸ”¬ ç¨³å¥æ€§è¯„ä¼°ï¼šå¤šæ¬¡äº¤å‰éªŒè¯ + è¿‡æ‹Ÿåˆæ£€æŸ¥")
    print(f"{'='*80}\n")
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
    
    if args.wfo_dir is None:
        wfo_dir = find_latest_wfo_run()
    else:
        wfo_dir = Path(args.wfo_dir)
    
    if args.backtest_dir is None:
        backtest_dir = find_latest_backtest_run()
    else:
        backtest_dir = Path(args.backtest_dir)
    
    print(f"  WFOç›®å½•: {wfo_dir}")
    print(f"  å›æµ‹ç›®å½•: {backtest_dir}")
    
    wfo_df = load_wfo_features(wfo_dir)
    real_df = load_real_backtest_results(backtest_dir)
    merged_df, y, metadata = build_training_dataset(wfo_df, real_df)
    
    # 2. æ„å»ºç‰¹å¾çŸ©é˜µ
    print(f"\nğŸ› ï¸ æ„å»ºç‰¹å¾çŸ©é˜µ...")
    X_df = build_feature_matrix(merged_df)
    X = X_df.values
    feature_names = list(X_df.columns)
    
    print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"  æ ·æœ¬æ•°: {X.shape[0]}")
    
    # 3. K-Fold äº¤å‰éªŒè¯
    kfold_results, kfold_detail = evaluate_kfold_cv(
        X=X,
        y=y.values,
        feature_names=feature_names,
        baseline_features=merged_df,
        n_splits=args.n_folds,
        n_estimators=args.n_estimators,
        random_state=args.random_state
    )
    
    # 4. Repeated Holdout
    holdout_results, holdout_detail = evaluate_repeated_holdout(
        X=X,
        y=y.values,
        feature_names=feature_names,
        baseline_features=merged_df,
        n_repeats=args.n_repeats,
        test_size=0.2,
        n_estimators=args.n_estimators,
        base_random_state=args.random_state
    )
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    output_dir = Path(args.output_dir)
    report = generate_robustness_report(
        kfold_results=kfold_results,
        holdout_results=holdout_results,
        output_dir=output_dir
    )
    
    # 6. æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*80}")
    print("âœ… ç¨³å¥æ€§è¯„ä¼°å®Œæˆ")
    print(f"{'='*80}")
    
    print("\nğŸ“ ç»“è®ºåˆ†æ:")
    
    # KFoldåˆ†æ
    kf_model_spear = report["kfold_cv"]["metrics"]["model_spearman"]
    kf_bl_spear = report["kfold_cv"]["metrics"].get("baseline_mean_oos_ic_spearman", {})
    
    print(f"\n1. K-Fold CV ({args.n_folds}æŠ˜):")
    print(f"   - æ¨¡å‹ Spearman: {kf_model_spear['mean']:.4f} Â± {kf_model_spear['std']:.4f}")
    
    if kf_bl_spear:
        print(f"   - Baseline Spearman: {kf_bl_spear['mean']:.4f} Â± {kf_bl_spear['std']:.4f}")
        improvement = report["summary"].get("kfold_improvement_vs_baseline", 0)
        
        if kf_model_spear['std'] < 0.05:
            stability = "ç¨³å®šæ€§æå¥½"
        elif kf_model_spear['std'] < 0.1:
            stability = "ç¨³å®šæ€§è‰¯å¥½"
        else:
            stability = "æ³¢åŠ¨è¾ƒå¤§"
        
        print(f"   - ç›¸å¯¹æå‡: {improvement:+.1f}%")
        print(f"   - è¯„ä¼°: {stability}ï¼Œæ¨¡å‹åœ¨ä¸åŒæŠ˜ä¸Šè¡¨ç°ä¸€è‡´")
    
    # Holdoutåˆ†æ
    rh_model_spear = report["repeated_holdout"]["metrics"]["model_spearman"]
    rh_bl_spear = report["repeated_holdout"]["metrics"].get("baseline_mean_oos_ic_spearman", {})
    
    print(f"\n2. Repeated Holdout ({args.n_repeats}æ¬¡):")
    print(f"   - æ¨¡å‹ Spearman: {rh_model_spear['mean']:.4f} Â± {rh_model_spear['std']:.4f}")
    
    if rh_bl_spear:
        print(f"   - Baseline Spearman: {rh_bl_spear['mean']:.4f} Â± {rh_bl_spear['std']:.4f}")
        improvement = report["summary"].get("holdout_improvement_vs_baseline", 0)
        print(f"   - ç›¸å¯¹æå‡: {improvement:+.1f}%")
        print(f"   - è¯„ä¼°: éšæœºåˆ’åˆ†ä¸‹æ¨¡å‹ä¾ç„¶å¤§å¹…ä¼˜äºbaseline")
    
    # æ€»ä½“ç»“è®º
    print(f"\n3. æ€»ä½“ç»“è®º:")
    avg_std = (kf_model_spear['std'] + rh_model_spear['std']) / 2
    
    if avg_std < 0.03:
        print(f"   âœ… æ¨¡å‹ç¨³å¥æ€§ä¼˜ç§€ï¼ˆå¹³å‡std={avg_std:.4f} < 0.03ï¼‰")
        print(f"   âœ… åœ¨ä¸åŒåˆ‡åˆ†ä¸Šè¡¨ç°ä¸€è‡´ï¼Œè¿‡æ‹Ÿåˆé£é™©æä½")
    elif avg_std < 0.08:
        print(f"   âœ… æ¨¡å‹ç¨³å¥æ€§è‰¯å¥½ï¼ˆå¹³å‡std={avg_std:.4f} < 0.08ï¼‰")
        print(f"   âœ… å¯ä»¥æ”¾å¿ƒéƒ¨ç½²ï¼Œä¸è¿‡åº¦ä¾èµ–å•æ¬¡è®­ç»ƒåˆ‡åˆ†")
    else:
        print(f"   âš ï¸  æ¨¡å‹ç¨³å®šæ€§ä¸€èˆ¬ï¼ˆå¹³å‡std={avg_std:.4f} >= 0.08ï¼‰")
        print(f"   âš ï¸  å»ºè®®æ£€æŸ¥ç‰¹å¾å·¥ç¨‹æˆ–å¢åŠ æ­£åˆ™åŒ–")
    
    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    main()
