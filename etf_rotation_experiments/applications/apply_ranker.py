#!/usr/bin/env python3
"""
åº”ç”¨å·²è®­ç»ƒçš„LTRæ¨¡å‹å¯¹æ–°WFOç»“æœæ’åº
"""
import argparse
from pathlib import Path
import pandas as pd
import warnings

warnings.filterwarnings('ignore')

from strategies.ml_ranker.data_loader import load_wfo_features
from strategies.ml_ranker.feature_engineer import build_feature_matrix
from strategies.ml_ranker.ltr_model import LTRRanker


def apply_ltr_ranking(model_path: str, wfo_dir: str | Path, output_path: str | Path = None, top_k: int = None, verbose: bool = True) -> pd.DataFrame:
    """
    åº”ç”¨LTRæ¨¡å‹å¯¹WFOç»“æœæ’åº (å¯å¤ç”¨å‡½æ•°)
    
    Args:
        model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (æ— æ‰©å±•å, å¦‚ ml_ranker/models/ltr_ranker)
        wfo_dir: WFOç»“æœç›®å½•
        output_path: è¾“å‡ºCSVè·¯å¾„ (å¯é€‰, é»˜è®¤ä¸º <wfo_dir>/ranked_combos.csv)
        top_k: ä¿å­˜ Top-K ç»“æœ (å¯é€‰, ä¸º None åˆ™ä¸å•ç‹¬ä¿å­˜)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        
    Returns:
        result_df: åŒ…å« ltr_score, ltr_rank çš„å®Œæ•´æ’åºç»“æœ
        
    Example:
        >>> result_df = apply_ltr_ranking(
        ...     model_path="ml_ranker/models/ltr_ranker",
        ...     wfo_dir="results/run_20251114_155420",
        ...     top_k=200
        ... )
        >>> print(result_df.head())
    """
    wfo_dir = Path(wfo_dir)
    
    if verbose:
        print(f"\n{'='*80}")
        print("ğŸ¯ åº”ç”¨LTRæ’åºæ¨¡å‹")
        print(f"{'='*80}\n")
    
    # 1. åŠ è½½æ¨¡å‹
    if verbose:
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    model = LTRRanker.load(model_path)
    if verbose:
        print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  ç‰¹å¾æ•°: {len(model.feature_names)}")
    
    # 2. åŠ è½½WFOæ•°æ®
    if verbose:
        print(f"\nğŸ“‚ åŠ è½½WFOæ•°æ®: {wfo_dir}")
    df_wfo = load_wfo_features(wfo_dir)
    if verbose:
        print(f"  âœ“ åŠ è½½ {len(df_wfo)} ä¸ªç­–ç•¥ç»„åˆ")
    
    # 3. æ„å»ºç‰¹å¾
    if verbose:
        print(f"\nğŸ› ï¸ æ„å»ºç‰¹å¾çŸ©é˜µ")
    X_df = build_feature_matrix(df_wfo)
    X = X_df.values
    feature_names = list(X_df.columns)
    if verbose:
        print(f"  âœ“ ç‰¹å¾ç»´åº¦: {X.shape}")
    
    # éªŒè¯ç‰¹å¾å¯¹é½
    if feature_names != model.feature_names:
        if verbose:
            print(f"  âš ï¸  ç‰¹å¾åç§°ä¸åŒ¹é…ï¼Œå°è¯•é‡æ–°æ’åˆ—...")
        feature_df = pd.DataFrame(X, columns=feature_names)
        X = feature_df[model.feature_names].values
        feature_names = model.feature_names
        if verbose:
            print(f"  âœ“ ç‰¹å¾å·²å¯¹é½")
    
    # 4. é¢„æµ‹æ’åº
    if verbose:
        print(f"\nğŸ¯ é¢„æµ‹æ’åºåˆ†æ•°")
    scores, ranks = model.predict(X)
    if verbose:
        print(f"  âœ“ é¢„æµ‹å®Œæˆ")
        print(f"  åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
    
    # 5. æ„å»ºç»“æœè¡¨
    if verbose:
        print(f"\nğŸ“Š ç”Ÿæˆæ’åºç»“æœ")
    result_df = df_wfo.copy()
    result_df["ltr_score"] = scores
    result_df["ltr_rank"] = ranks
    
    # æ·»åŠ åŸå§‹WFOæ’åç”¨äºå¯¹æ¯”
    result_df["wfo_rank"] = result_df["mean_oos_ic"].rank(ascending=False, method="min").astype(int)
    result_df["rank_change"] = result_df["wfo_rank"] - result_df["ltr_rank"]
    
    # æŒ‰LTRæ’åæ’åº
    result_df = result_df.sort_values("ltr_rank").reset_index(drop=True)
    
    # 6. ä¿å­˜ç»“æœ (å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„)
    if output_path is not None:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        result_df.to_csv(output_path, index=False, encoding="utf-8-sig")
        if verbose:
            print(f"\nğŸ’¾ å…¨é‡æ’åºç»“æœå·²ä¿å­˜: {output_path}")
        
        # ä¿å­˜Top-K (å¦‚æœæŒ‡å®š)
        if top_k is not None and top_k > 0:
            top_k_df = result_df.head(top_k)
            topk_path = output_path.parent / f"ranked_top{top_k}.csv"
            top_k_df.to_csv(topk_path, index=False, encoding="utf-8-sig")
            if verbose:
                print(f"ğŸ’¾ Top-{top_k} ç»“æœå·²ä¿å­˜: {topk_path}")
    
    # 7. æ˜¾ç¤ºTop-10 (å¦‚æœéœ€è¦)
    if verbose and len(result_df) > 0:
        display_k = min(10, len(result_df))
        print(f"\nğŸ† Top-{display_k} ç­–ç•¥ (LTRæ’åº):")
        print(f"{'='*80}")
        for idx, row in result_df.head(display_k).iterrows():
            combo_str = row['combo'] if len(row['combo']) <= 60 else row['combo'][:57] + '...'
            print(f"  #{int(row['ltr_rank']):3d}  {combo_str:60s}  "
                  f"score={row['ltr_score']:7.4f}  "
                  f"WFOæ’å: #{int(row['wfo_rank']):4d}  "
                  f"å˜åŒ–: {int(row['rank_change']):+4d}")
    
    return result_df


def parse_args():
    parser = argparse.ArgumentParser(description="åº”ç”¨LTRæ¨¡å‹å¯¹WFOç»“æœæ’åº")
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (æ— æ‰©å±•å, å¦‚ ml_ranker/models/ltr_ranker)"
    )
    parser.add_argument(
        "--wfo-dir",
        type=str,
        required=True,
        help="WFOç»“æœç›®å½•"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºCSVè·¯å¾„ (é»˜è®¤: <wfo-dir>/ranked_combos.csv)"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=50,
        help="è¾“å‡ºTop-Kç­–ç•¥"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    
    # è°ƒç”¨æ ¸å¿ƒå‡½æ•°
    result_df = apply_ltr_ranking(
        model_path=args.model,
        wfo_dir=args.wfo_dir,
        output_path=args.output if args.output else Path(args.wfo_dir) / "ranked_combos.csv",
        top_k=args.top_k,
        verbose=True
    )
    
    # 8. ç»Ÿè®¡æ‘˜è¦
    print(f"\n{'='*80}")
    print("ğŸ“ˆ æ’åºæ‘˜è¦")
    print(f"{'='*80}")
    
    print(f"  æ€»ç­–ç•¥æ•°: {len(result_df)}")
    print(f"  LTRåˆ†æ•°å‡å€¼: {result_df['ltr_score'].mean():.4f}")
    print(f"  LTRåˆ†æ•°æ ‡å‡†å·®: {result_df['ltr_score'].std():.4f}")
    
    # Top-Kçš„WFOæŒ‡æ ‡å¹³å‡å€¼
    top_k_df = result_df.head(args.top_k)
    print(f"\n  Top-{args.top_k} ç­–ç•¥çš„WFOæŒ‡æ ‡å¹³å‡:")
    for col in ["mean_oos_ic", "oos_sharpe_proxy", "stability_score", "mean_oos_pvalue"]:
        if col in top_k_df.columns:
            print(f"    {col:20s}: {top_k_df[col].mean():8.4f}")
    
    # ä¸WFO Top-Kå¯¹æ¯”
    wfo_topk = result_df.nsmallest(args.top_k, "wfo_rank")
    overlap = len(set(top_k_df["combo"]) & set(wfo_topk["combo"]))
    print(f"\n  ä¸WFO Top-{args.top_k} é‡å æ•°: {overlap}/{args.top_k} ({overlap/args.top_k*100:.1f}%)")
    
    print(f"\n{'='*80}")
    print("âœ… æ’åºå®Œæˆ")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
