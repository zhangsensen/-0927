"""
è¯„ä¼°æ¨¡å—: æ’åºè´¨é‡è¯„ä¼°æŒ‡æ ‡å’ŒæŠ¥å‘Šç”Ÿæˆ
"""
from __future__ import annotations

from typing import Dict, Any, Optional

import numpy as np
import pandas as pd
from scipy.stats import spearmanr
from sklearn.metrics import ndcg_score


def compute_spearman_correlation(
    y_true: np.ndarray,
    y_pred: np.ndarray
) -> float:
    """
    è®¡ç®—Spearmanæ’åºç›¸å…³ç³»æ•°
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        
    Returns:
        Spearmanç›¸å…³ç³»æ•°
    """
    corr, _ = spearmanr(y_true, y_pred)
    return corr


def compute_ndcg(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    k: Optional[int] = None
) -> float:
    """
    è®¡ç®—NDCG@Kåˆ†æ•°
    
    Args:
        y_true: çœŸå®ç›¸å…³æ€§åˆ†æ•°
        y_pred: é¢„æµ‹æ’åºåˆ†æ•°
        k: Top-K, Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨
        
    Returns:
        NDCGåˆ†æ•°
    """
    # sklearnçš„ndcg_scoreéœ€è¦2Dæ•°ç»„ä¸”ä¸èƒ½æœ‰è´Ÿå€¼
    # å°†y_trueå¹³ç§»åˆ°éè´ŸèŒƒå›´
    y_true_shifted = y_true - y_true.min() + 1e-6
    
    y_true_2d = y_true_shifted.reshape(1, -1)
    y_pred_2d = y_pred.reshape(1, -1)
    
    return ndcg_score(y_true_2d, y_pred_2d, k=k)


def compute_topk_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    k: int = 10
) -> Dict[str, float]:
    """
    è®¡ç®—Top-Kç›¸å…³æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
        k: Top-K
        
    Returns:
        æŒ‡æ ‡dictåŒ…å«å‘½ä¸­ç‡ã€å¹³å‡æ”¶ç›Šç­‰
    """
    # è·å–çœŸå®Top-Kç´¢å¼•
    true_topk_idx = np.argsort(y_true)[-k:][::-1]
    
    # è·å–é¢„æµ‹Top-Kç´¢å¼•
    pred_topk_idx = np.argsort(y_pred)[-k:][::-1]
    
    # å‘½ä¸­æ•°é‡
    hits = len(set(true_topk_idx) & set(pred_topk_idx))
    hit_rate = hits / k
    
    # é¢„æµ‹Top-Kçš„çœŸå®å¹³å‡å€¼
    pred_topk_true_mean = y_true[pred_topk_idx].mean()
    
    # çœŸå®Top-Kçš„çœŸå®å¹³å‡å€¼ (ç†è®ºæœ€ä¼˜)
    true_topk_true_mean = y_true[true_topk_idx].mean()
    
    # å…¨ä½“å¹³å‡å€¼ (baseline)
    overall_mean = y_true.mean()
    
    # æå‡å€æ•°
    lift_vs_baseline = pred_topk_true_mean / overall_mean if overall_mean != 0 else 0
    lift_vs_optimal = pred_topk_true_mean / true_topk_true_mean if true_topk_true_mean != 0 else 0
    
    return {
        f"top{k}_hit_rate": hit_rate,
        f"top{k}_hits": hits,
        f"top{k}_pred_mean": pred_topk_true_mean,
        f"top{k}_true_mean": true_topk_true_mean,
        f"top{k}_overall_mean": overall_mean,
        f"top{k}_lift_vs_baseline": lift_vs_baseline,
        f"top{k}_lift_vs_optimal": lift_vs_optimal,
    }


def compute_ranking_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    è®¡ç®—å®Œæ•´çš„æ’åºè¯„ä¼°æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®ç›®æ ‡å€¼
        y_pred: é¢„æµ‹æ’åºåˆ†æ•°
        metadata: å…ƒä¿¡æ¯ (å¯é€‰åŒ…å«comboç­‰)
        
    Returns:
        å®Œæ•´è¯„ä¼°æŒ‡æ ‡dict
    """
    metrics = {}
    
    # 1. Spearmanç›¸å…³æ€§
    metrics["spearman_corr"] = compute_spearman_correlation(y_true, y_pred)
    
    # 2. NDCG@K
    for k in [5, 10, 20, 50, 100]:
        if k <= len(y_true):
            metrics[f"ndcg@{k}"] = compute_ndcg(y_true, y_pred, k=k)
    
    # 3. Top-KæŒ‡æ ‡
    for k in [10, 20, 50]:
        if k <= len(y_true):
            topk_metrics = compute_topk_metrics(y_true, y_pred, k=k)
            metrics.update(topk_metrics)
    
    # 4. åŸºç¡€ç»Ÿè®¡
    metrics["n_samples"] = len(y_true)
    metrics["y_true_mean"] = float(np.mean(y_true))
    metrics["y_true_std"] = float(np.std(y_true))
    metrics["y_pred_mean"] = float(np.mean(y_pred))
    metrics["y_pred_std"] = float(np.std(y_pred))
    
    return metrics


def compare_with_baseline(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    baseline_scores: np.ndarray,
    baseline_name: str = "WFOåŸå§‹æ’åº"
) -> pd.DataFrame:
    """
    å¯¹æ¯”æ¨¡å‹é¢„æµ‹ä¸baselineæ’åº
    
    Args:
        y_true: çœŸå®ç›®æ ‡å€¼
        y_pred: æ¨¡å‹é¢„æµ‹åˆ†æ•°
        baseline_scores: baselineæ’åºåˆ†æ•° (å¦‚WFOçš„mean_oos_ic)
        baseline_name: baselineåç§°
        
    Returns:
        å¯¹æ¯”ç»“æœDataFrame
    """
    # è®¡ç®—æ¨¡å‹æŒ‡æ ‡
    model_metrics = compute_ranking_metrics(y_true, y_pred)
    
    # è®¡ç®—baselineæŒ‡æ ‡
    baseline_metrics = compute_ranking_metrics(y_true, baseline_scores)
    
    # æ„å»ºå¯¹æ¯”è¡¨
    comparison = []
    
    for key in sorted(model_metrics.keys()):
        if key in baseline_metrics and not key.startswith("y_") and not key.startswith("n_"):
            model_val = model_metrics[key]
            baseline_val = baseline_metrics[key]
            
            # è®¡ç®—æå‡
            if baseline_val != 0:
                improvement = (model_val - baseline_val) / abs(baseline_val) * 100
            else:
                improvement = 0
            
            comparison.append({
                "æŒ‡æ ‡": key,
                f"{baseline_name}": f"{baseline_val:.4f}",
                "LTRæ¨¡å‹": f"{model_val:.4f}",
                "æå‡(%)": f"{improvement:+.2f}%"
            })
    
    return pd.DataFrame(comparison)


def generate_evaluation_report(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    baseline_scores: np.ndarray,
    metadata: Optional[Dict[str, Any]] = None,
    feature_importance: Optional[pd.DataFrame] = None,
    output_path: Optional[str] = None
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š
    
    Args:
        y_true: çœŸå®ç›®æ ‡å€¼
        y_pred: æ¨¡å‹é¢„æµ‹åˆ†æ•°
        baseline_scores: baselineåˆ†æ•°
        metadata: å…ƒä¿¡æ¯ (åŒ…å«comboç­‰)
        feature_importance: ç‰¹å¾é‡è¦æ€§DataFrame
        output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„ (å¯é€‰)
        
    Returns:
        æŠ¥å‘Šdict
    """
    print(f"\n{'='*80}")
    print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*80}")
    
    # 1. è®¡ç®—æŒ‡æ ‡
    model_metrics = compute_ranking_metrics(y_true, y_pred, metadata)
    baseline_metrics = compute_ranking_metrics(y_true, baseline_scores, metadata)
    
    # 2. å¯¹æ¯”è¡¨
    comparison_df = compare_with_baseline(y_true, y_pred, baseline_scores)
    
    print("\nğŸ“Š æ¨¡å‹ vs Baseline å¯¹æ¯”:")
    print(comparison_df.to_string(index=False))
    
    # 3. Top-Kåˆ†æ
    print(f"\nğŸ† Top-10 ç­–ç•¥åˆ†æ:")
    
    # çœŸå®Top-10
    true_top10_idx = np.argsort(y_true)[-10:][::-1]
    print(f"  çœŸå®Top-10å¹³å‡æ”¶ç›Š: {y_true[true_top10_idx].mean():.4f}")
    
    # æ¨¡å‹é¢„æµ‹Top-10
    pred_top10_idx = np.argsort(y_pred)[-10:][::-1]
    pred_top10_true_mean = y_true[pred_top10_idx].mean()
    print(f"  æ¨¡å‹Top-10å¹³å‡æ”¶ç›Š: {pred_top10_true_mean:.4f}")
    
    # Baselineé¢„æµ‹Top-10
    baseline_top10_idx = np.argsort(baseline_scores)[-10:][::-1]
    baseline_top10_true_mean = y_true[baseline_top10_idx].mean()
    print(f"  Baseline Top-10å¹³å‡æ”¶ç›Š: {baseline_top10_true_mean:.4f}")
    
    # å‘½ä¸­åˆ†æ
    model_hits = len(set(true_top10_idx) & set(pred_top10_idx))
    baseline_hits = len(set(true_top10_idx) & set(baseline_top10_idx))
    print(f"  æ¨¡å‹å‘½ä¸­æ•°: {model_hits}/10")
    print(f"  Baselineå‘½ä¸­æ•°: {baseline_hits}/10")
    
    # 4. ç‰¹å¾é‡è¦æ€§
    if feature_importance is not None:
        print(f"\nğŸ” Top-15 é‡è¦ç‰¹å¾:")
        for idx, row in feature_importance.head(15).iterrows():
            print(f"  {row['feature']:30s}: {row['importance']:10.0f}")
    
    # 5. æ„å»ºæŠ¥å‘Šdict
    report = {
        "model_metrics": model_metrics,
        "baseline_metrics": baseline_metrics,
        "comparison": comparison_df.to_dict(orient="records"),
        "top10_analysis": {
            "true_mean": float(y_true[true_top10_idx].mean()),
            "model_pred_mean": float(pred_top10_true_mean),
            "baseline_pred_mean": float(baseline_top10_true_mean),
            "model_hits": int(model_hits),
            "baseline_hits": int(baseline_hits),
        }
    }
    
    if feature_importance is not None:
        report["feature_importance"] = feature_importance.head(20).to_dict(orient="records")
    
    # 6. ä¿å­˜æŠ¥å‘Š
    if output_path:
        import json
        from pathlib import Path
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    print(f"\n{'='*80}")
    
    return report


def create_ranking_comparison_df(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    baseline_scores: np.ndarray,
    combos: np.ndarray,
    top_n: int = 100
) -> pd.DataFrame:
    """
    åˆ›å»ºæ’åºå¯¹æ¯”è¡¨ (å±•ç¤ºTop-Nç­–ç•¥)
    
    Args:
        y_true: çœŸå®ç›®æ ‡å€¼
        y_pred: æ¨¡å‹é¢„æµ‹åˆ†æ•°
        baseline_scores: baselineåˆ†æ•°
        combos: ç­–ç•¥åç§°æ•°ç»„
        top_n: å±•ç¤ºå‰Nä¸ª
        
    Returns:
        æ’åºå¯¹æ¯”DataFrame
    """
    df = pd.DataFrame({
        "combo": combos,
        "true_value": y_true,
        "pred_score": y_pred,
        "baseline_score": baseline_scores,
    })
    
    # è®¡ç®—å„è‡ªæ’å
    df["true_rank"] = df["true_value"].rank(ascending=False, method="min").astype(int)
    df["pred_rank"] = df["pred_score"].rank(ascending=False, method="min").astype(int)
    df["baseline_rank"] = df["baseline_score"].rank(ascending=False, method="min").astype(int)
    
    # æŒ‰æ¨¡å‹é¢„æµ‹æ’åº
    df = df.sort_values("pred_rank").reset_index(drop=True)
    
    # åªä¿ç•™Top-N
    df_top = df.head(top_n).copy()
    
    # è®¡ç®—æ’åå˜åŒ–
    df_top["rank_change_vs_baseline"] = df_top["baseline_rank"] - df_top["pred_rank"]
    df_top["rank_gap_to_true"] = df_top["true_rank"] - df_top["pred_rank"]
    
    return df_top
