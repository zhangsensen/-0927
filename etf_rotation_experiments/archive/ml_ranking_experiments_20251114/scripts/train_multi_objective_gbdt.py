# -*- coding: utf-8 -*-
"""
@author: Copilot
@created: 2025-11-13
@description: å¤šç›®æ ‡å›å½’GBDTè®­ç»ƒ - å¹³è¡¡æ”¶ç›Šä¸é£é™©çš„å­¦ä¹ ç›®æ ‡
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr
import joblib
import json
import os
import glob

def calculate_target_scores(df):
    """è®¡ç®—ä¸åŒçš„ç›®æ ‡å‡½æ•°"""
    targets = {}
    
    # åŸºçº¿: çº¯ Sharpe
    targets['sharpe'] = df['sharpe']
    
    # ç›®æ ‡1: Sharpe - Î» * MaxDD (çº¿æ€§æƒ©ç½šå›æ’¤)
    targets['sharpe_minus_dd_0.3'] = df['sharpe'] - 0.3 * df['max_dd'].abs()
    targets['sharpe_minus_dd_0.5'] = df['sharpe'] - 0.5 * df['max_dd'].abs()
    targets['sharpe_minus_dd_1.0'] = df['sharpe'] - 1.0 * df['max_dd'].abs()
    
    # ç›®æ ‡2: Calmar Ratio (å¹´åŒ–æ”¶ç›Š / æœ€å¤§å›æ’¤)
    targets['calmar'] = df['annual_ret'] / (df['max_dd'].abs() + 1e-6)
    
    # ç›®æ ‡3: Sharpe / (1 + MaxDD) (æ¯”ä¾‹å½¢å¼)
    targets['sharpe_over_dd'] = df['sharpe'] / (1 + df['max_dd'].abs())
    
    # ç›®æ ‡4: åŠ æƒç»„åˆ (Sharpe^2 - DD)
    targets['sharpe2_minus_dd'] = df['sharpe']**2 - df['max_dd'].abs()
    
    return pd.DataFrame(targets)

def train_and_evaluate_model(X_train, y_train, X_test, y_test, model_name):
    """è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°"""
    model = GradientBoostingRegressor(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        min_samples_leaf=50,
        random_state=42,
        verbose=0
    )
    
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # è¯„ä¼° (ä½¿ç”¨ Spearman ç›¸å…³æ€§)
    train_spearman = spearmanr(y_train, y_train_pred)[0]
    test_spearman = spearmanr(y_test, y_test_pred)[0]
    
    # ä½†æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æ˜¯: é¢„æµ‹æ’åºä¸çœŸå®Sharpeçš„ç›¸å…³æ€§
    # (å› ä¸ºæœ€ç»ˆç›®æ ‡æ˜¯é€‰å‡ºé«˜Sharpeç­–ç•¥,è€Œéé«˜å¤åˆæŒ‡æ ‡ç­–ç•¥)
    
    results = {
        'model_name': model_name,
        'train_spearman': train_spearman,
        'test_spearman': test_spearman,
        'model': model
    }
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    run_id = '20251112_223854'
    
    print("="*80)
    print("ğŸ¯ å¤šç›®æ ‡å›å½’ GBDT è®­ç»ƒå®éªŒ")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    
    # åŠ è½½å›æµ‹ç»“æœ (åŒ…å«çœŸå® Sharpe, MaxDD ç­‰)
    backtest_files = glob.glob(f'results_combo_wfo/{run_id}_*/top3000_profit_backtest_slip2bps_{run_id}_*.csv')
    
    backtest_results = []
    for f in backtest_files:
        try:
            df = pd.read_csv(f)
            if 'combo' in df.columns and 'sharpe' in df.columns:
                # æå–éœ€è¦çš„åˆ—
                cols = ['combo', 'sharpe', 'max_dd', 'annual_ret']
                available_cols = [c for c in cols if c in df.columns]
                backtest_results.append(df[available_cols])
        except Exception as e:
            print(f"è¯»å– {f} å¤±è´¥: {e}")
    
    if not backtest_results:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›æµ‹ç»“æœæ–‡ä»¶ã€‚")
        return
    
    backtest_df = pd.concat(backtest_results, ignore_index=True)
    backtest_df = backtest_df.drop_duplicates(subset=['combo'], keep='last')
    
    print(f"âœ… å›æµ‹ç»“æœ: {len(backtest_df)} ä¸ªç­–ç•¥")
    
    # åŠ è½½ WFO ç‰¹å¾
    wfo_df = pd.read_parquet(f'results/run_{run_id}/ranking_blends/ranking_baseline.parquet')
    print(f"âœ… WFO ç‰¹å¾: {len(wfo_df)} ä¸ªç­–ç•¥")
    
    # 2. åˆå¹¶æ•°æ®
    print("\nğŸ”— åˆå¹¶æ•°æ®...")
    merged_df = backtest_df.merge(wfo_df, on='combo', how='inner')
    print(f"âœ… åˆå¹¶å: {len(merged_df)} ä¸ªç­–ç•¥")
    
    # 3. è®¡ç®—ä¸åŒçš„ç›®æ ‡å‡½æ•°
    print("\nğŸ“Š è®¡ç®—å¤šç›®æ ‡å‡½æ•°...")
    target_df = calculate_target_scores(merged_df)
    
    print(f"âœ… å·²è®¡ç®— {len(target_df.columns)} ä¸ªç›®æ ‡å‡½æ•°:")
    for col in target_df.columns:
        print(f"   - {col}")
    
    # 4. å‡†å¤‡ç‰¹å¾
    wfo_features = ['mean_oos_ic', 'oos_ic_std', 'positive_rate', 'stability_score', 'combo_size']
    
    # å¤„ç† combo_size å†²çª
    if 'combo_size' not in merged_df.columns:
        if 'combo_size_x' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_x']
        elif 'combo_size_y' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_y']
    
    X = merged_df[wfo_features].values
    X = np.nan_to_num(X, nan=0.0, posinf=10.0, neginf=-10.0)
    
    # ä¿å­˜çœŸå® Sharpe (ç”¨äºæœ€ç»ˆè¯„ä¼°)
    y_sharpe_real = merged_df['sharpe'].values
    
    # 5. æ•°æ®åˆ’åˆ†
    print("\nâœ‚ï¸  åˆ’åˆ†æ•°æ® (70% è®­ç»ƒ, 30% æµ‹è¯•)...")
    
    # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç¡®ä¿æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„åˆ’åˆ†
    indices = np.arange(len(X))
    train_idx, test_idx = train_test_split(indices, test_size=0.3, random_state=42)
    
    X_train, X_test = X[train_idx], X[test_idx]
    
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 6. è®­ç»ƒå¤šä¸ªæ¨¡å‹
    print("\n" + "="*80)
    print("ğŸŒ² è®­ç»ƒå¤šä¸ªç›®æ ‡å‡½æ•°çš„ GBDT æ¨¡å‹...")
    print("="*80)
    
    all_results = []
    
    for target_name in target_df.columns:
        print(f"\nè®­ç»ƒç›®æ ‡: {target_name}")
        
        y = target_df[target_name].values
        y_train, y_test = y[train_idx], y[test_idx]
        
        # è®­ç»ƒæ¨¡å‹
        result = train_and_evaluate_model(X_train, y_train, X_test, y_test, target_name)
        
        # å…³é”®è¯„ä¼°: é¢„æµ‹æ’åºä¸çœŸå®Sharpeçš„ç›¸å…³æ€§
        y_sharpe_test = y_sharpe_real[test_idx]
        
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹,ç„¶åè®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®Sharpeçš„ç›¸å…³æ€§
        y_test_pred = result['model'].predict(X_test)
        sharpe_spearman = spearmanr(y_test_pred, y_sharpe_test)[0]
        
        result['test_vs_real_sharpe_spearman'] = sharpe_spearman
        
        print(f"   è®­ç»ƒé›† Spearman (vs ç›®æ ‡): {result['train_spearman']:.4f}")
        print(f"   æµ‹è¯•é›† Spearman (vs ç›®æ ‡): {result['test_spearman']:.4f}")
        print(f"   æµ‹è¯•é›† Spearman (vs çœŸå®Sharpe): {sharpe_spearman:.4f} â­")
        
        all_results.append(result)
    
    # 7. å¯¹æ¯”åˆ†æ
    print("\n" + "="*80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    comparison_df = pd.DataFrame([{
        'ç›®æ ‡å‡½æ•°': r['model_name'],
        'è®­ç»ƒé›†ç›¸å…³æ€§': f"{r['train_spearman']:.4f}",
        'æµ‹è¯•é›†ç›¸å…³æ€§(vsç›®æ ‡)': f"{r['test_spearman']:.4f}",
        'æµ‹è¯•é›†ç›¸å…³æ€§(vsçœŸå®Sharpe)': f"{r['test_vs_real_sharpe_spearman']:.4f}"
    } for r in all_results])
    
    print(comparison_df.to_string(index=False))
    
    # 8. é€‰æ‹©æœ€ä½³æ¨¡å‹
    best_result = max(all_results, key=lambda x: x['test_vs_real_sharpe_spearman'])
    
    print("\n" + "="*80)
    print("ğŸ† æœ€ä½³æ¨¡å‹")
    print("="*80)
    print(f"   ç›®æ ‡å‡½æ•°: {best_result['model_name']}")
    print(f"   æµ‹è¯•é›† Spearman (vs çœŸå®Sharpe): {best_result['test_vs_real_sharpe_spearman']:.4f}")
    
    # 9. ä¿å­˜æœ€ä½³æ¨¡å‹
    output_dir = f'results/run_{run_id}'
    os.makedirs(output_dir, exist_ok=True)
    
    model_path = os.path.join(output_dir, 'gbdt_multi_objective.joblib')
    joblib.dump(best_result['model'], model_path)
    print(f"\nâœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜é…ç½®
    config = {
        'target_function': best_result['model_name'],
        'features': wfo_features,
        'train_spearman': float(best_result['train_spearman']),
        'test_spearman': float(best_result['test_spearman']),
        'test_vs_real_sharpe_spearman': float(best_result['test_vs_real_sharpe_spearman']),
        'n_train_samples': len(X_train),
        'n_test_samples': len(X_test)
    }
    
    config_path = os.path.join(output_dir, 'gbdt_multi_objective_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"âœ… é…ç½®å·²ä¿å­˜: {config_path}")
    
    # ä¿å­˜æ‰€æœ‰ç»“æœçš„å¯¹æ¯”
    comparison_path = os.path.join(output_dir, 'multi_objective_comparison.csv')
    comparison_df.to_csv(comparison_path, index=False)
    print(f"âœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_path}")
    
    # 10. æ€§èƒ½å¯¹æ¯”
    print("\n" + "="*80)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯” (vs åŸºçº¿)")
    print("="*80)
    
    baseline_spearman = 0.7129  # åŸºç¡€ GBDT (ç›®æ ‡=sharpe)
    improvement = best_result['test_vs_real_sharpe_spearman'] - baseline_spearman
    improvement_pct = (improvement / baseline_spearman) * 100
    
    print(f"   åŸºçº¿ GBDT (ç›®æ ‡=sharpe):        {baseline_spearman:.4f}")
    print(f"   å¤šç›®æ ‡ GBDT (ç›®æ ‡={best_result['model_name']}): {best_result['test_vs_real_sharpe_spearman']:.4f}")
    print(f"   æ”¹è¿›å¹…åº¦: {improvement:+.4f} ({improvement_pct:+.1f}%)")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. ä½¿ç”¨æœ€ä½³æ¨¡å‹å¯¹å®Œæ•´çš„12597ä¸ªç­–ç•¥é‡æ–°æ’åº")
    print("   2. è¿è¡Œ Top3000 çœŸå®å›æµ‹éªŒè¯")
    print("   3. ä¸ IC æ’åºå’ŒåŸºç¡€ ML æ’åºå¯¹æ¯”")

if __name__ == '__main__':
    main()
