# -*- coding: utf-8 -*-
"""
@author: Copilot
@created: 2025-11-13
@description: ä½¿ç”¨å¢å¼ºç‰¹å¾è®­ç»ƒ GBDT æ¨¡å‹
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from scipy.stats import spearmanr
import joblib
import json
import os

def main():
    """ä¸»å‡½æ•°"""
    run_id = '20251112_223854'
    
    print("="*80)
    print("ğŸš€ GBDT å¢å¼ºç‰¹å¾è®­ç»ƒå®éªŒ")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    
    # åŠ è½½å›æµ‹ç»“æœ (çœŸå® Sharpe)
    backtest_files = [
        f'results_combo_wfo/{run_id}_20251113_112641/top3000_profit_backtest_slip2bps_{run_id}_20251113_112641.csv',
        f'results_combo_wfo/{run_id}_20251113_112650/top3000_profit_backtest_slip2bps_{run_id}_20251113_112650.csv',
        f'results_combo_wfo/{run_id}_20251113_112657/top3000_profit_backtest_slip2bps_{run_id}_20251113_112657.csv',
        f'results_combo_wfo/{run_id}_20251113_112716/top3000_profit_backtest_slip2bps_{run_id}_20251113_112716.csv',
        f'results_combo_wfo/{run_id}_20251113_113852/top3000_profit_backtest_slip2bps_{run_id}_20251113_113852.csv',
        f'results_combo_wfo/{run_id}_20251113_114159/top3000_profit_backtest_slip2bps_{run_id}_20251113_114159.csv',
        f'results_combo_wfo/{run_id}_20251113_114610/top3000_profit_backtest_slip2bps_{run_id}_20251113_114610.csv',
    ]
    
    backtest_results = []
    for f in backtest_files:
        try:
            df = pd.read_csv(f)
            if 'combo' in df.columns and 'sharpe' in df.columns:
                backtest_results.append(df[['combo', 'sharpe']])
        except FileNotFoundError:
            pass
    
    if not backtest_results:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›æµ‹ç»“æœæ–‡ä»¶ã€‚")
        return
    
    backtest_df = pd.concat(backtest_results, ignore_index=True)
    backtest_df = backtest_df.drop_duplicates(subset=['combo'], keep='last')
    backtest_df.columns = ['combo', 'sharpe_real']
    
    print(f"âœ… å›æµ‹ç»“æœ: {len(backtest_df)} ä¸ªç­–ç•¥")
    
    # åŠ è½½ WFO ç‰¹å¾
    wfo_df = pd.read_parquet(f'results/run_{run_id}/ranking_blends/ranking_baseline.parquet')
    print(f"âœ… WFO ç‰¹å¾: {len(wfo_df)} ä¸ªç­–ç•¥, {len(wfo_df.columns)} ä¸ªç‰¹å¾")
    
    # åŠ è½½å¢å¼ºç‰¹å¾ (æ³¨æ„: generate_enhanced_features.py ä¿å­˜åˆ°äº† results/RUNID/ è€Œé results/run_RUNID/)
    enhanced_df = pd.read_parquet(f'results/{run_id}/enhanced_features.parquet')
    print(f"âœ… å¢å¼ºç‰¹å¾: {len(enhanced_df)} ä¸ªç­–ç•¥, {len(enhanced_df.columns)} ä¸ªç‰¹å¾")
    
    # 2. åˆå¹¶æ•°æ®
    print("\nğŸ”— åˆå¹¶æ•°æ®...")
    merged_df = backtest_df.merge(wfo_df, on='combo', how='inner')
    merged_df = merged_df.merge(enhanced_df, on='combo', how='inner', suffixes=('_wfo', '_enhanced'))
    
    print(f"âœ… åˆå¹¶å: {len(merged_df)} ä¸ªç­–ç•¥, {len(merged_df.columns)} ä¸ªåˆ—")
    
    # 3. å‡†å¤‡ç‰¹å¾
    print("\nğŸ“Š å‡†å¤‡ç‰¹å¾...")
    
    # WFO åŸºç¡€ç‰¹å¾
    wfo_features = ['mean_oos_ic', 'oos_ic_std', 'positive_rate', 'stability_score', 'combo_size']
    
    # å¢å¼ºç‰¹å¾ (ä»å›æµ‹æ‘˜è¦ä¸­æå–çš„)
    enhanced_features = [
        'calmar_ratio',
        'sortino_ratio', 
        'profit_factor',
        'win_rate',
        'max_consecutive_wins',
        'max_consecutive_losses',
        'sharpe_calmar_product',
        'dd_recovery_ratio',
        'sortino_sharpe_ratio',
        'win_rate_profit_composite',
        'win_loss_streak_ratio'
    ]
    
    # æ£€æŸ¥ç‰¹å¾å¯ç”¨æ€§
    available_wfo = [f for f in wfo_features if f in merged_df.columns]
    available_enhanced = [f for f in enhanced_features if f in merged_df.columns]
    
    # å¤„ç† combo_size åˆ—åå†²çª
    if 'combo_size' not in merged_df.columns:
        if 'combo_size_wfo' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_wfo']
        elif 'combo_size_enhanced' in merged_df.columns:
            merged_df['combo_size'] = merged_df['combo_size_enhanced']
    
    all_features = available_wfo + available_enhanced
    
    print(f"   WFO ç‰¹å¾ ({len(available_wfo)}): {available_wfo}")
    print(f"   å¢å¼ºç‰¹å¾ ({len(available_enhanced)}): {available_enhanced}")
    print(f"   æ€»è®¡: {len(all_features)} ä¸ªç‰¹å¾")
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    X = merged_df[all_features].values
    y = merged_df['sharpe_real'].values
    
    # å¤„ç† inf å’Œ nan
    X = np.nan_to_num(X, nan=0.0, posinf=10.0, neginf=-10.0)
    
    # 4. è®­ç»ƒæµ‹è¯•åˆ’åˆ†
    print("\nâœ‚ï¸  åˆ’åˆ†æ•°æ®...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # 5. è®­ç»ƒæ¨¡å‹
    print("\n" + "-"*80)
    print("ğŸŒ² è®­ç»ƒå¢å¼º GBDT æ¨¡å‹...")
    print("-"*80)
    
    model = GradientBoostingRegressor(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        min_samples_leaf=50,
        random_state=42,
        verbose=0
    )
    
    model.fit(X_train, y_train)
    
    # 6. è¯„ä¼°
    print("\nğŸ“Š æ¨¡å‹è¯„ä¼°...")
    
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    train_spearman = spearmanr(y_train, y_train_pred)[0]
    test_spearman = spearmanr(y_test, y_test_pred)[0]
    
    print(f"   è®­ç»ƒé›† Spearman: {train_spearman:.4f}")
    print(f"   æµ‹è¯•é›† Spearman: {test_spearman:.4f}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': all_features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ“ˆ Top 10 é‡è¦ç‰¹å¾:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:30s} {row['importance']:.4f}")
    
    # 7. ä¿å­˜æ¨¡å‹
    output_dir = f'results/run_{run_id}'
    os.makedirs(output_dir, exist_ok=True)
    
    model_path = os.path.join(output_dir, 'gbdt_enhanced.joblib')
    joblib.dump(model, model_path)
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜ç‰¹å¾åˆ—è¡¨
    feature_config = {
        'features': all_features,
        'wfo_features': available_wfo,
        'enhanced_features': available_enhanced,
        'train_spearman': float(train_spearman),
        'test_spearman': float(test_spearman)
    }
    
    config_path = os.path.join(output_dir, 'gbdt_enhanced_config.json')
    with open(config_path, 'w') as f:
        json.dump(feature_config, f, indent=2)
    print(f"âœ… é…ç½®å·²ä¿å­˜: {config_path}")
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    importance_path = os.path.join(output_dir, 'gbdt_enhanced_feature_importance.csv')
    feature_importance.to_csv(importance_path, index=False)
    print(f"âœ… ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {importance_path}")
    
    print("\n" + "="*80)
    print("ğŸ¯ è®­ç»ƒå®Œæˆ!")
    print("="*80)
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯” (ä¸åŸºç¡€ GBDT):")
    print(f"   åŸºç¡€ GBDT (5 ä¸ªç‰¹å¾):  æµ‹è¯•é›† Spearman = 0.7129")
    print(f"   å¢å¼º GBDT ({len(all_features)} ä¸ªç‰¹å¾): æµ‹è¯•é›† Spearman = {test_spearman:.4f}")
    print(f"   æ”¹è¿›å¹…åº¦: {(test_spearman - 0.7129):.4f} ({(test_spearman/0.7129 - 1)*100:+.1f}%)")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. è¿è¡Œ 'validate_ml_ranking_accuracy.py' éªŒè¯æ–°æ¨¡å‹çš„æ’åºæ•ˆæœ")
    print("   2. å¦‚æœæ•ˆæœæå‡,åº”ç”¨åˆ°å®Œæ•´çš„ run_20251112_223854")
    print("   3. é‡æ–°ç”Ÿæˆ ML æ’åºå¹¶éªŒè¯ Top3000")

if __name__ == '__main__':
    main()
