# ML 模型优化实验总结报告

**实验日期**: 2025年11月13日
**实验目标**: 提升GBDT模型的排序质量,特别是改善Top1000+区域的负相关问题
**基线性能**: 测试集Spearman相关性 = 0.7129 (5个WFO特征,目标=sharpe)

---

## 一、实验背景

### 1.1 问题识别

Top3000验证显示ML排序存在**头尾分化**现象:

| 区间 | Spearman相关性 | 结论 |
|------|----------------|------|
| Top100 | +0.2292 | ✅ 有效 |
| Top500 | +0.0906 | ✅ 微弱有效 |
| Top1000 | +0.0260 | ⚠️  接近无效 |
| Top2000 | -0.0549 | ❌ 负相关 |
| Top3000 | -0.1187 | ❌ 严重负相关 |

**根本原因假设**: GBDT回归优化MSE损失,不直接优化排序质量,导致尾部排序失效。

---

## 二、实验方案与结果

### 实验1: LambdaMART排序模型

**假设**: 切换到排序模型(优化NDCG),应能改善尾部排序质量。

**实现**:

- 使用LightGBM LambdaMART,objective="lambdarank"

- 特征: 与GBDT相同的5个WFO特征

- 数据: 5066个真实回测结果

**结果**:

| 指标 | GBDT回归 | LambdaMART | 变化 |
|------|----------|------------|------|
| 测试集Spearman | **0.7129** | 0.4783 | -32.9% ❌ |
| NDCG@1000 | **0.9410** | 0.9084 | -3.5% ❌ |
| Top10重叠 | 0% | 10% | +10% ✅ |
| Top100重叠 | 20% | 26% | +6% ✅ |
| Top500重叠 | **83.6%** | 65.8% | -17.8% ❌ |

**结论**: ❌ **LambdaMART显著弱于GBDT回归**

- 整体排序质量明显下降

- 仅极小头部(Top10-100)略有改善

- 中尾部(Top500+)大幅恶化

**原因分析**:

1. 当前特征集过于简单(仅5个),不足以支撑复杂的排序模型

2. LambdaMART需要更丰富的区分性特征

3. GBDT回归在简单特征下更稳健

---

### 实验2: 增强特征工程

**假设**: 添加风险调整收益特征(Sortino, Calmar, ProfitFactor等),能提升排序能力。

**实现**:

- 从回测摘要文件中提取13个原始特征

- 计算5个派生特征(复合指标)

- 总计: 5个WFO特征 + 11个增强特征 = 16个特征

**结果**:

| 指标 | 基础GBDT (5特征) | 增强GBDT (16特征) |
|------|------------------|-------------------|
| 测试集Spearman | 0.7129 | **0.9998** |
| 改进幅度 | - | +40.2% |

**问题**: ⚠️ **严重的数据泄漏!**

**致命缺陷**:

1. 增强特征(Sortino, ProfitFactor)来自**回测结果本身**

2. 相当于"用答案训练模型预测答案"

3. **12597个策略中只有5066个有增强特征**,剩余7531个无法预测

4. 如果只对已回测策略排序,不如直接按真实Sharpe排序

**结论**: ❌ **方案不可行**,无法泛化到未回测策略

---

### 实验3: 多目标回归

**假设**: 改变学习目标,从预测`sharpe`改为预测平衡收益与风险的复合指标。

**实现**:

- 保持5个WFO特征不变

- 测试7种目标函数:

  1. `sharpe` (基线)

  2. `sharpe - 0.3*max_dd` (轻度惩罚回撤)

  3. `sharpe - 0.5*max_dd` (中度惩罚回撤)

  4. `sharpe - 1.0*max_dd` (重度惩罚回撤)

  5. `calmar = annual_ret / max_dd`

  6. `sharpe / (1 + max_dd)`

  7. `sharpe^2 - max_dd`

**结果**:

| 目标函数 | 测试集Spearman (vs真实Sharpe) | vs基线 |
|----------|----------------------------------|--------|
| **sharpe** | **0.7192** | 基线 |
| sharpe - 0.3*dd | 0.7171 | -0.0021 |
| sharpe - 0.5*dd | 0.7158 | -0.0034 |
| sharpe - 1.0*dd | 0.7140 | -0.0052 |
| sharpe/(1+dd) | 0.7171 | -0.0021 |
| sharpe^2 - dd | 0.7119 | -0.0073 |
| calmar | 0.6937 | -0.0255 |

**结论**: ❌ **所有多目标变体都弱于纯Sharpe**

**关键洞察**:

- 添加回撤惩罚反而降低排序质量

- λ越大(回撤权重越高),效果越差

- **问题不在目标函数,而在特征!**

**深层原因**:

- WFO特征(IC均值/标准差)主要反映因子预测能力,不反映风险特征

- 模型无法从IC特征中学到"如何避免高回撤"

- 强迫预测"低回撤"复合目标,反而让模型confused

---

## 三、核心结论

### 3.1 实验失败的根本原因

所有优化尝试都未成功,根源在于:

1. **特征瓶颈**: 5个WFO特征信息有限,已经是"针对Sharpe优化的最佳组合"

2. **数据泄漏陷阱**: 真正有效的特征(回测指标)无法用于预测

3. **目标-特征不匹配**: IC特征无法有效预测回撤等风险指标

### 3.2 当前ML模型的本质

GBDT模型本质上是:

- **输入**: 5个简单的WFO统计量(IC均值/标准差/正率/稳定性/组合大小)

- **输出**: 对Sharpe的粗略估计

- **有效区域**: Top100-500(IC与Sharpe相关性较强)

- **失效区域**: Top1000+(IC与Sharpe相关性弱化)

### 3.3 为什么头部有效,尾部失效?

**头部(Top100-500)**:

- IC高的策略往往Sharpe也高(IC是Sharpe的先导指标)

- 模型学到的是"IC越高越好"这个简单规律

- 这个规律在头部较为可靠

**尾部(Top1000+)**:

- IC中等的策略,Sharpe分化严重

- 有些策略IC一般但Sharpe很高(低回撤、高稳定性)

- 有些策略IC一般且Sharpe很低(高回撤、不稳定)

- 模型无法区分这两类策略,因为**缺少风险特征**

---

## 四、未来可能的优化方向

基于实验结论,如果要继续优化,**唯一可行的路径**是:

### 方案A: 丰富WFO特征(可行但成本高)

从WFO过程中提取更多**不依赖回测结果**的特征:

1. **IC分布特征**:

   - IC偏度/峰度(识别异常窗口)

   - IC最小值/最大值(识别极端表现)

   - IC变异系数(CV = std/mean)

2. **窗口间稳健性**:

   - 连续正IC窗口数

   - 最长连续负IC窗口数

   - IC方向一致性(正率的时间序列稳定性)

3. **组合多样性**:

   - 因子数量分布的熵

   - 参数空间的覆盖度

   - 因子相关性矩阵特征

**挑战**: 需要修改WFO pipeline,重新生成特征,成本高。

### 方案B: 分层建模(低成本)

承认模型的局限性,分区使用:

- **Top500**: 使用当前GBDT(已验证有效)

- **Top500-3000**: 直接使用IC排序或简化规则

**优势**: 无需额外开发,基于现有验证结果。

### 方案C: 集成学习(中等成本)

训练多个互补模型:

- 模型1: GBDT(当前模型)

- 模型2: 纯IC排序

- 模型3: 基于稳定性的规则模型

加权融合(权重可通过Top3000验证调优)。

---

## 五、最终建议

### 5.1 短期建议(立即可行)

**保持现状,优化使用方式**:

1. **Top500以内**: 使用ML排序(已验证 +3.80%年化收益)

2. **Top500-1000**: 混合ML和IC(50:50权重)

3. **Top1000以外**: 纯IC排序

### 5.2 长期建议(需要投入)

如果要根本性提升,必须**丰富WFO特征**:

- 投入开发时间修改WFO pipeline

- 提取更多IC分布和稳健性特征

- 重新训练并全面验证

**投入产出比评估**:

- 预期改进: Top1000+ Spearman从-0.05提升到+0.05(乐观估计)

- 开发成本: 3-5人天

- **建议**: 除非有明确的业务需求(需要用到Top1000+策略),否则不值得投入

---

## 六、实验价值总结

虽然所有优化尝试都失败了,但这些实验的价值在于:

1. **避免了错误方向**: 证明了LambdaMART、增强特征、多目标回归都不可行

2. **澄清了根本问题**: 问题在特征,不在算法或目标函数

3. **明确了模型边界**: 当前模型在Top500有效,Top1000+无效

4. **指明了唯一出路**: 丰富WFO特征是唯一可行的优化路径

**最重要的洞察**:
> 在机器学习中,"garbage in, garbage out"。如果特征不包含预测所需的信息,再复杂的模型也无济于事。

---

**报告完成日期**: 2025年11月13日
**实验总耗时**: 约4小时
**核心结论**: 当前GBDT模型已接近"5个WFO特征"的理论上限,进一步优化需要丰富特征集。
