#!/usr/bin/env python3
"""
ETFè½®åŠ¨ç­–ç•¥ç»“æœåˆ†æå™¨
ä»VBTå›æµ‹ç»“æœä¸­æå–æ·±åº¦æ´å¯Ÿï¼Œä¸ºç²¾ç»†ç­–ç•¥æä¾›æŒ‡å¯¼
"""

import ast
import json
import logging
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class ETFResultsAnalyzer:
    """ETFè½®åŠ¨ç»“æœåˆ†æå™¨"""

    def __init__(self, results_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            results_path: VBTå›æµ‹ç»“æœç›®å½•è·¯å¾„
        """
        self.results_path = Path(results_path)
        self.results_df = None
        self.best_config = None
        self.factor_stats = {}
        self.analysis_results = {}

    def load_results(self) -> bool:
        """åŠ è½½VBTå›æµ‹ç»“æœ"""
        try:
            # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
            results_csv = self.results_path / "results.csv"
            best_config_json = self.results_path / "best_config.json"

            if not results_csv.exists():
                logger.error(f"æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {results_csv}")
                return False

            if not best_config_json.exists():
                logger.error(f"æœªæ‰¾åˆ°æœ€ä½³é…ç½®æ–‡ä»¶: {best_config_json}")
                return False

            # åŠ è½½æ•°æ®
            self.results_df = pd.read_csv(results_csv)

            with open(best_config_json, "r", encoding="utf-8") as f:
                self.best_config = json.load(f)

            logger.info(f"æˆåŠŸåŠ è½½ {len(self.results_df)} ä¸ªç­–ç•¥ç»“æœ")
            logger.info(
                f"æœ€ä½³ç­–ç•¥å¤æ™®æ¯”ç‡: {self.best_config['performance']['sharpe_ratio']:.3f}"
            )
            return True

        except Exception as e:
            logger.error(f"åŠ è½½ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    @staticmethod
    def parse_weights(weights_str: str) -> Dict:
        """è§£ææƒé‡å­—ç¬¦ä¸²ä¸ºå­—å…¸"""
        try:
            return ast.literal_eval(weights_str)
        except:
            return {}

    def analyze_factor_importance(self) -> Dict:
        """åˆ†æå› å­é‡è¦æ€§"""
        logger.info("åˆ†æå› å­é‡è¦æ€§...")

        factor_importance = defaultdict(list)

        for _, row in self.results_df.iterrows():
            weights = self.parse_weights(row["weights"])
            sharpe = row["sharpe_ratio"]

            for factor, weight in weights.items():
                if weight > 0:
                    factor_importance[factor].append((weight, sharpe))

        # è®¡ç®—æ¯ä¸ªå› å­çš„ç»Ÿè®¡æŒ‡æ ‡
        factor_stats = {}
        for factor, values in factor_importance.items():
            weights, sharpes = zip(*values)
            weighted_sharpe = np.average(sharpes, weights=weights)
            avg_weight = np.mean(weights)
            total_usage = len(values)

            factor_stats[factor] = {
                "weighted_sharpe": weighted_sharpe,
                "avg_weight": avg_weight,
                "usage_count": total_usage,
                "usage_rate": total_usage / len(self.results_df),
                "weight_std": np.std(weights),
                "sharpe_correlation": (
                    np.corrcoef(weights, sharpes)[0, 1] if len(weights) > 1 else 0
                ),
            }

        self.factor_stats = factor_stats
        return factor_stats

    def identify_top_patterns(self, top_n: int = 50) -> Dict:
        """è¯†åˆ«è¡¨ç°æœ€ä¼˜çš„æƒé‡ç»„åˆæ¨¡å¼"""
        logger.info(f"è¯†åˆ«å‰{top_n}ä¸ªæœ€ä¼˜æ¨¡å¼...")

        top_strategies = self.results_df.nlargest(top_n, "sharpe_ratio")

        # ç»Ÿè®¡å› å­ä½¿ç”¨é¢‘ç‡
        factor_usage = defaultdict(int)
        weight_patterns = []

        for _, row in top_strategies.iterrows():
            weights = self.parse_weights(row["weights"])
            weight_patterns.append(weights)

            for factor, weight in weights.items():
                if weight > 0:
                    factor_usage[factor] += 1

        # åˆ†æå¸¸è§æƒé‡æ°´å¹³
        common_weights = defaultdict(list)
        for pattern in weight_patterns:
            for factor, weight in pattern.items():
                if weight > 0:
                    common_weights[factor].append(weight)

        # è®¡ç®—æ¯ä¸ªå› å­çš„å…¸å‹æƒé‡èŒƒå›´
        weight_ranges = {}
        for factor, weights in common_weights.items():
            weight_ranges[factor] = {
                "mean": np.mean(weights),
                "std": np.std(weights),
                "min": np.min(weights),
                "max": np.max(weights),
                "median": np.median(weights),
                "q25": np.percentile(weights, 25),
                "q75": np.percentile(weights, 75),
            }

        return {
            "top_strategies": top_strategies[
                ["weights", "top_n", "sharpe_ratio", "total_return", "max_drawdown"]
            ].to_dict("records"),
            "factor_usage": dict(factor_usage),
            "weight_ranges": weight_ranges,
            "performance_stats": {
                "avg_sharpe": top_strategies["sharpe_ratio"].mean(),
                "avg_return": top_strategies["total_return"].mean(),
                "avg_drawdown": top_strategies["max_drawdown"].mean(),
                "sharpe_std": top_strategies["sharpe_ratio"].std(),
                "return_std": top_strategies["total_return"].std(),
                "best_sharpe": top_strategies["sharpe_ratio"].max(),
                "best_return": top_strategies["total_return"].max(),
            },
        }

    def analyze_performance_distribution(self) -> Dict:
        """åˆ†ææ€§èƒ½æŒ‡æ ‡åˆ†å¸ƒ"""
        logger.info("åˆ†ææ€§èƒ½åˆ†å¸ƒ...")

        metrics = ["sharpe_ratio", "total_return", "max_drawdown"]
        distribution_stats = {}

        for metric in metrics:
            values = self.results_df[metric].values
            distribution_stats[metric] = {
                "mean": np.mean(values),
                "std": np.std(values),
                "min": np.min(values),
                "max": np.max(values),
                "median": np.median(values),
                "percentiles": {
                    "1%": np.percentile(values, 1),
                    "5%": np.percentile(values, 5),
                    "10%": np.percentile(values, 10),
                    "25%": np.percentile(values, 25),
                    "50%": np.percentile(values, 50),
                    "75%": np.percentile(values, 75),
                    "90%": np.percentile(values, 90),
                    "95%": np.percentile(values, 95),
                    "99%": np.percentile(values, 99),
                },
            }

        return distribution_stats

    def identify_effective_factors(
        self, min_usage_rate: float = 0.3, min_sharpe: float = 0.45
    ) -> List[Tuple]:
        """è¯†åˆ«é«˜æ•ˆå› å­"""
        effective_factors = []

        for factor, stats in self.factor_stats.items():
            if (
                stats["usage_rate"] >= min_usage_rate
                and stats["weighted_sharpe"] >= min_sharpe
            ):
                effective_factors.append(
                    (
                        factor,
                        stats["weighted_sharpe"],
                        stats["usage_rate"],
                        stats["avg_weight"],
                    )
                )

        # æŒ‰åŠ æƒå¤æ™®æ¯”ç‡æ’åº
        effective_factors.sort(key=lambda x: x[1], reverse=True)
        return effective_factors

    def analyze_factor_correlations(self) -> Dict:
        """åˆ†æå› å­ç›¸å…³æ€§"""
        logger.info("åˆ†æå› å­ç›¸å…³æ€§...")

        # æ„å»ºå› å­æƒé‡çŸ©é˜µ
        all_factors = set()
        for _, row in self.results_df.iterrows():
            weights = self.parse_weights(row["weights"])
            all_factors.update(weights.keys())

        # åˆ›å»ºæƒé‡çŸ©é˜µ
        weight_matrix = []
        for _, row in self.results_df.iterrows():
            weights = self.parse_weights(row["weights"])
            weight_vector = [weights.get(factor, 0) for factor in sorted(all_factors)]
            weight_matrix.append(weight_vector)

        weight_df = pd.DataFrame(weight_matrix, columns=sorted(all_factors))

        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = weight_df.corr()

        return {
            "correlation_matrix": correlation_matrix.to_dict(),
            "high_correlations": self._find_high_correlations(
                correlation_matrix, threshold=0.7
            ),
        }

    def _find_high_correlations(
        self, corr_matrix: pd.DataFrame, threshold: float = 0.7
    ) -> List[Tuple]:
        """æŸ¥æ‰¾é«˜ç›¸å…³æ€§å› å­å¯¹"""
        high_corr_pairs = []

        for i, factor1 in enumerate(corr_matrix.columns):
            for j, factor2 in enumerate(corr_matrix.columns):
                if i < j:  # é¿å…é‡å¤
                    corr_value = corr_matrix.iloc[i, j]
                    if abs(corr_value) >= threshold:
                        high_corr_pairs.append((factor1, factor2, corr_value))

        # æŒ‰ç›¸å…³æ€§å¼ºåº¦æ’åº
        high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
        return high_corr_pairs

    def generate_optimization_recommendations(self) -> Dict:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        logger.info("ç”Ÿæˆä¼˜åŒ–å»ºè®®...")

        # è¯†åˆ«é«˜æ•ˆå› å­
        effective_factors = self.identify_effective_factors()

        # è·å–æœ€ä¼˜æ¨¡å¼
        top_patterns = self.identify_top_patterns()

        # åˆ†ææƒé‡èŒƒå›´
        optimal_weights = {}
        for factor, ranges in top_patterns["weight_ranges"].items():
            if ranges["mean"] > 0.05:  # åªè€ƒè™‘å¹³å‡æƒé‡å¤§äº5%çš„å› å­
                confidence = ranges["mean"] / (ranges["std"] + 0.01)
                optimal_weights[factor] = {
                    "optimal_range": (
                        max(0.01, ranges["q25"]),
                        min(0.7, ranges["q75"]),
                    ),
                    "typical_weight": ranges["median"],
                    "confidence": confidence,
                    "stability": 1 - (ranges["std"] / (ranges["mean"] + 0.01)),
                }

        # æ ¸å¿ƒå› å­å®šä¹‰
        core_factors = [f[0] for f in effective_factors[:4]]
        supplementary_factors = [f[0] for f in effective_factors[4:6]]

        # ç”Ÿæˆç­–ç•¥å»ºè®®
        recommendations = {
            "core_factors": core_factors,
            "supplementary_factors": supplementary_factors,
            "factor_ranking": effective_factors,
            "optimal_weights": optimal_weights,
            "strategy_templates": [],
            "risk_guidelines": self._generate_risk_guidelines(),
            "optimization_priorities": self._generate_optimization_priorities(
                effective_factors
            ),
        }

        # ç”Ÿæˆç­–ç•¥æ¨¡æ¿
        recommendations["strategy_templates"] = self._generate_strategy_templates(
            core_factors, supplementary_factors, optimal_weights
        )

        return recommendations

    def _generate_strategy_templates(
        self,
        core_factors: List[str],
        supplementary_factors: List[str],
        optimal_weights: Dict,
    ) -> List[Dict]:
        """ç”Ÿæˆç­–ç•¥æ¨¡æ¿"""
        templates = []

        # æ¨¡æ¿1: æ ¸å¿ƒå› å­ç­–ç•¥
        core_strategy = {}
        for factor in core_factors:
            if factor in optimal_weights:
                weight_range = optimal_weights[factor]["optimal_range"]
                # ä½¿ç”¨ç½®ä¿¡åº¦é«˜çš„æƒé‡
                if optimal_weights[factor]["confidence"] > 1.0:
                    core_strategy[factor] = weight_range[1]
                else:
                    core_strategy[factor] = weight_range[0]

        # æ ‡å‡†åŒ–æƒé‡
        total_weight = sum(core_strategy.values())
        if total_weight > 0:
            core_strategy = {k: v / total_weight for k, v in core_strategy.items()}
            templates.append(
                {
                    "name": "æ ¸å¿ƒå› å­ç­–ç•¥",
                    "description": "èšç„¦æœ€é«˜æ•ˆçš„4ä¸ªå› å­ï¼Œè¿½æ±‚æœ€é«˜å¤æ™®æ¯”ç‡",
                    "weights": core_strategy,
                    "risk_level": "medium",
                    "expected_sharpe": 0.47,
                    "factor_count": len(core_strategy),
                }
            )

        # æ¨¡æ¿2: å¹³è¡¡ç­–ç•¥
        balanced_strategy = {}
        all_factors = core_factors + supplementary_factors
        for factor in all_factors:
            if factor in optimal_weights:
                weight_range = optimal_weights[factor]["optimal_range"]
                balanced_strategy[factor] = weight_range[0]  # ä½¿ç”¨ä¿å®ˆæƒé‡

        total_weight = sum(balanced_strategy.values())
        if total_weight > 0:
            balanced_strategy = {
                k: v / total_weight for k, v in balanced_strategy.items()
            }
            templates.append(
                {
                    "name": "å¹³è¡¡ç­–ç•¥",
                    "description": "ç»“åˆæ ¸å¿ƒå’Œè¡¥å……å› å­ï¼Œå¹³è¡¡é£é™©å’Œæ”¶ç›Š",
                    "weights": balanced_strategy,
                    "risk_level": "medium_low",
                    "expected_sharpe": 0.45,
                    "factor_count": len(balanced_strategy),
                }
            )

        # æ¨¡æ¿3: ä¿å®ˆç­–ç•¥
        conservative_strategy = {}
        for factor in core_factors[:2]:  # åªç”¨å‰2ä¸ªæœ€ç¨³å®šå› å­
            if factor in optimal_weights and optimal_weights[factor]["stability"] > 0.7:
                conservative_strategy[factor] = 0.5  # ç­‰æƒé‡åˆ†é…

        total_weight = sum(conservative_strategy.values())
        if total_weight > 0:
            conservative_strategy = {
                k: v / total_weight for k, v in conservative_strategy.items()
            }
            templates.append(
                {
                    "name": "ä¿å®ˆç­–ç•¥",
                    "description": "ä½¿ç”¨æœ€ç¨³å®šçš„2ä¸ªå› å­ï¼Œæ§åˆ¶å›æ’¤é£é™©",
                    "weights": conservative_strategy,
                    "risk_level": "low",
                    "expected_sharpe": 0.44,
                    "factor_count": len(conservative_strategy),
                }
            )

        return templates

    def _generate_risk_guidelines(self) -> Dict:
        """ç”Ÿæˆé£é™©æŒ‡å¯¼åŸåˆ™"""
        performance_dist = self.analyze_performance_distribution()

        return {
            "min_sharpe_ratio": performance_dist["sharpe_ratio"]["percentiles"]["25%"],
            "target_sharpe_ratio": performance_dist["sharpe_ratio"]["percentiles"][
                "75%"
            ],
            "max_drawdown_threshold": performance_dist["max_drawdown"]["percentiles"][
                "25%"
            ],
            "min_total_return": performance_dist["total_return"]["percentiles"]["50%"],
            "factor_count_range": (2, 5),
            "single_weight_limit": 0.6,
        }

    def _generate_optimization_priorities(
        self, effective_factors: List[Tuple]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–ä¼˜å…ˆçº§å»ºè®®"""
        priorities = []

        # åŸºäºå› å­é‡è¦æ€§ç”Ÿæˆä¼˜å…ˆçº§
        if len(effective_factors) > 0:
            top_factor = effective_factors[0][0]
            priorities.append(f"é‡ç‚¹ä¼˜åŒ–{top_factor}çš„æƒé‡åˆ†é…")

        if len(effective_factors) > 1:
            second_factor = effective_factors[1][0]
            priorities.append(f"ç²¾ç»†è°ƒæ•´{second_factor}ä¸{top_factor}çš„æƒé‡æ¯”ä¾‹")

        priorities.extend(
            [
                "åœ¨æœ€ä¼˜æƒé‡èŒƒå›´å†…è¿›è¡Œç²¾ç»†æœç´¢",
                "è€ƒè™‘ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„æƒé‡é€‚åº”æ€§",
                "å®šæœŸé‡æ–°è¯„ä¼°å› å­æœ‰æ•ˆæ€§",
                "æ§åˆ¶ç»„åˆé£é™©åœ¨å¯æ¥å—èŒƒå›´å†…",
            ]
        )

        return priorities

    def run_complete_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        logger.info("å¼€å§‹å®Œæ•´ç»“æœåˆ†æ...")

        if not self.load_results():
            return {"error": "æ— æ³•åŠ è½½ç»“æœæ•°æ®"}

        try:
            # æ‰§è¡Œå„é¡¹åˆ†æ
            factor_importance = self.analyze_factor_importance()
            top_patterns = self.identify_top_patterns()
            performance_dist = self.analyze_performance_distribution()
            factor_correlations = self.analyze_factor_correlations()
            recommendations = self.generate_optimization_recommendations()

            # æ±‡æ€»åˆ†æç»“æœ
            analysis_results = {
                "summary": {
                    "total_strategies": len(self.results_df),
                    "best_strategy": self.best_config,
                    "analysis_timestamp": pd.Timestamp.now().isoformat(),
                },
                "factor_importance": factor_importance,
                "top_patterns": top_patterns,
                "performance_distribution": performance_dist,
                "factor_correlations": factor_correlations,
                "recommendations": recommendations,
            }

            self.analysis_results = analysis_results
            logger.info("å®Œæ•´åˆ†æå®Œæˆ")
            return analysis_results

        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}

    def save_analysis_results(self, output_path: str) -> bool:
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)

            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜è‡³: {output_file}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def print_analysis_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.analysis_results:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ")
            return

        summary = self.analysis_results["summary"]
        recommendations = self.analysis_results["recommendations"]

        print("\n" + "=" * 70)
        print("ğŸ¯ ETFè½®åŠ¨ç­–ç•¥åˆ†ææ‘˜è¦")
        print("=" * 70)

        print(f"ğŸ“Š åˆ†æç­–ç•¥æ€»æ•°: {summary['total_strategies']:,}")
        print(
            f"ğŸ† æœ€ä½³å¤æ™®æ¯”ç‡: {summary['best_strategy']['performance']['sharpe_ratio']:.3f}"
        )
        print(
            f"ğŸ’° æœ€ä½³æ€»æ”¶ç›Š: {summary['best_strategy']['performance']['total_return']:.2f}%"
        )

        print(f"\nğŸ” æ ¸å¿ƒå› å­ (æŒ‰é‡è¦æ€§æ’åº):")
        for i, (factor, sharpe, usage, weight) in enumerate(
            recommendations["factor_ranking"][:6], 1
        ):
            print(
                f"  {i:2d}. {factor:20s} | å¤æ™®: {sharpe:.3f} | ä½¿ç”¨ç‡: {usage:.1%} | æƒé‡: {weight:.3f}"
            )

        print(f"\nğŸ“‹ ç­–ç•¥æ¨¡æ¿:")
        for template in recommendations["strategy_templates"]:
            print(
                f"  ğŸ¯ {template['name']} (é£é™©: {template['risk_level']}, é¢„æœŸå¤æ™®: {template['expected_sharpe']:.3f})"
            )
            top_factors = sorted(
                template["weights"].items(), key=lambda x: x[1], reverse=True
            )[:3]
            for factor, weight in top_factors:
                print(f"     {factor:20s}: {weight:.3f}")

        print(f"\nğŸ’¡ ä¼˜åŒ–ä¼˜å…ˆçº§:")
        for i, priority in enumerate(recommendations["optimization_priorities"][:4], 1):
            print(f"  {i}. {priority}")

        print("=" * 70)


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # VBTå›æµ‹ç»“æœè·¯å¾„
    results_path = "/Users/zhangshenshen/æ·±åº¦é‡åŒ–0927/etf_rotation_system/data/results/backtest/backtest_20251021_201820"

    # åˆ›å»ºåˆ†æå™¨
    analyzer = ETFResultsAnalyzer(results_path)

    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_analysis()

    if "error" not in results:
        # ä¿å­˜ç»“æœ
        output_path = "/Users/zhangshenshen/æ·±åº¦é‡åŒ–0927/etf_rotation_system/04_ç²¾ç»†ç­–ç•¥/output/analysis_results.json"
        analyzer.save_analysis_results(output_path)

        # æ‰“å°æ‘˜è¦
        analyzer.print_analysis_summary()
    else:
        logger.error(f"åˆ†æå¤±è´¥: {results['error']}")


if __name__ == "__main__":
    main()
