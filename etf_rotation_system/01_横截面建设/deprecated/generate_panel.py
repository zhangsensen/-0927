#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ETFè½®åŠ¨å› å­é¢æ¿ç”Ÿæˆ - ç”Ÿäº§çº§"""
import json
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import yaml
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def validate_path(path: str, must_exist: bool = False) -> Path:
    """éªŒè¯å¹¶è§„èŒƒåŒ–è·¯å¾„"""
    try:
        p = Path(path).resolve()
        if must_exist and not p.exists():
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
        # é˜²æ­¢è·¯å¾„ç©¿è¶Š
        if ".." in str(p):
            raise ValueError(f"éæ³•è·¯å¾„: {path}")
        return p
    except Exception as e:
        logger.error(f"è·¯å¾„éªŒè¯å¤±è´¥: {path} - {e}")
        raise


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path) as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.warning(f"é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        return {
            "factor_generation": {
                "momentum_periods": [20, 63, 126, 252],
                "volatility_windows": [20, 60, 120],
                "rsi_windows": [6, 14, 24],
                "price_position_windows": [20, 60, 120],
                "volume_ratio_windows": [20, 60],
            }
        }


def load_price_data(data_dir: Path) -> pd.DataFrame:
    """åŠ è½½ä»·æ ¼æ•°æ®ï¼ˆå®Œæ•´OHLCVï¼‰"""
    logger.info(f"åŠ è½½ä»·æ ¼æ•°æ®: {data_dir}")
    files = sorted(data_dir.glob("*.parquet"))
    if not files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_dir}")

    prices = []
    for f in files:
        try:
            df = pd.read_parquet(f)
            symbol = f.stem.split("_")[0]
            df["symbol"] = symbol
            df["date"] = pd.to_datetime(df["trade_date"])

            if "vol" in df.columns and "volume" not in df.columns:
                df["volume"] = df["vol"]

            # å®Œæ•´OHLCV + amountæ•°æ®
            required_cols = ["date", "open", "high", "low", "close", "volume", "symbol"]
            if "amount" in df.columns:
                required_cols.append("amount")
            prices.append(df[required_cols])
        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥ {f.name}: {e}")
            continue

    if not prices:
        raise ValueError("æ— æœ‰æ•ˆæ•°æ®")

    price_df = pd.concat(prices, ignore_index=True)
    logger.info(f"åŠ è½½å®Œæˆ: {len(prices)} ä¸ªæ ‡çš„, {len(price_df)} æ¡è®°å½•")
    return price_df


def calculate_factors_single(args: Tuple[str, pd.DataFrame, Dict]) -> pd.DataFrame:
    """è®¡ç®—å•ä¸ªæ ‡çš„çš„å› å­ï¼ˆå¹¶è¡ŒåŒ–å•å…ƒï¼‰- 35ä¸ªå› å­"""
    symbol, symbol_data, config = args

    try:
        # æå–æ•°æ®
        open_p = symbol_data["open"].values
        high = symbol_data["high"].values
        low = symbol_data["low"].values
        close = symbol_data["close"].values
        volume = symbol_data["volume"].values
        dates = symbol_data["date"].values

        # è½¬ä¸ºSeriesä¾¿äºå‘é‡åŒ–
        s_open = pd.Series(open_p, index=symbol_data.index)
        s_high = pd.Series(high, index=symbol_data.index)
        s_low = pd.Series(low, index=symbol_data.index)
        s_close = pd.Series(close, index=symbol_data.index)
        s_vol = pd.Series(volume, index=symbol_data.index)

        factors = pd.DataFrame(index=symbol_data.index)
        factors["date"] = dates
        factors["symbol"] = symbol

        # ========== åŸæœ‰18ä¸ªå› å­ ==========

        # åŠ¨é‡å› å­ (4ä¸ª)
        for period in config["momentum_periods"]:
            factors[f"MOMENTUM_{period}D"] = (
                s_close / s_close.shift(period) - 1
            ).values

        # æ³¢åŠ¨ç‡å› å­ (3ä¸ª)
        ret = s_close.pct_change()
        for window in config["volatility_windows"]:
            factors[f"VOLATILITY_{window}D"] = (
                ret.rolling(window, min_periods=1).std() * np.sqrt(252)
            ).values

        # ğŸ”§ ä¿®å¤å›æ’¤å› å­ (2ä¸ª) - æ·»åŠ min_periods=1
        for window in [63, 126]:
            rolling_max = s_close.rolling(window, min_periods=1).max()
            dd = (s_close - rolling_max) / rolling_max
            factors[f"DRAWDOWN_{window}D"] = dd.values

        # åŠ¨é‡åŠ é€Ÿ (1ä¸ª)
        mom_short = s_close / s_close.shift(63) - 1
        mom_long = s_close / s_close.shift(252) - 1
        factors["MOM_ACCEL"] = (mom_short - mom_long).values

        # RSI (3ä¸ª)
        for window in config["rsi_windows"]:
            delta = s_close.diff()
            gain = delta.where(delta > 0, 0).rolling(window, min_periods=1).mean()
            loss = -delta.where(delta < 0, 0).rolling(window, min_periods=1).mean()
            rs = gain / (loss + 1e-10)
            rsi = 100 - (100 / (1 + rs))
            factors[f"RSI_{window}"] = rsi.values

        # ä»·æ ¼ä½ç½® (3ä¸ª)
        for window in config["price_position_windows"]:
            roll_high = s_high.rolling(window, min_periods=1).max()
            roll_low = s_low.rolling(window, min_periods=1).min()
            pos = (s_close - roll_low) / (roll_high - roll_low + 1e-10)
            factors[f"PRICE_POSITION_{window}D"] = pos.values

        # æˆäº¤é‡æ¯”ç‡ (2ä¸ª)
        for window in config["volume_ratio_windows"]:
            vol_ma = s_vol.rolling(window, min_periods=1).mean()
            factors[f"VOLUME_RATIO_{window}D"] = (s_vol / (vol_ma + 1e-10)).values

        # ========== æ–°å¢12ä¸ªå› å­ ==========

        # 1. OVERNIGHT_RETURN - éš”å¤œè·³ç©ºåŠ¨é‡
        prev_close = s_close.shift(1)
        factors["OVERNIGHT_RETURN"] = ((s_open - prev_close) / prev_close).values

        # 2. ATR_14 - çœŸå®æ³¢åŠ¨å¹…åº¦
        tr1 = s_high - s_low
        tr2 = (s_high - s_close.shift(1)).abs()
        tr3 = (s_low - s_close.shift(1)).abs()
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        factors["ATR_14"] = tr.rolling(14, min_periods=1).mean().values

        # 3. DOJI_PATTERN - åå­—æ˜Ÿå½¢æ€
        body = (s_close - s_open).abs()
        range_hl = s_high - s_low
        factors["DOJI_PATTERN"] = (body / (range_hl + 1e-10)).values

        # 4. INTRA_DAY_RANGE - æ—¥å†…æ³¢åŠ¨ç‡
        factors["INTRA_DAY_RANGE"] = ((s_high - s_low) / s_close).values

        # 5. BULLISH_ENGULFING - çœ‹æ¶¨åæ²¡å½¢æ€
        prev_open = s_open.shift(1)
        prev_body = (s_close.shift(1) - prev_open).abs()
        curr_body = (s_close - s_open).abs()
        is_bullish = (s_close > s_open) & (s_close.shift(1) < prev_open)
        is_engulfing = (
            (curr_body > prev_body)
            & (s_close > prev_open)
            & (s_open < s_close.shift(1))
        )
        factors["BULLISH_ENGULFING"] = (is_bullish & is_engulfing).astype(float).values

        # 6. HAMMER_PATTERN - é”¤å­çº¿åè½¬ä¿¡å·
        lower_shadow = s_close - s_low
        upper_shadow = s_high - s_close
        is_hammer = (lower_shadow > 2 * body) & (upper_shadow < body)
        factors["HAMMER_PATTERN"] = is_hammer.astype(float).values

        # 7. PRICE_IMPACT - ä»·æ ¼å†²å‡»ï¼ˆå¸‚åœºå¾®è§‚ç»“æ„ï¼‰
        price_change = s_close.pct_change().abs()
        vol_change = s_vol.pct_change().abs()
        factors["PRICE_IMPACT"] = (price_change / (vol_change + 1e-10)).values

        # 8. VOLUME_PRICE_TREND - é‡ä»·è¶‹åŠ¿ä¸€è‡´æ€§
        price_dir = (s_close > s_close.shift(1)).astype(float)
        vol_dir = (s_vol > s_vol.shift(1)).astype(float)
        vpt = (price_dir == vol_dir).astype(float).rolling(20, min_periods=1).mean()
        factors["VOLUME_PRICE_TREND"] = vpt.values

        # 9. VOL_MA_RATIO_5 - çŸ­æœŸæˆäº¤é‡åŠ¨æ€
        vol_ma5 = s_vol.rolling(5, min_periods=1).mean()
        factors["VOL_MA_RATIO_5"] = (s_vol / (vol_ma5 + 1e-10)).values

        # 10. VOL_VOLATILITY_20 - æˆäº¤é‡ç¨³å®šæ€§
        vol_std = s_vol.rolling(20, min_periods=1).std()
        vol_mean = s_vol.rolling(20, min_periods=1).mean()
        factors["VOL_VOLATILITY_20"] = (vol_std / (vol_mean + 1e-10)).values

        # 11. TRUE_RANGE - æ³¢åŠ¨ç‡ç»“æ„
        factors["TRUE_RANGE"] = (tr / s_close).values

        # 12. BUY_PRESSURE - æ—¥å†…ä»·æ ¼ä½ç½®
        factors["BUY_PRESSURE"] = ((s_close - s_low) / (s_high - s_low + 1e-10)).values

        # ========== æ–°å¢5ä¸ªèµ„é‡‘æµå› å­ ==========

        # éœ€è¦amountæ•°æ®ï¼Œä»åŸå§‹æ•°æ®ä¸­æå–
        if "amount" in symbol_data.columns:
            s_amount = pd.Series(symbol_data["amount"].values, index=symbol_data.index)
        else:
            # å¦‚æœæ²¡æœ‰amountæ•°æ®ï¼Œç”¨volume * closeä¼°ç®—
            s_amount = s_vol * s_close

        # 13. VWAP_DEVIATION - VWAPåç¦»åº¦ (èµ„é‡‘æµå¼ºå¼±)
        vwap = s_amount / (s_vol + 1e-10)  # æˆäº¤é¢/æˆäº¤é‡ = å‡ä»·
        factors["VWAP_DEVIATION"] = ((s_close - vwap) / (vwap + 1e-10)).values

        # 14. AMOUNT_SURGE_5D - æˆäº¤é¢çªå¢ (èµ„é‡‘æµå…¥ä¿¡å·)
        amount_ma5 = s_amount.rolling(5, min_periods=1).mean()
        amount_ma20 = s_amount.rolling(20, min_periods=1).mean()
        factors["AMOUNT_SURGE_5D"] = (amount_ma5 / (amount_ma20 + 1e-10) - 1).values

        # 15. PRICE_VOLUME_DIV - é‡ä»·èƒŒç¦» (èµ„é‡‘æµå‘ä¿¡å·)
        price_change = s_close.pct_change()
        vol_change = s_vol.pct_change()
        pv_divergence = (
            np.sign(price_change) * vol_change.rolling(5, min_periods=1).mean()
        )
        factors["PRICE_VOLUME_DIV"] = pv_divergence.values

        # 16. INTRADAY_POSITION - æ—¥å†…ä»·æ ¼ä½ç½® (Williams %Rç±»å‹ï¼Œéèµ„é‡‘æµ)
        # ä¿®å¤ï¼šé‡å‘½åä¸ºå‡†ç¡®åç§°ï¼Œé¿å…è¯¯å¯¼
        price_pos = (s_close - s_low) / (s_high - s_low + 1e-10)
        factors["INTRADAY_POSITION"] = price_pos.rolling(5, min_periods=1).mean().values

        # 17. LARGE_ORDER_SIGNAL - å¤§å•æµå…¥ (æœºæ„èµ„é‡‘æ´»åŠ¨)
        # ç”¨å‡ä»·å˜åŒ– + æˆäº¤é‡å¼‚å¸¸ä»£ç†å¤§å•æ´»åŠ¨
        avg_price = s_amount / (s_vol + 1e-10)
        avg_price_change = avg_price.pct_change()
        vol_ratio = s_vol / s_vol.rolling(20, min_periods=1).mean()
        large_order = ((avg_price_change > 0) & (vol_ratio > 1.2)).astype(float)
        factors["LARGE_ORDER_SIGNAL"] = large_order.values

        return factors

    except Exception as e:
        logger.error(f"å› å­è®¡ç®—å¤±è´¥ {symbol}: {e}")
        return pd.DataFrame()


def calculate_factors_parallel(
    price_df: pd.DataFrame, config: Dict, max_workers: int = 4
) -> pd.DataFrame:
    """å¹¶è¡Œè®¡ç®—å› å­"""
    symbols = sorted(price_df["symbol"].unique())
    logger.info(f"å¹¶è¡Œè®¡ç®—å› å­: {len(symbols)} ä¸ªæ ‡çš„, {max_workers} ä¸ªè¿›ç¨‹")

    # å‡†å¤‡ä»»åŠ¡
    tasks = []
    for symbol in symbols:
        symbol_data = price_df[price_df["symbol"] == symbol].sort_values("date")
        tasks.append((symbol, symbol_data, config["factor_generation"]))

    # å¹¶è¡Œæ‰§è¡Œ
    factors_list = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(calculate_factors_single, task): task[0] for task in tasks
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc="è®¡ç®—å› å­"):
            symbol = futures[future]
            try:
                result = future.result()
                if not result.empty:
                    factors_list.append(result)
            except Exception as e:
                logger.error(f"ä»»åŠ¡å¤±è´¥ {symbol}: {e}")

    if not factors_list:
        raise ValueError("æ— æœ‰æ•ˆå› å­æ•°æ®")

    panel = pd.concat(factors_list, ignore_index=True)
    panel = panel.set_index(["symbol", "date"]).sort_index()

    return panel


def save_results(panel: pd.DataFrame, output_dir: Path) -> Tuple[str, str]:
    """ä¿å­˜ç»“æœ - ä½¿ç”¨æ—¶é—´æˆ³å­ç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºæ—¶é—´æˆ³å­ç›®å½•
    timestamp_dir = output_dir / f"panel_{timestamp}"
    timestamp_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜é¢æ¿
    panel_file = timestamp_dir / "panel.parquet"
    panel.to_parquet(panel_file)
    logger.info(f"é¢æ¿å·²ä¿å­˜: {panel_file}")

    # ä¿å­˜å…ƒæ•°æ®
    meta = {
        "timestamp": timestamp,
        "etf_count": panel.index.get_level_values("symbol").nunique(),
        "factor_count": len(panel.columns),
        "data_points": len(panel),
        "coverage_rate": float(panel.notna().mean().mean()),
        "factors": panel.columns.tolist(),
        "date_range": {
            "start": str(panel.index.get_level_values("date").min().date()),
            "end": str(panel.index.get_level_values("date").max().date()),
        },
        "files": {"panel": str(panel_file), "directory": str(timestamp_dir)},
    }

    meta_file = timestamp_dir / "metadata.json"
    with open(meta_file, "w") as f:
        json.dump(meta, f, indent=2)
    logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {meta_file}")

    # è¾“å‡ºç»Ÿè®¡
    logger.info(f"é¢æ¿ç»Ÿè®¡:")
    logger.info(f"  æ ‡çš„æ•°: {meta['etf_count']}")
    logger.info(f"  å› å­æ•°: {meta['factor_count']}")
    logger.info(f"  æ•°æ®ç‚¹: {meta['data_points']}")
    logger.info(f"  è¦†ç›–ç‡: {meta['coverage_rate']:.2%}")
    logger.info(f"  ä¿å­˜ç›®å½•: {timestamp_dir}")

    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—æ–‡ä»¶
    log_file = timestamp_dir / "execution_log.txt"
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(f"ETFæ¨ªæˆªé¢å› å­é¢æ¿ç”Ÿæˆæ‰§è¡Œæ—¥å¿—\n")
        f.write(f"æ‰§è¡Œæ—¶é—´: {timestamp}\n")
        f.write(f"æ ‡çš„æ•°: {meta['etf_count']}\n")
        f.write(f"å› å­æ•°: {meta['factor_count']}\n")
        f.write(f"æ•°æ®ç‚¹: {meta['data_points']}\n")
        f.write(f"è¦†ç›–ç‡: {meta['coverage_rate']:.2%}\n")
        f.write(
            f"æ—¶é—´èŒƒå›´: {meta['date_range']['start']} è‡³ {meta['date_range']['end']}\n"
        )
        f.write(f"\nå› å­åˆ—è¡¨:\n")
        for i, factor in enumerate(meta["factors"], 1):
            f.write(f"  {i:2d}. {factor}\n")

    logger.info(f"æ‰§è¡Œæ—¥å¿—å·²ä¿å­˜: {log_file}")

    return str(panel_file), str(meta_file)


def generate_etf_panel(
    data_dir: str, output_dir: str, config_path: str = None, max_workers: int = 4
) -> Tuple[str, str]:
    """ç”ŸæˆETFå› å­é¢æ¿ï¼ˆä¸»å‡½æ•°ï¼‰"""
    logger.info("=" * 80)
    logger.info("ETFè½®åŠ¨å› å­é¢æ¿ç”Ÿæˆ")
    logger.info("=" * 80)

    try:
        # éªŒè¯è·¯å¾„
        data_dir_path = validate_path(data_dir, must_exist=True)
        output_dir_path = validate_path(output_dir)

        # åŠ è½½é…ç½®
        config = (
            load_config(config_path)
            if config_path
            else load_config("config/etf_config.yaml")
        )

        # åŠ è½½ä»·æ ¼æ•°æ®
        price_df = load_price_data(data_dir_path)

        # å¹¶è¡Œè®¡ç®—å› å­
        panel = calculate_factors_parallel(price_df, config, max_workers)

        # ä¿å­˜ç»“æœ
        return save_results(panel, output_dir_path)

    except Exception as e:
        logger.error(f"é¢æ¿ç”Ÿæˆå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ETFå› å­é¢æ¿ç”Ÿæˆ")
    parser.add_argument("--data-dir", default="raw/ETF/daily", help="æ•°æ®ç›®å½•")
    parser.add_argument(
        "--output-dir",
        default="etf_rotation_system/data/results/panels",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--workers", type=int, default=4, help="å¹¶è¡Œè¿›ç¨‹æ•°")

    args = parser.parse_args()

    try:
        panel_file, meta_file = generate_etf_panel(
            args.data_dir, args.output_dir, args.config, args.workers
        )
        logger.info("âœ… å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ å¤±è´¥: {e}")
        exit(1)
