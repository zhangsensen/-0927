#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¹¶è¡Œè®¡ç®—ä¼˜åŒ–ç‰ˆæœ¬ - ETFè½®åŠ¨å›æµ‹å¼•æ“
é€šè¿‡å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†æƒé‡ç»„åˆï¼Œå®ç°8-16å€æ€§èƒ½æå‡

ä¼˜åŒ–åŸç†ï¼š
- æƒé‡ç»„åˆç‹¬ç«‹è®¡ç®—ï¼Œå®Œç¾é€‚åˆå¹¶è¡Œ
- æ¯ä¸ªè¿›ç¨‹å¤„ç†éƒ¨åˆ†æƒé‡ç»„åˆ
- ç†è®ºåŠ é€Ÿæ¯”ï¼šCPUæ ¸å¿ƒæ•° Ã— å¹¶è¡Œæ•ˆç‡
"""

import itertools
import json
import logging
import multiprocessing as mp
import os
import sys
import time
from datetime import datetime
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

try:
    import vectorbt as vbt
    HAS_VBT = True
except ImportError:
    HAS_VBT = False
    print("è­¦å‘Š: æœªå®‰è£… vectorbtï¼Œè¯·è¿è¡Œ: pip install vectorbt")

# è®¾ç½®çº¿ç¨‹æ•°ä»¥é¿å…èµ„æºç«äº‰
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")


class ParallelBacktestEngine:
    """å¹¶è¡Œå›æµ‹å¼•æ“"""

    def __init__(
        self,
        n_workers: int = None,
        chunk_size: int = 10,
        enable_cache: bool = True,
        log_level: str = "INFO"
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œå›æµ‹å¼•æ“

        Args:
            n_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
            chunk_size: æ¯ä¸ªä»»åŠ¡å¤„ç†çš„æƒé‡ç»„åˆæ•°é‡
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            log_level: æ—¥å¿—çº§åˆ«
        """
        self.n_workers = n_workers or max(1, mp.cpu_count() - 1)
        self.chunk_size = chunk_size
        self.enable_cache = enable_cache

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, log_level.upper()),
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        self.logger.info(f"åˆå§‹åŒ–å¹¶è¡Œå›æµ‹å¼•æ“: {self.n_workers}å·¥ä½œè¿›ç¨‹, chunk_size={chunk_size}")

    def _load_factor_panel(self, panel_path: str) -> pd.DataFrame:
        """åŠ è½½å› å­é¢æ¿"""
        panel = pd.read_parquet(panel_path)
        if not isinstance(panel.index, pd.MultiIndex):
            raise ValueError("é¢æ¿å¿…é¡»æ˜¯ (symbol, date) MultiIndex")
        return panel

    def _load_price_data(self, price_dir: str) -> pd.DataFrame:
        """åŠ è½½ä»·æ ¼æ•°æ®"""
        import glob
        prices = []
        for f in sorted(glob.glob(f"{price_dir}/*.parquet")):
            df = pd.read_parquet(f)
            symbol = f.split("/")[-1].split("_")[0]
            df["symbol"] = symbol
            df["date"] = pd.to_datetime(df["trade_date"])
            prices.append(df[["date", "close", "symbol"]])

        price_df = pd.concat(prices, ignore_index=True)
        pivot = price_df.pivot(index="date", columns="symbol", values="close")
        return pivot.sort_index().ffill()

    def _load_top_factors(self, screening_csv: str, top_k: int = 10) -> List[str]:
        """ä»ç­›é€‰ç»“æœåŠ è½½Top Kå› å­"""
        df = pd.read_csv(screening_csv)
        col_name = "factor" if "factor" in df.columns else "panel_factor"
        return df.head(top_k)[col_name].tolist()

    def _calculate_composite_score(
        self,
        panel: pd.DataFrame,
        factors: List[str],
        weights: Dict[str, float],
        method: str = "zscore",
    ) -> pd.DataFrame:
        """è®¡ç®—å¤åˆå› å­å¾—åˆ† - å®Œå…¨å‘é‡åŒ–å®ç°"""
        # é‡å¡‘ä¸º (date, symbol) ç»“æ„
        factor_data = panel[factors].unstack(level="symbol")

        # å‘é‡åŒ–æ ‡å‡†åŒ–
        if method == "zscore":
            normalized = (
                factor_data - factor_data.mean(axis=1, skipna=True).values[:, np.newaxis]
            ) / (factor_data.std(axis=1, skipna=True).values[:, np.newaxis] + 1e-8)
        else:  # rank
            normalized = factor_data.rank(axis=1, pct=True) * 2 - 1

        # è·å–ç»´åº¦ä¿¡æ¯
        n_dates, n_total = normalized.shape
        n_factors = len(factors)
        n_symbols = n_total // n_factors

        # é‡å¡‘ä¸º (dates, symbols, factors) ç”¨äºçŸ©é˜µä¹˜æ³•
        reshaped = normalized.values.reshape(n_dates, n_symbols, n_factors)

        # å‘é‡åŒ–åŠ æƒæ±‚å’Œ
        weight_array = np.array([weights.get(f, 0) for f in factors])
        scores_array = np.sum(reshaped * weight_array[np.newaxis, np.newaxis, :], axis=2)

        # åˆ›å»ºç»“æœDataFrame
        symbols = [col[1] for col in normalized.columns[::n_factors]]
        scores = pd.DataFrame(scores_array, index=normalized.index, columns=symbols)

        return scores

    def _build_target_weights(self, scores: pd.DataFrame, top_n: int) -> pd.DataFrame:
        """æ„å»ºTop-Nç›®æ ‡æƒé‡"""
        ranks = scores.rank(axis=1, ascending=False, method="first")
        selection = ranks <= top_n
        weights = selection.astype(float)
        weights = weights.div(weights.sum(axis=1), axis=0).fillna(0.0)
        return weights

    def _backtest_topn_rotation(
        self,
        prices: pd.DataFrame,
        scores: pd.DataFrame,
        top_n: int = 5,
        rebalance_freq: int = 20,
        fees: float = 0.001,
        init_cash: float = 1_000_000,
    ) -> Dict[str, Any]:
        """Top-Nè½®åŠ¨å›æµ‹ - å‘é‡åŒ–å®ç°"""
        # å¯¹é½æ—¥æœŸ
        common_dates = prices.index.intersection(scores.index)
        prices = prices.loc[common_dates]
        scores = scores.loc[common_dates]

        # æ„å»ºç›®æ ‡æƒé‡
        weights = self._build_target_weights(scores, top_n)

        # å‘é‡åŒ–è°ƒä»“æ—¥æƒé‡æ›´æ–°
        rebalance_mask = pd.Series(
            np.arange(len(weights)) % rebalance_freq == 0, index=weights.index
        )
        rebalance_mask.iloc[0] = True

        # ä½¿ç”¨ ffill å‘å‰å¡«å……æƒé‡
        weights_ffill = weights.where(rebalance_mask, np.nan).ffill().fillna(0.0)

        # è®¡ç®—æ”¶ç›Š
        asset_returns = prices.pct_change().fillna(0.0)
        prev_weights = weights_ffill.shift().fillna(0.0)

        # å¯¹é½åˆ—å
        common_symbols = asset_returns.columns.intersection(prev_weights.columns)
        asset_returns_aligned = asset_returns[common_symbols]
        prev_weights_aligned = prev_weights[common_symbols]

        gross_returns = (prev_weights_aligned * asset_returns_aligned).sum(axis=1)

        # äº¤æ˜“æˆæœ¬
        weight_diff = weights_ffill.diff().abs().sum(axis=1).fillna(0.0)
        turnover = 0.5 * weight_diff
        net_returns = gross_returns - fees * turnover

        # å‡€å€¼æ›²çº¿
        equity = (1 + net_returns).cumprod() * init_cash

        # ç»Ÿè®¡æŒ‡æ ‡
        total_return = (equity.iloc[-1] / init_cash - 1) * 100
        periods_per_year = 252
        sharpe = (
            net_returns.mean() / net_returns.std() * np.sqrt(periods_per_year)
            if net_returns.std() > 0
            else 0
        )

        running_max = equity.cummax()
        drawdown = (equity / running_max - 1) * 100
        max_dd = drawdown.min()

        return {
            "total_return": total_return,
            "sharpe_ratio": sharpe,
            "max_drawdown": max_dd,
            "final_value": equity.iloc[-1],
            "turnover": turnover.sum(),
        }

    def _process_weight_chunk(
        self,
        weight_chunk: List[Tuple[float, ...]],
        factors: List[str],
        panel: pd.DataFrame,
        prices: pd.DataFrame,
        top_n_list: List[int],
        rebalance_freq: int = 20,
    ) -> List[Dict[str, Any]]:
        """å¤„ç†ä¸€ä¸ªæƒé‡ç»„åˆå— - åœ¨å•ä¸ªè¿›ç¨‹ä¸­æ‰§è¡Œ"""
        results = []

        for weights in weight_chunk:
            weight_dict = dict(zip(factors, weights))

            try:
                # è®¡ç®—å¾—åˆ†çŸ©é˜µ
                scores = self._calculate_composite_score(panel, factors, weight_dict)

                # æ‰¹é‡æµ‹è¯•æ‰€æœ‰Top-Nå€¼
                for top_n in top_n_list:
                    try:
                        result = self._backtest_topn_rotation(
                            prices=prices,
                            scores=scores,
                            top_n=top_n,
                            rebalance_freq=rebalance_freq,
                        )

                        results.append({
                            "weights": str(weight_dict),
                            "top_n": top_n,
                            "total_return": result["total_return"],
                            "sharpe_ratio": result["sharpe_ratio"],
                            "max_drawdown": result["max_drawdown"],
                            "final_value": result["final_value"],
                            "turnover": result["turnover"],
                        })

                    except Exception as e:
                        continue  # è·³è¿‡å¤±è´¥çš„ç»„åˆ

            except Exception as e:
                continue  # è·³è¿‡å¤±è´¥çš„ç»„åˆ

        return results

    def _generate_weight_combinations(
        self,
        factors: List[str],
        weight_grid: List[float],
        weight_sum_range: Tuple[float, float],
        max_combinations: int = 5000,
    ) -> List[Tuple[float, ...]]:
        """ç”Ÿæˆæœ‰æ•ˆçš„æƒé‡ç»„åˆ"""
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„æƒé‡ç»„åˆ
        weight_combos = list(itertools.product(weight_grid, repeat=len(factors)))

        # å‘é‡åŒ–è®¡ç®—æƒé‡å’Œ
        weight_array = np.array(weight_combos)
        weight_sums = np.sum(weight_array, axis=1)

        # å‘é‡åŒ–è¿‡æ»¤æœ‰æ•ˆç»„åˆ
        valid_mask = (weight_sums >= weight_sum_range[0]) & (weight_sums <= weight_sum_range[1])
        valid_indices = np.where(valid_mask)[0]

        # é™åˆ¶ç»„åˆæ•°
        if len(valid_indices) > max_combinations:
            valid_indices = valid_indices[:max_combinations]

        valid_combos = [weight_combos[i] for i in valid_indices]

        self.logger.info(f"æƒé‡ç»„åˆç”Ÿæˆå®Œæˆ: {len(valid_combos)}ä¸ªæœ‰æ•ˆç»„åˆ")
        return valid_combos

    def _chunk_weight_combinations(
        self,
        weight_combos: List[Tuple[float, ...]],
        chunk_size: int
    ) -> List[List[Tuple[float, ...]]]:
        """å°†æƒé‡ç»„åˆåˆ†å—"""
        chunks = []
        for i in range(0, len(weight_combos), chunk_size):
            chunk = weight_combos[i:i + chunk_size]
            chunks.append(chunk)
        return chunks

    def parallel_grid_search(
        self,
        panel_path: str,
        price_dir: str,
        screening_csv: str,
        factors: List[str],
        top_n_list: List[int] = [3, 5, 8],
        weight_grid: List[float] = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
        weight_sum_range: Tuple[float, float] = (0.7, 1.3),
        max_combinations: int = 5000,
        rebalance_freq: int = 20,
    ) -> pd.DataFrame:
        """å¹¶è¡Œç½‘æ ¼æœç´¢æƒé‡ç»„åˆ"""

        self.logger.info("å¼€å§‹å¹¶è¡Œç½‘æ ¼æœç´¢...")
        start_time = time.time()

        # åŠ è½½æ•°æ®
        self.logger.info("åŠ è½½æ•°æ®...")
        panel = self._load_factor_panel(panel_path)
        prices = self._load_price_data(price_dir)

        # ç”Ÿæˆæƒé‡ç»„åˆ
        weight_combos = self._generate_weight_combinations(
            factors, weight_grid, weight_sum_range, max_combinations
        )

        # åˆ†å—
        chunks = self._chunk_weight_combinations(weight_combos, self.chunk_size)
        total_tasks = len(chunks)

        self.logger.info(f"ä»»åŠ¡åˆ†å—å®Œæˆ: {total_tasks}ä¸ªä»»åŠ¡å—")
        self.logger.info(f"é¢„è®¡å¤„ç†: {len(weight_combos)}ä¸ªæƒé‡ç»„åˆ Ã— {len(top_n_list)}ä¸ªTop-Nå€¼")

        # åˆ›å»ºå·¥ä½œå‡½æ•°
        work_func = partial(
            self._process_weight_chunk,
            factors=factors,
            panel=panel,
            prices=prices,
            top_n_list=top_n_list,
            rebalance_freq=rebalance_freq,
        )

        # å¹¶è¡Œæ‰§è¡Œ
        all_results = []

        try:
            with mp.Pool(processes=self.n_workers) as pool:
                # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
                results_iter = pool.imap_unordered(work_func, chunks)

                for chunk_results in tqdm(
                    results_iter,
                    total=total_tasks,
                    desc=f"å¹¶è¡Œå¤„ç† ({self.n_workers}è¿›ç¨‹)"
                ):
                    all_results.extend(chunk_results)

        except Exception as e:
            self.logger.error(f"å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
            raise

        # å¤„ç†ç»“æœ
        processing_time = time.time() - start_time
        self.logger.info(f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
        self.logger.info(f"å…±å¤„ç† {len(all_results)} ä¸ªç­–ç•¥ç»“æœ")

        # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
        df = pd.DataFrame(all_results)
        if len(df) > 0:
            df = df.sort_values("sharpe_ratio", ascending=False)

            # è¾“å‡ºæœ€ä¼˜ç­–ç•¥
            best = df.iloc[0]
            self.logger.info(f"æœ€ä¼˜ç­–ç•¥: {best['weights']}, top_n={best['top_n']}, sharpe={best['sharpe_ratio']:.3f}")

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_strategies = len(weight_combos) * len(top_n_list)
            speed = total_strategies / processing_time
            self.logger.info(f"å¤„ç†é€Ÿåº¦: {speed:.1f}ç­–ç•¥/ç§’")

            # ä¼°ç®—åŠ é€Ÿæ¯”
            estimated_sequential_time = total_strategies / 142  # åŸºçº¿é€Ÿåº¦142ç­–/ç§’
            speedup = estimated_sequential_time / processing_time
            efficiency = speedup / self.n_workers * 100

            self.logger.info(f"é¢„ä¼°åŠ é€Ÿæ¯”: {speedup:.1f}x")
            self.logger.info(f"å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%")

        return df

    def run_parallel_backtest(
        self,
        panel_path: str,
        price_dir: str,
        screening_csv: str,
        output_dir: str,
        top_k: int = 10,
        top_n_list: List[int] = [3, 5, 8],
        rebalance_freq: int = 20,
        max_combinations: int = 5000,
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´çš„å¹¶è¡Œå›æµ‹"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        print("=" * 80)
        print("ETFè½®åŠ¨å›æµ‹å¼•æ“ - å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬")
        print("=" * 80)
        print(f"æ—¶é—´æˆ³: {timestamp}")
        print(f"å·¥ä½œè¿›ç¨‹æ•°: {self.n_workers}")
        print(f"å—å¤§å°: {self.chunk_size}")
        print(f"é¢æ¿: {panel_path}")
        print(f"ç­›é€‰: {screening_csv}")

        # åŠ è½½å› å­åˆ—è¡¨
        factors = self._load_top_factors(screening_csv, top_k)
        print(f"å› å­æ•°: {len(factors)}")
        print(f"å› å­: {factors}")

        # å¹¶è¡Œç½‘æ ¼æœç´¢
        print(f"\nå¼€å§‹å¹¶è¡Œå›æµ‹: {max_combinations}ä¸ªæƒé‡ç»„åˆ")
        start_time = time.time()

        results = self.parallel_grid_search(
            panel_path=panel_path,
            price_dir=price_dir,
            screening_csv=screening_csv,
            factors=factors,
            top_n_list=top_n_list,
            max_combinations=max_combinations,
            rebalance_freq=rebalance_freq,
        )

        total_time = time.time() - start_time
        print(f"\nå¹¶è¡Œå›æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        csv_file = output_path / f"parallel_backtest_results_{timestamp}.csv"
        results.to_csv(csv_file, index=False)
        print(f"ç»“æœä¿å­˜è‡³: {csv_file}")

        # è¾“å‡ºTop 10
        print("\nTop 10 ç­–ç•¥:")
        print(results.head(10).to_string(index=False))

        # ä¿å­˜æœ€ä¼˜ç­–ç•¥é…ç½®
        best = results.iloc[0]
        best_config = {
            "timestamp": timestamp,
            "engine_type": "parallel",
            "n_workers": self.n_workers,
            "chunk_size": self.chunk_size,
            "weights": best["weights"],
            "top_n": int(best["top_n"]),
            "rebalance_freq": rebalance_freq,
            "performance": {
                "total_return": float(best["total_return"]),
                "sharpe_ratio": float(best["sharpe_ratio"]),
                "max_drawdown": float(best["max_drawdown"]),
            },
            "factors": factors,
            "timing": {
                "total_time": total_time,
                "strategies_tested": len(results),
                "speed_per_second": len(results) / total_time,
            },
            "data_source": {"panel": panel_path, "screening": screening_csv},
        }

        config_file = output_path / f"parallel_best_strategy_{timestamp}.json"
        with open(config_file, "w") as f:
            json.dump(best_config, f, indent=2, ensure_ascii=False)
        print(f"æœ€ä¼˜é…ç½®ä¿å­˜è‡³: {config_file}")

        return results, best_config


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 4:
        print("ç”¨æ³•: python parallel_backtest_engine.py <panel> <price_dir> <screening_csv> [output_dir]")
        print("\nç¤ºä¾‹:")
        print("  python 03_vbtå›æµ‹/parallel_backtest_engine.py \\")
        print("    ../etf_cross_section_results/panel_20251018_024539.parquet \\")
        print("    ../../raw/ETF/daily \\")
        print("    dummy_screening.csv \\")
        print("    ./results")
        sys.exit(1)

    panel_path = sys.argv[1]
    price_dir = sys.argv[2]
    screening_csv = sys.argv[3]
    output_dir = sys.argv[4] if len(sys.argv) > 4 else "etf_rotation_system/strategies/results"

    # åˆ›å»ºå¹¶è¡Œå¼•æ“
    engine = ParallelBacktestEngine(
        n_workers=max(1, mp.cpu_count() - 1),  # ä½¿ç”¨é™¤ä¸»è¿›ç¨‹å¤–çš„æ‰€æœ‰CPUæ ¸å¿ƒ
        chunk_size=20,  # æ¯ä¸ªä»»åŠ¡å¤„ç†20ä¸ªæƒé‡ç»„åˆ
        enable_cache=True,
        log_level="INFO"
    )

    # è¿è¡Œå›æµ‹
    results, best_config = engine.run_parallel_backtest(
        panel_path=panel_path,
        price_dir=price_dir,
        screening_csv=screening_csv,
        output_dir=output_dir,
        top_k=10,
        top_n_list=[3, 5, 8, 10],
        rebalance_freq=20,
        max_combinations=5000,  # å¢åŠ åˆ°5000ä¸ªç»„åˆ
    )

    print("\nğŸ¯ å¹¶è¡Œä¼˜åŒ–æ€»ç»“:")
    print(f"å¤„ç†æ—¶é—´: {best_config['timing']['total_time']:.2f}ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {best_config['timing']['speed_per_second']:.1f}ç­–ç•¥/ç§’")
    print(f"å·¥ä½œè¿›ç¨‹: {best_config['n_workers']}ä¸ª")
    print(f"æœ€ä¼˜å¤æ™®æ¯”ç‡: {best_config['performance']['sharpe_ratio']:.3f}")
    print(f"æœ€ä¼˜æ”¶ç›Šç‡: {best_config['performance']['total_return']:.2f}%")


if __name__ == "__main__":
    main()