#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ETFè½®åŠ¨ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
å¯¹æ¯”ä¸åŒä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½è¡¨ç°ï¼Œæä¾›æœ€ä¼˜é…ç½®å»ºè®®
"""

import json
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from parallel_backtest_engine import ParallelBacktestEngine
from optimized_weight_generator import OptimizedWeightGenerator, WeightGenerationConfig, SearchStrategy


def create_benchmark_suite():
    """åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶"""
    print("åˆ›å»ºåŸºå‡†æµ‹è¯•å¥—ä»¶...")

    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            "name": "å°è§„æ¨¡æµ‹è¯•",
            "factors": 3,
            "max_combinations": 100,
            "top_n_list": [3, 5],
            "description": "3ä¸ªå› å­ï¼Œ100ä¸ªæƒé‡ç»„åˆ"
        },
        {
            "name": "ä¸­ç­‰è§„æ¨¡æµ‹è¯•",
            "factors": 5,
            "max_combinations": 500,
            "top_n_list": [3, 5, 8],
            "description": "5ä¸ªå› å­ï¼Œ500ä¸ªæƒé‡ç»„åˆ"
        },
        {
            "name": "å¤§è§„æ¨¡æµ‹è¯•",
            "factors": 8,
            "max_combinations": 1000,
            "top_n_list": [3, 5, 8, 10],
            "description": "8ä¸ªå› å­ï¼Œ1000ä¸ªæƒé‡ç»„åˆ"
        },
        {
            "name": "è¶…å¤§è§„æ¨¡æµ‹è¯•",
            "factors": 10,
            "max_combinations": 2000,
            "top_n_list": [3, 5, 8, 10],
            "description": "10ä¸ªå› å­ï¼Œ2000ä¸ªæƒé‡ç»„åˆ"
        }
    ]

    # ä¼˜åŒ–ç­–ç•¥é…ç½®
    strategy_configs = [
        {
            "name": "ä¸²è¡Œç½‘æ ¼æœç´¢",
            "n_workers": 1,
            "weight_strategy": SearchStrategy.GRID,
            "chunk_size": 1
        },
        {
            "name": "å¹¶è¡Œç½‘æ ¼æœç´¢",
            "n_workers": 4,
            "weight_strategy": SearchStrategy.GRID,
            "chunk_size": 20
        },
        {
            "name": "å¹¶è¡Œæ™ºèƒ½é‡‡æ ·",
            "n_workers": 4,
            "weight_strategy": SearchStrategy.SMART,
            "chunk_size": 20
        },
        {
            "name": "å¹¶è¡Œåˆ†å±‚æœç´¢",
            "n_workers": 4,
            "weight_strategy": SearchStrategy.HIERARCHICAL,
            "chunk_size": 20
        },
        {
            "name": "é«˜å¹¶å‘å¹¶è¡Œ",
            "n_workers": 8,
            "weight_strategy": SearchStrategy.SMART,
            "chunk_size": 50
        }
    ]

    return test_configs, strategy_configs


def generate_benchmark_data(n_dates: int = 126, n_symbols: int = 15) -> Tuple[str, str, str]:
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®"""
    print(f"ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®: {n_dates}å¤© Ã— {n_symbols}ä¸ªETF")

    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path("benchmark_data")
    test_dir.mkdir(exist_ok=True)

    # ç”Ÿæˆæ¨¡æ‹Ÿé¢æ¿æ•°æ®
    factors = [f"FACTOR_{i}" for i in range(10)]  # ç”Ÿæˆ10ä¸ªå› å­
    dates = pd.date_range('2023-01-01', periods=n_dates, freq='D')
    symbols = [f"ETF{i:03d}" for i in range(n_symbols)]

    # åˆ›å»º MultiIndex
    multi_index = pd.MultiIndex.from_product(
        [symbols, dates], names=['symbol', 'date']
    )

    # ç”Ÿæˆæœ‰æ„ä¹‰çš„å› å­æ•°æ®ï¼ˆåŠ å…¥ä¸€äº›è¶‹åŠ¿å’Œç›¸å…³æ€§ï¼‰
    np.random.seed(42)
    n_samples = len(multi_index)

    # åŸºç¡€éšæœºæ•°æ®
    factor_data = np.random.randn(n_samples, len(factors)) * 0.1

    # æ·»åŠ æ—¶é—´è¶‹åŠ¿
    time_trend = np.linspace(0, 0.5, n_dates)
    for i in range(len(factors)):
        factor_data[:, i] += np.repeat(time_trend, n_symbols)

    # æ·»åŠ å› å­é—´ç›¸å…³æ€§
    correlation_matrix = np.array([
        [1.0, 0.3, -0.2, 0.1, 0.0, 0.2, -0.1, 0.3, 0.1, -0.2],
        [0.3, 1.0, 0.1, -0.3, 0.2, 0.0, 0.3, -0.1, 0.2, 0.1],
        [-0.2, 0.1, 1.0, 0.2, -0.1, 0.3, 0.0, -0.2, 0.3, 0.1],
        [0.1, -0.3, 0.2, 1.0, 0.3, -0.1, 0.2, 0.0, -0.1, 0.3],
        [0.0, 0.2, -0.1, 0.3, 1.0, 0.1, -0.2, 0.3, 0.0, -0.1],
        [0.2, 0.0, 0.3, -0.1, 0.1, 1.0, 0.2, -0.3, 0.1, 0.2],
        [-0.1, 0.3, 0.0, 0.2, -0.2, 0.2, 1.0, 0.1, -0.3, 0.0],
        [0.3, -0.1, -0.2, 0.0, 0.3, -0.3, 0.1, 1.0, 0.2, -0.1],
        [0.1, 0.2, 0.3, -0.1, 0.0, 0.1, -0.3, 0.2, 1.0, 0.3],
        [-0.2, 0.1, 0.1, 0.3, -0.1, 0.2, 0.0, -0.1, 0.3, 1.0]
    ])

    # åº”ç”¨ç›¸å…³æ€§
    factor_data = factor_data @ correlation_matrix.T

    panel_df = pd.DataFrame(factor_data, index=multi_index, columns=factors)

    # ä¿å­˜é¢æ¿æ•°æ®
    panel_file = test_dir / "benchmark_panel.parquet"
    panel_df.to_parquet(panel_file)

    # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ˆæ›´çœŸå®çš„ä»·æ ¼åŠ¨æ€ï¼‰
    price_data = []
    base_prices = np.random.uniform(50, 200, n_symbols)  # åŸºç¡€ä»·æ ¼

    for symbol_idx, symbol in enumerate(symbols):
        base_price = base_prices[symbol_idx]
        prices = [base_price]

        # ç”Ÿæˆæœ‰è¶‹åŠ¿å’Œæ³¢åŠ¨çš„æ”¶ç›Šç‡
        for i in range(1, n_dates):
            # åŸºç¡€æ”¶ç›Šç‡
            base_return = np.random.randn() * 0.015  # 1.5%æ—¥æ³¢åŠ¨ç‡

            # æ·»åŠ è¶‹åŠ¿
            trend = 0.0002 * (i - n_dates/2) / n_dates  # è½»å¾®è¶‹åŠ¿

            # å‡å€¼å›å½’
            mean_reversion = -0.01 * (prices[-1] - base_price) / base_price

            total_return = base_return + trend + mean_reversion
            new_price = prices[-1] * (1 + total_return)
            prices.append(max(new_price, 1.0))  # ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿ

        for i, (date, price) in enumerate(zip(dates, prices)):
            price_data.append({
                'date': date,
                'symbol': symbol,
                'close': price,
                'trade_date': date.strftime('%Y%m%d')
            })

    price_df = pd.DataFrame(price_data)

    # ä¿å­˜æ¯ä¸ªETFçš„ä»·æ ¼æ•°æ®
    for symbol in symbols:
        symbol_data = price_df[price_df['symbol'] == symbol]
        price_file = test_dir / f"{symbol}_daily_20230101_20230601.parquet"
        symbol_data[['date', 'close', 'trade_date']].to_parquet(price_file)

    # ç”Ÿæˆæ¨¡æ‹Ÿç­›é€‰ç»“æœï¼ˆæœ‰æ„ä¹‰çš„å› å­æ’åºï¼‰
    factor_performance = []
    np.random.seed(42)

    for i, factor in enumerate(factors):
        # æ¨¡æ‹Ÿå› å­è¡¨ç°ï¼Œå‰å‡ ä¸ªå› å­è¡¨ç°æ›´å¥½
        base_ic = 0.05 - i * 0.005  # ICé€’å‡
        base_sharpe = 1.5 - i * 0.1   # å¤æ™®æ¯”ç‡é€’å‡

        ic_mean = base_ic + np.random.randn() * 0.01
        ic_std = abs(0.15 + np.random.randn() * 0.05)
        sharpe_ratio = base_sharpe + np.random.randn() * 0.3

        factor_performance.append({
            'factor': factor,
            'ic_mean': ic_mean,
            'ic_std': ic_std,
            'sharpe_ratio': sharpe_ratio,
            'ir': ic_mean / ic_std if ic_std > 0 else 0
        })

    screening_df = pd.DataFrame(factor_performance)
    screening_df = screening_df.sort_values('sharpe_ratio', ascending=False)

    # ä¿å­˜ç­›é€‰ç»“æœ
    screening_file = test_dir / "benchmark_screening.csv"
    screening_df.to_csv(screening_file, index=False)

    print(f"åŸºå‡†æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ:")
    print(f"  é¢æ¿æ•°æ®: {panel_file}")
    print(f"  ä»·æ ¼æ•°æ®: {test_dir}/ETF*_daily_*.parquet")
    print(f"  ç­›é€‰ç»“æœ: {screening_file}")

    return str(panel_file), str(test_dir), str(screening_file)


def run_single_benchmark(
    test_config: Dict,
    strategy_config: Dict,
    panel_path: str,
    price_dir: str,
    screening_path: str
) -> Dict[str, Any]:
    """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
    test_name = f"{test_config['name']} - {strategy_config['name']}"
    print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")

    try:
        # åˆ›å»ºæƒé‡ç”Ÿæˆå™¨é…ç½®
        weight_config = WeightGenerationConfig(
            strategy=strategy_config['weight_strategy'],
            max_combinations=test_config['max_combinations']
        )

        # åˆ›å»ºå¹¶è¡Œå¼•æ“
        engine = ParallelBacktestEngine(
            n_workers=strategy_config['n_workers'],
            chunk_size=strategy_config['chunk_size']
        )

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        memory_start = get_memory_usage()

        # è¿è¡Œå›æµ‹
        results, config = engine.run_parallel_backtest(
            panel_path=panel_path,
            price_dir=price_dir,
            screening_csv=screening_path,
            output_dir=f"benchmark_results/{test_config['name']}/{strategy_config['name']}",
            top_k=test_config['factors'],
            top_n_list=test_config['top_n_list'],
            max_combinations=test_config['max_combinations']
        )

        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        memory_end = get_memory_usage()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        execution_time = end_time - start_time
        memory_used = memory_end - memory_start
        strategies_tested = len(results)
        speed = strategies_tested / execution_time

        # è·å–æœ€ä¼˜ç­–ç•¥æ€§èƒ½
        best_strategy = results.iloc[0] if len(results) > 0 else None

        benchmark_result = {
            'test_name': test_name,
            'test_config': test_config,
            'strategy_config': strategy_config,
            'execution_time': execution_time,
            'memory_used_mb': memory_used,
            'strategies_tested': strategies_tested,
            'speed_per_second': speed,
            'best_sharpe_ratio': best_strategy['sharpe_ratio'] if best_strategy is not None else None,
            'best_return': best_strategy['total_return'] if best_strategy is not None else None,
            'success': True,
            'error': None
        }

        print(f"âœ… æµ‹è¯•å®Œæˆ: {execution_time:.2f}ç§’, {speed:.1f}ç­–ç•¥/ç§’")
        return benchmark_result

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return {
            'test_name': test_name,
            'test_config': test_config,
            'strategy_config': strategy_config,
            'execution_time': None,
            'memory_used_mb': None,
            'strategies_tested': 0,
            'speed_per_second': 0,
            'best_sharpe_ratio': None,
            'best_return': None,
            'success': False,
            'error': str(e)
        }


def get_memory_usage() -> float:
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    try:
        import psutil
        import os
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    except ImportError:
        return 0.0


def run_full_benchmark_suite():
    """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    print("ETFè½®åŠ¨ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    panel_path, price_dir, screening_path = generate_benchmark_data()

    # è·å–æµ‹è¯•é…ç½®
    test_configs, strategy_configs = create_benchmark_suite()

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    all_results = []

    for test_config in test_configs:
        for strategy_config in strategy_configs:
            result = run_single_benchmark(
                test_config, strategy_config,
                panel_path, price_dir, screening_path
            )
            all_results.append(result)

    return all_results


def analyze_benchmark_results(results: List[Dict]):
    """åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ"""
    print("\n" + "=" * 80)
    print("åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)

    # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
    df = pd.DataFrame(results)

    # åŸºæœ¬ç»Ÿè®¡
    total_tests = len(results)
    successful_tests = len(results[results['success']])
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸæµ‹è¯•æ•°: {successful_tests}")
    print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")

    if successful_tests == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return

    # æŒ‰æµ‹è¯•è§„æ¨¡åˆ†æ
    print(f"\nğŸ“Š æŒ‰æµ‹è¯•è§„æ¨¡åˆ†æ:")
    scale_analysis = df[df['success']].groupby('test_config.apply(lambda x: x["factors"])').agg({
        'execution_time': ['mean', 'std'],
        'speed_per_second': ['mean', 'std'],
        'memory_used_mb': ['mean']
    }).round(2)
    print(scale_analysis)

    # æŒ‰ç­–ç•¥åˆ†æ
    print(f"\nğŸ“Š æŒ‰ä¼˜åŒ–ç­–ç•¥åˆ†æ:")
    strategy_analysis = df[df['success']].groupby('strategy_config.apply(lambda x: x["name"])').agg({
        'execution_time': ['mean', 'std'],
        'speed_per_second': ['mean', 'std'],
        'best_sharpe_ratio': ['mean'],
        'success': 'count'
    }).round(2)
    print(strategy_analysis)

    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸš€ æ€§èƒ½å¯¹æ¯”:")
    successful_df = df[df['success']]

    # æ‰¾å‡ºæœ€å¿«çš„é…ç½®
    fastest = successful_df.loc[successful_df['speed_per_second'].idxmax()]
    print(f"æœ€å¿«é…ç½®: {fastest['test_name']}")
    print(f"  é€Ÿåº¦: {fastest['speed_per_second']:.1f}ç­–ç•¥/ç§’")
    print(f"  æ—¶é—´: {fastest['execution_time']:.2f}ç§’")

    # æ‰¾å‡ºæœ€ä¼˜ç­–ç•¥è´¨é‡çš„é…ç½®
    best_quality = successful_df.loc[successful_df['best_sharpe_ratio'].idxmax()]
    print(f"æœ€ä¼˜ç­–ç•¥è´¨é‡: {best_quality['test_name']}")
    print(f"  æœ€ä½³å¤æ™®æ¯”ç‡: {best_quality['best_sharpe_ratio']:.3f}")
    print(f"  æœ€ä½³æ”¶ç›Šç‡: {best_quality['best_return']:.2f}%")

    # è®¡ç®—å¹¶è¡ŒåŠ é€Ÿæ•ˆæœ
    print(f"\nâš¡ å¹¶è¡ŒåŠ é€Ÿæ•ˆæœ:")
    serial_results = successful_df[successful_df['strategy_config.apply(lambda x: x["n_workers"])'] == 1]
    parallel_results = successful_df[successful_df['strategy_config.apply(lambda x: x["n_workers"])'] > 1]

    if len(serial_results) > 0 and len(parallel_results) > 0:
        # æ‰¾åˆ°ç›¸åŒçš„æµ‹è¯•é…ç½®è¿›è¡Œå¯¹æ¯”
        for _, serial_row in serial_results.iterrows():
            test_factors = serial_row['test_config']['factors']
            matching_parallel = parallel_results[
                parallel_results['test_config.apply(lambda x: x["factors"])'] == test_factors
            ]

            if len(matching_parallel) > 0:
                best_parallel = matching_parallel.loc[matching_parallel['speed_per_second'].idxmax()]
                speedup = best_parallel['speed_per_second'] / serial_row['speed_per_second']
                efficiency = speedup / best_parallel['strategy_config']['n_workers'] * 100

                print(f"  {test_factors}å› å­æµ‹è¯•:")
                print(f"    ä¸²è¡Œ: {serial_row['speed_per_second']:.1f}ç­–ç•¥/ç§’")
                print(f"    å¹¶è¡Œ: {best_parallel['speed_per_second']:.1f}ç­–ç•¥/ç§’")
                print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
                print(f"    å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%")

    return df


def create_performance_visualizations(df: pd.DataFrame):
    """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ“ˆ ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨...")

    # è®¾ç½®å›¾è¡¨æ ·å¼
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")

    # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
    successful_df = df[df['success']].copy()

    if len(successful_df) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æ•°æ®ç”¨äºå¯è§†åŒ–")
        return

    # åˆ›å»ºå›¾è¡¨ç›®å½•
    viz_dir = Path("benchmark_results/visualizations")
    viz_dir.mkdir(parents=True, exist_ok=True)

    # 1. æ‰§è¡Œæ—¶é—´å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))

    # æå–å› å­æ•°é‡å’Œæ‰§è¡Œæ—¶é—´
    factor_counts = [r['test_config']['factors'] for r in successful_df['test_config']]
    strategy_names = [r['name'] for r in successful_df['strategy_config']]

    # åˆ›å»ºåˆ†ç»„æŸ±çŠ¶å›¾
    unique_factors = sorted(set(factor_counts))
    unique_strategies = list(set(strategy_names))

    x = np.arange(len(unique_factors))
    width = 0.8 / len(unique_strategies)

    fig, ax = plt.subplots(figsize=(14, 8))

    for i, strategy in enumerate(unique_strategies):
        strategy_times = []
        for factors in unique_factors:
            mask = (successful_df['test_config.apply(lambda x: x["factors"])'] == factors) & \
                   (successful_df['strategy_config.apply(lambda x: x["name"])'] == strategy)
            strategy_data = successful_df[mask]
            if len(strategy_data) > 0:
                strategy_times.append(strategy_data['execution_time'].mean())
            else:
                strategy_times.append(0)

        ax.bar(x + i * width, strategy_times, width, label=strategy)

    ax.set_xlabel('å› å­æ•°é‡')
    ax.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
    ax.set_title('ä¸åŒç­–ç•¥çš„æ‰§è¡Œæ—¶é—´å¯¹æ¯”')
    ax.set_xticks(x + width * (len(unique_strategies) - 1) / 2)
    ax.set_xticklabels([f'{f}å› å­' for f in unique_factors])
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(viz_dir / 'execution_time_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 2. å¤„ç†é€Ÿåº¦å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))

    fig, ax = plt.subplots(figsize=(14, 8))

    for i, strategy in enumerate(unique_strategies):
        strategy_speeds = []
        for factors in unique_factors:
            mask = (successful_df['test_config.apply(lambda x: x["factors"])'] == factors) & \
                   (successful_df['strategy_config.apply(lambda x: x["name"])'] == strategy)
            strategy_data = successful_df[mask]
            if len(strategy_data) > 0:
                strategy_speeds.append(strategy_data['speed_per_second'].mean())
            else:
                strategy_speeds.append(0)

        ax.bar(x + i * width, strategy_speeds, width, label=strategy)

    ax.set_xlabel('å› å­æ•°é‡')
    ax.set_ylabel('å¤„ç†é€Ÿåº¦ (ç­–ç•¥/ç§’)')
    ax.set_title('ä¸åŒç­–ç•¥çš„å¤„ç†é€Ÿåº¦å¯¹æ¯”')
    ax.set_xticks(x + width * (len(unique_strategies) - 1) / 2)
    ax.set_xticklabels([f'{f}å› å­' for f in unique_factors])
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(viz_dir / 'processing_speed_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 3. å†…å­˜ä½¿ç”¨å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 8))

    fig, ax = plt.subplots(figsize=(14, 8))

    for i, strategy in enumerate(unique_strategies):
        strategy_memory = []
        for factors in unique_factors:
            mask = (successful_df['test_config.apply(lambda x: x["factors"])'] == factors) & \
                   (successful_df['strategy_config.apply(lambda x: x["name"])'] == strategy)
            strategy_data = successful_df[mask]
            if len(strategy_data) > 0:
                strategy_memory.append(strategy_data['memory_used_mb'].mean())
            else:
                strategy_memory.append(0)

        ax.bar(x + i * width, strategy_memory, width, label=strategy)

    ax.set_xlabel('å› å­æ•°é‡')
    ax.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    ax.set_title('ä¸åŒç­–ç•¥çš„å†…å­˜ä½¿ç”¨å¯¹æ¯”')
    ax.set_xticks(x + width * (len(unique_strategies) - 1) / 2)
    ax.set_xticklabels([f'{f}å› å­' for f in unique_factors])
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(viz_dir / 'memory_usage_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()

    # 4. æ•£ç‚¹å›¾ï¼šé€Ÿåº¦ vs è´¨é‡ trade-off
    plt.figure(figsize=(12, 8))

    speeds = successful_df['speed_per_second']
    sharpe_ratios = successful_df['best_sharpe_ratio']
    colors = [r['strategy_config']['n_workers'] for r in successful_df['strategy_config']]

    scatter = plt.scatter(speeds, sharpe_ratios, c=colors, s=100, alpha=0.7, cmap='viridis')
    plt.colorbar(scatter, label='å·¥ä½œè¿›ç¨‹æ•°')

    plt.xlabel('å¤„ç†é€Ÿåº¦ (ç­–ç•¥/ç§’)')
    plt.ylabel('æœ€ä¼˜å¤æ™®æ¯”ç‡')
    plt.title('é€Ÿåº¦ä¸ç­–ç•¥è´¨é‡çš„æƒè¡¡')
    plt.grid(True, alpha=0.3)

    # æ·»åŠ æ ‡æ³¨
    for i, row in successful_df.iterrows():
        plt.annotate(row['test_name'].split(' - ')[1],
                    (row['speed_per_second'], row['best_sharpe_ratio']),
                    fontsize=8, alpha=0.7)

    plt.tight_layout()
    plt.savefig(viz_dir / 'speed_vs_quality_tradeoff.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {viz_dir}")


def generate_benchmark_report(df: pd.DataFrame):
    """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    print("\nğŸ“ ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")

    successful_df = df[df['success']]

    if len(successful_df) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•æ•°æ®")
        return

    # è®¡ç®—å…³é”®æŒ‡æ ‡
    fastest_config = successful_df.loc[successful_df['speed_per_second'].idxmax()]
    best_quality_config = successful_df.loc[successful_df['best_sharpe_ratio'].idxmax()]

    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_speed = successful_df['speed_per_second'].mean()
    avg_time = successful_df['execution_time'].mean()
    avg_memory = successful_df['memory_used_mb'].mean()

    # è®¡ç®—å¹¶è¡ŒåŠ é€Ÿæ¯”
    serial_configs = successful_df[successful_df['strategy_config.apply(lambda x: x["n_workers"])'] == 1]
    parallel_configs = successful_df[successful_df['strategy_config.apply(lambda x: x["n_workers"])'] > 1]

    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_tests": len(df),
            "successful_tests": len(successful_df),
            "success_rate": len(successful_df) / len(df) * 100,
            "average_speed": avg_speed,
            "average_time": avg_time,
            "average_memory_mb": avg_memory
        },
        "best_performance": {
            "fastest": {
                "name": fastest_config['test_name'],
                "speed": fastest_config['speed_per_second'],
                "time": fastest_config['execution_time'],
                "memory_mb": fastest_config['memory_used_mb']
            },
            "best_quality": {
                "name": best_quality_config['test_name'],
                "sharpe_ratio": best_quality_config['best_sharpe_ratio'],
                "return": best_quality_config['best_return'],
                "speed": best_quality_config['speed_per_second']
            }
        },
        "parallel_analysis": {
            "serial_avg_speed": serial_configs['speed_per_second'].mean() if len(serial_configs) > 0 else 0,
            "parallel_avg_speed": parallel_configs['speed_per_second'].mean() if len(parallel_configs) > 0 else 0,
            "speedup": (parallel_configs['speed_per_second'].mean() / serial_configs['speed_per_second'].mean()
                      if len(serial_configs) > 0 and len(parallel_configs) > 0 else 0)
        },
        "recommendations": generate_recommendations(successful_df),
        "detailed_results": df.to_dict('records')
    }

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("benchmark_results/performance_report.json")
    report_file.parent.mkdir(parents=True, exist_ok=True)

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    markdown_report = generate_markdown_report(report)
    markdown_file = Path("benchmark_results/PERFORMANCE_REPORT.md")

    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(markdown_report)

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"  JSONæ ¼å¼: {report_file}")
    print(f"  Markdownæ ¼å¼: {markdown_file}")

    return report


def generate_recommendations(df: pd.DataFrame) -> List[str]:
    """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    recommendations = []

    # åˆ†ææœ€ä¼˜é…ç½®
    fastest = df.loc[df['speed_per_second'].idxmax()]
    best_quality = df.loc[df['best_sharpe_ratio'].idxmax()]

    # åŸºäºç»“æœç”Ÿæˆå»ºè®®
    recommendations.append("ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")

    # å¹¶è¡ŒåŒ–å»ºè®®
    parallel_configs = df[df['strategy_config.apply(lambda x: x["n_workers"])'] > 1]
    if len(parallel_configs) > 0:
        avg_parallel_speed = parallel_configs['speed_per_second'].mean()
        avg_speed = df['speed_per_second'].mean()
        if avg_parallel_speed > avg_speed * 2:
            recommendations.append("  â€¢ å¼ºçƒˆæ¨èä½¿ç”¨å¹¶è¡Œè®¡ç®—ï¼Œå¹³å‡åŠ é€Ÿæ¯”è¶…è¿‡2å€")

    # ç­–ç•¥é€‰æ‹©å»ºè®®
    strategy_performance = df.groupby('strategy_config.apply(lambda x: x["name"])')['speed_per_second'].mean()
    best_strategy = strategy_performance.idxmax()
    recommendations.append(f"  â€¢ æ¨èä½¿ç”¨ {best_strategy} ç­–ç•¥ï¼Œå¹³å‡é€Ÿåº¦æœ€å¿«")

    # å·¥ä½œè¿›ç¨‹æ•°å»ºè®®
    worker_performance = df.groupby('strategy_config.apply(lambda x: x["n_workers"])')['speed_per_second'].mean()
    if len(worker_performance) > 1:
        best_workers = worker_performance.idxmax()
        recommendations.append(f"  â€¢ æ¨èä½¿ç”¨ {best_workers} ä¸ªå·¥ä½œè¿›ç¨‹")

    # å†…å­˜ä½¿ç”¨å»ºè®®
    high_memory = df[df['memory_used_mb'] > 1000]
    if len(high_memory) > 0:
        recommendations.append("  â€¢ æ³¨æ„å†…å­˜ä½¿ç”¨ï¼Œå»ºè®®ç›‘æ§å¤§è§„æ¨¡å›æµ‹çš„å†…å­˜æ¶ˆè€—")

    # è´¨é‡vsé€Ÿåº¦æƒè¡¡å»ºè®®
    if fastest['test_name'] != best_quality['test_name']:
        recommendations.append("  â€¢ å­˜åœ¨é€Ÿåº¦ä¸è´¨é‡çš„æƒè¡¡ï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„é…ç½®")
        recommendations.append(f"    - è¿½æ±‚é€Ÿåº¦: {fastest['test_name']}")
        recommendations.append(f"    - è¿½æ±‚è´¨é‡: {best_quality['test_name']}")

    return recommendations


def generate_markdown_report(report: Dict) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
    md = []

    # æ ‡é¢˜
    md.append("# ETFè½®åŠ¨ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    md.append("")
    md.append(f"**ç”Ÿæˆæ—¶é—´**: {report['timestamp']}")
    md.append("")

    # æ‰§è¡Œæ‘˜è¦
    md.append("## ğŸ“Š æ‰§è¡Œæ‘˜è¦")
    md.append("")
    summary = report['summary']
    md.append(f"- **æ€»æµ‹è¯•æ•°**: {summary['total_tests']}")
    md.append(f"- **æˆåŠŸç‡**: {summary['success_rate']:.1f}%")
    md.append(f"- **å¹³å‡å¤„ç†é€Ÿåº¦**: {summary['average_speed']:.1f} ç­–ç•¥/ç§’")
    md.append(f"- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {summary['average_time']:.2f} ç§’")
    md.append(f"- **å¹³å‡å†…å­˜ä½¿ç”¨**: {summary['average_memory_mb']:.1f} MB")
    md.append("")

    # æœ€ä½³æ€§èƒ½é…ç½®
    md.append("## ğŸ† æœ€ä½³æ€§èƒ½é…ç½®")
    md.append("")

    fastest = report['best_performance']['fastest']
    md.append("### âš¡ æœ€å¿«é…ç½®")
    md.append(f"- **é…ç½®**: {fastest['name']}")
    md.append(f"- **é€Ÿåº¦**: {fastest['speed']:.1f} ç­–ç•¥/ç§’")
    md.append(f"- **æ‰§è¡Œæ—¶é—´**: {fastest['time']:.2f} ç§’")
    md.append(f"- **å†…å­˜ä½¿ç”¨**: {fastest['memory_mb']:.1f} MB")
    md.append("")

    best_quality = report['best_performance']['best_quality']
    md.append("### ğŸ¯ æœ€ä¼˜ç­–ç•¥è´¨é‡")
    md.append(f"- **é…ç½®**: {best_quality['name']}")
    md.append(f"- **å¤æ™®æ¯”ç‡**: {best_quality['sharpe_ratio']:.3f}")
    md.append(f"- **æ”¶ç›Šç‡**: {best_quality['return']:.2f}%")
    md.append(f"- **é€Ÿåº¦**: {best_quality['speed']:.1f} ç­–ç•¥/ç§’")
    md.append("")

    # å¹¶è¡Œåˆ†æ
    md.append("## âš¡ å¹¶è¡Œè®¡ç®—æ•ˆæœ")
    md.append("")
    parallel = report['parallel_analysis']
    if parallel['speedup'] > 0:
        md.append(f"- **ä¸²è¡Œå¹³å‡é€Ÿåº¦**: {parallel['serial_avg_speed']:.1f} ç­–ç•¥/ç§’")
        md.append(f"- **å¹¶è¡Œå¹³å‡é€Ÿåº¦**: {parallel['parallel_avg_speed']:.1f} ç­–ç•¥/ç§’")
        md.append(f"- **å¹³å‡åŠ é€Ÿæ¯”**: {parallel['speedup']:.2f}x")
    else:
        md.append("- æ— æ³•è®¡ç®—å¹¶è¡ŒåŠ é€Ÿæ¯”ï¼ˆç¼ºå°‘å¯¹æ¯”æ•°æ®ï¼‰")
    md.append("")

    # ä¼˜åŒ–å»ºè®®
    md.append("## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®")
    md.append("")
    for rec in report['recommendations']:
        md.append(rec)
    md.append("")

    # ç»“è®º
    md.append("## ğŸ“ˆ ç»“è®º")
    md.append("")
    md.append("é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬éªŒè¯äº†å¹¶è¡Œè®¡ç®—å’Œæ™ºèƒ½æƒé‡ç”Ÿæˆç­–ç•¥çš„æ˜¾è‘—æ•ˆæœã€‚")
    md.append("å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨æ¨èçš„é…ç½®ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚")
    md.append("")

    return "\n".join(md)


def main():
    """ä¸»å‡½æ•°"""
    print("ETFè½®åŠ¨ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)

    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = run_full_benchmark_suite()

        # åˆ†æç»“æœ
        df = analyze_benchmark_results(results)

        # åˆ›å»ºå¯è§†åŒ–
        create_performance_visualizations(df)

        # ç”ŸæˆæŠ¥å‘Š
        report = generate_benchmark_report(df)

        print("\n" + "=" * 80)
        print("ğŸ¯ åŸºå‡†æµ‹è¯•å®Œæˆæ€»ç»“")
        print("=" * 80)

        if len(results) > 0 and any(r['success'] for r in results):
            summary = report['summary']
            fastest = report['best_performance']['fastest']
            best_quality = report['best_performance']['best_quality']

            print(f"âœ… æˆåŠŸå®Œæˆ {summary['successful_tests']}/{summary['total_tests']} ä¸ªæµ‹è¯•")
            print(f"ğŸ“ˆ å¹³å‡å¤„ç†é€Ÿåº¦: {summary['average_speed']:.1f} ç­–ç•¥/ç§’")
            print(f"âš¡ æœ€å¿«é…ç½®: {fastest['name']} ({fastest['speed']:.1f} ç­–ç•¥/ç§’)")
            print(f"ğŸ¯ æœ€ä¼˜è´¨é‡: {best_quality['name']} (å¤æ™® {best_quality['sharpe_ratio']:.3f})")
            print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: benchmark_results/PERFORMANCE_REPORT.md")

            # æ˜¾ç¤ºä¸»è¦å»ºè®®
            print(f"\nğŸ’¡ ä¸»è¦å»ºè®®:")
            for rec in report['recommendations'][1:4]:  # æ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                print(f"  {rec}")
        else:
            print("âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ")

    except Exception as e:
        print(f"\nâŒ åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()