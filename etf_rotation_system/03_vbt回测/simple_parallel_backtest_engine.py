#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ç®€åŒ–çš„å¹¶è¡Œå›æµ‹å¼•æ“ - æ¢å¤åŸå§‹æ€§èƒ½

åŸºäºåŸå§‹parallel_backtest_engine.pyï¼Œåªæ·»åŠ æœ€å°åŒ–çš„é…ç½®æ”¯æŒ
ç¡®ä¿æ€§èƒ½ä¸å—å½±å“
"""

import json
import logging
import multiprocessing as mp
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
import vectorbt as vbt
import yaml
from tqdm import tqdm


class SimpleParallelBacktestEngine:
    """ç®€åŒ–çš„å¹¶è¡Œå›æµ‹å¼•æ“ - ä¿æŒåŸå§‹é«˜æ€§èƒ½"""

    def __init__(self, config_file: str = None):
        """
        åˆå§‹åŒ–å¼•æ“

        Args:
            config_file: å¯é€‰çš„é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # é»˜è®¤é…ç½®ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
        self.n_workers = max(1, mp.cpu_count() - 1)
        self.chunk_size = 20
        self.enable_cache = True
        self.log_level = "INFO"

        # å¦‚æœæä¾›é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½é…ç½®
        if config_file:
            self._load_simple_config(config_file)

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, self.log_level.upper()),
            format="%(asctime)s - %(levelname)s - %(message)s",
        )
        self.logger = logging.getLogger(__name__)

    def _load_simple_config(self, config_file: str):
        """åŠ è½½ç®€åŒ–é…ç½®ï¼Œåªè¦†ç›–å¿…è¦çš„å‚æ•°"""
        try:
            with open(config_file, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)

            # åªåŠ è½½å¿…è¦çš„é…ç½®é¡¹
            if "parallel_config" in config:
                parallel_config = config["parallel_config"]
                self.n_workers = parallel_config.get("n_workers", self.n_workers)
                self.chunk_size = parallel_config.get("chunk_size", self.chunk_size)
                self.enable_cache = parallel_config.get(
                    "enable_cache", self.enable_cache
                )
                self.log_level = parallel_config.get("log_level", self.log_level)

            # åŠ è½½æ•°æ®è·¯å¾„
            self.data_paths = config.get("data_paths", {})

        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

    def generate_weight_combinations(
        self, factor_names: List[str], max_combinations: int = 10000
    ) -> List[Dict[str, float]]:
        """
        ç”Ÿæˆæƒé‡ç»„åˆ - ç¡®å®šæ€§ç½‘æ ¼æœç´¢ï¼ˆå¯¹é½configurableå¼•æ“ï¼‰
        æ¶ˆé™¤éšæœºæ€§ï¼Œä¿è¯å¯å¤ç°
        """
        import itertools

        weight_options = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        n_factors = len(factor_names)

        # ä½¿ç”¨ç¡®å®šæ€§ç½‘æ ¼ç”Ÿæˆ
        combinations = []
        combo_generator = itertools.product(weight_options, repeat=n_factors)

        for combo in combo_generator:
            weight_sum = sum(combo)

            # æƒé‡å’Œçº¦æŸ
            if 0.8 <= weight_sum <= 1.2:
                weights = dict(zip(factor_names, combo))
                combinations.append(weights)

                if len(combinations) >= max_combinations:
                    break

        return combinations

    def _process_weight_chunk(
        self, args: Tuple[List[Dict], pd.DataFrame, List[str], Dict, List[int]]
    ) -> List[Dict]:
        """
        å¤„ç†æƒé‡å—ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒçš„å‘é‡åŒ–é€»è¾‘ï¼‰
        """
        weight_combinations, factor_data, factor_names, backtest_config, top_n_list = (
            args
        )

        results = []

        # é¢„è®¡ç®—ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
        factor_matrix = factor_data[factor_names].values
        factor_matrix_3d = factor_matrix.reshape(len(factor_data), len(factor_names), 1)

        for weight_dict in weight_combinations:
            try:
                # æ„å»ºæƒé‡æ•°ç»„ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
                weight_array = np.array(
                    [weight_dict.get(factor, 0.0) for factor in factor_names]
                )

                # å‘é‡åŒ–è®¡ç®—ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
                scores_3d = np.einsum("cf,dsf->cds", weight_array, factor_matrix_3d)
                scores = scores_3d.squeeze()

                # æ ‡å‡†åŒ–åˆ†æ•°
                scores = (scores - scores.mean()) / (scores.std() + 1e-8)

                for top_n in top_n_list:
                    try:
                        # é€‰æ‹©é¡¶éƒ¨èµ„äº§
                        selected_indices = np.argpartition(-scores, top_n)[:top_n]
                        selected_indices = selected_indices[
                            np.argsort(-scores[selected_indices])
                        ]

                        # è®¡ç®—æƒé‡
                        weights = np.zeros(len(factor_data.columns[3:]))
                        weights[selected_indices] = 1.0 / top_n

                        # åˆ›å»ºä»·æ ¼çŸ©é˜µ
                        price_matrix = factor_data.iloc[:, 3:].values

                        # æ‰§è¡Œå›æµ‹
                        portfolio = vbt.Portfolio.from_orders(
                            price_matrix,
                            np.tile(weights, (len(factor_data), 1)),
                            init_cash=backtest_config["init_cash"],
                            fees=backtest_config["fees"],
                        )

                        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                        total_return = portfolio.total_return() * 100
                        sharpe_ratio = portfolio.sharpe_ratio() * np.sqrt(252)
                        max_drawdown = portfolio.max_drawdown() * 100
                        final_value = portfolio.final_value()
                        turnover = portfolio.trades.count() / len(factor_data) * 100

                        results.append(
                            {
                                "weights": str(weight_dict),
                                "top_n": top_n,
                                "total_return": total_return,
                                "sharpe_ratio": sharpe_ratio,
                                "max_drawdown": max_drawdown,
                                "final_value": final_value,
                                "turnover": turnover,
                            }
                        )

                    except Exception as e:
                        self.logger.warning(
                            f"å›æµ‹å¤±è´¥: {weight_dict}, top_n={top_n}: {e}"
                        )
                        continue

            except Exception as e:
                self.logger.warning(f"æƒé‡å¤„ç†å¤±è´¥: {weight_dict}: {e}")
                continue

        return results

    def run_backtest(
        self,
        panel_file: str,
        price_dir: str,
        screening_file: str,
        output_dir: str = None,
        max_combinations: int = 10000,
    ) -> pd.DataFrame:
        """
        è¿è¡Œå›æµ‹ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        print("=" * 80)
        print("ETFè½®åŠ¨å›æµ‹å¼•æ“ - ç®€åŒ–é…ç½®å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬")
        print("=" * 80)
        print(f"æ—¶é—´æˆ³: {timestamp}")
        print(f"å·¥ä½œè¿›ç¨‹æ•°: {self.n_workers}")
        print(f"å—å¤§å°: {self.chunk_size}")

        # åŠ è½½æ•°æ®
        print(f"é¢æ¿: {panel_file}")
        print(f"ç­›é€‰: {screening_file}")

        panel_data = pd.read_parquet(panel_file)
        screening_data = pd.read_csv(screening_file)

        # è·å–å› å­ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
        top_factors = screening_data.nlargest(8, "ic_mean")["factor"].tolist()
        print(f"å› å­æ•°: {len(top_factors)}")
        print(f"å› å­: {top_factors}")

        # å‡†å¤‡æ•°æ®ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
        factor_data = panel_data[
            top_factors + ["open", "high", "low", "close", "volume"]
        ].copy()
        factor_data = factor_data.dropna()

        # ç”Ÿæˆæƒé‡ç»„åˆ
        print(f"\nå¼€å§‹ç®€åŒ–é…ç½®å¹¶è¡Œå›æµ‹: {max_combinations}ä¸ªæƒé‡ç»„åˆ")
        start_time = time.time()

        weight_combinations = self.generate_weight_combinations(
            top_factors, max_combinations
        )

        # å›æµ‹é…ç½® - Aè‚¡ ETF æˆæœ¬æ¨¡å‹
        backtest_config = {
            "init_cash": 1000000,
            "fees": 0.003,  # Aè‚¡ ETF: ä½£é‡‘0.2% + å°èŠ±ç¨0.1% = 0.3% å¾€è¿”
        }
        top_n_list = [3, 5, 8]

        # å¹¶è¡Œå¤„ç†ï¼ˆä¸åŸå§‹ç‰ˆæœ¬ç›¸åŒï¼‰
        chunk_size = self.chunk_size
        chunks = [
            weight_combinations[i : i + chunk_size]
            for i in range(0, len(weight_combinations), chunk_size)
        ]

        args_list = [
            (chunk, factor_data, top_factors, backtest_config, top_n_list)
            for chunk in chunks
        ]

        with mp.Pool(self.n_workers) as pool:
            results = []
            with tqdm(total=len(chunks), desc="å¹¶è¡Œå¤„ç†") as pbar:
                for chunk_results in pool.imap_unordered(
                    self._process_weight_chunk, args_list
                ):
                    results.extend(chunk_results)
                    pbar.update(1)

        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)

        if not results_df.empty:
            # æŒ‰å¤æ™®æ¯”ç‡æ’åº
            results_df = results_df.sort_values("sharpe_ratio", ascending=False)

            # è¾“å‡ºç»“æœ
            total_time = time.time() - start_time
            print(f"\nç®€åŒ–é…ç½®å¹¶è¡Œå›æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(
                f"æœ€ä¼˜ç­–ç•¥: {eval(results_df.iloc[0]['weights'])}, top_n={results_df.iloc[0]['top_n']}, sharpe={results_df.iloc[0]['sharpe_ratio']:.3f}"
            )

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_strategies = len(results_df)
            speed = total_strategies / total_time
            print(f"\nğŸ¯ ç®€åŒ–é…ç½®å¹¶è¡Œä¼˜åŒ–æ€»ç»“:")
            print(f"å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"å¤„ç†é€Ÿåº¦: {speed:.1f}ç­–ç•¥/ç§’")
            print(f"å·¥ä½œè¿›ç¨‹: {self.n_workers}ä¸ª")
            print(f"æœ€ä¼˜å¤æ™®æ¯”ç‡: {results_df.iloc[0]['sharpe_ratio']:.3f}")
            print(f"æœ€ä¼˜æ”¶ç›Šç‡: {results_df.iloc[0]['total_return']:.2f}%")
        else:
            print("æ²¡æœ‰æœ‰æ•ˆçš„å›æµ‹ç»“æœ")
            total_time = time.time() - start_time
            print(f"ç®€åŒ–é…ç½®å¹¶è¡Œå›æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

        # ä¿å­˜ç»“æœï¼ˆåˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼‰
        if output_dir and not results_df.empty:
            output_path = Path(output_dir)
            timestamp_folder = output_path / f"backtest_{timestamp}"
            timestamp_folder.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜CSVç»“æœ
            csv_file = timestamp_folder / "results.csv"
            results_df.to_csv(csv_file, index=False)
            print(f"ç»“æœä¿å­˜è‡³: {csv_file}")

            # ä¿å­˜æœ€ä¼˜é…ç½®
            best_config = {
                "timestamp": timestamp,
                "engine_type": "simple_parallel",
                "weights": results_df.iloc[0]["weights"],
                "top_n": int(results_df.iloc[0]["top_n"]),
                "performance": {
                    "total_return": float(results_df.iloc[0]["total_return"]),
                    "sharpe_ratio": float(results_df.iloc[0]["sharpe_ratio"]),
                    "max_drawdown": float(results_df.iloc[0]["max_drawdown"]),
                },
                "factors": top_factors,
                "timing": {
                    "total_time": total_time,
                    "strategies_tested": len(results_df),
                    "speed_per_second": speed,
                },
            }

            config_file = timestamp_folder / "best_config.json"
            with open(config_file, "w") as f:
                json.dump(best_config, f, indent=2)
            print(f"æœ€ä¼˜é…ç½®ä¿å­˜è‡³: {config_file}")

            # ä¿å­˜æ—¥å¿—æ–‡ä»¶
            log_file = timestamp_folder / "backtest.log"
            with open(log_file, "w") as f:
                f.write(f"ETFè½®åŠ¨å›æµ‹å¼•æ“ - ç®€åŒ–é…ç½®å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬\n")
                f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
                f.write(f"å·¥ä½œè¿›ç¨‹æ•°: {self.n_workers}\n")
                f.write(f"å—å¤§å°: {self.chunk_size}\n")
                f.write(f"æœ€å¤§ç»„åˆæ•°: {max_combinations}\n")
                f.write(f"é¢æ¿: {panel_file}\n")
                f.write(f"ç­›é€‰: {screening_file}\n\n")
                f.write(f"ç®€åŒ–é…ç½®å¹¶è¡Œå›æµ‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’\n")
                f.write(
                    f"æœ€ä¼˜ç­–ç•¥: {eval(results_df.iloc[0]['weights'])}, top_n={int(results_df.iloc[0]['top_n'])}, sharpe={results_df.iloc[0]['sharpe_ratio']:.3f}\n"
                )
            print(f"æ—¥å¿—æ–‡ä»¶ä¿å­˜è‡³: {log_file}")

        return results_df


def main():
    """ä¸»å‡½æ•°"""
    import sys

    if len(sys.argv) < 4:
        print(
            "ç”¨æ³•: python simple_parallel_backtest_engine.py <panel> <price_dir> <screening_csv> [output_dir] [config_file]"
        )
        print("ç¤ºä¾‹:")
        print("  python simple_parallel_backtest_engine.py \\")
        print("    ../data/panels/panel.parquet \\")
        print("    ../../raw/ETF/daily \\")
        print("    ../data/screening/passed_factors.csv \\")
        print("    ./results \\")
        print("    parallel_backtest_config.yaml  # å¯é€‰é…ç½®æ–‡ä»¶")
        return

    panel_file = sys.argv[1]
    price_dir = sys.argv[2]
    screening_file = sys.argv[3]
    output_dir = (
        sys.argv[4]
        if len(sys.argv) > 4
        else "/Users/zhangshenshen/æ·±åº¦é‡åŒ–0927/etf_rotation_system/data/results/backtest"
    )
    config_file = sys.argv[5] if len(sys.argv) > 5 else None

    # åˆ›å»ºå¼•æ“
    engine = SimpleParallelBacktestEngine(config_file)

    # è¿è¡Œå›æµ‹
    results = engine.run_backtest(
        panel_file=panel_file,
        price_dir=price_dir,
        screening_file=screening_file,
        output_dir=output_dir,
        max_combinations=10000,
    )

    print(f"\nâœ… ç®€åŒ–é…ç½®å¹¶è¡Œå›æµ‹å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} ä¸ªç­–ç•¥ç»“æœ")


if __name__ == "__main__":
    main()
