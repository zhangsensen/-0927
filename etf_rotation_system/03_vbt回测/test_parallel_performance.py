#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""å¹¶è¡Œå›æµ‹å¼•æ“æ€§èƒ½æµ‹è¯•è„šæœ¬
å¯¹æ¯”ä¸²è¡Œå’Œå¹¶è¡Œç‰ˆæœ¬çš„æ€§èƒ½å·®å¼‚ï¼ŒéªŒè¯åŠ é€Ÿæ•ˆæœ
"""

import json
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd

from parallel_backtest_engine import ParallelBacktestEngine


def generate_test_data():
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    print("ç”Ÿæˆæµ‹è¯•æ•°æ®...")

    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = Path("test_data")
    test_dir.mkdir(exist_ok=True)

    # ç”Ÿæˆæ¨¡æ‹Ÿé¢æ¿æ•°æ®
    n_dates = 252  # ä¸€å¹´çš„äº¤æ˜“æ—¥
    n_symbols = 20  # 20ä¸ªETF
    n_factors = 10  # 10ä¸ªå› å­

    dates = pd.date_range('2023-01-01', periods=n_dates, freq='D')
    symbols = [f"ETF{i:03d}" for i in range(n_symbols)]
    factors = [f"FACTOR_{i}" for i in range(n_factors)]

    # åˆ›å»º MultiIndex
    multi_index = pd.MultiIndex.from_product(
        [symbols, dates], names=['symbol', 'date']
    )

    # ç”Ÿæˆéšæœºå› å­æ•°æ®
    np.random.seed(42)
    factor_data = np.random.randn(len(multi_index), n_factors)
    panel_df = pd.DataFrame(factor_data, index=multi_index, columns=factors)

    # ä¿å­˜é¢æ¿æ•°æ®
    panel_file = test_dir / "test_panel.parquet"
    panel_df.to_parquet(panel_file)

    # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
    price_data = []
    for symbol in symbols:
        # éšæœºæ¸¸èµ°ä»·æ ¼
        initial_price = 100 + np.random.randn() * 10
        returns = np.random.randn(n_dates) * 0.02  # 2%æ—¥æ³¢åŠ¨ç‡
        prices = [initial_price]
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))

        for i, (date, price) in enumerate(zip(dates, prices)):
            price_data.append({
                'date': date,
                'symbol': symbol,
                'close': price,
                'trade_date': date.strftime('%Y%m%d')
            })

    price_df = pd.DataFrame(price_data)

    # ä¿å­˜æ¯ä¸ªETFçš„ä»·æ ¼æ•°æ®
    for symbol in symbols:
        symbol_data = price_df[price_df['symbol'] == symbol]
        price_file = test_dir / f"{symbol}_daily_20230101_20231231.parquet"
        symbol_data[['date', 'close', 'trade_date']].to_parquet(price_file)

    # ç”Ÿæˆæ¨¡æ‹Ÿç­›é€‰ç»“æœ
    screening_data = pd.DataFrame({
        'factor': factors,
        'ic_mean': np.random.randn(n_factors) * 0.05,
        'ic_std': np.abs(np.random.randn(n_factors) * 0.1),
        'sharpe_ratio': np.random.randn(n_factors) * 2,
    })
    screening_df = screening_data.sort_values('sharpe_ratio', ascending=False)

    # ä¿å­˜ç­›é€‰ç»“æœ
    screening_file = test_dir / "test_screening.csv"
    screening_df.to_csv(screening_file, index=False)

    print(f"æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ:")
    print(f"  é¢æ¿æ•°æ®: {panel_file}")
    print(f"  ä»·æ ¼æ•°æ®: {test_dir}/ETF*_daily_*.parquet")
    print(f"  ç­›é€‰ç»“æœ: {screening_file}")

    return str(panel_file), str(test_dir), str(screening_file)


def test_serial_vs_parallel():
    """æµ‹è¯•ä¸²è¡Œvså¹¶è¡Œæ€§èƒ½"""
    print("\n" + "=" * 80)
    print("ä¸²è¡Œ vs å¹¶è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    panel_path, price_dir, screening_path = generate_test_data()

    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "å°è§„æ¨¡æµ‹è¯•", "max_combos": 100, "top_k": 3},
        {"name": "ä¸­ç­‰è§„æ¨¡æµ‹è¯•", "max_combos": 500, "top_k": 5},
        {"name": "å¤§è§„æ¨¡æµ‹è¯•", "max_combos": 1000, "top_k": 8},
    ]

    results = []

    for config in test_configs:
        print(f"\nğŸ§ª {config['name']}:")
        print(f"  æƒé‡ç»„åˆ: {config['max_combos']}")
        print(f"  å› å­æ•°é‡: {config['top_k']}")

        # ä¸²è¡Œæµ‹è¯•
        print("\nğŸ”„ ä¸²è¡Œæ‰§è¡Œæµ‹è¯•...")
        start_time = time.time()

        # è¿™é‡Œåº”è¯¥è°ƒç”¨åŸæœ‰çš„ä¸²è¡Œç‰ˆæœ¬
        # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬ç”¨å•è¿›ç¨‹å¹¶è¡Œå¼•æ“æ¨¡æ‹Ÿä¸²è¡Œ
        serial_engine = ParallelBacktestEngine(n_workers=1, chunk_size=1)
        serial_results, serial_config = serial_engine.run_parallel_backtest(
            panel_path=panel_path,
            price_dir=price_dir,
            screening_csv=screening_path,
            output_dir="test_results/serial",
            top_k=config['top_k'],
            top_n_list=[3, 5],
            max_combinations=config['max_combos'],
        )

        serial_time = time.time() - start_time
        serial_speed = len(serial_results) / serial_time

        print(f"  ä¸²è¡Œè€—æ—¶: {serial_time:.2f}ç§’")
        print(f"  ä¸²è¡Œé€Ÿåº¦: {serial_speed:.1f}ç­–ç•¥/ç§’")

        # å¹¶è¡Œæµ‹è¯•
        print("\nğŸš€ å¹¶è¡Œæ‰§è¡Œæµ‹è¯•...")
        start_time = time.time()

        # å¤šè¿›ç¨‹å¹¶è¡Œ
        import multiprocessing as mp
        n_workers = max(1, mp.cpu_count() - 1)
        parallel_engine = ParallelBacktestEngine(
            n_workers=n_workers,
            chunk_size=20
        )
        parallel_results, parallel_config = parallel_engine.run_parallel_backtest(
            panel_path=panel_path,
            price_dir=price_dir,
            screening_csv=screening_path,
            output_dir="test_results/parallel",
            top_k=config['top_k'],
            top_n_list=[3, 5],
            max_combinations=config['max_combos'],
        )

        parallel_time = time.time() - start_time
        parallel_speed = len(parallel_results) / parallel_time

        print(f"  å¹¶è¡Œè€—æ—¶: {parallel_time:.2f}ç§’")
        print(f"  å¹¶è¡Œé€Ÿåº¦: {parallel_speed:.1f}ç­–ç•¥/ç§’")

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = serial_time / parallel_time if parallel_time > 0 else float('inf')
        efficiency = speedup / n_workers * 100 if n_workers > 0 else 0

        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"  å¹¶è¡Œæ•ˆç‡: {efficiency:.1f}%")
        print(f"  å·¥ä½œè¿›ç¨‹: {n_workers}")

        # è®°å½•ç»“æœ
        results.append({
            'test_name': config['name'],
            'max_combos': config['max_combos'],
            'top_k': config['top_k'],
            'n_workers': n_workers,
            'serial_time': serial_time,
            'parallel_time': parallel_time,
            'speedup': speedup,
            'efficiency': efficiency,
            'serial_speed': serial_speed,
            'parallel_speed': parallel_speed,
            'strategies_tested': len(serial_results),
        })

    return results


def analyze_performance_results(results):
    """åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ"""
    print("\n" + "=" * 80)
    print("æ€§èƒ½æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)

    # åˆ›å»ºç»“æœDataFrame
    df = pd.DataFrame(results)

    print("\nğŸ“Š è¯¦ç»†ç»“æœè¡¨:")
    print(df[['test_name', 'max_combos', 'serial_time', 'parallel_time', 'speedup', 'efficiency']].round(2))

    # åˆ†æåŠ é€Ÿæ¯”
    avg_speedup = df['speedup'].mean()
    avg_efficiency = df['efficiency'].mean()

    print(f"\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    print(f"  å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
    print(f"  å¹³å‡å¹¶è¡Œæ•ˆç‡: {avg_efficiency:.1f}%")
    print(f"  æœ€ä¼˜åŠ é€Ÿæ¯”: {df['speedup'].max():.2f}x ({df.loc[df['speedup'].idxmax(), 'test_name']})")

    # é¢„ä¼°å®é™…ä»»åŠ¡æ€§èƒ½
    print(f"\nğŸ”® å®é™…ä»»åŠ¡æ€§èƒ½é¢„ä¼°:")
    actual_combos = 5000
    actual_top_n = 4  # [3,5,8,10]
    actual_strategies = actual_combos * actual_top_n

    for _, row in df.iterrows():
        # åŸºäºæµ‹è¯•ç»“æœé¢„ä¼°
        serial_time_per_strategy = row['serial_time'] / row['strategies_tested']
        parallel_time_per_strategy = row['parallel_time'] / row['strategies_tested']

        estimated_serial = actual_strategies * serial_time_per_strategy
        estimated_parallel = actual_strategies * parallel_time_per_strategy

        print(f"  {row['test_name']} é¢„ä¼°:")
        print(f"    ä¸²è¡Œ: {estimated_serial:.1f}ç§’ ({estimated_serial/60:.1f}åˆ†é’Ÿ)")
        print(f"    å¹¶è¡Œ: {estimated_parallel:.1f}ç§’")
        print(f"    åŠ é€Ÿæ¯”: {estimated_serial/estimated_parallel:.2f}x")

    # ä¿å­˜ç»“æœ
    results_file = Path("test_results/performance_comparison.json")
    results_file.parent.mkdir(exist_ok=True)

    comparison_data = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'avg_speedup': avg_speedup,
            'avg_efficiency': avg_efficiency,
            'max_speedup': df['speedup'].max(),
        },
        'detailed_results': df.to_dict('records'),
        'hardware_info': {
            'cpu_count': mp.cpu_count(),
            'workers_used': int(df.iloc[0]['n_workers']),
        }
    }

    with open(results_file, 'w') as f:
        json.dump(comparison_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœä¿å­˜è‡³: {results_file}")


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "=" * 80)
    print("è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 80)

    panel_path, price_dir, screening_path = generate_test_data()

    # æµ‹è¯•å•è¿›ç¨‹
    print("\nğŸ§ª å•è¿›ç¨‹æµ‹è¯•...")
    single_engine = ParallelBacktestEngine(n_workers=1)
    start_time = time.time()
    try:
        results, config = single_engine.run_parallel_backtest(
            panel_path, price_dir, screening_path,
            output_dir="test_results/single",
            max_combinations=50
        )
        single_time = time.time() - start_time
        print(f"âœ… å•è¿›ç¨‹æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {single_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ å•è¿›ç¨‹æµ‹è¯•å¤±è´¥: {e}")

    # æµ‹è¯•å¤§é‡å°ä»»åŠ¡
    print("\nğŸ§ª å¤§é‡å°ä»»åŠ¡æµ‹è¯•...")
    chunk_engine = ParallelBacktestEngine(n_workers=4, chunk_size=5)
    start_time = time.time()
    try:
        results, config = chunk_engine.run_parallel_backtest(
            panel_path, price_dir, screening_path,
            output_dir="test_results/chunk",
            max_combinations=200
        )
        chunk_time = time.time() - start_time
        print(f"âœ… å¤§é‡å°ä»»åŠ¡æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {chunk_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ å¤§é‡å°ä»»åŠ¡æµ‹è¯•å¤±è´¥: {e}")

    # æµ‹è¯•å†…å­˜ä½¿ç”¨
    print("\nğŸ§ª å†…å­˜ä½¿ç”¨æµ‹è¯•...")
    import psutil
    import os

    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB

    memory_engine = ParallelBacktestEngine(n_workers=2)
    try:
        results, config = memory_engine.run_parallel_backtest(
            panel_path, price_dir, screening_path,
            output_dir="test_results/memory",
            max_combinations=100
        )
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = memory_after - memory_before

        print(f"âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•æˆåŠŸ")
        print(f"  å†…å­˜ä½¿ç”¨: {memory_before:.1f}MB â†’ {memory_after:.1f}MB (+{memory_used:.1f}MB)")
        print(f"  æ¯ç­–ç•¥å†…å­˜: {memory_used/len(results)*1024:.2f}KB")

    except Exception as e:
        print(f"âŒ å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("å¹¶è¡Œå›æµ‹å¼•æ“æ€§èƒ½æµ‹è¯•")
    print("=" * 80)

    import multiprocessing as mp
    print(f"ç¡¬ä»¶ä¿¡æ¯: {mp.cpu_count()}ä¸ªCPUæ ¸å¿ƒ")

    try:
        # ä¸»è¦æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        results = test_serial_vs_parallel()

        # åˆ†æç»“æœ
        analyze_performance_results(results)

        # è¾¹ç•Œæƒ…å†µæµ‹è¯•
        test_edge_cases()

        print("\n" + "=" * 80)
        print("ğŸ¯ æµ‹è¯•å®Œæˆæ€»ç»“")
        print("=" * 80)
        print("âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")
        print("âœ… ç»“æœå·²ä¿å­˜åˆ° test_results/ ç›®å½•")
        print("\nğŸ’¡ å»ºè®®:")
        print("1. å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨ 6-8 ä¸ªå·¥ä½œè¿›ç¨‹")
        print("2. chunk_size å»ºè®® 10-20ï¼Œå¹³è¡¡ä»»åŠ¡åˆ†é…å’Œå†…å­˜ä½¿ç”¨")
        print("3. å¤§è§„æ¨¡å›æµ‹æ—¶æ³¨æ„ç›‘æ§å†…å­˜ä½¿ç”¨")
        print("4. å¯æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´å¹¶è¡Œå‚æ•°")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()