#!/usr/bin/env python3
"""
è´¨é‡ä¿è¯æ£€æŸ¥ - ä¸“ä¸šçº§å› å­ç­›é€‰ç³»ç»Ÿ
ä½œè€…ï¼šé‡åŒ–é¦–å¸­å·¥ç¨‹å¸ˆ
ç‰ˆæœ¬ï¼š2.0.0
æ—¥æœŸï¼š2025-09-29

æ£€æŸ¥å†…å®¹ï¼š
1. ä»£ç è´¨é‡å®¡æŸ¥
2. æµ‹è¯•è¦†ç›–ç‡éªŒè¯
3. æ€§èƒ½åŸºå‡†éªŒè¯
4. é…ç½®ç³»ç»ŸéªŒè¯
5. æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥
6. ç³»ç»Ÿé›†æˆéªŒè¯
"""

import sys
import subprocess
import time
from pathlib import Path
import importlib.util
import ast
import re
from typing import Dict, List, Tuple, Any

class QualityAssuranceChecker:
    """è´¨é‡ä¿è¯æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.results = {}
        
    def check_code_quality(self) -> Dict[str, Any]:
        """ä»£ç è´¨é‡æ£€æŸ¥"""
        print("ğŸ” ä»£ç è´¨é‡æ£€æŸ¥")
        print("-" * 60)
        
        quality_results = {
            'file_structure': self.check_file_structure(),
            'import_structure': self.check_import_structure(),
            'function_complexity': self.check_function_complexity(),
            'documentation': self.check_code_documentation(),
            'naming_conventions': self.check_naming_conventions()
        }
        
        return quality_results
    
    def check_file_structure(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡ä»¶ç»“æ„"""
        print("ğŸ“ æ£€æŸ¥æ–‡ä»¶ç»“æ„...")
        
        required_files = [
            'professional_factor_screener.py',
            'config_loader.py',
            'test_basic_functionality.py',
            'performance_benchmark.py',
            'README.md',
            'config/screening_config.yaml'
        ]
        
        existing_files = []
        missing_files = []
        
        for file_path in required_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                existing_files.append(file_path)
                print(f"  âœ… {file_path}")
            else:
                missing_files.append(file_path)
                print(f"  âŒ {file_path} (ç¼ºå¤±)")
        
        return {
            'existing_files': existing_files,
            'missing_files': missing_files,
            'structure_score': len(existing_files) / len(required_files)
        }
    
    def check_import_structure(self) -> Dict[str, Any]:
        """æ£€æŸ¥å¯¼å…¥ç»“æ„"""
        print("ğŸ“¦ æ£€æŸ¥å¯¼å…¥ç»“æ„...")
        
        main_file = self.project_root / 'professional_factor_screener.py'
        if not main_file.exists():
            return {'error': 'Main file not found'}
        
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    for alias in node.names:
                        imports.append(f"{module}.{alias.name}")
            
            # æ£€æŸ¥å…³é”®ä¾èµ–
            required_imports = [
                'pandas', 'numpy', 'scipy', 'statsmodels', 'vectorbt'
            ]
            
            missing_imports = []
            for req in required_imports:
                if not any(req in imp for imp in imports):
                    missing_imports.append(req)
            
            print(f"  ğŸ“Š æ€»å¯¼å…¥æ•°: {len(imports)}")
            print(f"  âœ… å¿…éœ€ä¾èµ–: {len(required_imports) - len(missing_imports)}/{len(required_imports)}")
            
            if missing_imports:
                print(f"  âŒ ç¼ºå¤±ä¾èµ–: {missing_imports}")
            
            return {
                'total_imports': len(imports),
                'required_imports': required_imports,
                'missing_imports': missing_imports,
                'import_score': (len(required_imports) - len(missing_imports)) / len(required_imports)
            }
            
        except Exception as e:
            print(f"  âŒ å¯¼å…¥æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_function_complexity(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‡½æ•°å¤æ‚åº¦"""
        print("ğŸ”§ æ£€æŸ¥å‡½æ•°å¤æ‚åº¦...")
        
        main_file = self.project_root / 'professional_factor_screener.py'
        if not main_file.exists():
            return {'error': 'Main file not found'}
        
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            functions = []
            classes = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # è®¡ç®—å‡½æ•°è¡Œæ•°
                    lines = node.end_lineno - node.lineno + 1 if hasattr(node, 'end_lineno') else 0
                    functions.append({
                        'name': node.name,
                        'lines': lines,
                        'args': len(node.args.args)
                    })
                elif isinstance(node, ast.ClassDef):
                    classes.append({
                        'name': node.name,
                        'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                    })
            
            # åˆ†æå¤æ‚åº¦
            long_functions = [f for f in functions if f['lines'] > 50]
            complex_functions = [f for f in functions if f['args'] > 8]
            
            print(f"  ğŸ“Š æ€»å‡½æ•°æ•°: {len(functions)}")
            print(f"  ğŸ“Š æ€»ç±»æ•°: {len(classes)}")
            print(f"  âš ï¸  é•¿å‡½æ•° (>50è¡Œ): {len(long_functions)}")
            print(f"  âš ï¸  å¤æ‚å‡½æ•° (>8å‚æ•°): {len(complex_functions)}")
            
            return {
                'total_functions': len(functions),
                'total_classes': len(classes),
                'long_functions': long_functions,
                'complex_functions': complex_functions,
                'complexity_score': 1 - (len(long_functions) + len(complex_functions)) / max(len(functions), 1)
            }
            
        except Exception as e:
            print(f"  âŒ å¤æ‚åº¦æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_code_documentation(self) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç æ–‡æ¡£"""
        print("ğŸ“ æ£€æŸ¥ä»£ç æ–‡æ¡£...")
        
        main_file = self.project_root / 'professional_factor_screener.py'
        if not main_file.exists():
            return {'error': 'Main file not found'}
        
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç»Ÿè®¡æ–‡æ¡£å­—ç¬¦ä¸²
            docstring_pattern = r'"""[\s\S]*?"""'
            docstrings = re.findall(docstring_pattern, content)
            
            # ç»Ÿè®¡æ³¨é‡Š
            comment_pattern = r'#.*'
            comments = re.findall(comment_pattern, content)
            
            # ç»Ÿè®¡ä»£ç è¡Œ
            lines = content.split('\n')
            code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
            
            doc_coverage = len(docstrings) / max(len(code_lines) / 20, 1)  # ä¼°ç®—æ–‡æ¡£è¦†ç›–ç‡
            
            print(f"  ğŸ“Š æ–‡æ¡£å­—ç¬¦ä¸²: {len(docstrings)}")
            print(f"  ğŸ“Š æ³¨é‡Šè¡Œæ•°: {len(comments)}")
            print(f"  ğŸ“Š ä»£ç è¡Œæ•°: {len(code_lines)}")
            print(f"  ğŸ“ˆ æ–‡æ¡£è¦†ç›–ç‡: {doc_coverage:.1%}")
            
            return {
                'docstrings': len(docstrings),
                'comments': len(comments),
                'code_lines': len(code_lines),
                'doc_coverage': doc_coverage,
                'doc_score': min(doc_coverage, 1.0)
            }
            
        except Exception as e:
            print(f"  âŒ æ–‡æ¡£æ£€æŸ¥å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_naming_conventions(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‘½åè§„èŒƒ"""
        print("ğŸ·ï¸  æ£€æŸ¥å‘½åè§„èŒƒ...")
        
        main_file = self.project_root / 'professional_factor_screener.py'
        if not main_file.exists():
            return {'error': 'Main file not found'}
        
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            # æ£€æŸ¥ç±»å (PascalCase)
            class_names = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_names.append(node.name)
            
            # æ£€æŸ¥å‡½æ•°å (snake_case)
            function_names = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    function_names.append(node.name)
            
            # éªŒè¯å‘½åè§„èŒƒ
            pascal_case_pattern = r'^[A-Z][a-zA-Z0-9]*$'
            snake_case_pattern = r'^[a-z_][a-z0-9_]*$'
            
            valid_class_names = [name for name in class_names if re.match(pascal_case_pattern, name)]
            valid_function_names = [name for name in function_names if re.match(snake_case_pattern, name) or name.startswith('_')]
            
            class_score = len(valid_class_names) / max(len(class_names), 1)
            function_score = len(valid_function_names) / max(len(function_names), 1)
            
            print(f"  ğŸ“Š ç±»åè§„èŒƒ: {len(valid_class_names)}/{len(class_names)} ({class_score:.1%})")
            print(f"  ğŸ“Š å‡½æ•°åè§„èŒƒ: {len(valid_function_names)}/{len(function_names)} ({function_score:.1%})")
            
            return {
                'class_names': class_names,
                'function_names': function_names,
                'valid_class_names': valid_class_names,
                'valid_function_names': valid_function_names,
                'class_score': class_score,
                'function_score': function_score,
                'naming_score': (class_score + function_score) / 2
            }
            
        except Exception as e:
            print(f"  âŒ å‘½åæ£€æŸ¥å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_test_coverage(self) -> Dict[str, Any]:
        """æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡"""
        print("\nğŸ§ª æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥")
        print("-" * 60)
        
        test_files = [
            'test_basic_functionality.py',
            'tests/test_professional_screener.py'
        ]
        
        existing_tests = []
        test_results = {}
        
        for test_file in test_files:
            test_path = self.project_root / test_file
            if test_path.exists():
                existing_tests.append(test_file)
                print(f"  âœ… {test_file}")
                
                # è¿è¡ŒåŸºç¡€åŠŸèƒ½æµ‹è¯•
                if test_file == 'test_basic_functionality.py':
                    try:
                        print(f"  ğŸƒ è¿è¡Œ {test_file}...")
                        result = subprocess.run(
                            [sys.executable, str(test_path)],
                            capture_output=True,
                            text=True,
                            timeout=60
                        )
                        
                        if result.returncode == 0:
                            print(f"    âœ… æµ‹è¯•é€šè¿‡")
                            test_results[test_file] = 'PASSED'
                        else:
                            print(f"    âŒ æµ‹è¯•å¤±è´¥")
                            test_results[test_file] = 'FAILED'
                            
                    except subprocess.TimeoutExpired:
                        print(f"    â° æµ‹è¯•è¶…æ—¶")
                        test_results[test_file] = 'TIMEOUT'
                    except Exception as e:
                        print(f"    âŒ æµ‹è¯•é”™è¯¯: {str(e)}")
                        test_results[test_file] = 'ERROR'
            else:
                print(f"  âŒ {test_file} (ç¼ºå¤±)")
        
        coverage_score = len([r for r in test_results.values() if r == 'PASSED']) / max(len(test_files), 1)
        
        return {
            'existing_tests': existing_tests,
            'test_results': test_results,
            'coverage_score': coverage_score
        }
    
    def check_performance_benchmarks(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ€§èƒ½åŸºå‡†"""
        print("\nâš¡ æ€§èƒ½åŸºå‡†æ£€æŸ¥")
        print("-" * 60)
        
        benchmark_file = self.project_root / 'performance_benchmark.py'
        
        if not benchmark_file.exists():
            print("  âŒ æ€§èƒ½åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨")
            return {'error': 'Benchmark file not found'}
        
        print("  âœ… æ€§èƒ½åŸºå‡†æ–‡ä»¶å­˜åœ¨")
        
        # æ£€æŸ¥æ€§èƒ½æŠ¥å‘Š
        report_files = list(self.project_root.glob('performance_report_*.txt'))
        
        if report_files:
            latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
            print(f"  âœ… æœ€æ–°æ€§èƒ½æŠ¥å‘Š: {latest_report.name}")
            
            # è§£ææ€§èƒ½æŠ¥å‘Š
            try:
                with open(latest_report, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–å…³é”®æ€§èƒ½æŒ‡æ ‡
                ic_performance = re.search(r'ICè®¡ç®—æ€§èƒ½: ğŸŸ¢ ä¼˜ç§€ \((\d+\.?\d*) å› å­/ç§’\)', content)
                memory_efficiency = re.search(r'å†…å­˜ä½¿ç”¨æ•ˆç‡: (\S+) \((\d+\.?\d*)%\)', content)
                parallel_efficiency = re.search(r'å¹¶è¡Œå¤„ç†æ•ˆç‡: (\S+) \((\d+\.?\d*)%\)', content)
                
                performance_metrics = {}
                
                if ic_performance:
                    throughput = float(ic_performance.group(1))
                    performance_metrics['ic_throughput'] = throughput
                    print(f"    ğŸ“Š ICè®¡ç®—ååé‡: {throughput} å› å­/ç§’")
                
                if memory_efficiency:
                    mem_grade = memory_efficiency.group(1)
                    mem_percent = float(memory_efficiency.group(2))
                    performance_metrics['memory_efficiency'] = mem_percent
                    print(f"    ğŸ“Š å†…å­˜æ•ˆç‡: {mem_percent}% ({mem_grade})")
                
                if parallel_efficiency:
                    par_grade = parallel_efficiency.group(1)
                    par_percent = float(parallel_efficiency.group(2))
                    performance_metrics['parallel_efficiency'] = par_percent
                    print(f"    ğŸ“Š å¹¶è¡Œæ•ˆç‡: {par_percent}% ({par_grade})")
                
                # è®¡ç®—æ€§èƒ½å¾—åˆ†
                performance_score = 0
                if 'ic_throughput' in performance_metrics:
                    performance_score += min(performance_metrics['ic_throughput'] / 500, 1.0) * 0.5
                if 'memory_efficiency' in performance_metrics:
                    performance_score += performance_metrics['memory_efficiency'] / 100 * 0.3
                if 'parallel_efficiency' in performance_metrics:
                    performance_score += performance_metrics['parallel_efficiency'] / 100 * 0.2
                
                return {
                    'report_exists': True,
                    'latest_report': latest_report.name,
                    'performance_metrics': performance_metrics,
                    'performance_score': performance_score
                }
                
            except Exception as e:
                print(f"  âŒ æ€§èƒ½æŠ¥å‘Šè§£æå¤±è´¥: {str(e)}")
                return {'error': f'Report parsing failed: {str(e)}'}
        else:
            print("  âš ï¸  æœªæ‰¾åˆ°æ€§èƒ½æŠ¥å‘Š")
            return {'warning': 'No performance reports found'}
    
    def check_configuration_system(self) -> Dict[str, Any]:
        """æ£€æŸ¥é…ç½®ç³»ç»Ÿ"""
        print("\nâš™ï¸  é…ç½®ç³»ç»Ÿæ£€æŸ¥")
        print("-" * 60)
        
        config_files = [
            'config/screening_config.yaml',
            'config_loader.py'
        ]
        
        config_results = {}
        
        for config_file in config_files:
            config_path = self.project_root / config_file
            if config_path.exists():
                print(f"  âœ… {config_file}")
                config_results[config_file] = 'EXISTS'
            else:
                print(f"  âŒ {config_file} (ç¼ºå¤±)")
                config_results[config_file] = 'MISSING'
        
        # æµ‹è¯•é…ç½®åŠ è½½
        try:
            config_loader_path = self.project_root / 'config_loader.py'
            if config_loader_path.exists():
                print("  ğŸ”§ æµ‹è¯•é…ç½®åŠ è½½...")
                
                result = subprocess.run(
                    [sys.executable, str(config_loader_path)],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                if result.returncode == 0:
                    print("    âœ… é…ç½®åŠ è½½æµ‹è¯•é€šè¿‡")
                    config_results['loader_test'] = 'PASSED'
                else:
                    print("    âŒ é…ç½®åŠ è½½æµ‹è¯•å¤±è´¥")
                    config_results['loader_test'] = 'FAILED'
            
        except Exception as e:
            print(f"    âŒ é…ç½®æµ‹è¯•é”™è¯¯: {str(e)}")
            config_results['loader_test'] = 'ERROR'
        
        config_score = len([r for r in config_results.values() if r in ['EXISTS', 'PASSED']]) / len(config_results)
        
        return {
            'config_results': config_results,
            'config_score': config_score
        }
    
    def check_documentation_completeness(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§"""
        print("\nğŸ“š æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥")
        print("-" * 60)
        
        doc_files = [
            'README.md',
            'factor_system_optimization_plan.md'
        ]
        
        doc_results = {}
        
        for doc_file in doc_files:
            doc_path = self.project_root / doc_file
            if doc_path.exists():
                print(f"  âœ… {doc_file}")
                
                # æ£€æŸ¥æ–‡æ¡£é•¿åº¦
                with open(doc_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    word_count = len(content.split())
                    
                print(f"    ğŸ“Š å­—æ•°: {word_count}")
                
                if word_count > 1000:
                    doc_results[doc_file] = 'COMPREHENSIVE'
                elif word_count > 500:
                    doc_results[doc_file] = 'ADEQUATE'
                else:
                    doc_results[doc_file] = 'BRIEF'
            else:
                print(f"  âŒ {doc_file} (ç¼ºå¤±)")
                doc_results[doc_file] = 'MISSING'
        
        # æ£€æŸ¥READMEå…³é”®ç« èŠ‚
        readme_path = self.project_root / 'README.md'
        if readme_path.exists():
            with open(readme_path, 'r', encoding='utf-8') as f:
                readme_content = f.read()
            
            required_sections = [
                'ç³»ç»Ÿæ¦‚è¿°', 'å¿«é€Ÿå¼€å§‹', '5ç»´åº¦ç­›é€‰æ¡†æ¶', 
                'é…ç½®ç³»ç»Ÿ', 'æ€§èƒ½åŸºå‡†', 'ä½¿ç”¨ç¤ºä¾‹'
            ]
            
            missing_sections = []
            for section in required_sections:
                if section not in readme_content:
                    missing_sections.append(section)
            
            section_score = (len(required_sections) - len(missing_sections)) / len(required_sections)
            
            print(f"  ğŸ“Š READMEç« èŠ‚å®Œæ•´æ€§: {section_score:.1%}")
            if missing_sections:
                print(f"    âš ï¸  ç¼ºå¤±ç« èŠ‚: {missing_sections}")
            
            doc_results['readme_sections'] = section_score
        
        doc_score = len([r for r in doc_results.values() if r in ['COMPREHENSIVE', 'ADEQUATE']]) / max(len(doc_files), 1)
        
        return {
            'doc_results': doc_results,
            'doc_score': doc_score
        }
    
    def run_system_integration_test(self) -> Dict[str, Any]:
        """è¿è¡Œç³»ç»Ÿé›†æˆæµ‹è¯•"""
        print("\nğŸ”— ç³»ç»Ÿé›†æˆæµ‹è¯•")
        print("-" * 60)
        
        integration_results = {}
        
        # æµ‹è¯•ä¸»è¦æ¨¡å—å¯¼å…¥
        modules_to_test = [
            'professional_factor_screener',
            'config_loader'
        ]
        
        for module_name in modules_to_test:
            module_path = self.project_root / f'{module_name}.py'
            if module_path.exists():
                try:
                    print(f"  ğŸ”§ æµ‹è¯•æ¨¡å—å¯¼å…¥: {module_name}")
                    
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    
                    print(f"    âœ… å¯¼å…¥æˆåŠŸ")
                    integration_results[module_name] = 'IMPORT_SUCCESS'
                    
                except Exception as e:
                    print(f"    âŒ å¯¼å…¥å¤±è´¥: {str(e)}")
                    integration_results[module_name] = 'IMPORT_FAILED'
            else:
                print(f"  âŒ æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {module_name}")
                integration_results[module_name] = 'FILE_MISSING'
        
        integration_score = len([r for r in integration_results.values() if r == 'IMPORT_SUCCESS']) / len(modules_to_test)
        
        return {
            'integration_results': integration_results,
            'integration_score': integration_score
        }
    
    def generate_quality_report(self, all_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = []
        report.append("="*100)
        report.append("ä¸“ä¸šçº§å› å­ç­›é€‰ç³»ç»Ÿ - è´¨é‡ä¿è¯æŠ¥å‘Š")
        report.append("="*100)
        report.append(f"æ£€æŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ç‰ˆæœ¬: 2.0.0")
        report.append("")
        
        # è®¡ç®—æ€»ä½“è´¨é‡å¾—åˆ†
        total_score = 0
        score_count = 0
        
        # ä»£ç è´¨é‡
        if 'code_quality' in all_results:
            report.append("ğŸ” ä»£ç è´¨é‡è¯„ä¼°")
            report.append("-" * 50)
            
            cq = all_results['code_quality']
            
            if 'file_structure' in cq and 'structure_score' in cq['file_structure']:
                score = cq['file_structure']['structure_score']
                report.append(f"æ–‡ä»¶ç»“æ„: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'import_structure' in cq and 'import_score' in cq['import_structure']:
                score = cq['import_structure']['import_score']
                report.append(f"å¯¼å…¥ç»“æ„: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'function_complexity' in cq and 'complexity_score' in cq['function_complexity']:
                score = cq['function_complexity']['complexity_score']
                report.append(f"å‡½æ•°å¤æ‚åº¦: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'documentation' in cq and 'doc_score' in cq['documentation']:
                score = cq['documentation']['doc_score']
                report.append(f"ä»£ç æ–‡æ¡£: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'naming_conventions' in cq and 'naming_score' in cq['naming_conventions']:
                score = cq['naming_conventions']['naming_score']
                report.append(f"å‘½åè§„èŒƒ: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            report.append("")
        
        # æµ‹è¯•è¦†ç›–ç‡
        if 'test_coverage' in all_results:
            report.append("ğŸ§ª æµ‹è¯•è¦†ç›–ç‡")
            report.append("-" * 50)
            
            tc = all_results['test_coverage']
            if 'coverage_score' in tc:
                score = tc['coverage_score']
                report.append(f"æµ‹è¯•è¦†ç›–ç‡: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'test_results' in tc:
                for test_file, result in tc['test_results'].items():
                    status = "âœ…" if result == "PASSED" else "âŒ"
                    report.append(f"  {status} {test_file}: {result}")
            
            report.append("")
        
        # æ€§èƒ½åŸºå‡†
        if 'performance' in all_results:
            report.append("âš¡ æ€§èƒ½åŸºå‡†")
            report.append("-" * 50)
            
            perf = all_results['performance']
            if 'performance_score' in perf:
                score = perf['performance_score']
                report.append(f"æ€§èƒ½å¾—åˆ†: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            if 'performance_metrics' in perf:
                metrics = perf['performance_metrics']
                for key, value in metrics.items():
                    if key == 'ic_throughput':
                        report.append(f"  ICè®¡ç®—ååé‡: {value} å› å­/ç§’")
                    elif key == 'memory_efficiency':
                        report.append(f"  å†…å­˜æ•ˆç‡: {value}%")
                    elif key == 'parallel_efficiency':
                        report.append(f"  å¹¶è¡Œæ•ˆç‡: {value}%")
            
            report.append("")
        
        # é…ç½®ç³»ç»Ÿ
        if 'configuration' in all_results:
            report.append("âš™ï¸ é…ç½®ç³»ç»Ÿ")
            report.append("-" * 50)
            
            config = all_results['configuration']
            if 'config_score' in config:
                score = config['config_score']
                report.append(f"é…ç½®ç³»ç»Ÿ: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            report.append("")
        
        # æ–‡æ¡£å®Œæ•´æ€§
        if 'documentation' in all_results:
            report.append("ğŸ“š æ–‡æ¡£å®Œæ•´æ€§")
            report.append("-" * 50)
            
            doc = all_results['documentation']
            if 'doc_score' in doc:
                score = doc['doc_score']
                report.append(f"æ–‡æ¡£å®Œæ•´æ€§: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            report.append("")
        
        # ç³»ç»Ÿé›†æˆ
        if 'integration' in all_results:
            report.append("ğŸ”— ç³»ç»Ÿé›†æˆ")
            report.append("-" * 50)
            
            integration = all_results['integration']
            if 'integration_score' in integration:
                score = integration['integration_score']
                report.append(f"ç³»ç»Ÿé›†æˆ: {score:.1%} {'ğŸŸ¢' if score > 0.8 else 'ğŸŸ¡' if score > 0.6 else 'ğŸ”´'}")
                total_score += score
                score_count += 1
            
            report.append("")
        
        # æ€»ä½“è¯„ä¼°
        if score_count > 0:
            overall_score = total_score / score_count
            
            report.append("ğŸ† æ€»ä½“è´¨é‡è¯„ä¼°")
            report.append("-" * 50)
            report.append(f"æ€»ä½“å¾—åˆ†: {overall_score:.1%}")
            
            if overall_score >= 0.9:
                grade = "ğŸŸ¢ ä¼˜ç§€ (Açº§)"
                recommendation = "ç³»ç»Ÿè´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨"
            elif overall_score >= 0.8:
                grade = "ğŸŸ¡ è‰¯å¥½ (Bçº§)"
                recommendation = "ç³»ç»Ÿè´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œå°å¹…ä¼˜åŒ–åä½¿ç”¨"
            elif overall_score >= 0.7:
                grade = "ğŸŸ  åˆæ ¼ (Cçº§)"
                recommendation = "ç³»ç»ŸåŸºæœ¬åˆæ ¼ï¼Œéœ€è¦è¿›è¡Œä¼˜åŒ–æ”¹è¿›"
            else:
                grade = "ğŸ”´ éœ€æ”¹è¿› (Dçº§)"
                recommendation = "ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›æ‰èƒ½æŠ•å…¥ä½¿ç”¨"
            
            report.append(f"è´¨é‡ç­‰çº§: {grade}")
            report.append(f"å»ºè®®: {recommendation}")
        
        report.append("")
        report.append("="*100)
        
        return "\n".join(report)

def run_quality_assurance():
    """è¿è¡Œè´¨é‡ä¿è¯æ£€æŸ¥"""
    print("ğŸš€ å¯åŠ¨ä¸“ä¸šçº§å› å­ç­›é€‰ç³»ç»Ÿè´¨é‡ä¿è¯æ£€æŸ¥")
    print("="*100)
    
    checker = QualityAssuranceChecker()
    all_results = {}
    
    try:
        # 1. ä»£ç è´¨é‡æ£€æŸ¥
        all_results['code_quality'] = checker.check_code_quality()
        
        # 2. æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥
        all_results['test_coverage'] = checker.check_test_coverage()
        
        # 3. æ€§èƒ½åŸºå‡†æ£€æŸ¥
        all_results['performance'] = checker.check_performance_benchmarks()
        
        # 4. é…ç½®ç³»ç»Ÿæ£€æŸ¥
        all_results['configuration'] = checker.check_configuration_system()
        
        # 5. æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥
        all_results['documentation'] = checker.check_documentation_completeness()
        
        # 6. ç³»ç»Ÿé›†æˆæµ‹è¯•
        all_results['integration'] = checker.run_system_integration_test()
        
        # 7. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = checker.generate_quality_report(all_results)
        
        # è¾“å‡ºæŠ¥å‘Š
        print("\n" + quality_report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = checker.project_root / f"quality_assurance_report_{time.strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(quality_report)
        
        print(f"\nğŸ“„ è´¨é‡ä¿è¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è´¨é‡ä¿è¯æ£€æŸ¥å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_quality_assurance()
    sys.exit(0 if success else 1)

