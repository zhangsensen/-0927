#!/usr/bin/env python3
"""
æ‰¹é‡å› å­å¤„ç†å™¨ - ä¼ä¸šçº§å¤šè‚¡ç¥¨å› å­è®¡ç®—ç³»ç»Ÿ
æ”¯æŒ raw/ ç›®å½•ä¸‹æ‰€æœ‰è‚¡ç¥¨çš„å¹¶è¡Œå› å­è®¡ç®—ä¸å­˜å‚¨

è®¾è®¡åŸåˆ™ï¼š
1. é«˜æ€§èƒ½ï¼šå¹¶è¡Œå¤„ç† + å†…å­˜ä¼˜åŒ–
2. å®¹é”™æ€§ï¼šå•è‚¡ç¥¨å¤±è´¥ä¸å½±å“æ•´ä½“
3. å¯ç›‘æ§ï¼šè¯¦ç»†è¿›åº¦å’Œæ€§èƒ½æŒ‡æ ‡
4. å¯æ‰©å±•ï¼šæ”¯æŒæ–°å¸‚åœºå’Œæ—¶é—´æ¡†æ¶
"""

import logging
import multiprocessing as mp
import os
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import pandas as pd
import psutil
from tqdm import tqdm

# ğŸ”§ ä¿®å¤ï¼šç¡®ä¿å­è¿›ç¨‹èƒ½æ‰¾åˆ° factor_system æ¨¡å—
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    from factor_system.factor_generation.config import get_config, setup_logging
except ImportError:
    # ç›¸å¯¹å¯¼å…¥
    from config import get_config, setup_logging
try:
    from factor_system.factor_generation.data_validator import DataValidator
    from factor_system.factor_generation.enhanced_factor_calculator import (
        EnhancedFactorCalculator,
        IndicatorConfig,
    )
    from factor_system.factor_generation.integrated_resampler import IntegratedResampler
except ImportError:
    # ç›¸å¯¹å¯¼å…¥
    from data_validator import DataValidator
    from enhanced_factor_calculator import EnhancedFactorCalculator, IndicatorConfig
    from integrated_resampler import IntegratedResampler

logger = logging.getLogger(__name__)


@dataclass
class StockInfo:
    """è‚¡ç¥¨ä¿¡æ¯"""

    symbol: str
    market: str  # HK, US
    timeframes: List[str]
    file_paths: Dict[str, str]  # timeframe -> file_path


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""

    total_stocks: int = 0
    processed_stocks: int = 0
    failed_stocks: int = 0
    total_factors: int = 0
    processing_time: float = 0.0
    memory_peak_mb: float = 0.0

    @property
    def success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        if self.total_stocks == 0:
            return 0.0
        return (self.processed_stocks / self.total_stocks) * 100

    @property
    def total_factors_generated(self) -> int:
        """å…¼å®¹æ€§å±æ€§ï¼Œè¿”å›æ€»å› å­æ•°"""
        return self.total_factors


class BatchFactorProcessor:
    """æ‰¹é‡å› å­å¤„ç†å™¨"""

    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.config = get_config(config_path)
        if self.config.config_data is None:
            self.config.load_config()

        self.stats = ProcessingStats()
        self.failed_stocks: List[Tuple[str, str]] = []  # (symbol, error)

        # æ€§èƒ½é…ç½®
        perf_config = self.config.get("performance", {})
        self.max_workers = min(
            mp.cpu_count(), perf_config.get("max_workers", mp.cpu_count())
        )
        self.memory_limit_gb = perf_config.get("memory_limit_gb", 8)

        # è¾“å‡ºé…ç½®
        output_config = self.config.get("output", {})
        self.output_dir = Path(output_config.get("directory", "./factor_output"))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.separate_by_market = output_config.get("separate_by_market", True)
        self.enable_validation = output_config.get("enable_validation", True)

        # æ‰¹å¤„ç†æ—¶é—´æˆ³ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
        self.batch_timestamp = None

        # é‡é‡‡æ ·é…ç½®
        self.enable_resampling = self.config.get("resampling", {}).get("enable", True)
        self.temp_resample_dir = Path(
            self.config.get("resampling", {}).get("temp_dir", "./temp_resampled")
        )
        self.resampler = IntegratedResampler() if self.enable_resampling else None

        # æ•°æ®æ ¡å¯¹é…ç½®
        self.validator = DataValidator() if self.enable_validation else None
        self.validation_results = []

        logger.info(f"æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æœ€å¤§å¹¶è¡Œæ•°: {self.max_workers}")
        logger.info(f"å†…å­˜é™åˆ¶: {self.memory_limit_gb}GB")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"é‡é‡‡æ ·åŠŸèƒ½: {'å¯ç”¨' if self.enable_resampling else 'ç¦ç”¨'}")

    def discover_stocks(self, raw_dir: str) -> Dict[str, StockInfo]:
        """å‘ç°æ‰€æœ‰è‚¡ç¥¨åŠå…¶æ•°æ®æ–‡ä»¶"""
        raw_path = Path(raw_dir)
        stocks = {}

        logger.info(f"æ‰«æåŸå§‹æ•°æ®ç›®å½•: {raw_path}")

        # æ‰«ææ‰€æœ‰å¸‚åœº
        for market_dir in raw_path.iterdir():
            if not market_dir.is_dir():
                continue

            market = market_dir.name
            logger.info(f"æ‰«æå¸‚åœº: {market}")

            # æŒ‰è‚¡ç¥¨åˆ†ç»„æ–‡ä»¶
            stock_files = {}
            for file_path in market_dir.glob("*.parquet"):
                # è§£ææ–‡ä»¶åï¼š0700HK_5min_2025-03-05_2025-09-01.parquet
                parts = file_path.stem.split("_")
                if len(parts) < 2:
                    continue

                symbol = parts[0]
                original_timeframe = parts[1]

                # ğŸ”§ æ ‡å‡†åŒ–æ—¶é—´æ¡†æ¶æ ‡ç­¾ï¼š15m->15min, 30m->30min, 60m->60min
                if self.resampler:
                    timeframe = self.resampler.normalize_timeframe_label(
                        original_timeframe
                    )
                    if original_timeframe != timeframe:
                        logger.debug(
                            f"æ ‡å‡†åŒ–æ—¶é—´æ¡†æ¶: {original_timeframe} -> {timeframe}"
                        )
                else:
                    timeframe = original_timeframe

                if symbol not in stock_files:
                    stock_files[symbol] = {}
                # ğŸ”§ å…³é”®ï¼šå¦‚æœåŒä¸€ä¸ªæ ‡å‡†åŒ–æ—¶é—´æ¡†æ¶æœ‰å¤šä¸ªæ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨åŸå§‹æ–‡ä»¶
                if timeframe not in stock_files[symbol]:
                    stock_files[symbol][timeframe] = str(file_path)

            # åˆ›å»ºè‚¡ç¥¨ä¿¡æ¯
            for symbol, files in stock_files.items():
                # ğŸ”§ æ ‡å‡†åŒ–è‚¡ç¥¨ç¬¦å·ï¼š0700HK -> 0700.HK
                if symbol.endswith(market):
                    clean_symbol = symbol[: -len(market)]
                    standardized_symbol = f"{clean_symbol}.{market}"
                else:
                    standardized_symbol = f"{symbol}.{market}"

                stocks[standardized_symbol] = StockInfo(
                    symbol=standardized_symbol,
                    market=market,
                    timeframes=list(files.keys()),
                    file_paths=files,
                )

        logger.info(f"å‘ç° {len(stocks)} åªè‚¡ç¥¨")
        for symbol, info in list(stocks.items())[:5]:  # æ˜¾ç¤ºå‰5åª
            logger.info(f"  {symbol}: {len(info.timeframes)} ä¸ªæ—¶é—´æ¡†æ¶")

        return stocks

    def validate_stock_data(self, stock_info: StockInfo) -> bool:
        """éªŒè¯è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§"""
        required_columns = {"timestamp", "open", "high", "low", "close", "volume"}

        for timeframe, file_path in stock_info.file_paths.items():
            try:
                # å¿«é€Ÿæ£€æŸ¥ï¼šè¯»å–æ–‡ä»¶å¹¶æ£€æŸ¥å‰å‡ è¡Œ
                df = pd.read_parquet(file_path)
                if df.empty:
                    logger.warning(f"{stock_info.symbol} {timeframe}: æ•°æ®ä¸ºç©º")
                    return False

                # åªæ£€æŸ¥å‰å‡ è¡Œçš„åˆ—
                sample_df = df.head(5)
                if not required_columns.issubset(set(sample_df.columns)):
                    logger.warning(f"{stock_info.symbol} {timeframe}: ç¼ºå°‘å¿…è¦åˆ—")
                    return False

            except Exception as e:
                logger.warning(f"{stock_info.symbol} {timeframe}: æ•°æ®è¯»å–å¤±è´¥ - {e}")
                return False

        return True

    def process_single_stock(self, stock_info: StockInfo) -> Tuple[str, bool, str, int]:
        """å¤„ç†å•åªè‚¡ç¥¨çš„æ‰€æœ‰æ—¶é—´æ¡†æ¶

        Returns:
            (symbol, success, error_msg, factor_count)
        """
        symbol = stock_info.symbol

        try:
            # å†…å­˜ç›‘æ§
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB

            total_factors = 0

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡é‡‡æ ·ç”Ÿæˆç¼ºå¤±æ—¶é—´æ¡†æ¶
            complete_file_paths = stock_info.file_paths.copy()

            if self.enable_resampling and self.resampler:
                # è·å–é…ç½®ä¸­éœ€è¦çš„æ—¶é—´æ¡†æ¶
                required_timeframes = self.config.get("timeframes", {}).get(
                    "enabled",
                    ["1min", "2min", "3min", "5min", "15min", "30min", "60min", "1day"],
                )

                # ç¡®ä¿æ‰€æœ‰æ—¶é—´æ¡†æ¶æ•°æ®å­˜åœ¨
                complete_file_paths = self.resampler.ensure_all_timeframes(
                    symbol,
                    stock_info.file_paths,
                    required_timeframes,
                    self.temp_resample_dir,
                )

                logger.debug(
                    f"{symbol}: å®Œæˆæ—¶é—´æ¡†æ¶æ£€æŸ¥ï¼Œå…± {len(complete_file_paths)} ä¸ªæ—¶é—´æ¡†æ¶"
                )

            # åˆå§‹åŒ–å› å­è®¡ç®—å™¨ - å¤ç”¨å®ä¾‹é¿å…é‡å¤åˆå§‹åŒ–
            from pathlib import Path

            from factor_system.factor_engine.batch_calculator import (
                BatchFactorCalculator,
            )

            # ğŸ”§ Linuså¼ä¿®å¤ï¼šä½¿ç”¨ ProjectPaths ç»Ÿä¸€è·¯å¾„ç®¡ç†
            from factor_system.utils import get_project_root

            project_root = get_project_root()
            calculator = BatchFactorCalculator(
                raw_data_dir=project_root,
                enable_cache=True,
            )

            # ä¸ºæ¯ä¸ªæ—¶é—´æ¡†æ¶è®¡ç®—å› å­
            for timeframe, file_path in complete_file_paths.items():

                # è¯»å–æ•°æ®
                df = pd.read_parquet(file_path)

                # æ•°æ®é¢„å¤„ç†
                if "timestamp" in df.columns:
                    df["timestamp"] = pd.to_datetime(df["timestamp"])
                    df.set_index("timestamp", inplace=True)

                is_resampled = timeframe not in stock_info.file_paths

                factors_df = pd.DataFrame()

                if not is_resampled:
                    # ä¼˜å…ˆä½¿ç”¨FactorEngineï¼ˆå…·å¤‡ç¼“å­˜èƒ½åŠ›ï¼‰
                    factors_df = calculator.calculate_all_factors(
                        symbol=symbol,
                        timeframe=timeframe,
                        start_date=df.index.min(),
                        end_date=df.index.max(),
                    )

                    if isinstance(factors_df.index, pd.MultiIndex):
                        factors_df = factors_df.xs(symbol, level="symbol")

                    if not factors_df.empty:
                        factors_df = factors_df.reindex(df.index)

                if factors_df is None or factors_df.empty:
                    # ğŸ”§ å¯¹è¡¥å……æ—¶é—´æ¡†æ¶æˆ–ç¼“å­˜æœªå‘½ä¸­çš„åœºæ™¯ï¼Œç›´æ¥æ¶ˆè´¹DataFrame
                    factors_df = calculator.calculate_factors_from_df(df, timeframe)

                if factors_df is not None and not factors_df.empty:
                    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå°†ä»·æ ¼æ•°æ®åˆå¹¶åˆ°å› å­æ•°æ®ä¸­
                    price_columns = ["open", "high", "low", "close", "volume"]
                    available_price_columns = [
                        col for col in price_columns if col in df.columns
                    ]

                    if available_price_columns:
                        # å°†ä»·æ ¼æ•°æ®æ·»åŠ åˆ°å› å­æ•°æ®ä¸­
                        price_data = df[available_price_columns]

                        # åˆå¹¶å› å­æ•°æ®å’Œä»·æ ¼æ•°æ®
                        combined_df = pd.concat([price_data, factors_df], axis=1)

                        logger.debug(
                            f"{symbol} {timeframe}: åˆå¹¶æ•°æ® - ä»·æ ¼åˆ—: {len(available_price_columns)}, å› å­åˆ—: {len(factors_df.columns)}"
                        )
                    else:
                        combined_df = factors_df
                        logger.warning(
                            f"{symbol} {timeframe}: æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®åˆ—ï¼Œä»…ä¿å­˜å› å­æ•°æ®"
                        )

                    # ä¿å­˜åˆå¹¶åçš„æ•°æ® - æŒ‰å¸‚åœºåˆ†åˆ«å­˜å‚¨
                    timestamp_suffix = (
                        f"_{self.batch_timestamp}" if self.batch_timestamp else ""
                    )
                    if self.separate_by_market:
                        market_dir = self.output_dir / stock_info.market
                        output_path = (
                            market_dir
                            / timeframe
                            / f"{symbol}_{timeframe}_factors{timestamp_suffix}.parquet"
                        )
                    else:
                        output_path = (
                            self.output_dir
                            / timeframe
                            / f"{symbol}_{timeframe}_factors{timestamp_suffix}.parquet"
                        )

                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    # ä¿å­˜åˆå¹¶åçš„æ•°æ®ï¼ˆåŒ…å«ä»·æ ¼+å› å­ï¼‰
                    combined_df.to_parquet(
                        output_path, compression="snappy", index=True
                    )

                    total_factors += len(factors_df.columns)
                    logger.debug(
                        f"{symbol} {timeframe}: {len(factors_df.columns)} ä¸ªå› å­ + {len(available_price_columns)} ä¸ªä»·æ ¼åˆ—"
                    )

                # å†…å­˜æ£€æŸ¥
                current_memory = process.memory_info().rss / 1024 / 1024
                if current_memory > self.memory_limit_gb * 1024:
                    logger.warning(f"{symbol}: å†…å­˜ä½¿ç”¨è¿‡é«˜ {current_memory:.1f}MB")

            end_memory = process.memory_info().rss / 1024 / 1024
            memory_used = end_memory - start_memory

            logger.info(
                f"âœ… {symbol}: {total_factors} ä¸ªå› å­, å†…å­˜: {memory_used:.1f}MB"
            )
            return symbol, True, "", total_factors

        except Exception as e:
            error_msg = f"{symbol} å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return symbol, False, error_msg, 0

    def process_batch(
        self, stocks: Dict[str, StockInfo], batch_size: Optional[int] = None
    ) -> ProcessingStats:
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è‚¡ç¥¨"""

        if batch_size is None:
            batch_size = len(stocks)

        # éªŒè¯æ•°æ®
        logger.info("éªŒè¯è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§...")
        valid_stocks = {}
        for symbol, stock_info in stocks.items():
            if self.validate_stock_data(stock_info):
                valid_stocks[symbol] = stock_info
            else:
                self.failed_stocks.append((symbol, "æ•°æ®éªŒè¯å¤±è´¥"))

        logger.info(f"æœ‰æ•ˆè‚¡ç¥¨: {len(valid_stocks)}/{len(stocks)}")

        # æ›´æ–°ç»Ÿè®¡
        self.stats.total_stocks = len(valid_stocks)
        start_time = time.time()

        # å¹¶è¡Œå¤„ç†
        logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(valid_stocks)} åªè‚¡ç¥¨...")

        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_symbol = {
                executor.submit(self.process_single_stock, stock_info): symbol
                for symbol, stock_info in valid_stocks.items()
            }

            # å¤„ç†ç»“æœ
            with tqdm(total=len(valid_stocks), desc="å¤„ç†è‚¡ç¥¨") as pbar:
                for future in as_completed(future_to_symbol):
                    symbol = future_to_symbol[future]

                    try:
                        symbol_result, success, error_msg, factor_count = (
                            future.result()
                        )

                        if success:
                            self.stats.processed_stocks += 1
                            self.stats.total_factors += factor_count
                        else:
                            self.stats.failed_stocks += 1
                            self.failed_stocks.append((symbol_result, error_msg))

                        pbar.update(1)
                        pbar.set_postfix(
                            {
                                "processed": self.stats.processed_stocks,
                                "failed": self.stats.failed_stocks,
                                "factors": self.stats.total_factors,
                            }
                        )

                    except Exception as e:
                        self.stats.failed_stocks += 1
                        self.failed_stocks.append((symbol, f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"))
                        pbar.update(1)

        # å®Œæˆç»Ÿè®¡
        self.stats.processing_time = time.time() - start_time
        self.stats.memory_peak_mb = psutil.Process().memory_info().rss / 1024 / 1024

        if self.failed_stocks:
            preview = ", ".join(f"{sym}: {err}" for sym, err in self.failed_stocks[:5])
            logger.error(
                f"å¤„ç†å¤±è´¥ {len(self.failed_stocks)} åªè‚¡ç¥¨ (å‰5æ¡): {preview}"
            )

            # è¾“å‡ºå®Œæ•´å¤±è´¥åˆ—è¡¨ä¾›æ’æŸ¥
            failed_summary = "\n".join(
                f"{sym}: {err}" for sym, err in self.failed_stocks
            )
            logger.error(f"å¤„ç†å¤±è´¥è‚¡ç¥¨æ˜ç»†:\n{failed_summary}")

        # æ¸…ç†ä¸´æ—¶é‡é‡‡æ ·æ–‡ä»¶
        if self.enable_resampling and self.config.get("resampling", {}).get(
            "cleanup_temp", True
        ):
            self.cleanup_temp_files()

        return self.stats

    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶é‡é‡‡æ ·æ–‡ä»¶"""
        try:
            if self.temp_resample_dir.exists():
                import shutil

                shutil.rmtree(self.temp_resample_dir)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶é‡é‡‡æ ·ç›®å½•: {self.temp_resample_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    def generate_report(self) -> str:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("æ‰¹é‡å› å­å¤„ç†æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"æ€»è‚¡ç¥¨æ•°: {self.stats.total_stocks}")
        report.append(f"æˆåŠŸå¤„ç†: {self.stats.processed_stocks}")
        report.append(f"å¤„ç†å¤±è´¥: {self.stats.failed_stocks}")
        report.append(
            f"æˆåŠŸç‡: {self.stats.processed_stocks/self.stats.total_stocks*100:.1f}%"
        )
        report.append(f"æ€»å› å­æ•°: {self.stats.total_factors}")
        report.append(f"å¤„ç†æ—¶é—´: {self.stats.processing_time:.1f}ç§’")
        report.append(f"å†…å­˜å³°å€¼: {self.stats.memory_peak_mb:.1f}MB")

        if self.failed_stocks:
            report.append("\nå¤±è´¥è‚¡ç¥¨:")
            for symbol, error in self.failed_stocks[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                report.append(f"  {symbol}: {error}")
            if len(self.failed_stocks) > 10:
                report.append(f"  ... è¿˜æœ‰ {len(self.failed_stocks)-10} ä¸ªå¤±è´¥")

        report.append("=" * 60)

        report_text = "\n".join(report)

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "batch_processing_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_text)

        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_text


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    from factor_system.utils import get_raw_data_dir

    parser = argparse.ArgumentParser(description="æ‰¹é‡å› å­å¤„ç†å™¨")
    parser.add_argument(
        "--raw-dir",
        default=str(get_raw_data_dir()),
        help="åŸå§‹æ•°æ®ç›®å½•",
    )
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--workers", type=int, help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--memory-limit", type=float, help="å†…å­˜é™åˆ¶(GB)")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    setup_logging(timestamp)

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = BatchFactorProcessor(args.config)

    # ä¼ é€’æ—¶é—´æˆ³åˆ°å¤„ç†å™¨ç”¨äºæ–‡ä»¶å‘½å
    processor.batch_timestamp = timestamp

    # è¦†ç›–é…ç½®
    if args.workers:
        processor.max_workers = args.workers
    if args.memory_limit:
        processor.memory_limit_gb = args.memory_limit

    try:
        # å‘ç°è‚¡ç¥¨
        stocks = processor.discover_stocks(args.raw_dir)

        if not stocks:
            logger.error("æœªå‘ç°ä»»ä½•è‚¡ç¥¨æ•°æ®")
            return

        # æ‰¹é‡å¤„ç†
        stats = processor.process_batch(stocks)

        # ç”ŸæˆæŠ¥å‘Š
        report = processor.generate_report()
        print(report)

        logger.info("æ‰¹é‡å¤„ç†å®Œæˆ")

    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
    except Exception as e:
        logger.error(f"å¤„ç†å¼‚å¸¸: {e}")
        raise


if __name__ == "__main__":
    main()
