#!/usr/bin/env python3
"""
å®Œæ•´å› å­å¤„ç†ç®¡é“ - æ”¯æŒHK/USåˆ†åˆ«å­˜å‚¨å’Œå®Œæ•´æ—¶é—´æ¡†æ¶é‡é‡‡æ ·

åŠŸèƒ½ç‰¹æ€§ï¼š
1. æ¸…é™¤å†å²æ•°æ®ï¼ŒæŒ‰HK/USåˆ†åˆ«å­˜å‚¨
2. ä»1miné‡é‡‡æ ·ç”Ÿæˆï¼š2min, 3min, 5min, 15min, 30min, 60min, 2h, 4h, 1day
3. æ•°æ®æ ¡å¯¹ï¼šå¯¹æ¯”é‡é‡‡æ ·æ•°æ®ä¸åŸå§‹æ•°æ®
4. å¹¶è¡Œå¤„ç†154ä¸ªæŠ€æœ¯æŒ‡æ ‡
5. ç”Ÿæˆå®Œæ•´çš„å¤„ç†å’Œæ ¡å¯¹æŠ¥å‘Š
"""

import logging
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from batch_factor_processor import BatchFactorProcessor
from config import setup_logging


def main():
    """ä¸»å¤„ç†å‡½æ•°"""
    print("ğŸš€ å®Œæ•´å› å­å¤„ç†ç®¡é“")
    print("=" * 60)
    print("åŠŸèƒ½ï¼šHK/USåˆ†åˆ«å­˜å‚¨ + å®Œæ•´æ—¶é—´æ¡†æ¶é‡é‡‡æ · + æ•°æ®æ ¡å¯¹")
    print("æ—¶é—´æ¡†æ¶ï¼š1min, 2min, 3min, 5min, 15min, 30min, 60min, 2h, 4h, 1day")
    print("=" * 60)

    # è®¾ç½®æ—¥å¿—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = setup_logging(f"complete_pipeline_{timestamp}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print()

    try:
        # 1. åˆå§‹åŒ–å¤„ç†å™¨
        print("ğŸ”§ åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨...")
        processor = BatchFactorProcessor()
        print(f"   âš™ï¸ é…ç½®æ–‡ä»¶: {processor.config.config_file}")
        print(f"   âœ… é‡é‡‡æ ·åŠŸèƒ½: {'å¯ç”¨' if processor.enable_resampling else 'ç¦ç”¨'}")
        print(f"   âœ… æ•°æ®æ ¡å¯¹: {'å¯ç”¨' if processor.enable_validation else 'ç¦ç”¨'}")
        print(f"   âœ… æŒ‰å¸‚åœºå­˜å‚¨: {'å¯ç”¨' if processor.separate_by_market else 'ç¦ç”¨'}")
        print(f"   âœ… æœ€å¤§å¹¶è¡Œæ•°: {processor.max_workers}")
        print(f"   âœ… è¾“å‡ºç›®å½•: {processor.output_dir}")
        print()

        # 2. æ‰«æåŸå§‹æ•°æ®
        print("ğŸ” æ‰«æåŸå§‹æ•°æ®...")
        # ğŸ”§ Linuså¼ä¿®å¤ï¼šä½¿ç”¨ ProjectPaths ç»Ÿä¸€è·¯å¾„ç®¡ç†
        from factor_system.utils import get_raw_data_dir
        raw_dir = str(get_raw_data_dir())
        stocks = processor.discover_stocks(raw_dir)

        # æŒ‰å¸‚åœºåˆ†ç»„ç»Ÿè®¡
        hk_stocks = {k: v for k, v in stocks.items() if v.market == "HK"}
        us_stocks = {k: v for k, v in stocks.items() if v.market == "US"}

        print(f"   âœ… å‘ç° {len(stocks)} åªè‚¡ç¥¨")
        print(f"   ğŸ“Š HKå¸‚åœº: {len(hk_stocks)} åªè‚¡ç¥¨")
        print(f"   ğŸ“Š USå¸‚åœº: {len(us_stocks)} åªè‚¡ç¥¨")
        print()

        # æ˜¾ç¤ºæ—¶é—´æ¡†æ¶ç»Ÿè®¡
        print("ğŸ“Š æ—¶é—´æ¡†æ¶ç»Ÿè®¡:")
        timeframe_stats = {}
        for stock_info in stocks.values():
            for tf in stock_info.timeframes:
                timeframe_stats[tf] = timeframe_stats.get(tf, 0) + 1

        for tf, count in sorted(timeframe_stats.items()):
            print(f"   {tf}: {count} åªè‚¡ç¥¨")
        print()

        # 3. æ¼”ç¤ºé‡é‡‡æ ·éœ€æ±‚
        print("ğŸ”„ é‡é‡‡æ ·éœ€æ±‚åˆ†æ...")
        required_timeframes = processor.config.get("timeframes", {}).get("enabled", [])
        print(f"   éœ€è¦çš„æ—¶é—´æ¡†æ¶: {required_timeframes}")

        # ç»Ÿè®¡éœ€è¦é‡é‡‡æ ·çš„è‚¡ç¥¨
        resample_needed = 0
        for stock_info in stocks.values():
            if processor.resampler:
                missing_tfs = processor.resampler.find_missing_timeframes(
                    stock_info.file_paths, required_timeframes
                )
                if missing_tfs:
                    resample_needed += 1

        print(f"   éœ€è¦é‡é‡‡æ ·çš„è‚¡ç¥¨: {resample_needed} åª")
        print()

        # 4. æ‰§è¡Œæ‰¹é‡å¤„ç†
        print("âš¡ å¼€å§‹æ‰¹é‡å¤„ç†...")
        print(f"   å¤„ç†æ¨¡å¼ï¼šå®Œæ•´å¤„ç†æ‰€æœ‰ {len(stocks)} åªè‚¡ç¥¨")
        print()

        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        stats = processor.process_batch(stocks)

        # 5. å¤„ç†ç»“æœç»Ÿè®¡
        print("\nğŸ“ˆ å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"   æ€»è‚¡ç¥¨æ•°: {stats.total_stocks}")
        print(f"   æˆåŠŸå¤„ç†: {stats.processed_stocks}")
        print(f"   å¤„ç†å¤±è´¥: {stats.failed_stocks}")
        print(f"   æˆåŠŸç‡: {stats.success_rate:.1f}%")
        print(f"   æ€»å› å­æ•°: {stats.total_factors_generated}")
        print(f"   å¤„ç†æ—¶é—´: {stats.processing_time:.1f}ç§’")
        print(f"   å†…å­˜å³°å€¼: {stats.memory_peak_mb:.1f}MB")
        print()

        # 6. è¾“å‡ºç›®å½•ç»“æ„
        print("ğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
        if processor.separate_by_market:
            for market in ["HK", "US"]:
                market_dir = processor.output_dir / market
                if market_dir.exists():
                    print(f"   {market}/")
                    for tf_dir in sorted(market_dir.iterdir()):
                        if tf_dir.is_dir():
                            files = list(tf_dir.glob("*.parquet"))
                            if files:
                                print(f"     {tf_dir.name}/: {len(files)} ä¸ªå› å­æ–‡ä»¶")
        else:
            for tf_dir in sorted(processor.output_dir.iterdir()):
                if tf_dir.is_dir():
                    files = list(tf_dir.glob("*.parquet"))
                    if files:
                        print(f"   {tf_dir.name}/: {len(files)} ä¸ªå› å­æ–‡ä»¶")
        print()

        # 7. æ•°æ®æ ¡å¯¹ç»“æœ
        if processor.enable_validation and processor.validation_results:
            print("ğŸ” æ•°æ®æ ¡å¯¹ç»“æœ:")
            validation_stats = {}
            for result in processor.validation_results:
                status = result.get("status", "UNKNOWN")
                validation_stats[status] = validation_stats.get(status, 0) + 1

            for status, count in validation_stats.items():
                print(f"   {status}: {count}")

            # ç”Ÿæˆæ ¡å¯¹æŠ¥å‘Š
            report_path = processor.output_dir / f"validation_report_{timestamp}.txt"
            processor.validator.generate_validation_report(
                processor.validation_results, report_path
            )
            print(f"   ğŸ“‹ æ ¡å¯¹æŠ¥å‘Š: {report_path}")
        print()

        # 8. é‡é‡‡æ ·æ–‡ä»¶æ¸…ç†çŠ¶æ€
        if processor.enable_resampling:
            cleanup_enabled = processor.config.get("resampling", {}).get(
                "cleanup_temp", True
            )
            print(f"ğŸ§¹ ä¸´æ—¶é‡é‡‡æ ·æ–‡ä»¶: {'å·²æ¸…ç†' if cleanup_enabled else 'ä¿ç•™'}")
        print()

        # 9. æ€§èƒ½åˆ†æ
        print("âš¡ æ€§èƒ½åˆ†æ:")
        if stats.processed_stocks > 0:
            avg_factors_per_stock = (
                stats.total_factors_generated / stats.processed_stocks
            )
            avg_time_per_stock = stats.processing_time / stats.processed_stocks
            print(f"   å¹³å‡æ¯åªè‚¡ç¥¨å› å­æ•°: {avg_factors_per_stock:.0f}")
            print(f"   å¹³å‡æ¯åªè‚¡ç¥¨å¤„ç†æ—¶é—´: {avg_time_per_stock:.2f}ç§’")
            print(
                f"   å› å­ç”Ÿæˆé€Ÿåº¦: {stats.total_factors_generated / stats.processing_time:.0f} å› å­/ç§’"
            )

            # æŒ‰å¸‚åœºç»Ÿè®¡
            if processor.separate_by_market:
                hk_processed = sum(1 for s in stocks.values() if s.market == "HK")
                us_processed = sum(1 for s in stocks.values() if s.market == "US")
                print(f"   HKå¸‚åœºå¤„ç†: {hk_processed} åªè‚¡ç¥¨")
                print(f"   USå¸‚åœºå¤„ç†: {us_processed} åªè‚¡ç¥¨")
        print()

        print("ğŸ‰ å®Œæ•´ç®¡é“å¤„ç†æˆåŠŸå®Œæˆï¼")
        print("=" * 60)
        print("âœ¨ æ‰€æœ‰è‚¡ç¥¨å·²æŒ‰HK/USåˆ†åˆ«å­˜å‚¨")
        print("âœ¨ å®Œæ•´æ—¶é—´æ¡†æ¶é‡é‡‡æ ·å·²å®Œæˆ")
        print("âœ¨ 154ä¸ªæŠ€æœ¯æŒ‡æ ‡åœ¨å¤šæ—¶é—´æ¡†æ¶ä¸‹å¹¶è¡Œè®¡ç®—")
        print("âœ¨ æ•°æ®æ ¡å¯¹å’ŒéªŒè¯å·²å®Œæˆ")
        print("âœ¨ å†…å­˜ç®¡ç†å’Œé”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„")

        return True

    except Exception as e:
        print(f"âŒ ç®¡é“å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
