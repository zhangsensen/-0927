#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ETFæ¨ªæˆªé¢æ‰¹é‡å› å­è®¡ç®—å¼•æ“
æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—ï¼Œé«˜æ•ˆå¤„ç†800-1200ä¸ªå€™é€‰å› å­
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
import multiprocessing as mp
from datetime import datetime, timedelta
import pickle
import gc
import psutil

from factor_system.factor_engine import api
from factor_system.utils import safe_operation, FactorSystemError
from .candidate_factor_generator import ETFCandidateFactorGenerator, FactorVariant

logger = logging.getLogger(__name__)


@dataclass
class CalculationTask:
    """è®¡ç®—ä»»åŠ¡å®šä¹‰"""
    task_id: str
    variant: FactorVariant
    symbols: List[str]
    timeframe: str
    start_date: datetime
    end_date: datetime


@dataclass
class CalculationResult:
    """è®¡ç®—ç»“æœ"""
    task_id: str
    variant_id: str
    factor_data: pd.DataFrame
    success: bool
    error_message: Optional[str] = None
    calculation_time: Optional[float] = None


class BatchFactorCalculator:
    """æ‰¹é‡å› å­è®¡ç®—å™¨"""

    def __init__(self, max_workers: Optional[int] = None, batch_size: int = 50):
        """
        åˆå§‹åŒ–æ‰¹é‡è®¡ç®—å™¨

        Args:
            max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°-1
            batch_size: æ¯æ‰¹å¤„ç†çš„å› å­æ•°é‡
        """
        self.max_workers = max_workers or max(1, mp.cpu_count() - 1)
        self.batch_size = batch_size
        self.memory_threshold = 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼

        logger.info(f"æ‰¹é‡å› å­è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°: {self.max_workers}")
        logger.info(f"æ‰¹é‡å¤§å°: {self.batch_size}")

    def _check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_percent = psutil.virtual_memory().percent / 100
        if memory_percent > self.memory_threshold:
            logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent:.1%}, æ‰§è¡Œåƒåœ¾å›æ”¶")
            gc.collect()
        return memory_percent

    def calculate_factors(self,
                         symbols: List[str],
                         factor_ids: List[str],
                         start_date: datetime,
                         end_date: datetime,
                         timeframe: str = 'daily',
                         max_workers: Optional[int] = None,
                         factor_registry = None) -> pd.DataFrame:
        """
        è®¡ç®—å› å­ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            symbols: ETFä»£ç åˆ—è¡¨
            factor_ids: å› å­IDåˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            timeframe: æ—¶é—´æ¡†æ¶
            max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
            factor_registry: å› å­æ³¨å†Œè¡¨ï¼ˆç”¨äºåŠ¨æ€å› å­ï¼‰
            
        Returns:
            ç»Ÿä¸€æ ¼å¼DataFrame: MultiIndex(date, symbol), columns=factor_ids
        """
        logger.info(f"å¼€å§‹è®¡ç®—å› å­: {len(symbols)}åªETF, {len(factor_ids)}ä¸ªå› å­")
        
        try:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„factor_registry
            if factor_registry is not None:
                # ä½¿ç”¨ETFä¸“ç”¨æ³¨å†Œè¡¨è®¡ç®—åŠ¨æ€å› å­
                all_data = []
                
                for factor_id in factor_ids:
                    try:
                        # ä»æ³¨å†Œè¡¨è·å–å› å­ä¿¡æ¯
                        factor_info = factor_registry.get_factor(factor_id)
                        if factor_info is None:
                            # å›é€€åˆ°å…¨å±€API
                            factor_result = api.calculate_factors(
                                factor_ids=[factor_id],
                                symbols=symbols,
                                timeframe=timeframe,
                                start_date=start_date,
                                end_date=end_date
                            )
                        else:
                            # ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„å› å­å‡½æ•°ç›´æ¥è®¡ç®—
                            # è¿™é‡Œéœ€è¦åŠ è½½æ•°æ®å¹¶è°ƒç”¨å› å­å‡½æ•°
                            factor_result = self._calculate_factor_from_registry(
                                factor_id, factor_info, symbols, start_date, end_date, timeframe
                            )
                        
                        if factor_result is not None and not factor_result.empty:
                            if factor_id in factor_result.columns:
                                all_data.append(factor_result[[factor_id]])
                            elif len(factor_result.columns) > 0:
                                temp_df = factor_result.iloc[:, [0]].copy()
                                temp_df.columns = [factor_id]
                                all_data.append(temp_df)
                                
                    except Exception as e:
                        logger.warning(f"å› å­ {factor_id} è®¡ç®—å¤±è´¥: {str(e)}")
                        continue
                
                if not all_data:
                    logger.warning("æ‰€æœ‰å› å­è®¡ç®—å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
                    return pd.DataFrame()
                
                result_df = pd.concat(all_data, axis=1)
                
            else:
                # ä½¿ç”¨å…¨å±€APIè®¡ç®—
                all_data = []
                
                for factor_id in factor_ids:
                    try:
                        factor_result = api.calculate_factors(
                            factor_ids=[factor_id],
                            symbols=symbols,
                            timeframe=timeframe,
                            start_date=start_date,
                            end_date=end_date
                        )
                        
                        if factor_result is not None and not factor_result.empty:
                            if factor_id in factor_result.columns:
                                all_data.append(factor_result[[factor_id]])
                            elif len(factor_result.columns) > 0:
                                temp_df = factor_result.iloc[:, [0]].copy()
                                temp_df.columns = [factor_id]
                                all_data.append(temp_df)
                                
                    except Exception as e:
                        logger.warning(f"å› å­ {factor_id} è®¡ç®—å¤±è´¥: {str(e)}")
                        continue
                
                if not all_data:
                    logger.warning("æ‰€æœ‰å› å­è®¡ç®—å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame")
                    return pd.DataFrame()
                
                result_df = pd.concat(all_data, axis=1)
            
            # ç¡®ä¿MultiIndexæ ¼å¼ (date, symbol)
            if not isinstance(result_df.index, pd.MultiIndex):
                if 'date' in result_df.columns and 'symbol' in result_df.columns:
                    result_df = result_df.set_index(['date', 'symbol'])
            
            logger.info(f"å› å­è®¡ç®—å®Œæˆ: {result_df.shape}")
            return result_df
            
        except Exception as e:
            logger.error(f"å› å­è®¡ç®—å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return pd.DataFrame()
    
    def _calculate_factor_from_registry(self, factor_id: str, factor_info,
                                       symbols: List[str], start_date: datetime,
                                       end_date: datetime, timeframe: str) -> pd.DataFrame:
        """ä»æ³¨å†Œè¡¨ä¸­çš„å› å­å‡½æ•°è®¡ç®—å› å­å€¼"""
        try:
            # ğŸ”¥ å…³é”®å®ç°ï¼šç›´æ¥ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„å› å­å‡½æ•°
            # 1. åŠ è½½æ•°æ®
            from factor_system.factor_engine.providers.etf_cross_section_provider import ETFCrossSectionDataManager
            data_manager = ETFCrossSectionDataManager()
            
            # 2. è·å–æ—¶é—´åºåˆ—æ•°æ®
            price_data = data_manager.get_time_series_data(
                start_date.strftime('%Y-%m-%d'),
                end_date.strftime('%Y-%m-%d'),
                symbols
            )
            
            if price_data.empty:
                logger.warning(f"å› å­ {factor_id}: æ— æ•°æ®")
                return pd.DataFrame()
            
            # 3. æŒ‰symbolåˆ†ç»„è®¡ç®—å› å­
            results = []
            factor_func = factor_info.function
            
            for etf_code in symbols:
                etf_data = price_data[price_data['etf_code'] == etf_code].copy()
                if etf_data.empty:
                    continue
                
                try:
                    # è°ƒç”¨å› å­å‡½æ•°
                    factor_values = factor_func(etf_data)
                    
                    # æ„å»ºç»“æœDataFrame
                    result_df = pd.DataFrame({
                        'date': etf_data['trade_date'],
                        'symbol': etf_code,
                        factor_id: factor_values
                    })
                    results.append(result_df)
                    
                except Exception as e:
                    logger.debug(f"å› å­ {factor_id} è®¡ç®—å¤±è´¥ ({etf_code}): {str(e)}")
                    continue
            
            if not results:
                return pd.DataFrame()
            
            # 4. åˆå¹¶ç»“æœ
            combined = pd.concat(results, ignore_index=True)
            combined['date'] = pd.to_datetime(combined['date'], format='%Y%m%d')
            combined = combined.set_index(['date', 'symbol'])
            
            return combined
            
        except Exception as e:
            logger.error(f"ä»æ³¨å†Œè¡¨è®¡ç®—å› å­å¤±è´¥ {factor_id}: {str(e)}")
            # å›é€€åˆ°API
            return api.calculate_factors(
                factor_ids=[factor_id],
                symbols=symbols,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date
            )

    def _calculate_single_factor(self, task: CalculationTask) -> CalculationResult:
        """è®¡ç®—å•ä¸ªå› å­"""
        start_time = datetime.now()

        try:
            # è§£æå› å­å‚æ•°
            factor_id = task.variant.base_factor_id
            parameters = task.variant.parameters

            # æ„å»ºå› å­è®¡ç®—å‚æ•°
            calc_params = {
                "factor_ids": [factor_id],
                "symbols": task.symbols,
                "timeframe": task.timeframe,
                "start_date": task.start_date,
                "end_date": task.end_date
            }

            # æ·»åŠ å› å­å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if parameters:
                calc_params.update(parameters)

            # æ‰§è¡Œå› å­è®¡ç®—
            factor_data = api.calculate_factors(**calc_params)

            if factor_data is None or factor_data.empty:
                return CalculationResult(
                    task_id=task.task_id,
                    variant_id=task.variant_id,
                    factor_data=pd.DataFrame(),
                    success=False,
                    error_message="å› å­è®¡ç®—è¿”å›ç©ºç»“æœ"
                )

            # é‡å‘½ååˆ—ä»¥åŒ…å«å˜ä½“ID
            if not factor_data.empty:
                factor_columns = [col for col in factor_data.columns if col not in ['symbol', 'date']]
                rename_dict = {}
                for col in factor_columns:
                    rename_dict[col] = f"{task.variant.variant_id}_{col}"
                factor_data = factor_data.rename(columns=rename_dict)

            calculation_time = (datetime.now() - start_time).total_seconds()

            return CalculationResult(
                task_id=task.task_id,
                variant_id=task.variant_id,
                factor_data=factor_data,
                success=True,
                calculation_time=calculation_time
            )

        except Exception as e:
            calculation_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"å› å­è®¡ç®—å¤±è´¥: {str(e)}"
            logger.error(f"{task.task_id}: {error_msg}")

            return CalculationResult(
                task_id=task.task_id,
                variant_id=task.variant_id,
                factor_data=pd.DataFrame(),
                success=False,
                error_message=error_msg,
                calculation_time=calculation_time
            )

    def _process_batch(self, batch_tasks: List[CalculationTask]) -> List[CalculationResult]:
        """å¤„ç†ä¸€æ‰¹è®¡ç®—ä»»åŠ¡"""
        results = []

        # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        with ProcessPoolExecutor(max_workers=min(self.max_workers, len(batch_tasks))) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self._calculate_single_factor, task): task
                for task in batch_tasks
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)

                    # è¿›åº¦æŠ¥å‘Š
                    if result.success:
                        logger.debug(f"âœ… {task.task_id} å®Œæˆ ({result.calculation_time:.3f}s)")
                    else:
                        logger.warning(f"âŒ {task.task_id} å¤±è´¥: {result.error_message}")

                except Exception as e:
                    logger.error(f"âŒ {task.task_id} å¼‚å¸¸: {str(e)}")
                    results.append(CalculationResult(
                        task_id=task.task_id,
                        variant_id=task.variant.variant_id,
                        factor_data=pd.DataFrame(),
                        success=False,
                        error_message=f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                    ))

        return results

    def calculate_factors_batch(self,
                              variants: List[FactorVariant],
                              symbols: List[str],
                              timeframe: str,
                              start_date: datetime,
                              end_date: datetime,
                              output_dir: str) -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è®¡ç®—å› å­

        Args:
            variants: å› å­å˜ä½“åˆ—è¡¨
            symbols: ETFä»£ç åˆ—è¡¨
            timeframe: æ—¶é—´æ¡†æ¶
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            å› å­æ•°æ®å­—å…¸ {variant_id: factor_data}
        """
        logger.info(f"å¼€å§‹æ‰¹é‡è®¡ç®— {len(variants)} ä¸ªå› å­")
        logger.info(f"æ ‡çš„æ± : {len(symbols)} ä¸ªETF")
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date.date()} ~ {end_date.date()}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºè®¡ç®—ä»»åŠ¡
        tasks = []
        for i, variant in enumerate(variants):
            task = CalculationTask(
                task_id=f"task_{i:04d}",
                variant=variant,
                symbols=symbols,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date
            )
            tasks.append(task)

        # åˆ†æ‰¹å¤„ç†
        all_results = {}
        total_batches = (len(tasks) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(tasks))
            batch_tasks = tasks[start_idx:end_idx]

            logger.info(f"å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ ({len(batch_tasks)} ä¸ªå› å­)")

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            memory_percent = self._check_memory_usage()

            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results = self._process_batch(batch_tasks)

            # ä¿å­˜ä¸­é—´ç»“æœ
            for result in batch_results:
                all_results[result.variant_id] = result

                # ä¿å­˜åˆ°æ–‡ä»¶
                if result.success and not result.factor_data.empty:
                    factor_file = output_path / f"{result.variant_id}.parquet"
                    result.factor_data.to_parquet(factor_file, index=False)

            # æ‰¹æ¬¡ç»Ÿè®¡
            success_count = sum(1 for r in batch_results if r.success)
            total_time = sum(r.calculation_time for r in batch_results if r.calculation_time)
            avg_time = total_time / len(batch_results) if batch_results else 0

            logger.info(f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ: {success_count}/{len(batch_tasks)} æˆåŠŸ, "
                       f"å¹³å‡è€—æ—¶ {avg_time:.3f}s")

            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()

        # ä¿å­˜è®¡ç®—ç»Ÿè®¡
        self._save_calculation_stats(all_results, output_path)

        # è¿”å›æˆåŠŸçš„å› å­æ•°æ®
        successful_factors = {}
        for variant_id, result in all_results.items():
            if result.success and not result.factor_data.empty:
                successful_factors[variant_id] = result.factor_data

        logger.info(f"æ‰¹é‡è®¡ç®—å®Œæˆ: {len(successful_factors)}/{len(variants)} ä¸ªå› å­æˆåŠŸ")

        return successful_factors

    def _save_calculation_stats(self, results: Dict[str, CalculationResult], output_path: Path):
        """ä¿å­˜è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_tasks": len(results),
            "successful_tasks": sum(1 for r in results.values() if r.success),
            "failed_tasks": sum(1 for r in results.values() if not r.success),
            "total_calculation_time": sum(r.calculation_time or 0 for r in results.values()),
            "average_calculation_time": np.mean([r.calculation_time for r in results.values() if r.calculation_time]),
            "successful_variants": [variant_id for variant_id, r in results.items() if r.success],
            "failed_variants": [(variant_id, r.error_message) for variant_id, r in results.items() if not r.success],
            "generated_at": datetime.now().isoformat()
        }

        stats_file = output_path / "calculation_stats.json"
        import json
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        logger.info(f"è®¡ç®—ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file}")

    def load_calculated_factors(self, input_dir: str) -> Dict[str, pd.DataFrame]:
        """åŠ è½½å·²è®¡ç®—çš„å› å­æ•°æ®"""
        input_path = Path(input_dir)
        factors = {}

        for factor_file in input_path.glob("*.parquet"):
            variant_id = factor_file.stem
            try:
                factor_data = pd.read_parquet(factor_file)
                factors[variant_id] = factor_data
                logger.debug(f"åŠ è½½å› å­: {variant_id} ({len(factor_data)} æ¡è®°å½•)")
            except Exception as e:
                logger.error(f"åŠ è½½å› å­å¤±è´¥ {variant_id}: {str(e)}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(factors)} ä¸ªå› å­æ•°æ®")
        return factors


@safe_operation
def calculate_all_etf_factors(symbols: List[str],
                            start_date: str,
                            end_date: str,
                            timeframe: str = "daily",
                            output_dir: str = None) -> Dict[str, pd.DataFrame]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè®¡ç®—æ‰€æœ‰ETFå› å­

    Args:
        symbols: ETFä»£ç åˆ—è¡¨
        start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        timeframe: æ—¶é—´æ¡†æ¶
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        å› å­æ•°æ®å­—å…¸
    """
    # å‚æ•°å¤„ç†
    start_dt = pd.to_datetime(start_date)
    end_dt = pd.to_datetime(end_date)

    if output_dir is None:
        output_dir = f"factor_system/factor_output/etf_cross_section/calculated_factors_{start_date}_{end_date}"

    # ç”Ÿæˆå€™é€‰å› å­
    generator = ETFCandidateFactorGenerator()
    variants = generator.generate_all_variants()

    logger.info(f"å‡†å¤‡è®¡ç®— {len(variants)} ä¸ªå€™é€‰å› å­")

    # æ‰¹é‡è®¡ç®—
    calculator = BatchFactorCalculator()
    factors = calculator.calculate_factors_batch(
        variants=variants,
        symbols=symbols,
        timeframe=timeframe,
        start_date=start_dt,
        end_date=end_dt,
        output_dir=output_dir
    )

    return factors


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # æµ‹è¯•å‚æ•°
    test_symbols = ['510300.SH', '159915.SZ', '515030.SH']
    start_date = "2025-09-01"
    end_date = "2025-10-14"

    # è®¡ç®—å› å­
    factors = calculate_all_etf_factors(
        symbols=test_symbols,
        start_date=start_date,
        end_date=end_date
    )

    print(f"è®¡ç®—å®Œæˆï¼Œè·å¾— {len(factors)} ä¸ªå› å­æ•°æ®")