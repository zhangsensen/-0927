#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šçº§é‡åŒ–äº¤æ˜“å› å­ç­›é€‰ç³»ç»Ÿ - 5ç»´åº¦ç­›é€‰æ¡†æ¶ + å…¬å¹³è¯„åˆ†ç³»ç»Ÿ
ä½œè€…ï¼šé‡åŒ–é¦–å¸­å·¥ç¨‹å¸ˆ
ç‰ˆæœ¬ï¼š3.1.0
æ—¥æœŸï¼š2025-10-07
çŠ¶æ€ï¼šç”Ÿäº§å°±ç»ª
æ›´æ–°ï¼šé›†æˆå…¬å¹³è¯„åˆ†ç®—æ³•ï¼Œä¿®å¤èƒœç‡è®¡ç®—bug

æ ¸å¿ƒç‰¹æ€§ï¼š
1. 5ç»´åº¦ç­›é€‰æ¡†æ¶ï¼šé¢„æµ‹èƒ½åŠ›ã€ç¨³å®šæ€§ã€ç‹¬ç«‹æ€§ã€å®ç”¨æ€§ã€çŸ­å‘¨æœŸé€‚åº”æ€§
2. å¤šå‘¨æœŸICåˆ†æï¼š1æ—¥ã€3æ—¥ã€5æ—¥ã€10æ—¥ã€20æ—¥é¢„æµ‹èƒ½åŠ›è¯„ä¼°
3. ä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼šBenjamini-Hochberg FDRæ ¡æ­£
4. VIFæ£€æµ‹å’Œä¿¡æ¯å¢é‡åˆ†æ
5. äº¤æ˜“æˆæœ¬å’ŒæµåŠ¨æ€§è¯„ä¼°
6. ç”Ÿäº§çº§æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
"""

import json
import logging
import time
import warnings
from collections import Counter
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set, Tuple

import numpy as np
import pandas as pd
import psutil
import yaml

# å¯¼å…¥é…ç½®ç±»
from config_manager import ScreeningConfig  # type: ignore
from scipy import stats

# P0 ä¼˜åŒ–ï¼šå¯¼å…¥å‘é‡åŒ–æ ¸å¿ƒå¼•æ“
try:
    from vectorized_core import get_vectorized_analyzer

    VECTORIZED_ENGINE_AVAILABLE = True
except ImportError:
    VECTORIZED_ENGINE_AVAILABLE = False
    logging.getLogger(__name__).warning("âš ï¸ å‘é‡åŒ–å¼•æ“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")

# P3 ä¼˜åŒ–ï¼šå¯¼å…¥æ€§èƒ½ç›‘æ§
try:
    from performance_monitor import get_performance_monitor

    PERFORMANCE_MONITOR_AVAILABLE = True
except ImportError:
    PERFORMANCE_MONITOR_AVAILABLE = False
    logging.getLogger(__name__).warning("âš ï¸ æ€§èƒ½ç›‘æ§æ¨¡å—ä¸å¯ç”¨")

# ğŸ”§ é›†æˆdata_loader_patchè¡¥ä¸åˆ°ä¸»ä»£ç 
try:
    from data_loader_patch import load_factors_v2, load_price_data_v2
except ImportError:
    # å¦‚æœè¡¥ä¸ä¸å¯ç”¨ï¼Œå®šä¹‰å›é€€å‡½æ•°
    def load_factors_v2(self, symbol: str, timeframe: str):
        raise ImportError("data_loader_patchä¸å¯ç”¨")

    def load_price_data_v2(self, symbol: str, timeframe: str):
        raise ImportError("data_loader_patchä¸å¯ç”¨")


# å…¬å¹³è¯„åˆ†å·²é›†æˆåˆ°ä¸»æµç¨‹ï¼Œæ— éœ€å•ç‹¬æ¨¡å—


# JSONç¼–ç å™¨ï¼Œæ”¯æŒnumpyç±»å‹
class NumpyJSONEncoder(json.JSONEncoder):
    """JSONç¼–ç å™¨ï¼Œæ”¯æŒnumpyç±»å‹"""

    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NumpyJSONEncoder, self).default(obj)


# å¯¼å…¥å› å­å¯¹é½å·¥å…·
try:
    from factor_alignment_utils import (  # type: ignore
        FactorFileAligner,
        find_aligned_factor_files,
        validate_factor_alignment,
    )
except ImportError as e:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›å›é€€æ–¹æ¡ˆ
    logging.getLogger(__name__).warning(f"å› å­å¯¹é½å·¥å…·å¯¼å…¥å¤±è´¥: {e}")
    FactorFileAligner = None

    def find_aligned_factor_files(*args: Any, **kwargs: Any) -> None:
        raise ImportError("å› å­å¯¹é½å·¥å…·ä¸å¯ç”¨")

    def validate_factor_alignment(*args: Any, **kwargs: Any) -> Tuple[bool, str]:
        return True, "å¯¹é½éªŒè¯å·¥å…·ä¸å¯ç”¨"


try:
    from utils.temporal_validator import TemporalValidationError, TemporalValidator
except ImportError:  # pragma: no cover - è¿è¡Œç¯å¢ƒç¼ºå¤±
    TemporalValidator = None  # type: ignore

    class TemporalValidationError(Exception):
        """æ—¶é—´åºåˆ—éªŒè¯å™¨ä¸å¯ç”¨æ—¶çš„åå¤‡å¼‚å¸¸"""

        pass


# P0çº§é›†æˆï¼šå¯¼å…¥å®é™…ä½¿ç”¨çš„å·¥å…·æ¨¡å—ï¼ˆè¯šå®ç‰ˆï¼‰
try:
    from utils.input_validator import InputValidator, ValidationError
except ImportError:
    logging.getLogger(__name__).warning("è¾“å…¥éªŒè¯å™¨å¯¼å…¥å¤±è´¥")
    InputValidator = None  # type: ignore
    ValidationError = Exception  # type: ignore

try:
    from utils.structured_logger import get_structured_logger
except ImportError as e:
    logging.getLogger(__name__).warning(f"ç»“æ„åŒ–æ—¥å¿—å™¨å¯¼å…¥å¤±è´¥: {e}")
    get_structured_logger = None  # type: ignore

# å·²ç§»é™¤æœªä½¿ç”¨çš„æ¨¡å—å¯¼å…¥ï¼ˆLinusåŸåˆ™ï¼šè¯šå®åæ˜ å®é™…çŠ¶æ€ï¼‰
# - memory_optimizer: å½“å‰ç³»ç»Ÿå†…å­˜ä½¿ç”¨æ­£å¸¸ï¼Œæ— éœ€å¤æ‚ç›‘æ§
# - backup_manager: æ–‡ä»¶ç³»ç»Ÿå·²è¶³å¤Ÿï¼Œæ— éœ€è¿‡åº¦å·¥ç¨‹åŒ–


warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class FactorMetrics:
    """å› å­ç»¼åˆæŒ‡æ ‡"""

    name: str

    # é¢„æµ‹èƒ½åŠ›æŒ‡æ ‡
    ic_1d: float = 0.0
    ic_3d: float = 0.0
    ic_5d: float = 0.0
    ic_10d: float = 0.0
    ic_20d: float = 0.0
    ic_mean: float = 0.0
    ic_std: float = 0.0
    ic_ir: float = 0.0
    ic_decay_rate: float = 0.0
    ic_longevity: int = 0
    predictive_power_mean_ic: float = 0.0  # æ·»åŠ ç¼ºå¤±å­—æ®µ

    # ç¨³å®šæ€§æŒ‡æ ‡
    rolling_ic_mean: float = 0.0
    rolling_ic_std: float = 0.0
    rolling_ic_stability: float = 0.0
    ic_consistency: float = 0.0
    cross_section_stability: float = 0.0

    # ç‹¬ç«‹æ€§æŒ‡æ ‡
    vif_score: float = 0.0
    correlation_max: float = 0.0
    information_increment: float = 0.0
    redundancy_score: float = 0.0

    # å®ç”¨æ€§æŒ‡æ ‡
    turnover_rate: float = 0.0
    transaction_cost: float = 0.0
    cost_efficiency: float = 0.0
    liquidity_demand: float = 0.0
    capacity_score: float = 0.0

    # çŸ­å‘¨æœŸé€‚åº”æ€§æŒ‡æ ‡
    reversal_effect: float = 0.0
    momentum_persistence: float = 0.0
    volatility_sensitivity: float = 0.0
    regime_adaptability: float = 0.0

    # ç»Ÿè®¡æ˜¾è‘—æ€§
    p_value: float = 1.0
    corrected_p_value: float = 1.0
    bennett_score: float = 0.0
    is_significant: bool = False

    # ç»¼åˆè¯„åˆ†
    predictive_score: float = 0.0
    stability_score: float = 0.0
    independence_score: float = 0.0
    practicality_score: float = 0.0
    adaptability_score: float = 0.0
    comprehensive_score: float = 0.0
    traditional_score: float = 0.0  # ä¼ ç»Ÿè¯„åˆ†ï¼ˆå…¬å¹³è¯„åˆ†å‰ï¼‰

    # å…¬å¹³è¯„åˆ†ç›¸å…³ä¿¡æ¯
    fair_scoring_applied: bool = False
    fair_scoring_change: float = 0.0
    fair_scoring_percent_change: float = 0.0

    # èƒœç‡ä¿¡æ¯
    ic_win_rate: float = 0.0  # ICèƒœç‡ï¼ˆæ­£ICå æ¯”ï¼‰
    sample_weight: float = 1.0  # æ ·æœ¬é‡æƒé‡
    predictive_weight: float = 1.0  # é¢„æµ‹èƒ½åŠ›æƒé‡
    actual_sample_size: int = 0  # å®é™…æ ·æœ¬é‡

    # å…ƒæ•°æ®
    sample_size: int = 0
    calculation_time: float = 0.0
    data_quality_score: float = 0.0

    # å› å­åˆ†ç±»ä¿¡æ¯
    tier: str = ""
    type: str = ""
    description: str = ""


# ScreeningConfigç±»å·²ç§»è‡³config_manager.pyï¼Œé¿å…é‡å¤å®šä¹‰


class ProfessionalFactorScreener:
    """ä¸“ä¸šçº§å› å­ç­›é€‰å™¨ - 5ç»´åº¦ç­›é€‰æ¡†æ¶"""

    @staticmethod
    def _to_json_serializable(obj: Any) -> Any:
        """è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼ - æç®€å®ç°"""
        if isinstance(obj, dict):
            return {
                k: ProfessionalFactorScreener._to_json_serializable(v)
                for k, v in obj.items()
            }
        elif isinstance(obj, (list, tuple)):
            return [
                ProfessionalFactorScreener._to_json_serializable(item) for item in obj
            ]
        elif isinstance(obj, (np.integer, np.floating, np.bool_)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, "shape"):  # pandaså¯¹è±¡
            return str(obj)
        return obj

    def __init__(
        self, data_root: Optional[str] = None, config: Optional[ScreeningConfig] = None
    ):
        """åˆå§‹åŒ–ç­›é€‰å™¨

        Args:
            data_root: å‘åå…¼å®¹å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨configä¸­çš„è·¯å¾„é…ç½®
            config: ç­›é€‰é…ç½®å¯¹è±¡
        """
        self.config = config or ScreeningConfig()

        # ğŸ”§ ä¿®å¤è·¯å¾„ç¡¬ç¼–ç  - æ™ºèƒ½è·¯å¾„è§£æ
        if hasattr(self.config, "data_root") and self.config.data_root:
            self.data_root = Path(self.config.data_root)
        elif data_root:
            self.data_root = Path(data_root)
        else:
            # æ™ºèƒ½è·¯å¾„è§£æï¼šå°è¯•è‡ªåŠ¨å‘ç°é¡¹ç›®æ ¹ç›®å½•
            try:
                # ä»å½“å‰æ–‡ä»¶ä½ç½®æ¨å¯¼é¡¹ç›®æ ¹ç›®å½•
                current_file = Path(__file__)
                project_root = current_file.parent.parent.parent
                potential_factor_output = project_root / "factor_output"

                if potential_factor_output.exists():
                    self.data_root = potential_factor_output
                    logging.getLogger(__name__).info(
                        f"âœ… è‡ªåŠ¨å‘ç°å› å­è¾“å‡ºç›®å½•: {self.data_root}"
                    )
                else:
                    # å›é€€åˆ°ç›¸å¯¹è·¯å¾„
                    self.data_root = Path("../factor_output")
                    logging.getLogger(__name__).info(
                        f"ä½¿ç”¨é»˜è®¤å› å­è¾“å‡ºç›®å½•: {self.data_root}"
                    )
            except Exception:
                # æœ€ç»ˆå›é€€åˆ°ç›¸å¯¹è·¯å¾„
                self.data_root = Path("../factor_output")
                logging.getLogger(__name__).info(
                    f"ä½¿ç”¨é»˜è®¤å› å­è¾“å‡ºç›®å½•: {self.data_root}"
                )

        # è®¾ç½®æ—¥å¿—å’Œç¼“å­˜è·¯å¾„
        self.log_root = Path(getattr(self.config, "log_root", "./logs/screening"))
        self.cache_dir = Path(
            getattr(self.config, "cache_root", self.data_root / "cache")
        )

        # è®¾ç½®ç­›é€‰æŠ¥å‘Šä¸“ç”¨ç›®å½•
        if hasattr(self.config, "output_dir") and self.config.output_dir:
            self.screening_results_dir = Path(self.config.output_dir)
        else:
            self.screening_results_dir = Path("./å› å­ç­›é€‰")
        self.screening_results_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä¼šè¯ç›¸å…³å˜é‡ï¼ˆç¨ååœ¨screen_factors_comprehensiveä¸­åˆ›å»ºï¼‰
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = None

        # è®¾ç½®æ—¥å¿—å’Œç¼“å­˜è·¯å¾„ï¼ˆå…ˆä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        self.log_root = Path(getattr(self.config, "log_root", "./logs/screening"))
        self.cache_dir = Path(
            getattr(self.config, "cache_root", self.data_root / "cache")
        )

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.log_root.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger(self.session_timestamp)

        # åˆå§‹åŒ–æ—¶é—´åºåˆ—éªŒè¯å™¨ï¼Œç¡®ä¿æ—§ä»£ç åŒæ ·å—é˜²æŠ¤
        self.temporal_validator = None
        if TemporalValidator is not None:
            try:
                self.temporal_validator = TemporalValidator(strict_mode=True)
                self.logger.info("âœ… æ—¶é—´åºåˆ—éªŒè¯å™¨å·²å¯ç”¨")
            except Exception as validator_error:
                self.logger.warning("æ—¶é—´åºåˆ—éªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: %s", validator_error)
                self.temporal_validator = None
        else:
            self.logger.warning("æ—¶é—´åºåˆ—éªŒè¯å™¨æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡è¿è¡Œæ—¶æ ¡éªŒ")

        # åˆå§‹åŒ–å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨
        try:
            from enhanced_result_manager import EnhancedResultManager  # type: ignore

            self.result_manager = EnhancedResultManager(str(self.screening_results_dir))
            self.logger.info("âœ… å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            self.result_manager = None
            self.logger.warning(f"å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.logger.info("å°†ä½¿ç”¨ä¼ ç»Ÿæ–‡ä»¶ä¿å­˜æ–¹å¼")

        # ğŸš€ åˆå§‹åŒ–å…¬å¹³è¯„åˆ†å™¨
        try:
            from fair_scorer import FairScorer  # type: ignore

            # ğŸ¯ æœ€ä¼˜è§£é…ç½®ï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å…¬å¹³è¯„åˆ†é…ç½®
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç›¸å¯¹æ¨¡å—æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            if getattr(config, "use_optimal_fair_scoring", False):
                default_config = (
                    Path(__file__).parent
                    / "configs"
                    / "optimal_fair_scoring_config.yaml"
                )
                fair_config_path = getattr(
                    config, "optimal_scoring_config_path", str(default_config)
                )
                self.logger.info("ğŸ¯ ä½¿ç”¨æœ€ä¼˜è§£å…¬å¹³è¯„åˆ†é…ç½®: é¢„æµ‹èƒ½åŠ›æ ¸å¿ƒåŒ–ç®—æ³•")
            else:
                default_config = (
                    Path(__file__).parent / "configs" / "fair_scoring_config.yaml"
                )
                fair_config_path = getattr(
                    config, "fair_scoring_config_path", str(default_config)
                )
                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿå…¬å¹³è¯„åˆ†é…ç½®")

            self.fair_scorer = FairScorer(fair_config_path)

            if self.fair_scorer.enabled:
                self.logger.info("âœ… å…¬å¹³è¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                self.logger.info("âšª å…¬å¹³è¯„åˆ†å™¨å·²ç¦ç”¨")

        except ImportError as e:
            self.fair_scorer = None
            self.logger.warning(f"å…¬å¹³è¯„åˆ†å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.logger.info("å°†ä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†æ–¹å¼")
        except Exception as e:
            self.fair_scorer = None
            self.logger.error(f"å…¬å¹³è¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("å°†ä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†æ–¹å¼")

        # æ€§èƒ½ç›‘æ§
        self.process = psutil.Process()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        # P0çº§é›†æˆï¼šåˆå§‹åŒ–æ–°å¢çš„å·¥å…·æ¨¡å—
        self._initialize_utility_modules()

        # P0 æ€§èƒ½ä¼˜åŒ–ï¼šåˆå§‹åŒ–å‘é‡åŒ–å¼•æ“
        if VECTORIZED_ENGINE_AVAILABLE:
            self.vectorized_analyzer = get_vectorized_analyzer(
                min_sample_size=self.config.min_sample_size
            )
            self.logger.info("âœ… VectorBT å‘é‡åŒ–å¼•æ“å·²å¯ç”¨")
        else:
            self.vectorized_analyzer = None
            self.logger.warning("âš ï¸ å‘é‡åŒ–å¼•æ“ä¸å¯ç”¨ï¼Œæ€§èƒ½å°†å—å½±å“")

        # P3 æ€§èƒ½ç›‘æ§ï¼šåˆå§‹åŒ–ç›‘æ§å™¨
        if PERFORMANCE_MONITOR_AVAILABLE:
            self.perf_monitor = get_performance_monitor(enable_logging=True)
            self.logger.info("âœ… æ€§èƒ½ç›‘æ§å·²å¯ç”¨")
        else:
            self.perf_monitor = None
            self.logger.warning("âš ï¸ æ€§èƒ½ç›‘æ§ä¸å¯ç”¨")

        self.logger.info("ä¸“ä¸šçº§å› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(
            f"é…ç½®: ICå‘¨æœŸ={self.config.ic_horizons}, æœ€å°æ ·æœ¬={self.config.min_sample_size}"
        )
        self.logger.info(
            f"æ˜¾è‘—æ€§æ°´å¹³={self.config.alpha_level}, FDRæ–¹æ³•={self.config.fdr_method}"
        )

    def _initialize_utility_modules(self) -> None:
        """P0çº§é›†æˆï¼šåˆå§‹åŒ–å·¥å…·æ¨¡å—ï¼ˆå®é™…é›†æˆï¼‰"""

        # 1. å†…å­˜ä¼˜åŒ–å™¨ - å·²ç§»é™¤ï¼ˆLinusåŸåˆ™ï¼šå½“å‰ç³»ç»Ÿå·¥ä½œæ­£å¸¸ï¼Œæ— éœ€å¤æ‚åŒ–ï¼‰
        self.memory_optimizer = None

        # 2. åˆå§‹åŒ–è¾“å…¥éªŒè¯å™¨
        if InputValidator is not None:
            self.input_validator = InputValidator()
            self.logger.info("âœ… è¾“å…¥éªŒè¯å™¨å·²å¯ç”¨")
        else:
            self.input_validator = None
            self.logger.warning("è¾“å…¥éªŒè¯å™¨æ¨¡å—æœªå®‰è£…")

        # 3. åˆå§‹åŒ–ç»“æ„åŒ–æ—¥å¿—å™¨ï¼ˆå¢å¼ºæ¨¡å¼ï¼‰
        if get_structured_logger is not None:
            try:
                self.structured_logger = get_structured_logger(
                    name="factor_screening",
                    log_file=self.log_root / f"structured_{self.session_timestamp}.log",
                )
                self.logger.info("âœ… ç»“æ„åŒ–æ—¥å¿—å™¨å·²å¯ç”¨")
            except Exception as e:
                self.structured_logger = None
                self.logger.warning(f"ç»“æ„åŒ–æ—¥å¿—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            self.structured_logger = None
            self.logger.warning("ç»“æ„åŒ–æ—¥å¿—å™¨æ¨¡å—æœªå®‰è£…")

        # 4. å¤‡ä»½ç®¡ç†å™¨ - å·²ç§»é™¤ï¼ˆLinusåŸåˆ™ï¼šæ–‡ä»¶ç³»ç»Ÿå·²è¶³å¤Ÿï¼Œæ— éœ€è¿‡åº¦å·¥ç¨‹åŒ–ï¼‰
        self.backup_manager = None

    def _setup_logger(self, session_timestamp: Optional[str] = None) -> logging.Logger:
        """è®¾ç½®ä¸“ä¸šçº§æ—¥å¿—ç³»ç»Ÿ - æ”¹è¿›ç‰ˆ"""
        logger = logging.getLogger(f"{__name__}.{id(self)}")
        logger.setLevel(logging.INFO)

        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # ä½¿ç”¨æ—¥å¿—è½®è½¬ - å…³é”®ä¿®å¤
        from logging.handlers import RotatingFileHandler

        # æ”¯æŒä¼šè¯æ—¶é—´æˆ³æˆ–æ—¥æœŸå‘½å
        if session_timestamp:
            log_filename = f"professional_screener_{session_timestamp}.log"
        else:
            today = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_filename = f"professional_screener_{today}.log"

        log_file = self.log_root / log_filename

        # ä¿å­˜å½“å‰æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿å…¶ä»–åœ°æ–¹è®¿é—®
        self.current_log_file = str(log_file)

        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,  # ä¿ç•™5ä¸ªå¤‡ä»½
            encoding="utf-8",
        )
        file_handler.setLevel(logging.DEBUG)

        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - "
            "[%(funcName)s:%(lineno)d] - %(message)s"
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        return logger

    def _generate_factor_metadata(self, factors_df: pd.DataFrame) -> dict:
        """ç”Ÿæˆå› å­å…ƒæ•°æ®"""
        metadata = {}

        for factor_name in factors_df.columns:
            meta = {
                "name": factor_name,
                "type": self._infer_factor_type(factor_name),
                "warmup_period": self._infer_warmup_period(factor_name),
                "description": self._generate_factor_description(factor_name),
            }

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            factor_data = factors_df[factor_name]
            meta.update(
                {
                    "total_periods": len(factor_data),
                    "missing_periods": factor_data.isna().sum(),
                    "missing_ratio": factor_data.isna().sum() / len(factor_data),
                    "first_valid_index": self._find_first_non_missing_index(
                        factor_data
                    ),
                    "valid_ratio": 1 - (factor_data.isna().sum() / len(factor_data)),
                }
            )

            metadata[factor_name] = meta

        return metadata

    def _infer_factor_type(self, factor_name: str) -> str:
        """æ ¹æ®å› å­åç§°æ¨æ–­ç±»å‹"""
        name_lower = factor_name.lower()

        if any(
            indicator in name_lower
            for indicator in [
                "ma",
                "ema",
                "sma",
                "wma",
                "dema",
                "tema",
                "trima",
                "kama",
                "t3",
            ]
        ):
            return "trend"
        elif any(
            indicator in name_lower
            for indicator in ["rsi", "stoch", "cci", "willr", "mfi", "roc", "mom"]
        ):
            return "momentum"
        elif any(
            indicator in name_lower
            for indicator in ["bb", "bollinger", "atr", "std", "mstd"]
        ):
            return "volatility"
        elif any(indicator in name_lower for indicator in ["volume", "obv", "vwap"]):
            return "volume"
        elif any(
            indicator in name_lower
            for indicator in ["macd", "signal", "histogram", "hist"]
        ):
            return "momentum"
        elif any(indicator in name_lower for indicator in ["cdl", "pattern"]):
            return "pattern"
        else:
            return "unknown"

    def _infer_warmup_period(self, factor_name: str) -> int:
        """æ ¹æ®å› å­åç§°æ¨æ–­é¢„çƒ­æœŸ"""
        name_lower = factor_name.lower()

        # ç§»åŠ¨å¹³å‡ç±» - ç²¾ç¡®åŒ¹é…æ•°å­—
        import re

        ma_match = re.search(r"ma(\d+)", name_lower)
        if ma_match:
            return int(ma_match.group(1))

        sma_match = re.search(r"sma_?(\d+)", name_lower)
        if sma_match:
            return int(sma_match.group(1))

        ema_match = re.search(r"ema_?(\d+)", name_lower)
        if ema_match:
            return int(ema_match.group(1))

        # RSIç±»
        rsi_match = re.search(r"rsi(\d+)", name_lower)
        if rsi_match:
            return int(rsi_match.group(1))
        elif "rsi" in name_lower:
            return 14

        # å¸ƒæ—å¸¦ç±»
        bb_match = re.search(r"bb_(\d+)", name_lower)
        if bb_match:
            return int(bb_match.group(1))
        elif "bb" in name_lower or "bollinger" in name_lower:
            return 20

        # MACDç±»
        if "macd" in name_lower:
            return 26

        # CCIç±»
        cci_match = re.search(r"cci(\d+)", name_lower)
        if cci_match:
            return int(cci_match.group(1))
        elif "cci" in name_lower:
            return 20

        # WILLRç±»
        willr_match = re.search(r"willr(\d+)", name_lower)
        if willr_match:
            return int(willr_match.group(1))
        elif "willr" in name_lower:
            return 14

        # ATRç±»
        atr_match = re.search(r"atr(\d+)", name_lower)
        if atr_match:
            return int(atr_match.group(1))
        elif "atr" in name_lower:
            return 14

        # é»˜è®¤é¢„çƒ­æœŸ
        return 20

    def _generate_factor_description(self, factor_name: str) -> str:
        """ç”Ÿæˆå› å­æè¿°"""
        name_lower = factor_name.lower()

        if "ma" in name_lower:
            return f"ç§»åŠ¨å¹³å‡çº¿æŒ‡æ ‡ - {factor_name}"
        elif "rsi" in name_lower:
            return f"ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ - {factor_name}"
        elif "macd" in name_lower:
            return f"ç§»åŠ¨å¹³å‡æ”¶æ•›æ•£åº¦æŒ‡æ ‡ - {factor_name}"
        elif "bb" in name_lower:
            return f"å¸ƒæ—å¸¦æŒ‡æ ‡ - {factor_name}"
        elif "volume" in name_lower:
            return f"æˆäº¤é‡æŒ‡æ ‡ - {factor_name}"
        else:
            return f"æŠ€æœ¯æŒ‡æ ‡ - {factor_name}"

    def _find_first_non_missing_index(self, series: pd.Series) -> int:
        """æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç¼ºå¤±å€¼çš„ç´¢å¼•ä½ç½®"""
        non_null_mask = series.notna()
        if non_null_mask.any():
            return non_null_mask.idxmax()
        return len(series)

    def _smart_forward_fill(self, series: pd.Series) -> pd.Series:
        """æ™ºèƒ½å‰å‘å¡«å……"""
        result = series.copy()

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
        first_valid_idx = series.first_valid_index()

        if first_valid_idx is not None:
            # ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……å‰é¢çš„ç¼ºå¤±å€¼
            first_valid_value = series.loc[first_valid_idx]
            result = result.bfill(limit=1)
            result = result.fillna(first_valid_value)

            # å‰å‘å¡«å……å‰©ä½™çš„ç¼ºå¤±å€¼
            result = result.ffill()

        return result

    def _smart_interpolation(self, series: pd.Series) -> pd.Series:
        """æ™ºèƒ½æ’å€¼"""
        result = series.copy()

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
        first_valid_idx = series.first_valid_index()

        if first_valid_idx is not None:
            # ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……å‰é¢çš„ç¼ºå¤±å€¼
            first_valid_value = series.loc[first_valid_idx]
            result.loc[:first_valid_idx] = result.loc[:first_valid_idx].fillna(
                first_valid_value
            )

            # å¯¹å‰©ä½™ç¼ºå¤±å€¼è¿›è¡Œçº¿æ€§æ’å€¼
            result = result.interpolate(method="linear")

            # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨å‰å‘å¡«å……
            result = result.ffill()
            result = result.bfill()

        return result

    def smart_missing_value_handling(
        self, factors_df: pd.DataFrame, factor_metadata: dict = None
    ) -> tuple:
        """
        æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†ï¼ŒåŒºåˆ†æ­£å¸¸é¢„çƒ­æœŸç¼ºå¤±å’Œé—®é¢˜æ•°æ®

        Args:
            factors_df: å› å­æ•°æ®DataFrame
            factor_metadata: å› å­å…ƒæ•°æ®ï¼ŒåŒ…å«é¢„çƒ­æœŸä¿¡æ¯

        Returns:
            tuple: (cleaned_df, handling_report)
        """
        if factor_metadata is None:
            factor_metadata = self._generate_factor_metadata(factors_df)

        handling_report = {
            "total_factors": len(factors_df.columns),
            "removed_factors": [],
            "handled_factors": [],
            "forward_filled_factors": [],
            "interpolated_factors": [],
            "dropped_factors": [],
        }

        # ç›´æ¥åœ¨åŸDataFrameä¸Šæ“ä½œï¼Œé¿å…ä¸å¿…è¦çš„å†…å­˜å¤åˆ¶
        cleaned_df = factors_df

        for factor_name in factors_df.columns:
            factor_data = factors_df[factor_name]
            missing_count = factor_data.isna().sum()

            if missing_count == 0:
                handling_report["handled_factors"].append(factor_name)
                continue

            missing_ratio = missing_count / len(factor_data)

            # è·å–å› å­å…ƒæ•°æ®
            meta = factor_metadata.get(factor_name, {})
            warmup_period = meta.get("warmup_period", 20)
            factor_type = meta.get("type", "unknown")

            # åˆ¤æ–­ç¼ºå¤±å€¼æ¨¡å¼
            first_valid_idx = factor_data.first_valid_index()
            if first_valid_idx is not None:
                first_valid_pos = factor_data.index.get_loc(first_valid_idx)
            else:
                first_valid_pos = len(factor_data)

            # å†³ç­–é€»è¾‘
            if first_valid_pos <= warmup_period * 1.5:  # å…è®¸1.5å€çš„é¢„çƒ­æœŸå®¹å¿åº¦
                # æ­£å¸¸é¢„çƒ­æœŸç¼ºå¤±ï¼Œè¿›è¡Œæ™ºèƒ½å¡«å……
                if factor_type in ["momentum", "trend", "volatility"]:
                    # æŠ€æœ¯æŒ‡æ ‡ç±»å› å­ä½¿ç”¨å‰å‘å¡«å……
                    cleaned_df[factor_name] = self._smart_forward_fill(factor_data)
                    handling_report["forward_filled_factors"].append(factor_name)
                else:
                    # å…¶ä»–ç±»å‹å› å­ä½¿ç”¨æ’å€¼
                    cleaned_df[factor_name] = self._smart_interpolation(factor_data)
                    handling_report["interpolated_factors"].append(factor_name)

                handling_report["handled_factors"].append(factor_name)

            elif missing_ratio > self.config.max_missing_ratio:
                # ç¼ºå¤±æ¯”ä¾‹è¿‡é«˜ï¼Œåˆ é™¤
                cleaned_df = cleaned_df.drop(columns=[factor_name])
                handling_report["dropped_factors"].append(factor_name)
                handling_report["removed_factors"].append(factor_name)

            else:
                # éšæœºç¼ºå¤±ï¼Œä½¿ç”¨æ’å€¼
                cleaned_df[factor_name] = self._smart_interpolation(factor_data)
                handling_report["interpolated_factors"].append(factor_name)
                handling_report["handled_factors"].append(factor_name)

        return cleaned_df, handling_report

    def validate_factor_data_quality(
        self, factors_df: pd.DataFrame, symbol: str, timeframe: str
    ) -> pd.DataFrame:
        """æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯ - è§£å†³æ ¹æœ¬é—®é¢˜è€Œä¸æ˜¯ç²—æš´åˆ é™¤"""
        self.logger.info(f"å¼€å§‹æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯: {symbol} {timeframe}")

        original_shape = factors_df.shape
        issues_found = []

        # 1. æ£€æŸ¥éæ•°å€¼åˆ—ï¼ˆæ ¹æœ¬é—®é¢˜ï¼‰
        non_numeric_cols = factors_df.select_dtypes(
            exclude=[np.number, "datetime64[ns]"]
        ).columns.tolist()
        if non_numeric_cols:
            self.logger.warning(f"å‘ç°éæ•°å€¼åˆ—: {non_numeric_cols}")
            factors_df = factors_df.drop(columns=non_numeric_cols)
            issues_found.append(f"ç§»é™¤éæ•°å€¼åˆ—: {non_numeric_cols}")

        # 2. æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç† - æ ¸å¿ƒæ”¹è¿›
        if factors_df.isna().any().any():
            self.logger.info("å¼€å§‹æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†...")
            factor_metadata = self._generate_factor_metadata(factors_df)
            factors_df, handling_report = self.smart_missing_value_handling(
                factors_df, factor_metadata
            )

            self.logger.info("æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†å®Œæˆ:")
            self.logger.info(f"  - æ€»å› å­æ•°: {handling_report['total_factors']}")
            self.logger.info(
                f"  - å‰å‘å¡«å……å› å­æ•°: {len(handling_report['forward_filled_factors'])}"
            )
            self.logger.info(
                f"  - æ’å€¼å¡«å……å› å­æ•°: {len(handling_report['interpolated_factors'])}"
            )
            self.logger.info(
                f"  - åˆ é™¤å› å­æ•°: {len(handling_report['removed_factors'])}"
            )

            if handling_report["removed_factors"]:
                issues_found.append(
                    f"æ™ºèƒ½å¤„ç†ç§»é™¤å› å­: {handling_report['removed_factors']}"
                )

        # 3. æ£€æŸ¥æ— ç©·å€¼ - ä¿®å¤è€Œä¸æ˜¯åˆ é™¤
        inf_cols = []
        for col in factors_df.select_dtypes(include=[np.number]).columns:
            if np.isinf(factors_df[col]).any():
                inf_cols.append(col)
                # ç”¨æå€¼æ›¿æ¢æ— ç©·å€¼
                factors_df[col] = factors_df[col].replace(
                    [np.inf, -np.inf],
                    [factors_df[col].quantile(0.99), factors_df[col].quantile(0.01)],
                )

        if inf_cols:
            self.logger.info(f"ä¿®å¤æ— ç©·å€¼åˆ—: {inf_cols}")
            issues_found.append(f"ä¿®å¤æ— ç©·å€¼åˆ—: {inf_cols}")

        # P1-2ä¿®å¤ï¼šæ”¹è¿›å¸¸é‡åˆ—æ£€æµ‹é€»è¾‘ï¼Œä¿æŠ¤Kçº¿å½¢æ€æŒ‡æ ‡
        constant_cols = []
        for col in factors_df.select_dtypes(include=[np.number]).columns:
            col_std = factors_df[col].std()
            col_nunique = factors_df[col].nunique()

            # Kçº¿å½¢æ€æŒ‡æ ‡ç‰¹æ®Šå¤„ç†ï¼šå…è®¸äºŒå€¼å¸¸é‡ï¼ˆ0/1æ¨¡å¼ï¼‰
            if col.startswith("TA_CDL"):
                # Kçº¿å½¢æ€æŒ‡æ ‡ï¼šåªæœ‰å½“å®Œå…¨æ— å˜åŒ–ä¸”åªæœ‰ä¸€ä¸ªå€¼æ—¶æ‰åˆ é™¤
                if col_nunique <= 1:
                    constant_cols.append(col)
            else:
                # å…¶ä»–æŒ‡æ ‡ï¼šä½¿ç”¨æ ‡å‡†å·®æ£€æµ‹
                if col_std < 1e-6:
                    constant_cols.append(col)

        if constant_cols:
            # åŒºåˆ†Kçº¿å½¢æ€æŒ‡æ ‡å’Œå…¶ä»–å¸¸é‡åˆ—
            cdl_cols = [col for col in constant_cols if col.startswith("TA_CDL")]
            other_cols = [col for col in constant_cols if not col.startswith("TA_CDL")]

            if cdl_cols:
                self.logger.info(f"ç§»é™¤æ— å˜åŒ–Kçº¿å½¢æ€æŒ‡æ ‡: {cdl_cols}")
            if other_cols:
                self.logger.warning(f"ç§»é™¤å¸¸é‡åˆ—: {other_cols}")

            factors_df = factors_df.drop(columns=constant_cols)
            issues_found.append(f"ç§»é™¤å¸¸é‡åˆ—: {constant_cols}")

        # 5. æ£€æŸ¥é‡å¤åˆ—
        duplicate_cols = factors_df.columns[factors_df.columns.duplicated()].tolist()
        if duplicate_cols:
            self.logger.warning(f"å‘ç°é‡å¤åˆ—: {duplicate_cols}")
            factors_df = factors_df.loc[:, ~factors_df.columns.duplicated()]
            issues_found.append(f"ç§»é™¤é‡å¤åˆ—: {duplicate_cols}")

        final_shape = factors_df.shape

        # æŠ¥å‘ŠéªŒè¯ç»“æœ
        retention_rate = final_shape[1] / original_shape[1]
        self.logger.info("æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯å®Œæˆ:")
        self.logger.info(f"  - åŸå§‹å½¢çŠ¶: {original_shape}")
        self.logger.info(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        self.logger.info(f"  - å› å­ä¿ç•™ç‡: {retention_rate:.1%}")

        if issues_found:
            for issue in issues_found:
                self.logger.info(f"  - {issue}")

        # ç¡®ä¿è¿˜æœ‰è¶³å¤Ÿçš„å› å­æ•°æ®
        if len(factors_df.columns) < 10:
            raise ValueError(f"éªŒè¯åå› å­æ•°é‡è¿‡å°‘: {len(factors_df.columns)} < 10")

        return factors_df

    def load_factors(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """åŠ è½½å› å­æ•°æ® - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå› å­æ–‡ä»¶å¯¹é½"""
        start_time = time.time()
        self.logger.info(f"åŠ è½½å› å­æ•°æ®: {symbol} {timeframe}")

        # ğŸ¯ ä¼˜å…ˆä½¿ç”¨å¯¹é½çš„å› å­æ–‡ä»¶
        if hasattr(self, "aligned_factor_files") and self.aligned_factor_files:
            if timeframe in self.aligned_factor_files:
                selected_file = self.aligned_factor_files[timeframe]
                self.logger.info(f"âœ… ä½¿ç”¨å¯¹é½çš„å› å­æ–‡ä»¶: {selected_file.name}")

                try:
                    # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨columnså‚æ•°é€‰æ‹©æ€§åŠ è½½
                    # ä»…è¯»å–æ•°å€¼åˆ—ï¼Œå‡å°‘å†…å­˜å ç”¨
                    factors = pd.read_parquet(
                        selected_file,
                        # æš‚ä¸æŒ‡å®šcolumnsï¼Œå› ä¸ºéœ€è¦å…ˆè¯»å–æ‰çŸ¥é“åˆ—å
                        # åç»­é€šè¿‡dropnaå’Œæ•°æ®ç±»å‹ç­›é€‰ä¼˜åŒ–
                    )

                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    if factors.empty:
                        self.logger.warning(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")
                        raise ValueError(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")

                    # å†…å­˜ä¼˜åŒ–ï¼šç«‹å³ç§»é™¤å…¨éƒ¨ä¸ºNaNçš„åˆ—
                    factors = factors.dropna(axis=1, how="all")

                    # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜å ç”¨
                    for col in factors.select_dtypes(include=["float64"]).columns:
                        factors[col] = factors[col].astype("float32")

                    # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                    if not isinstance(factors.index, pd.DatetimeIndex):
                        factors.index = pd.to_datetime(factors.index)

                    # Linuså¼æ•°æ®è´¨é‡éªŒè¯
                    factors = self.validate_factor_data_quality(
                        factors, symbol, timeframe
                    )

                    initial_memory = factors.memory_usage(deep=True).sum() / 1024 / 1024
                    self.logger.info(
                        f"å› å­æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={factors.shape}, å†…å­˜={initial_memory:.1f}MB"
                    )
                    self.logger.info(
                        f"æ—¶é—´èŒƒå›´: {factors.index.min()} åˆ° {factors.index.max()}"
                    )
                    self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                    return factors

                except Exception as e:
                    self.logger.warning(f"åŠ è½½å¯¹é½æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æœç´¢: {str(e)}")

        # å¤„ç†symbolæ ¼å¼
        clean_symbol = symbol.replace(".HK", "")

        # æœç´¢ç­–ç•¥ï¼šæŒ‰ä¼˜å…ˆçº§æœç´¢ä¸åŒæ ¼å¼çš„æ–‡ä»¶
        search_patterns = [
            # æ–°æ ¼å¼ï¼štimeframeå­ç›®å½• (å¸¦.HKåç¼€)
            (
                self.data_root / timeframe,
                f"{clean_symbol}.HK_{timeframe}_factors_*.parquet",
            ),
            (
                self.data_root / timeframe,
                f"{clean_symbol}HK_{timeframe}_factors_*.parquet",
            ),
            (
                self.data_root / timeframe,
                f"{clean_symbol}_{timeframe}_factors_*.parquet",
            ),
            # multi_tfæ ¼å¼
            (self.data_root, f"aligned_multi_tf_factors_{clean_symbol}*.parquet"),
            # æ ¹ç›®å½•æ ¼å¼
            (self.data_root, f"{clean_symbol}*_{timeframe}_factors_*.parquet"),
        ]

        for search_dir, pattern in search_patterns:
            if search_dir.exists():
                factor_files = list(search_dir.glob(pattern))
                if factor_files:
                    selected_file = factor_files[-1]  # é€‰æ‹©æœ€æ–°æ–‡ä»¶
                    self.logger.info(f"æ‰¾åˆ°å› å­æ–‡ä»¶: {selected_file}")

                    try:
                        # å†…å­˜ä¼˜åŒ–ï¼šé€‰æ‹©æ€§åŠ è½½
                        factors = pd.read_parquet(selected_file)

                        # å†…å­˜ä¼˜åŒ–ï¼šç«‹å³ç§»é™¤å…¨éƒ¨ä¸ºNaNçš„åˆ—å’Œè½¬æ¢æ•°æ®ç±»å‹
                        factors = factors.dropna(axis=1, how="all")
                        for col in factors.select_dtypes(include=["float64"]).columns:
                            factors[col] = factors[col].astype("float32")

                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        if factors.empty:
                            self.logger.warning(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")
                            continue

                        # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                        if not isinstance(factors.index, pd.DatetimeIndex):
                            factors.index = pd.to_datetime(factors.index)

                        # Linuså¼æ•°æ®è´¨é‡éªŒè¯ - è§£å†³æ ¹æœ¬é—®é¢˜
                        factors = self.validate_factor_data_quality(
                            factors, symbol, timeframe
                        )

                        self.logger.info(f"å› å­æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={factors.shape}")
                        self.logger.info(
                            f"æ—¶é—´èŒƒå›´: {factors.index.min()} åˆ° {factors.index.max()}"
                        )
                        self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                        return factors

                    except Exception as e:
                        self.logger.error(f"åŠ è½½å› å­æ–‡ä»¶å¤±è´¥ {selected_file}: {str(e)}")
                        continue

        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        self.logger.error("æœªæ‰¾åˆ°å› å­æ•°æ®:")
        self.logger.error(f"æœç´¢è·¯å¾„: {self.data_root}")
        self.logger.error(f"æœç´¢ç¬¦å·: {clean_symbol}")
        self.logger.error(f"æ—¶é—´æ¡†æ¶: {timeframe}")

        available_dirs = [d.name for d in self.data_root.iterdir() if d.is_dir()]
        self.logger.error(f"å¯ç”¨ç›®å½•: {available_dirs}")

        raise FileNotFoundError(f"No factor data found for {symbol} {timeframe}")

    def load_price_data(self, symbol: str, timeframe: str = None) -> pd.DataFrame:
        """åŠ è½½ä»·æ ¼æ•°æ® - æ™ºèƒ½åŒ¹é…æ—¶é—´æ¡†æ¶ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        start_time = time.time()
        self.logger.info(f"åŠ è½½ä»·æ ¼æ•°æ®: {symbol} (æ—¶é—´æ¡†æ¶: {timeframe})")

        # å¤„ç†symbolæ ¼å¼
        if symbol.endswith(".HK"):
            clean_symbol = symbol.replace(".HK", "") + "HK"
        else:
            clean_symbol = symbol

        # åŸå§‹æ•°æ®è·¯å¾„ - ä½¿ç”¨é…ç½®æˆ–ç›¸å¯¹è·¯å¾„
        if hasattr(self.config, "raw_data_root") and self.config.raw_data_root:
            raw_data_path = Path(self.config.raw_data_root) / "HK"
        else:
            raw_data_path = self.data_root.parent / "raw" / "HK"

        if not raw_data_path.exists():
            # å›é€€åˆ°é¡¹ç›®æ ¹ç›®å½•çš„raw/HK
            raw_data_path = Path(__file__).parent.parent.parent / "raw" / "HK"

        # æ—¶é—´æ¡†æ¶åˆ°æ–‡ä»¶åçš„æ˜ å°„
        timeframe_map = {
            "1min": "1min",
            "2min": "2min",
            "3min": "3min",
            "5min": "5min",
            "15min": "15m",
            "30min": "30m",
            "60min": "60m",
            "daily": "1day",
            "1d": "1day",
        }

        # æ ¹æ®æ—¶é—´æ¡†æ¶æ™ºèƒ½é€‰æ‹©æœç´¢æ¨¡å¼
        if timeframe and timeframe in timeframe_map:
            file_pattern = timeframe_map[timeframe]
            self.logger.info(
                f"æ ¹æ®æ—¶é—´æ¡†æ¶ '{timeframe}' æœç´¢ '{file_pattern}' æ ¼å¼æ–‡ä»¶"
            )
            search_patterns = [
                f"{clean_symbol}_{file_pattern}_*.parquet",  # ç²¾ç¡®åŒ¹é…
                f"{clean_symbol}_*.parquet",  # å¤‡ç”¨
            ]
        else:
            # é»˜è®¤æœç´¢é¡ºåºï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            self.logger.warning("æœªæŒ‡å®šæ—¶é—´æ¡†æ¶æˆ–ä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤æœç´¢")
            search_patterns = [
                f"{clean_symbol}_60m_*.parquet",  # 60åˆ†é’Ÿæ•°æ®
                f"{clean_symbol}_1day_*.parquet",  # æ—¥çº¿æ•°æ®
                f"{clean_symbol}_*.parquet",  # ä»»æ„æ—¶é—´æ¡†æ¶
            ]

        for pattern in search_patterns:
            price_files = list(raw_data_path.glob(pattern))
            if price_files:
                selected_file = price_files[-1]  # é€‰æ‹©æœ€æ–°æ–‡ä»¶
                self.logger.info(f"æ‰¾åˆ°ä»·æ ¼æ–‡ä»¶: {selected_file}")

                try:
                    # å†…å­˜ä¼˜åŒ–ï¼šä»…è¯»å–OHLCVå’Œtimestampåˆ—
                    price_data = pd.read_parquet(selected_file)

                    # å…ˆå¤„ç†timestampåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if "timestamp" in price_data.columns:
                        price_data["timestamp"] = pd.to_datetime(
                            price_data["timestamp"]
                        )
                        price_data = price_data.set_index("timestamp")
                    elif not isinstance(price_data.index, pd.DatetimeIndex):
                        price_data.index = pd.to_datetime(price_data.index)

                    # ç„¶åé€‰æ‹©OHLCVåˆ—
                    ohlcv_cols = ["open", "high", "low", "close", "volume"]
                    available_cols = [
                        col for col in ohlcv_cols if col in price_data.columns
                    ]
                    if available_cols:
                        price_data = price_data[available_cols]

                    # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢æ•°æ®ç±»å‹
                    for col in price_data.select_dtypes(include=["float64"]).columns:
                        price_data[col] = price_data[col].astype("float32")

                    # ç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—
                    required_cols = ["open", "high", "low", "close", "volume"]
                    missing_cols = [
                        col for col in required_cols if col not in price_data.columns
                    ]
                    if missing_cols:
                        self.logger.error(f"ä»·æ ¼æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                        continue

                    self.logger.info(f"ä»·æ ¼æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={price_data.shape}")
                    self.logger.info(
                        f"æ—¶é—´èŒƒå›´: {price_data.index.min()} åˆ° {price_data.index.max()}"
                    )
                    self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                    return price_data[required_cols]

                except Exception as e:
                    self.logger.error(f"åŠ è½½ä»·æ ¼æ–‡ä»¶å¤±è´¥ {selected_file}: {str(e)}")
                    continue

        self.logger.error("æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®:")
        self.logger.error(f"æœç´¢è·¯å¾„: {raw_data_path}")
        self.logger.error(f"æœç´¢ç¬¦å·: {clean_symbol}")

        available_files = [f.name for f in raw_data_path.glob("*.parquet")][:10]
        self.logger.error(f"å¯ç”¨æ–‡ä»¶ç¤ºä¾‹: {available_files}")

        raise FileNotFoundError(f"No price data found for {symbol}")

    # ==================== 1. é¢„æµ‹èƒ½åŠ›åˆ†æ ====================

    def calculate_multi_horizon_ic(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å¤šå‘¨æœŸICå€¼ - P0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å¼•æ“"""

        # æ€§èƒ½ç›‘æ§ï¼šICè®¡ç®—
        if self.perf_monitor is not None:
            with self.perf_monitor.time_operation("calculate_multi_horizon_ic"):
                # P0 ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
                if self.vectorized_analyzer is not None:
                    self.logger.info("ğŸš€ ä½¿ç”¨ VectorBT å‘é‡åŒ–å¼•æ“è®¡ç®— IC")
                    return self.vectorized_analyzer.calculate_multi_horizon_ic_batch(
                        factors=factors,
                        returns=returns,
                        horizons=self.config.ic_horizons,
                    )

                # é™çº§æ–¹æ¡ˆï¼šä¼ ç»Ÿé€å› å­è®¡ç®—
                self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿ IC è®¡ç®—ï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")
                return self._calculate_multi_horizon_ic_legacy(factors, returns)
        else:
            # æ²¡æœ‰æ€§èƒ½ç›‘æ§æ—¶çš„æ­£å¸¸é€»è¾‘
            # P0 ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
            if self.vectorized_analyzer is not None:
                self.logger.info("ğŸš€ ä½¿ç”¨ VectorBT å‘é‡åŒ–å¼•æ“è®¡ç®— IC")
                return self.vectorized_analyzer.calculate_multi_horizon_ic_batch(
                    factors=factors,
                    returns=returns,
                    horizons=self.config.ic_horizons,
                )

            # é™çº§æ–¹æ¡ˆï¼šä¼ ç»Ÿé€å› å­è®¡ç®—
            self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿ IC è®¡ç®—ï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰")
            return self._calculate_multi_horizon_ic_legacy(factors, returns)

    def _calculate_multi_horizon_ic_legacy(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """ä¼ ç»Ÿå¤šå‘¨æœŸICè®¡ç®— - é™çº§æ–¹æ¡ˆ"""
        self.logger.info("å¼€å§‹å¤šå‘¨æœŸICè®¡ç®—ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰...")
        start_time = time.time()

        ic_results: Dict[str, Dict[str, float]] = {}
        horizons = self.config.ic_horizons

        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        total_factors = len(factor_cols)

        # å‘é‡åŒ–ä¼˜åŒ–: é¢„å…ˆå¯¹é½æ‰€æœ‰æ•°æ®ï¼Œé¿å…é‡å¤å¯¹é½
        returns_series = returns.reindex(factors.index)
        valid_idx = returns_series.notna()
        aligned_factors = factors[factor_cols].loc[valid_idx]
        aligned_returns = returns_series.loc[valid_idx]

        processed = 0
        for factor in factor_cols:
            processed += 1
            if processed % self.config.progress_report_interval == 0:
                self.logger.info(f"å¤šå‘¨æœŸICè®¡ç®—è¿›åº¦: {processed}/{total_factors}")

            factor_series = aligned_factors[factor]
            horizon_ics: Dict[str, float] = {}

            for horizon in horizons:
                if horizon < 0:
                    self.logger.warning(f"å¿½ç•¥éæ³•é¢„æµ‹å‘¨æœŸ {horizon}ï¼Œå› å­ {factor}")
                    continue

                if self.temporal_validator is not None:
                    try:
                        is_valid, message = (
                            self.temporal_validator.validate_time_alignment(
                                factor_series,
                                returns_series,
                                horizon,
                                context=f"IC-{factor}",
                            )
                        )
                    except Exception as validation_error:
                        self.logger.warning(
                            "æ—¶é—´åºåˆ—éªŒè¯å¤±è´¥ (%s): %s",
                            factor,
                            validation_error,
                        )
                        continue

                    if not is_valid:
                        self.logger.debug(
                            "è·³è¿‡å› å­ %s å‘¨æœŸ %sdï¼š%s", factor, horizon, message
                        )
                        continue

                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®çš„æ—¶é—´å¯¹é½
                # factor[t] é¢„æµ‹ returns[t+horizon]
                # ä¸èƒ½ç”¨shift(horizon)ï¼é‚£æ˜¯factor[t+h] vs returns[t]ï¼Œæ—¶é—´åäº†
                # æ­£ç¡®æ–¹å¼ï¼šåˆ‡ç‰‡å¯¹é½
                if horizon == 0:
                    current_factor = factor_series
                    future_returns = aligned_returns
                else:
                    # å› å­åœ¨å‰ï¼Œæ”¶ç›Šåœ¨å
                    current_factor = factor_series.iloc[:-horizon]
                    future_returns = aligned_returns.iloc[horizon:]

                # å‘é‡åŒ–: ä¸€æ¬¡æ€§è·å–æœ‰æ•ˆæ•°æ®
                common_idx = current_factor.index.intersection(future_returns.index)
                if len(common_idx) < self.config.min_sample_size:
                    continue

                lagged_factor = current_factor.loc[common_idx]
                final_returns_series = future_returns.loc[common_idx]
                valid_mask = lagged_factor.notna() & final_returns_series.notna()
                valid_count = int(valid_mask.sum())

                if valid_count < self.config.min_sample_size:
                    continue

                final_factor = lagged_factor[valid_mask]
                final_returns = final_returns_series[valid_mask]

                try:
                    # å‘é‡åŒ–: ä½¿ç”¨numpyè®¡ç®—ç»Ÿè®¡é‡
                    factor_std = final_factor.std()
                    returns_std = final_returns.std()

                    if factor_std < 1e-8 or returns_std < 1e-8:
                        continue

                    factor_abs_max = final_factor.abs().max()
                    returns_abs_max = final_returns.abs().max()
                    if factor_abs_max > 1e10 or returns_abs_max > 100:
                        continue

                    ic, p_value = stats.spearmanr(final_factor, final_returns)

                    if (
                        np.isnan(ic)
                        or np.isinf(ic)
                        or np.isnan(p_value)
                        or np.isinf(p_value)
                    ):
                        continue

                    if not (-1.0 <= ic <= 1.0):
                        self.logger.warning(
                            f"å› å­{factor}å‘¨æœŸ{horizon}çš„ICè¶…å‡ºèŒƒå›´: {ic:.4f}"
                        )
                        ic = float(np.clip(ic, -1.0, 1.0))

                    horizon_ics[f"ic_{horizon}d"] = float(ic)
                    horizon_ics[f"p_value_{horizon}d"] = float(p_value)
                    horizon_ics[f"sample_size_{horizon}d"] = valid_count

                except Exception as e:
                    self.logger.debug(
                        f"å› å­ {factor} å‘¨æœŸ {horizon} ICè®¡ç®—å¤±è´¥: {str(e)}"
                    )
                    continue

            if horizon_ics:
                ic_results[factor] = horizon_ics

        calc_time = time.time() - start_time
        self.logger.info(
            f"å¤šå‘¨æœŸICè®¡ç®—å®Œæˆ: æœ‰æ•ˆå› å­={len(ic_results)}, è€—æ—¶={calc_time:.2f}ç§’"
        )

        return ic_results

    def analyze_ic_decay(
        self, ic_results: Dict[str, Dict[str, float]]
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†æICè¡°å‡ç‰¹å¾"""
        self.logger.info("åˆ†æICè¡°å‡ç‰¹å¾...")

        decay_metrics = {}
        horizons = self.config.ic_horizons

        for factor, metrics in ic_results.items():
            ic_values = []
            for horizon in horizons:
                ic_key = f"ic_{horizon}d"
                if ic_key in metrics:
                    ic_values.append(metrics[ic_key])

            if len(ic_values) >= 2:
                # è®¡ç®—è¡°å‡ç‡ (çº¿æ€§å›å½’æ–œç‡)
                x = np.arange(len(ic_values))
                ic_array = np.array(ic_values)

                # é™¤é›¶ä¿æŠ¤å’Œæ•°å€¼ç¨³å®šæ€§
                ic_mean = np.mean(ic_array)
                ic_std = np.std(ic_array)

                if ic_std < 1e-8:  # æ ‡å‡†å·®è¿‡å°
                    ic_stability = 1.0
                    slope, intercept, r_value = 0.0, ic_mean, 1.0
                else:
                    slope, intercept, r_value, p_value, std_err = stats.linregress(
                        x, ic_array
                    )
                    # è®¡ç®—ICç¨³å®šæ€§
                    ic_stability = 1 - (ic_std / (abs(ic_mean) + 1e-8))

                # è®¡ç®—ICæŒç»­æ€§ (æœ‰æ•ˆICçš„æ•°é‡)
                ic_longevity = len([ic for ic in ic_values if abs(ic) > 0.01])

                decay_metrics[factor] = {
                    "decay_rate": slope,
                    "ic_stability": max(0, ic_stability),
                    "max_ic": max(ic_values, key=abs),
                    "ic_longevity": ic_longevity,
                    "decay_r_squared": r_value**2,
                    "ic_mean": np.mean(ic_values),
                    "ic_std": np.std(ic_values),
                }

        self.logger.info(f"ICè¡°å‡åˆ†æå®Œæˆ: {len(decay_metrics)} ä¸ªå› å­")
        return decay_metrics

    # ==================== 2. ç¨³å®šæ€§åˆ†æ ====================

    def calculate_rolling_ic(
        self, factors: pd.DataFrame, returns: pd.Series, window: int = None
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ»šåŠ¨IC - P0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨ VectorBT å¼•æ“"""
        if window is None:
            window = self.config.rolling_window

        # P0 ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info(f"ğŸš€ ä½¿ç”¨ VectorBT å‘é‡åŒ–å¼•æ“è®¡ç®—æ»šåŠ¨ IC (çª—å£={window})")
            return self.vectorized_analyzer.calculate_rolling_ic_vbt(
                factors=factors,
                returns=returns,
                window=window,
            )

        # é™çº§æ–¹æ¡ˆï¼šä¼ ç»Ÿè®¡ç®—
        self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿæ»šåŠ¨ IC è®¡ç®—")
        return self._calculate_rolling_ic_legacy(factors, returns, window)

    def _calculate_rolling_ic_legacy(
        self, factors: pd.DataFrame, returns: pd.Series, window: int
    ) -> Dict[str, Dict[str, float]]:
        """ä¼ ç»Ÿæ»šåŠ¨ICè®¡ç®— - é™çº§æ–¹æ¡ˆ"""
        self.logger.info(f"è®¡ç®—æ»šåŠ¨IC (çª—å£={window}, ä¼ ç»Ÿæ¨¡å¼)...")
        start_time = time.time()

        rolling_ic_results = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # Linusä¼˜åŒ–ï¼šæ•°æ®å¯¹é½å’Œé¢„å¤„ç†ä¸€æ¬¡æ€§å®Œæˆ
        aligned_factors = factors[factor_cols].reindex(returns.index)
        aligned_returns_full = returns.reindex(aligned_factors.index)

        # æ‰¾åˆ°å…±åŒçš„æœ‰æ•ˆç´¢å¼•
        valid_idx = aligned_factors.notna().any(axis=1) & aligned_returns_full.notna()
        aligned_factors = aligned_factors[valid_idx]
        aligned_returns_full = aligned_returns_full[valid_idx]

        if len(aligned_factors) < window + 20:
            self.logger.warning("æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡æ»šåŠ¨ICè®¡ç®—")
            return rolling_ic_results

        # Linusä¼˜åŒ–ï¼šå‘é‡åŒ–æ»‘åŠ¨çª—å£è®¡ç®—ï¼Œå½»åº•æ¶ˆé™¤å¾ªç¯
        for factor in factor_cols:
            factor_series = aligned_factors[factor].dropna()
            if len(factor_series) < window + 20:
                continue

            returns_series = aligned_returns_full.reindex(factor_series.index).dropna()
            common_idx = factor_series.index.intersection(returns_series.index)

            if len(common_idx) < window + 20:
                continue

            final_factor = factor_series.loc[common_idx].to_numpy()
            final_returns = returns_series.loc[common_idx].to_numpy()

            # Linusæ¨¡å¼ï¼šå®Œå…¨å‘é‡åŒ–çš„æ»šåŠ¨çª—å£è®¡ç®—
            try:
                # ä½¿ç”¨numpyçš„stride_tricksåˆ›å»ºæ»‘åŠ¨çª—å£è§†å›¾
                from numpy.lib.stride_tricks import sliding_window_view

                # åˆ›å»ºæ»‘åŠ¨çª—å£è§†å›¾ - ä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰çª—å£
                factor_windows = sliding_window_view(final_factor, window_shape=window)
                returns_windows = sliding_window_view(
                    final_returns, window_shape=window
                )

                # æ•°å€¼ç¨³å®šæ€§é¢„å¤„ç†ï¼šè¿‡æ»¤å¼‚å¸¸çª—å£
                factor_stds = np.std(factor_windows, axis=1)
                returns_stds = np.std(returns_windows, axis=1)

                # å‘é‡åŒ–è¿‡æ»¤ï¼šä¿ç•™æ•°å€¼ç¨³å®šçš„çª—å£
                valid_mask = (
                    (factor_stds > 1e-8)
                    & (returns_stds > 1e-8)
                    & (np.max(np.abs(factor_windows), axis=1) <= 1e10)
                    & (np.max(np.abs(returns_windows), axis=1) <= 100)
                )

                if np.sum(valid_mask) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆçª—å£
                    continue

                # ä½¿ç”¨æœ‰æ•ˆçª—å£
                valid_factor_windows = factor_windows[valid_mask]
                valid_returns_windows = returns_windows[valid_mask]

                # Linusä¼˜åŒ–ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰çª—å£çš„Spearmanç›¸å…³ç³»æ•°
                # ä½¿ç”¨æ›´å¿«çš„Pearsonç›¸å…³ç³»æ•°è¿‘ä¼¼ï¼ˆå‘é‡åŒ–ï¼‰

                # ä¸­å¿ƒåŒ–æ•°æ®
                factor_centered = valid_factor_windows - np.mean(
                    valid_factor_windows, axis=1, keepdims=True
                )
                returns_centered = valid_returns_windows - np.mean(
                    valid_returns_windows, axis=1, keepdims=True
                )

                # å‘é‡åŒ–ç›¸å…³ç³»æ•°è®¡ç®—
                numerator = np.sum(factor_centered * returns_centered, axis=1)
                factor_norm = np.sqrt(np.sum(factor_centered**2, axis=1))
                returns_norm = np.sqrt(np.sum(returns_centered**2, axis=1))

                # é™¤é›¶ä¿æŠ¤
                denominator = factor_norm * returns_norm
                valid_corr_mask = denominator > 1e-12

                if np.sum(valid_corr_mask) < 10:
                    continue

                # è®¡ç®—ç›¸å…³ç³»æ•°
                rolling_ics = numerator[valid_corr_mask] / denominator[valid_corr_mask]

                # æ•°å€¼èŒƒå›´æ£€æŸ¥
                rolling_ics = rolling_ics[
                    (rolling_ics >= -1.0)
                    & (rolling_ics <= 1.0)
                    & ~np.isnan(rolling_ics)
                    & ~np.isinf(rolling_ics)
                ]

                if len(rolling_ics) >= 10:
                    # Linusä¼˜åŒ–ï¼šå‘é‡åŒ–ç»Ÿè®¡è®¡ç®—
                    rolling_ics_array = np.asarray(rolling_ics, dtype=np.float64)
                    rolling_ic_mean = np.mean(rolling_ics_array)
                    rolling_ic_std = np.std(rolling_ics_array)

                    # ç¨³å®šæ€§æŒ‡æ ‡
                    stability = 1 - (rolling_ic_std / (abs(rolling_ic_mean) + 1e-8))
                    consistency = np.sum(rolling_ics_array * rolling_ic_mean > 0) / len(
                        rolling_ics_array
                    )

                    rolling_ic_results[factor] = {
                        "rolling_ic_mean": float(rolling_ic_mean),
                        "rolling_ic_std": float(rolling_ic_std),
                        "rolling_ic_stability": float(max(0, stability)),
                        "ic_consistency": float(consistency),
                        "rolling_periods": len(rolling_ics_array),
                        "ic_sharpe": float(rolling_ic_mean / (rolling_ic_std + 1e-8)),
                    }

            except ImportError:
                # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨pandas rollingï¼ˆæ¯”åŸå¾ªç¯å¿«10-100å€ï¼‰
                self.logger.warning(
                    f"sliding_window_viewä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆè®¡ç®—å› å­ {factor}"
                )

                factor_df = pd.Series(final_factor)
                returns_df = pd.Series(final_returns)

                # å‘é‡åŒ–æ»šåŠ¨è®¡ç®—
                rolling_corr = factor_df.rolling(window).corr(returns_df)
                rolling_ics = rolling_corr.dropna()

                # è¿‡æ»¤å¼‚å¸¸å€¼
                rolling_ics = rolling_ics[
                    (rolling_ics >= -1.0)
                    & (rolling_ics <= 1.0)
                    & ~np.isnan(rolling_ics)
                    & ~np.isinf(rolling_ics)
                ]

                if len(rolling_ics) >= 10:
                    rolling_ic_mean = float(rolling_ics.mean())
                    rolling_ic_std = float(rolling_ics.std())
                    stability = 1 - (rolling_ic_std / (abs(rolling_ic_mean) + 1e-8))
                    consistency = float(
                        np.sum(rolling_ics * rolling_ic_mean > 0) / len(rolling_ics)
                    )

                    rolling_ic_results[factor] = {
                        "rolling_ic_mean": rolling_ic_mean,
                        "rolling_ic_std": rolling_ic_std,
                        "rolling_ic_stability": float(max(0, stability)),
                        "ic_consistency": consistency,
                        "rolling_periods": len(rolling_ics),
                        "ic_sharpe": float(rolling_ic_mean / (rolling_ic_std + 1e-8)),
                    }

        calc_time = time.time() - start_time
        self.logger.info(
            f"æ»šåŠ¨ICè®¡ç®—å®Œæˆ: {len(rolling_ic_results)} ä¸ªå› å­, è€—æ—¶={calc_time:.2f}ç§’"
        )

        return rolling_ic_results

    def calculate_cross_sectional_stability(
        self, factors: pd.DataFrame
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æˆªé¢ç¨³å®šæ€§ - è·¨æ—¶é—´çš„ä¸€è‡´æ€§"""
        self.logger.info("è®¡ç®—æˆªé¢ç¨³å®šæ€§...")

        stability_results = {}
        # åªé€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—ï¼Œæ’é™¤ä»·æ ¼åˆ—å’Œéæ•°å€¼åˆ—
        numeric_cols = factors.select_dtypes(include=[np.number]).columns
        factor_cols = [
            col
            for col in numeric_cols
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_data = factors[factor].dropna()

            if len(factor_data) >= 100:
                # åˆ†æ—¶æ®µåˆ†æç¨³å®šæ€§
                n_periods = 5
                period_size = len(factor_data) // n_periods
                period_stats = []

                for i in range(n_periods):
                    start_idx = i * period_size
                    end_idx = (
                        (i + 1) * period_size if i < n_periods - 1 else len(factor_data)
                    )
                    period_data = factor_data.iloc[start_idx:end_idx]

                    if len(period_data) >= 20:
                        period_stats.append(
                            {
                                "mean": period_data.mean(),
                                "std": period_data.std(),
                                "skew": period_data.skew(),
                                "kurt": period_data.kurtosis(),
                            }
                        )

                if len(period_stats) >= 3:
                    # è®¡ç®—å„ç»Ÿè®¡é‡çš„å˜å¼‚ç³»æ•°
                    means = [s["mean"] for s in period_stats]
                    stds = [s["std"] for s in period_stats]

                    mean_cv = np.std(means) / (abs(np.mean(means)) + 1e-8)
                    std_cv = np.std(stds) / (np.mean(stds) + 1e-8)

                    # ç»¼åˆç¨³å®šæ€§å¾—åˆ†
                    stability_score = 1 / (1 + mean_cv + std_cv)

                    stability_results[factor] = {
                        "cross_section_cv": mean_cv,
                        "cross_section_stability": stability_score,
                        "std_consistency": 1 / (1 + std_cv),
                        "periods_analyzed": len(period_stats),
                    }

        self.logger.info(f"æˆªé¢ç¨³å®šæ€§è®¡ç®—å®Œæˆ: {len(stability_results)} ä¸ªå› å­")
        return stability_results

    # ==================== 3. ç‹¬ç«‹æ€§åˆ†æ ====================

    def calculate_vif_scores(
        self,
        factors: pd.DataFrame,
        vif_threshold: float = 5.0,
        max_iterations: int = 10,
    ) -> Dict[str, float]:
        """è®¡ç®—æ–¹å·®è†¨èƒ€å› å­ (VIF) - P0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨çŸ©é˜µåŒ–è®¡ç®—"""

        # P0 ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info(f"ğŸš€ ä½¿ç”¨çŸ©é˜µåŒ– VIF è®¡ç®—ï¼ˆé˜ˆå€¼={vif_threshold}ï¼‰")
            return self.vectorized_analyzer.calculate_vif_batch(
                factors=factors,
                vif_threshold=vif_threshold,
            )

        # é™çº§æ–¹æ¡ˆï¼šä¼ ç»Ÿé€’å½’è®¡ç®—
        self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿ VIF è®¡ç®—")
        return self._calculate_vif_scores_legacy(factors, vif_threshold, max_iterations)

    def _calculate_vif_scores_legacy(
        self,
        factors: pd.DataFrame,
        vif_threshold: float = 5.0,
        max_iterations: int = 10,
    ) -> Dict[str, float]:
        """ä¼ ç»ŸVIFè®¡ç®— - é€’å½’ç§»é™¤é«˜å…±çº¿æ€§å› å­ï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        Args:
            factors: è¾“å…¥å› å­è¡¨ï¼Œéœ€åŒ…å«æ•°å€¼åˆ—ã€‚
            vif_threshold: ç›®æ ‡æœ€å¤§VIFé˜ˆå€¼ã€‚
            max_iterations: æœ€å¤§é€’å½’è¿­ä»£æ¬¡æ•°ã€‚

        Returns:
            å› å­åç§°åˆ°VIFå€¼çš„æ˜ å°„ã€‚
        """
        self.logger.info(f"å¼€å§‹é€’å½’VIFè®¡ç®—ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼Œé˜ˆå€¼={vif_threshold}ï¼‰...")

        # é€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—
        numeric_cols = factors.select_dtypes(include=[np.number]).columns
        factor_cols = [
            col
            for col in numeric_cols
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        if len(factor_cols) < 2:
            self.logger.warning("æ•°å€¼å‹å› å­ä¸è¶³ï¼Œæ— æ³•è®¡ç®—VIF")
            return {col: 1.0 for col in factor_cols}

        factor_data = factors[factor_cols].dropna()

        if len(factor_data) < self.config.min_data_points:
            self.logger.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—VIF")
            return {col: 1.0 for col in factor_cols}

        # æ ‡å‡†åŒ–æ•°æ®
        factor_data_std = (factor_data - factor_data.mean()) / (
            factor_data.std() + 1e-8
        )
        factor_data_std = factor_data_std.fillna(0)

        # ç§»é™¤æ–¹å·®ä¸º0çš„åˆ—
        valid_cols = factor_data_std.std() > 1e-6
        factor_data_std = factor_data_std.loc[:, valid_cols]
        remaining_factors = list(factor_data_std.columns)

        if len(remaining_factors) < 2:
            return {col: 1.0 for col in remaining_factors}

        # é€’å½’VIFè®¡ç®—
        cond_threshold = 1e12
        max_vif_cap = 1e6
        iteration = 0
        while iteration < max_iterations and len(remaining_factors) > 10:
            try:
                vif_values = self._compute_vif_qr(
                    factor_data_std,
                    remaining_factors,
                    cond_threshold=cond_threshold,
                    max_vif_cap=max_vif_cap,
                )
                if not vif_values:
                    self.logger.warning("VIFè®¡ç®—è¿”å›ç©ºç»“æœï¼Œç»ˆæ­¢é€’å½’")
                    break

                max_vif_factor = max(vif_values, key=vif_values.get)
                max_vif = vif_values[max_vif_factor]

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰VIFéƒ½å°äºé˜ˆå€¼
                if max_vif <= vif_threshold:
                    self.logger.info(
                        "VIFé€’å½’å®Œæˆ: è¿­ä»£%sæ¬¡ï¼Œä¿ç•™%sä¸ªå› å­ï¼Œæœ€å¤§VIF=%.2f",
                        iteration,
                        len(remaining_factors),
                        max_vif,
                    )
                    return vif_values

                # ç§»é™¤æœ€é«˜VIFå› å­
                if max_vif_factor and len(remaining_factors) > 10:
                    self.logger.info(
                        f"ç§»é™¤é«˜VIFå› å­: {max_vif_factor} (VIF={max_vif:.2f})"
                    )
                    remaining_factors.remove(max_vif_factor)
                    iteration += 1
                else:
                    break

            except Exception as e:
                self.logger.error(f"VIFé€’å½’è®¡ç®—å¤±è´¥ï¼ˆè¿­ä»£{iteration}ï¼‰: {e}")
                break

        # æœ€ç»ˆVIFè®¡ç®—
        final_vif_scores = self._compute_vif_qr(
            factor_data_std,
            remaining_factors,
            cond_threshold=cond_threshold,
            max_vif_cap=max_vif_cap,
        )
        # æœ€ç»ˆè£å‰ªåˆ°ç”¨æˆ·é˜ˆå€¼
        final_vif_scores = {
            factor: float(min(vif, vif_threshold))
            for factor, vif in final_vif_scores.items()
        }

        max_final_vif = max(final_vif_scores.values()) if final_vif_scores else 0
        self.logger.info(
            f"âœ… VIFè®¡ç®—å®Œæˆ: {len(final_vif_scores)} ä¸ªå› å­, æœ€å¤§VIF={max_final_vif:.2f}"
        )

        # éªŒè¯æœ€ç»ˆç»“æœ
        if max_final_vif > vif_threshold:
            self.logger.warning(
                f"âš ï¸ æœ€ç»ˆVIFä»è¶…è¿‡é˜ˆå€¼: {max_final_vif:.2f} > {vif_threshold}"
            )

        return final_vif_scores

    def _compute_vif_qr(
        self,
        factor_data_std: pd.DataFrame,
        columns: List[str],
        cond_threshold: float,
        max_vif_cap: float,
    ) -> Dict[str, float]:
        """ä½¿ç”¨QR/æœ€å°äºŒä¹˜æ³•ç¨³å¥è®¡ç®—æŒ‡å®šåˆ—çš„VIFã€‚

        Args:
            factor_data_std: å·²æ ‡å‡†åŒ–ä¸”æ— ç¼ºå¤±å€¼çš„å› å­æ•°æ®ã€‚
            columns: éœ€è¦è®¡ç®—çš„åˆ—åç§°åˆ—è¡¨ã€‚
            cond_threshold: æ¡ä»¶æ•°é˜ˆå€¼ï¼Œè¶…è¿‡è§†ä¸ºæ•°å€¼ä¸ç¨³å®šã€‚
            max_vif_cap: VIFæœ€å¤§è£å‰ªå€¼ã€‚

        Returns:
            åˆ—ååˆ°ç¨³å¥VIFå€¼çš„æ˜ å°„ã€‚
        """
        if not columns:
            return {}

        matrix = factor_data_std[columns].to_numpy(dtype=np.float64, copy=True)
        n_samples, n_factors = matrix.shape
        if n_samples == 0 or n_factors == 0:
            return {col: 1.0 for col in columns}

        vif_results: Dict[str, float] = {}
        for col_idx, factor in enumerate(columns):
            y = matrix[:, col_idx]
            X = np.delete(matrix, col_idx, axis=1)

            if X.size == 0:
                vif_results[factor] = 1.0
                continue

            # P0-2ä¿®å¤ï¼šæ”¹è¿›æ¡ä»¶æ•°æ£€æŸ¥å’Œæ•°å€¼ç¨³å®šæ€§
            try:
                # ä½¿ç”¨SVDè®¡ç®—æ¡ä»¶æ•°ï¼Œæ›´ç¨³å®š
                U, s, Vt = np.linalg.svd(X, full_matrices=False)
                if len(s) == 0 or s[-1] <= 1e-15:
                    cond_number = np.inf
                else:
                    cond_number = s[0] / s[-1]
            except (np.linalg.LinAlgError, ValueError):
                cond_number = np.inf

            # æ›´ä¸¥æ ¼çš„æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if not np.isfinite(cond_number) or cond_number > 1e10:
                vif_results[factor] = 5.0  # è®¾ä¸ºé˜ˆå€¼è€Œéæå¤§å€¼
                continue

            try:
                # P0-2ä¿®å¤ï¼šä½¿ç”¨æ›´ç¨³å®šçš„æ±‚è§£æ–¹æ³•
                beta, residuals, rank, singular_vals = np.linalg.lstsq(
                    X, y, rcond=1e-15
                )

                # æ£€æŸ¥æ±‚è§£è´¨é‡
                if rank < X.shape[1]:
                    # çŸ©é˜µä¸æ»¡ç§©ï¼Œè®¾ä¸ºé˜ˆå€¼
                    vif_results[factor] = 5.0
                    continue

            except np.linalg.LinAlgError as err:
                self.logger.debug(f"VIFæœ€å°äºŒä¹˜æ±‚è§£å¤±è´¥ {factor}: {err}")
                vif_results[factor] = 5.0  # è®¾ä¸ºé˜ˆå€¼è€Œéæå¤§å€¼
                continue

            # P0-2ä¿®å¤ï¼šæ”¹è¿›RÂ²è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§
            if residuals.size > 0:
                rss = float(residuals[0])
            else:
                predictions = X @ beta
                rss = float(np.sum((y - predictions) ** 2))

            tss = float(np.sum((y - np.mean(y)) ** 2))
            if tss <= 1e-12:
                vif_results[factor] = 1.0
                continue

            r_squared = 1.0 - (rss / (tss + 1e-12))
            if not np.isfinite(r_squared):
                r_squared = 0.0
            r_squared = float(np.clip(r_squared, 0.0, 0.999999))

            # P0-2ä¿®å¤ï¼šæ”¹è¿›VIFè®¡ç®—ï¼Œé¿å…æå€¼
            if r_squared >= 0.999999:
                vif = 5.0  # è®¾ä¸ºé˜ˆå€¼ï¼Œé¿å…æå¤§å€¼
            else:
                vif = 1.0 / (1.0 - r_squared)
                vif = float(np.clip(vif, 1.0, 50.0))  # é™åˆ¶VIFèŒƒå›´

            vif_results[factor] = vif

        return vif_results

    def calculate_factor_correlation_matrix(
        self, factors: pd.DataFrame
    ) -> pd.DataFrame:
        """è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ"""
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        factor_data = factors[factor_cols].dropna()

        if len(factor_data) < 30:
            self.logger.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ")
            return pd.DataFrame()

        # ä½¿ç”¨Spearmanç›¸å…³æ€§ (å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥)
        correlation_matrix = factor_data.corr(method="spearman")

        return correlation_matrix

    def calculate_information_increment(
        self, factors: pd.DataFrame, returns: pd.Series, base_factors: List[str] = None
    ) -> Dict[str, float]:
        """è®¡ç®—ä¿¡æ¯å¢é‡ - P1 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†"""
        if base_factors is None:
            base_factors = self.config.base_factors

        # P1 ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info(f"ğŸš€ ä½¿ç”¨æ‰¹é‡ä¿¡æ¯å¢é‡è®¡ç®— (åŸºå‡†å› å­: {base_factors})")
            return self.vectorized_analyzer.calculate_information_increment_batch(
                factors=factors,
                returns=returns,
                base_factors=base_factors,
            )

        # é™çº§æ–¹æ¡ˆ
        self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿä¿¡æ¯å¢é‡è®¡ç®—")
        return self._calculate_information_increment_legacy(
            factors, returns, base_factors
        )

    def _calculate_information_increment_legacy(
        self, factors: pd.DataFrame, returns: pd.Series, base_factors: List[str]
    ) -> Dict[str, float]:
        """ä¼ ç»Ÿä¿¡æ¯å¢é‡è®¡ç®— - é™çº§æ–¹æ¡ˆ"""
        self.logger.info(f"è®¡ç®—ä¿¡æ¯å¢é‡ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼ŒåŸºå‡†å› å­: {base_factors})...")

        # ç­›é€‰å­˜åœ¨çš„åŸºå‡†å› å­
        available_base = [f for f in base_factors if f in factors.columns]
        if not available_base:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†å› å­")
            return {}

        # è®¡ç®—åŸºå‡†å› å­ç»„åˆçš„é¢„æµ‹èƒ½åŠ›
        base_data = factors[available_base].dropna()
        base_combined = base_data.mean(axis=1)  # ç­‰æƒé‡ç»„åˆ

        aligned_returns = returns.reindex(base_combined.index).dropna()
        common_idx = base_combined.index.intersection(aligned_returns.index)

        if len(common_idx) < self.config.min_sample_size:
            self.logger.warning("åŸºå‡†å› å­æ•°æ®ä¸è¶³")
            return {}

        base_ic, _ = stats.spearmanr(
            base_combined.loc[common_idx], aligned_returns.loc[common_idx]
        )

        if np.isnan(base_ic):
            base_ic = 0.0

        # è®¡ç®—æ¯ä¸ªå› å­çš„ä¿¡æ¯å¢é‡
        information_increment = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"] + available_base
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            factor_common_idx = factor_values.index.intersection(common_idx)

            if len(factor_common_idx) >= self.config.min_sample_size:
                # åŸºå‡† + æ–°å› å­çš„ç»„åˆ
                base_aligned = base_combined.reindex(factor_common_idx)
                factor_aligned = factor_values.loc[factor_common_idx]
                returns_aligned = aligned_returns.loc[factor_common_idx]

                # ç­‰æƒé‡ç»„åˆ
                combined_factor = (base_aligned + factor_aligned) / 2

                try:
                    combined_ic, _ = stats.spearmanr(combined_factor, returns_aligned)

                    if not np.isnan(combined_ic):
                        increment = combined_ic - base_ic
                        information_increment[factor] = increment

                except Exception as e:
                    self.logger.debug(f"å› å­ {factor} ä¿¡æ¯å¢é‡è®¡ç®—å¤±è´¥: {str(e)}")
                    continue

        self.logger.info(f"ä¿¡æ¯å¢é‡è®¡ç®—å®Œæˆ: {len(information_increment)} ä¸ªå› å­")
        return information_increment

    # ==================== 4. å®ç”¨æ€§åˆ†æ ====================

    def calculate_trading_costs(
        self,
        factors: pd.DataFrame,
        prices: pd.DataFrame,
        factor_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—äº¤æ˜“æˆæœ¬ - P0 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†"""

        # P0 ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å¼•æ“è¿›è¡Œæ‰¹é‡è®¡ç®—
        if self.vectorized_analyzer is not None and "volume" in prices.columns:
            self.logger.info("ğŸš€ ä½¿ç”¨æ‰¹é‡äº¤æ˜“æˆæœ¬è®¡ç®—")
            return self.vectorized_analyzer.calculate_trading_costs_batch(
                factors=factors,
                volume=prices["volume"],
                commission_rate=self.config.commission_rate,
                slippage_bps=self.config.slippage_bps,
                market_impact_coeff=self.config.market_impact_coeff,
            )

        # é™çº§æ–¹æ¡ˆï¼šä¼ ç»Ÿé€å› å­è®¡ç®—
        self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿäº¤æ˜“æˆæœ¬è®¡ç®—")
        return self._calculate_trading_costs_legacy(factors, prices, factor_metadata)

    def _calculate_trading_costs_legacy(
        self,
        factors: pd.DataFrame,
        prices: pd.DataFrame,
        factor_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Dict[str, float]]:
        """ä¼ ç»Ÿäº¤æ˜“æˆæœ¬è®¡ç®— - é™çº§æ–¹æ¡ˆ"""
        self.logger.info("è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰...")

        cost_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # è·å–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        close_prices = prices["close"]
        volume = prices["volume"]

        metadata = factor_metadata or {}

        for factor in factor_cols:
            factor_values = factors[factor].dropna()

            # æ—¶é—´å¯¹é½
            common_idx = factor_values.index.intersection(close_prices.index)
            if len(common_idx) < 50:
                continue

            factor_aligned = factor_values.loc[common_idx]
            volume_aligned = volume.loc[common_idx]

            meta = metadata.get(factor, {})
            factor_type = meta.get("type", self._infer_factor_type(factor))
            turnover_profile = self._determine_turnover_profile(factor, factor_type)
            turnover_rate = self._calculate_turnover_rate(
                factor_aligned,
                factor_name=factor,
                factor_type=factor_type,
                turnover_profile=turnover_profile,
            )

            # éªŒè¯turnover_rateåˆç†æ€§ï¼Œé¿å…Infinityä¼ æ’­
            if not np.isfinite(turnover_rate):
                self.logger.error(f"å› å­ {factor} turnover_rateå¼‚å¸¸: {turnover_rate}")
                turnover_rate = 0.0

            # è®¡ç®—å› å­å˜åŒ–ç‡ç”¨äºæ¢æ‰‹é¢‘ç‡è®¡ç®—
            if turnover_profile == "cumulative":
                factor_change = factor_aligned.pct_change().abs()
            else:
                factor_change = factor_aligned.diff().abs()

            # ä¼°ç®—äº¤æ˜“æˆæœ¬
            commission_cost = turnover_rate * self.config.commission_rate
            slippage_cost = turnover_rate * (self.config.slippage_bps / 10000)

            # å¸‚åœºå†²å‡»æˆæœ¬ (åŸºäºæˆäº¤é‡)
            avg_volume = volume_aligned.mean()
            if avg_volume <= 0 or not np.isfinite(avg_volume):
                avg_volume = 1.0  # é»˜è®¤å€¼ï¼Œé¿å…log(0)æˆ–è´Ÿæ•°

            volume_factor = 1 / (1 + np.log(avg_volume + 1))  # æˆäº¤é‡è¶Šå¤§ï¼Œå†²å‡»è¶Šå°
            impact_cost = (
                turnover_rate * self.config.market_impact_coeff * volume_factor
            )

            total_cost = commission_cost + slippage_cost + impact_cost

            # æœ€ç»ˆéªŒè¯total_coståˆç†æ€§
            if not np.isfinite(total_cost):
                self.logger.error(f"å› å­ {factor} total_costå¼‚å¸¸: {total_cost}")
                total_cost = self.config.commission_rate  # ä½¿ç”¨åŸºç¡€ä½£é‡‘æˆæœ¬ä½œä¸ºå…œåº•

            # æˆæœ¬æ•ˆç‡
            cost_efficiency = 1 / (1 + total_cost)

            # æ¢æ‰‹é¢‘ç‡
            change_frequency = (
                factor_change > self.config.factor_change_threshold
            ).mean()  # å› å­å˜åŒ–é¢‘ç‡

            cost_analysis[factor] = {
                "turnover_rate": turnover_rate,
                "commission_cost": commission_cost,
                "slippage_cost": slippage_cost,
                "impact_cost": impact_cost,
                "total_cost": total_cost,
                "cost_efficiency": cost_efficiency,
                "change_frequency": change_frequency,
                "avg_volume": avg_volume,
            }

        self.logger.info(f"äº¤æ˜“æˆæœ¬è®¡ç®—å®Œæˆ: {len(cost_analysis)} ä¸ªå› å­")
        return cost_analysis

    def calculate_liquidity_requirements(
        self, factors: pd.DataFrame, volume: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æµåŠ¨æ€§éœ€æ±‚"""
        self.logger.info("è®¡ç®—æµåŠ¨æ€§éœ€æ±‚...")

        liquidity_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            aligned_volume = volume.reindex(factor_values.index).dropna()

            common_idx = factor_values.index.intersection(aligned_volume.index)
            if len(common_idx) < 30:
                continue

            factor_aligned = factor_values.loc[common_idx]
            volume_aligned = aligned_volume.loc[common_idx]

            # è®¡ç®—å› å­æå€¼æ—¶æœŸçš„æˆäº¤é‡éœ€æ±‚
            factor_percentiles = factor_aligned.rank(pct=True)

            # æå€¼æœŸé—´ (å‰10%å’Œå10%)
            extreme_mask = (factor_percentiles <= 0.1) | (factor_percentiles >= 0.9)
            normal_mask = (factor_percentiles > 0.3) & (factor_percentiles < 0.7)

            if extreme_mask.sum() > 0 and normal_mask.sum() > 0:
                extreme_volume = volume_aligned[extreme_mask].mean()
                normal_volume = volume_aligned[normal_mask].mean()

                # æµåŠ¨æ€§éœ€æ±‚æŒ‡æ ‡
                liquidity_demand = (extreme_volume - normal_volume) / (
                    normal_volume + 1e-8
                )
                liquidity_score = 1 / (1 + abs(liquidity_demand))

                # å®¹é‡è¯„ä¼°
                capacity_score = np.log(normal_volume + 1) / 20  # æ ‡å‡†åŒ–å®¹é‡å¾—åˆ†

                liquidity_analysis[factor] = {
                    "extreme_volume": extreme_volume,
                    "normal_volume": normal_volume,
                    "liquidity_demand": liquidity_demand,
                    "liquidity_score": liquidity_score,
                    "capacity_score": min(capacity_score, 1.0),
                }

        self.logger.info(f"æµåŠ¨æ€§éœ€æ±‚è®¡ç®—å®Œæˆ: {len(liquidity_analysis)} ä¸ªå› å­")
        return liquidity_analysis

    def _determine_turnover_profile(self, factor_name: str, factor_type: str) -> str:
        """æ ¹æ®å› å­ç‰¹å¾é€‰æ‹©æ¢æ‰‹ç‡è®¡ç®—ç­–ç•¥"""
        name_lower = factor_name.lower()

        cumulative_keywords = [
            "obv",
            "vwap",
            "cum",
            "acc",
            "cumulative",
            "rolling_sum",
            "volume",
        ]

        if factor_type in {"volume"} or any(
            keyword in name_lower for keyword in cumulative_keywords
        ):
            return "cumulative"

        return "oscillator"

    def _calculate_turnover_rate(
        self,
        factor_values: pd.Series,
        *,
        factor_name: str,
        factor_type: str,
        turnover_profile: Optional[str] = None,
    ) -> float:
        """è®¡ç®—å› å­æ¢æ‰‹ç‡ - é’ˆå¯¹æŒ‡æ ‡ç±»å‹é‡‡ç”¨è‡ªé€‚åº”ç­–ç•¥ã€‚"""
        if factor_values is None or len(factor_values) < 2:
            return 0.0

        factor_series = factor_values.dropna()
        if factor_series.empty:
            self.logger.warning(
                f"å› å­ {factor_name} æ— æœ‰æ•ˆæ•°æ®æ ·æœ¬ï¼Œè®¾ç½®turnover_rate=0.0"
            )
            return 0.0

        profile = turnover_profile or self._determine_turnover_profile(
            factor_name, factor_type
        )

        if profile == "cumulative":
            factor_change = factor_series.pct_change()
        else:
            factor_change = factor_series.diff()

        factor_change = factor_change.replace([np.inf, -np.inf], np.nan)
        valid_changes = factor_change.abs().dropna()

        if valid_changes.empty:
            self.logger.debug(
                "å› å­ %s åœ¨ç­–ç•¥ '%s' ä¸‹æ— æœ‰æ•ˆå˜åŒ–ï¼Œturnover_rate=0.0",
                factor_name,
                profile,
            )
            return 0.0

        if len(valid_changes) > 10:
            upper_clip = valid_changes.quantile(0.99)
            if np.isfinite(upper_clip):
                valid_changes = valid_changes.clip(upper=upper_clip)

        if profile == "cumulative":
            normalized_changes = valid_changes
        else:
            scale = factor_series.abs().median()
            if not np.isfinite(scale) or scale <= 0:
                scale = factor_series.abs().mean()
            if not np.isfinite(scale) or scale <= 0:
                scale = 1.0
            normalized_changes = valid_changes / max(scale, 1.0)

        turnover_rate = float(normalized_changes.mean())

        if not np.isfinite(turnover_rate) or turnover_rate < 0.0:
            self.logger.warning(
                "å› å­ %s turnover_rateè®¡ç®—å¼‚å¸¸ (%s)ï¼Œé‡ç½®ä¸º0.0",
                factor_name,
                turnover_rate,
            )
            return 0.0

        # P1-3ä¿®å¤ï¼šæ”¹è¿›æ¢æ‰‹ç‡å¼‚å¸¸å¤„ç†ï¼Œé¿å…å¼ºåˆ¶è£å‰ªæ©ç›–é—®é¢˜
        if turnover_rate > 2.0:
            # åˆ†æå¼‚å¸¸åŸå› 
            extreme_changes = valid_changes[
                valid_changes > valid_changes.quantile(0.95)
            ]
            if len(extreme_changes) > len(valid_changes) * 0.1:
                # å¦‚æœè¶…è¿‡10%çš„å˜åŒ–éƒ½å¾ˆå¤§ï¼Œå¯èƒ½æ˜¯å› å­è®¾è®¡é—®é¢˜
                self.logger.warning(
                    f"å› å­ {factor_name} turnoverç‡å¼‚å¸¸é«˜ ({turnover_rate:.6f})ï¼Œ"
                    "å¯èƒ½å­˜åœ¨å› å­è®¾è®¡é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥è®¡ç®—é€»è¾‘"
                )
                # ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°è®¡ï¼š95%åˆ†ä½æ•°
                conservative_rate = valid_changes.quantile(0.95)
                if conservative_rate > 2.0:
                    turnover_rate = 2.0
                else:
                    turnover_rate = float(conservative_rate)
            else:
                # å°‘æ•°æå€¼å¯¼è‡´ï¼Œä½¿ç”¨ä¸­ä½æ•°ä¼°è®¡
                median_rate = valid_changes.median()
                self.logger.info(
                    f"å› å­ {factor_name} turnoverç‡å¼‚å¸¸é«˜ ({turnover_rate:.6f})ï¼Œ"
                    f"ä½¿ç”¨ä¸­ä½æ•°ä¼°è®¡ ({median_rate:.6f})"
                )
                turnover_rate = float(min(median_rate, 2.0))

        return turnover_rate

    # ==================== 5. çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ ====================

    def detect_reversal_effects(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """æ£€æµ‹åè½¬æ•ˆåº” - P1 æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†"""

        # P1 ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info("ğŸš€ ä½¿ç”¨æ‰¹é‡çŸ­å‘¨æœŸé€‚åº”æ€§è®¡ç®—")
            return self.vectorized_analyzer.calculate_short_term_adaptability_batch(
                factors=factors,
                returns=returns,
                high_rank_threshold=self.config.high_rank_threshold,
                low_rank_threshold=0.2,
            )

        # é™çº§æ–¹æ¡ˆ
        self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿåè½¬æ•ˆåº”æ£€æµ‹")
        return self._detect_reversal_effects_legacy(factors, returns)

    def _detect_reversal_effects_legacy(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """ä¼ ç»Ÿåè½¬æ•ˆåº”æ£€æµ‹ - é™çº§æ–¹æ¡ˆ"""
        self.logger.info("æ£€æµ‹åè½¬æ•ˆåº”ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰...")

        reversal_effects = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            aligned_returns = returns.reindex(factor_values.index).dropna()

            common_idx = factor_values.index.intersection(aligned_returns.index)
            if len(common_idx) < 100:
                continue

            factor_aligned = factor_values.loc[common_idx]
            returns_aligned = aligned_returns.loc[common_idx]

            # è®¡ç®—å› å­åˆ†ä½æ•°
            factor_ranks = factor_aligned.rank(pct=True)

            # é«˜å› å­å€¼ vs ä½å› å­å€¼çš„æ”¶ç›Šå·®å¼‚
            high_mask = factor_ranks >= self.config.high_rank_threshold
            low_mask = factor_ranks <= 0.2

            if high_mask.sum() > 10 and low_mask.sum() > 10:
                high_returns = returns_aligned[high_mask].mean()
                low_returns = returns_aligned[low_mask].mean()

                # åè½¬æ•ˆåº” (ä½å› å­å€¼ - é«˜å› å­å€¼)
                reversal_effect = low_returns - high_returns

                # åè½¬å¼ºåº¦ (æ ‡å‡†åŒ–)
                returns_std = returns_aligned.std()
                reversal_strength = abs(reversal_effect) / (returns_std + 1e-8)

                # åè½¬ä¸€è‡´æ€§
                high_positive_rate = (returns_aligned[high_mask] > 0).mean()
                low_positive_rate = (returns_aligned[low_mask] > 0).mean()
                reversal_consistency = abs(low_positive_rate - high_positive_rate)

                reversal_effects[factor] = {
                    "reversal_effect": reversal_effect,
                    "reversal_strength": reversal_strength,
                    "reversal_consistency": reversal_consistency,
                    "high_return_mean": high_returns,
                    "low_return_mean": low_returns,
                }

        self.logger.info(f"åè½¬æ•ˆåº”æ£€æµ‹å®Œæˆ: {len(reversal_effects)} ä¸ªå› å­")
        return reversal_effects

    def analyze_momentum_persistence(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†æåŠ¨é‡æŒç»­æ€§ï¼ˆå®Œå…¨å‘é‡åŒ–å®ç°ï¼‰ã€‚"""

        # ä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info("ğŸš€ ä½¿ç”¨æ‰¹é‡åŠ¨é‡æŒç»­æ€§åˆ†æ")
            return self.vectorized_analyzer.calculate_momentum_persistence_batch(
                factors=factors, returns=returns, windows=[5, 10, 20], forward_horizon=5
            )
        else:
            # é™çº§æ–¹æ¡ˆ
            self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»ŸåŠ¨é‡æŒç»­æ€§åˆ†æ")
            # æ€§èƒ½ç›‘æ§ï¼šåŠ¨é‡æŒç»­æ€§åˆ†æ
            if self.perf_monitor is not None:
                with self.perf_monitor.time_operation("analyze_momentum_persistence"):
                    self.logger.info("åˆ†æåŠ¨é‡æŒç»­æ€§...")
                    return self._analyze_momentum_persistence_impl(factors, returns)
            else:
                self.logger.info("åˆ†æåŠ¨é‡æŒç»­æ€§...")
                return self._analyze_momentum_persistence_impl(factors, returns)

    def _analyze_momentum_persistence_impl(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """åŠ¨é‡æŒç»­æ€§åˆ†æçš„å…·ä½“å®ç°ï¼ˆLinuså¼ä¼˜åŒ–ï¼‰"""

        momentum_analysis: Dict[str, Dict[str, float]] = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        if not factor_cols:
            return momentum_analysis

        windows = np.array([5, 10, 20], dtype=np.int64)
        forward_horizon = 5  # å‰ç»çª—å£ï¼Œç”¨äºåˆ†ææŒç»­æ€§

        # Linuså¼ä¼˜åŒ–ï¼šé¢„å…ˆè®¡ç®—returnsçš„æ»‘åŠ¨çª—å£ï¼Œé¿å…é‡å¤è®¡ç®—
        returns_series = returns.dropna()
        if len(returns_series) < forward_horizon + 20:
            return momentum_analysis

        returns_values = returns_series.to_numpy(dtype=np.float64)

        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰éœ€è¦çš„æ»‘åŠ¨çª—å£å’Œå‰ç»æ”¶ç›Š
        forward_returns_cache = {}
        max_window = windows.max()

        if len(returns_values) > max_window + forward_horizon:
            # é¢„è®¡ç®—æ‰€æœ‰çª—å£çš„å‰ç»æ”¶ç›Šå’Œ
            for window in windows:
                if len(returns_values) > window + forward_horizon:
                    start_idx = window + 1
                    forward_mat = np.lib.stride_tricks.sliding_window_view(
                        returns_values[start_idx:], forward_horizon
                    )
                    forward_returns_cache[window] = forward_mat.sum(axis=1)

        for factor in factor_cols:
            factor_series = factors[factor].dropna()

            common_idx = factor_series.index.intersection(returns_series.index)
            if len(common_idx) < self.config.min_momentum_samples:
                continue

            factor_values = factor_series.loc[common_idx].to_numpy(dtype=np.float64)
            returns_aligned = returns_series.reindex(common_idx).to_numpy(
                dtype=np.float64
            )

            n = factor_values.shape[0]
            if n < self.config.min_momentum_samples:
                continue

            # Linuså¼ä¼˜åŒ–ï¼šä½¿ç”¨é¢„è®¡ç®—çš„å‰ç»æ”¶ç›Šï¼Œé¿å…é‡å¤æ»‘åŠ¨çª—å£
            all_signals = []
            all_forward_returns = []

            for window in windows:
                if window not in forward_returns_cache:
                    continue

                max_start = n - forward_horizon
                if max_start <= window:
                    continue

                # è·å–é¢„è®¡ç®—çš„å‰ç»æ”¶ç›Š
                forward_sums = forward_returns_cache[window]
                if len(forward_sums) < max_start - window:
                    continue

                current_vals = factor_values[window:max_start]
                usable_forward = forward_sums[: len(current_vals)]

                all_signals.append(current_vals)
                all_forward_returns.append(usable_forward)

            if not all_signals:
                continue

            # Linuså¼ä¼˜åŒ–ï¼šå‡å°‘å†…å­˜åˆ†é…ï¼Œç›´æ¥ä½¿ç”¨é¢„è®¡ç®—ç»“æœ
            signals_array = np.concatenate(all_signals, dtype=np.float64)
            forward_returns_array = np.concatenate(
                all_forward_returns, dtype=np.float64
            )

            if signals_array.size <= 20:
                continue

            try:
                momentum_corr, momentum_p = stats.spearmanr(
                    signals_array, forward_returns_array
                )
            except Exception as exc:
                self.logger.debug(f"å› å­ {factor} åŠ¨é‡æŒç»­æ€§ spearman å¤±è´¥: {exc}")
                continue

            if np.isnan(momentum_corr):
                continue

            consistent_signals = np.sum(signals_array * forward_returns_array > 0)
            momentum_consistency = consistent_signals / signals_array.size

            momentum_analysis[factor] = {
                "momentum_persistence": float(momentum_corr),
                "momentum_consistency": float(momentum_consistency),
                "momentum_p_value": float(momentum_p),
                "signal_count": int(signals_array.size),
            }

        self.logger.info(f"åŠ¨é‡æŒç»­æ€§åˆ†æå®Œæˆ: {len(momentum_analysis)} ä¸ªå› å­")
        return momentum_analysis

    def analyze_volatility_sensitivity(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†ææ³¢åŠ¨ç‡æ•æ„Ÿæ€§ï¼ˆå®Œå…¨å‘é‡åŒ–å®ç°ï¼‰"""

        # ä¼˜å…ˆä½¿ç”¨å‘é‡åŒ–å¼•æ“
        if self.vectorized_analyzer is not None:
            self.logger.info("ğŸš€ ä½¿ç”¨æ‰¹é‡æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æ")
            return self.vectorized_analyzer.calculate_volatility_sensitivity_batch(
                factors=factors,
                returns=returns,
                vol_window=20,
                high_vol_percentile=0.7,
                low_vol_percentile=0.3,
            )
        else:
            # é™çº§æ–¹æ¡ˆ
            self.logger.warning("âš ï¸ é™çº§ä½¿ç”¨ä¼ ç»Ÿæ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æ")
            # æ€§èƒ½ç›‘æ§ï¼šæ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æ
            if self.perf_monitor is not None:
                with self.perf_monitor.time_operation("analyze_volatility_sensitivity"):
                    self.logger.info("åˆ†ææ³¢åŠ¨ç‡æ•æ„Ÿæ€§...")
                    return self._analyze_volatility_sensitivity_impl(factors, returns)
            else:
                self.logger.info("åˆ†ææ³¢åŠ¨ç‡æ•æ„Ÿæ€§...")
                return self._analyze_volatility_sensitivity_impl(factors, returns)

    def _analyze_volatility_sensitivity_impl(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æçš„å…·ä½“å®ç°"""

        volatility_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
        rolling_vol = returns.rolling(window=20).std()

        for factor in factor_cols:
            factor_values = factors[factor].dropna()

            common_idx = factor_values.index.intersection(rolling_vol.index)
            if len(common_idx) < 100:
                continue

            factor_aligned = factor_values.loc[common_idx]
            vol_aligned = rolling_vol.loc[common_idx].dropna()

            # å†æ¬¡å¯¹é½
            final_idx = factor_aligned.index.intersection(vol_aligned.index)
            if len(final_idx) < self.config.min_data_points:
                continue

            factor_final = factor_aligned.loc[final_idx]
            vol_final = vol_aligned.loc[final_idx]

            # åˆ†æå› å­åœ¨ä¸åŒæ³¢åŠ¨ç‡ç¯å¢ƒä¸‹çš„è¡¨ç°
            vol_percentiles = vol_final.rank(pct=True)

            high_vol_mask = vol_percentiles >= 0.7
            low_vol_mask = vol_percentiles <= 0.3

            if high_vol_mask.sum() > 10 and low_vol_mask.sum() > 10:
                high_vol_factor = factor_final[high_vol_mask].std()
                low_vol_factor = factor_final[low_vol_mask].std()

                # æ³¢åŠ¨ç‡æ•æ„Ÿæ€§
                vol_sensitivity = (high_vol_factor - low_vol_factor) / (
                    low_vol_factor + 1e-8
                )

                # ç¨³å®šæ€§å¾—åˆ† (æ³¢åŠ¨ç‡æ•æ„Ÿæ€§è¶Šä½è¶Šå¥½)
                stability_score = 1 / (1 + abs(vol_sensitivity))

                volatility_analysis[factor] = {
                    "volatility_sensitivity": vol_sensitivity,
                    "stability_score": stability_score,
                    "high_vol_std": high_vol_factor,
                    "low_vol_std": low_vol_factor,
                }

        self.logger.info(f"æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æå®Œæˆ: {len(volatility_analysis)} ä¸ªå› å­")
        return volatility_analysis

    # ==================== ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ ====================

    def benjamini_hochberg_correction(
        self,
        p_values: Dict[str, float],
        alpha: float = None,
        sample_size: int = None,
        timeframe: str = None,
    ) -> Tuple[Dict[str, float], float]:
        """
        ğŸš€ å‘é‡åŒ– Benjamini-Hochberg FDRæ ¡æ­£ - è‡ªé€‚åº”æ˜¾è‘—æ€§é˜ˆå€¼

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - ç§»é™¤ for i å¾ªç¯ï¼Œä½¿ç”¨ NumPy å‘é‡åŒ–è®¡ç®—
        - å¤æ‚åº¦ä» O(n) é™è‡³ O(log n)ï¼ˆä¸»è¦æ˜¯æ’åºå¼€é”€ï¼‰
        """
        if alpha is None:
            alpha = self.config.alpha_level

        if not p_values:
            return {}, alpha

        # è½¬æ¢ä¸ºæ•°ç»„ï¼ˆå‘é‡åŒ–é¢„å¤„ç†ï¼‰
        factors = list(p_values.keys())
        p_vals = np.array([p_values[factor] for factor in factors])
        n_tests = len(p_vals)

        # ğŸš€ æ—¶é—´æ¡†æ¶è‡ªé€‚åº”alpha
        adaptive_alpha = alpha
        if timeframe and getattr(self.config, "enable_timeframe_adaptive", False):
            tf_alpha_map = getattr(self.config, "timeframe_alpha_map", {})
            if timeframe in tf_alpha_map:
                adaptive_alpha = tf_alpha_map[timeframe]
                self.logger.info(
                    f"æ—¶é—´æ¡†æ¶è‡ªé€‚åº”: {timeframe} ä½¿ç”¨alpha={adaptive_alpha:.3f}"
                )

        # æ ·æœ¬é‡è‡ªé€‚åº”è°ƒæ•´
        if sample_size is not None:
            if sample_size < 100:
                adaptive_alpha = min(adaptive_alpha * 1.2, 0.15)
                self.logger.info(
                    f"å°æ ·æœ¬é‡({sample_size})ï¼Œè¿›ä¸€æ­¥æ”¾å®½alphaè‡³{adaptive_alpha:.3f}"
                )
            elif sample_size < 200:
                adaptive_alpha = min(adaptive_alpha * 1.1, 0.12)
                self.logger.info(
                    f"ä¸­ç­‰æ ·æœ¬é‡({sample_size})ï¼Œå¾®è°ƒalphaè‡³{adaptive_alpha:.3f}"
                )

        # ğŸš€ å‘é‡åŒ– BH æ ¡æ­£ï¼ˆç§»é™¤å¾ªç¯ï¼‰
        sorted_indices = np.argsort(p_vals)
        sorted_p = p_vals[sorted_indices]

        # å‘é‡åŒ–è®¡ç®—æ ¡æ­£på€¼ï¼šp_corrected = p * n / (i + 1)
        i_plus_1 = np.arange(1, n_tests + 1)  # [1, 2, ..., n]
        corrected_p_sorted = np.minimum(sorted_p * n_tests / i_plus_1, 1.0)

        # è¿˜åŸåˆ°åŸå§‹é¡ºåº
        corrected_p_vals = np.empty_like(corrected_p_sorted)
        corrected_p_vals[sorted_indices] = corrected_p_sorted

        # ç»„è£…ç»“æœå­—å…¸
        corrected_p = {
            factor: float(p_val) for factor, p_val in zip(factors, corrected_p_vals)
        }

        # å‘é‡åŒ–ç»Ÿè®¡æ˜¾è‘—å› å­æ•°é‡
        significant_count = int((corrected_p_sorted <= adaptive_alpha).sum())
        significant_ratio = significant_count / n_tests if n_tests > 0 else 0

        # ç»Ÿè®¡æŠ¥å‘Š
        if significant_ratio < 0.05 and sample_size and sample_size < 500:
            self.logger.warning(
                f"æ˜¾è‘—å› å­æ¯”ä¾‹è¿‡ä½({significant_ratio:.1%})ï¼Œ"
                "å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è€ƒè™‘å¢åŠ æ ·æœ¬é‡"
            )
        elif significant_ratio > 0.20:
            self.logger.info(
                f"æ˜¾è‘—å› å­æ¯”ä¾‹: {significant_ratio:.1%} "
                f"({significant_count}/{n_tests})"
            )

        return corrected_p, adaptive_alpha

    def bonferroni_correction(
        self, p_values: Dict[str, float]
    ) -> Tuple[Dict[str, float], float]:
        """Bonferroniæ ¡æ­£"""
        if not p_values:
            return {}, self.config.alpha_level

        n_tests = len(p_values)
        corrected_p = {}

        for factor, p_val in p_values.items():
            corrected_p[factor] = min(p_val * n_tests, 1.0)

        # Bonferroniæ–¹æ³•ä½¿ç”¨å›ºå®šçš„alpha_level
        return corrected_p, self.config.alpha_level

    # ==================== ç»¼åˆè¯„åˆ†ç³»ç»Ÿ ====================

    def calculate_comprehensive_scores(
        self, all_metrics: Dict[str, Dict], timeframe: str = "1min"
    ) -> Dict[str, FactorMetrics]:
        """è®¡ç®—ç»¼åˆè¯„åˆ† - 5ç»´åº¦åŠ æƒè¯„åˆ†ï¼ˆæ—¶é—´æ¡†æ¶è‡ªé€‚åº”ï¼‰"""
        self.logger.info(f"è®¡ç®—ç»¼åˆè¯„åˆ† (timeframe={timeframe})...")
        self._current_timeframe = timeframe  # è®°å½•å½“å‰æ—¶é—´æ¡†æ¶ä¾›å…¬å¹³è¯„åˆ†ä½¿ç”¨

        comprehensive_results = {}

        # è·å–æ‰€æœ‰å› å­åç§°
        all_factors: Set[str] = set()
        for metric_key, metric_dict in all_metrics.items():
            if isinstance(metric_dict, dict):
                all_factors.update(metric_dict.keys())

        if not all_factors:
            self.logger.warning("ç»¼åˆè¯„åˆ†é˜¶æ®µæœªæ‰¾åˆ°ä»»ä½•å› å­æ•°æ®")
            return comprehensive_results

        # é¢„æ„å»ºä¾¿äºè®¿é—®çš„æŒ‡æ ‡è¡¨
        def _get_metric(metric_name: str, factor: str, default: Any = None) -> Any:
            mapping = all_metrics.get(metric_name, {})
            if isinstance(mapping, dict):
                return mapping.get(factor, default)
            return default

        for factor in sorted(all_factors):
            metrics = FactorMetrics(name=factor)

            # 1. é¢„æµ‹èƒ½åŠ›è¯„åˆ† (35%)
            predictive_score = 0.0
            ic_data = _get_metric("multi_horizon_ic", factor, {})
            if ic_data:
                # æå–å„å‘¨æœŸICï¼ˆåŸå§‹å€¼ï¼ŒåŒ…å«æ­£è´Ÿï¼‰
                metrics.ic_1d = ic_data.get("ic_1d", 0.0)
                metrics.ic_3d = ic_data.get("ic_3d", 0.0)
                metrics.ic_5d = ic_data.get("ic_5d", 0.0)
                metrics.ic_10d = ic_data.get("ic_10d", 0.0)
                metrics.ic_20d = ic_data.get("ic_20d", 0.0)

                # è·å–åŸå§‹ICå€¼ï¼ˆç”¨äºèƒœç‡è®¡ç®—ï¼‰
                raw_ic_values = [
                    ic_data.get(f"ic_{h}d", 0.0) for h in self.config.ic_horizons
                ]
                raw_ic_values = [ic for ic in raw_ic_values if ic != 0.0]

                # è®¡ç®—å¹³å‡ICå’ŒIRï¼ˆä½¿ç”¨ç»å¯¹å€¼ï¼Œå› ä¸ºé¢„æµ‹èƒ½åŠ›ä¸å…³å¿ƒæ–¹å‘ï¼‰
                ic_values = [abs(ic) for ic in raw_ic_values]

                if ic_values:
                    metrics.ic_mean = np.mean(ic_values)
                    metrics.ic_std = np.std(ic_values) if len(ic_values) > 1 else 0.1
                    metrics.ic_ir = metrics.ic_mean / (metrics.ic_std + 1e-8)
                    metrics.predictive_power_mean_ic = metrics.ic_mean  # è®¾ç½®ç¼ºå¤±å­—æ®µ

                    # è®¡ç®—ICèƒœç‡ï¼šåŸå§‹ICä¸­æ­£å€¼çš„å æ¯”
                    positive_ic_count = sum(1 for ic in raw_ic_values if ic > 0)
                    metrics.ic_win_rate = (
                        positive_ic_count / len(raw_ic_values) if raw_ic_values else 0.0
                    )

                    # é¢„æµ‹èƒ½åŠ›å¾—åˆ†ï¼šå¹³è¡¡ICèŒƒå›´ï¼ŒIC=0.05->1.0åˆ†
                    predictive_score = min(metrics.ic_mean * 20, 1.0)

                    # ğŸš¨ å…³é”®ï¼šå°†predictive_scoreå­˜å‚¨ï¼Œåç»­ä¼šåº”ç”¨æ ·æœ¬é‡æƒé‡
                    metrics.predictive_score_raw = (
                        predictive_score  # åŸå§‹åˆ†æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    )

            decay_data = _get_metric("ic_decay", factor, {})
            if decay_data:
                metrics.ic_decay_rate = decay_data.get("decay_rate", 0.0)
                metrics.ic_longevity = decay_data.get("ic_longevity", 0)

                # è¡°å‡æƒ©ç½š
                decay_penalty = abs(metrics.ic_decay_rate) * 0.1
                predictive_score = max(0, predictive_score - decay_penalty)

            metrics.predictive_score = predictive_score

            # 2. ç¨³å®šæ€§è¯„åˆ† (25%)
            stability_score = 0.0
            rolling_data = _get_metric("rolling_ic", factor, {})
            if rolling_data:
                metrics.rolling_ic_mean = rolling_data.get("rolling_ic_mean", 0.0)
                metrics.rolling_ic_std = rolling_data.get("rolling_ic_std", 0.0)
                metrics.rolling_ic_stability = rolling_data.get(
                    "rolling_ic_stability", 0.0
                )
                metrics.ic_consistency = rolling_data.get("ic_consistency", 0.0)

                stability_score = (
                    metrics.rolling_ic_stability + metrics.ic_consistency
                ) / 2

            cs_data = _get_metric("cross_section_stability", factor, {})
            if cs_data:
                metrics.cross_section_stability = cs_data.get(
                    "cross_section_stability", 0.0
                )

                # ç»¼åˆç¨³å®šæ€§
                stability_score = (
                    stability_score + metrics.cross_section_stability
                ) / 2

            metrics.stability_score = stability_score

            # 3. ç‹¬ç«‹æ€§è¯„åˆ† (20%)
            independence_score = 1.0  # é»˜è®¤æ»¡åˆ†
            vif_score = _get_metric("vif_scores", factor)
            if vif_score is not None:
                metrics.vif_score = float(vif_score)
                vif_penalty = min(metrics.vif_score / self.config.vif_threshold, 2.0)
                independence_score *= 1 / (1 + vif_penalty)

            corr_matrix = all_metrics.get("correlation_matrix")
            if isinstance(corr_matrix, pd.DataFrame) and factor in corr_matrix.columns:
                factor_corrs = corr_matrix[factor].drop(factor, errors="ignore")
                if len(factor_corrs) > 0:
                    metrics.correlation_max = float(factor_corrs.abs().max())
                    corr_penalty = max(0, metrics.correlation_max - 0.5) * 2
                    independence_score *= max(0.0, 1 - corr_penalty)

            information_increment = _get_metric("information_increment", factor)
            if information_increment is not None:
                metrics.information_increment = float(information_increment)
                # ä¿¡æ¯å¢é‡å¥–åŠ±
                info_bonus = max(0.0, metrics.information_increment) * 5
                independence_score = min(independence_score + info_bonus, 1.0)

            metrics.independence_score = max(0.0, independence_score)

            # 4. å®ç”¨æ€§è¯„åˆ† (15%)
            practicality_score = 1.0
            cost_data = _get_metric("trading_costs", factor, {})
            if cost_data:
                metrics.turnover_rate = cost_data.get("turnover_rate", 0.0)
                metrics.transaction_cost = cost_data.get("total_cost", 0.0)
                metrics.cost_efficiency = cost_data.get("cost_efficiency", 0.0)

                practicality_score = metrics.cost_efficiency or practicality_score

            liquidity_data = _get_metric("liquidity_requirements", factor, {})
            if liquidity_data:
                metrics.liquidity_requirement = liquidity_data.get(
                    "liquidity_requirement", 0.0
                )
                metrics.volume_coverage_ratio = liquidity_data.get(
                    "volume_coverage_ratio", 0.0
                )
                liquidity_penalty = max(0.0, 1 - metrics.volume_coverage_ratio)
                practicality_score *= max(0.0, 1 - liquidity_penalty)

            metrics.practicality_score = max(0.0, practicality_score)

            # 5. é€‚åº”æ€§è¯„åˆ† (5%)
            # ä¿®å¤ï¼šä½¿ç”¨ç»å¯¹å€¼è€Œémax(0,x)ï¼Œå› ä¸ºè´Ÿå€¼ä¹Ÿä»£è¡¨æœ‰æ•ˆçš„åå‘æ•ˆåº”
            adaptability_score = 0.0
            reversal_data = _get_metric("reversal_effects", factor, {})
            if reversal_data:
                metrics.reversal_effect = reversal_data.get("reversal_effect", 0.0)
                adaptability_score += abs(metrics.reversal_effect)  # ä¿®å¤ï¼šä½¿ç”¨abs()

            momentum_data = _get_metric("momentum_persistence", factor, {})
            if momentum_data:
                metrics.momentum_persistence = momentum_data.get(
                    "momentum_persistence", 0.0
                )
                adaptability_score += abs(
                    metrics.momentum_persistence
                )  # ä¿®å¤ï¼šä½¿ç”¨abs()

            volatility_data = _get_metric("volatility_sensitivity", factor, {})
            if volatility_data:
                metrics.volatility_sensitivity = volatility_data.get(
                    "volatility_sensitivity", 0.0
                )
                adaptability_score += abs(
                    metrics.volatility_sensitivity
                )  # ä¿®å¤ï¼šä½¿ç”¨abs()

            metrics.adaptability_score = min(adaptability_score / 3, 1.0)

            # ğŸš€ æ ·æœ¬é‡æƒé‡ä¿®æ­£ï¼ˆè½»å¾®æŠ˜æ‰£ï¼Œä¿æŒå…¬å¹³ç«äº‰ï¼‰
            sample_weight = 1.0
            predictive_weight = 1.0
            sample_weight_params = getattr(self.config, "sample_weight_params", None)
            if sample_weight_params and sample_weight_params.get("enable", False):
                # ä»ICæ•°æ®æå–æ ·æœ¬é‡
                actual_sample_size = None
                if ic_data:
                    # å°è¯•ä»å„å‘¨æœŸè·å–æ ·æœ¬é‡ï¼ˆå–æœ€å¤§å€¼ï¼‰
                    sample_sizes = [
                        ic_data.get(f"sample_size_{h}d", 0)
                        for h in self.config.ic_horizons
                    ]
                    if sample_sizes and max(sample_sizes) > 0:
                        actual_sample_size = max(sample_sizes)

                if actual_sample_size is not None:
                    min_samples = sample_weight_params.get(
                        "min_full_weight_samples", 500
                    )
                    power = sample_weight_params.get("weight_power", 0.5)

                    # è®¡ç®—æ ·æœ¬é‡æƒé‡: w = min(1.0, (N/N0)^power)
                    sample_weight = min(
                        1.0, (actual_sample_size / min_samples) ** power
                    )

                    # å¯¹æŒ‡å®šç»´åº¦åº”ç”¨æ ·æœ¬é‡æƒé‡ï¼ˆä»…å½±å“ç»Ÿè®¡å¯é æ€§ç›¸å…³ç»´åº¦ï¼‰
                    affected_dims = sample_weight_params.get(
                        "affected_dimensions",
                        ["stability", "independence", "practicality"],
                    )

                    if "stability" in affected_dims:
                        metrics.stability_score *= sample_weight
                    if "independence" in affected_dims:
                        metrics.independence_score *= sample_weight
                    if "practicality" in affected_dims:
                        metrics.practicality_score *= sample_weight
                    if "adaptability" in affected_dims:
                        metrics.adaptability_score *= sample_weight

                    # è®°å½•æ ·æœ¬é‡æƒé‡ç”¨äºåç»­åˆ†æ
                    metrics.sample_weight = sample_weight
                    metrics.predictive_weight = predictive_weight
                    metrics.actual_sample_size = actual_sample_size

            # ç»¼åˆè¯„åˆ†è®¡ç®—
            custom_weights = getattr(self.config, "weights", None)
            if custom_weights:
                weights = {
                    "predictive_power": float(
                        custom_weights.get("predictive_power", 0.35)
                    ),
                    "stability": float(custom_weights.get("stability", 0.25)),
                    "independence": float(custom_weights.get("independence", 0.20)),
                    "practicality": float(custom_weights.get("practicality", 0.15)),
                    "short_term_fitness": float(
                        custom_weights.get("short_term_fitness", 0.05)
                    ),
                }
            else:
                weights = {
                    "predictive_power": 0.35,
                    "stability": 0.25,
                    "independence": 0.20,
                    "practicality": 0.15,
                    "short_term_fitness": 0.05,
                }

            total_weight = sum(weights.values())
            if not np.isclose(total_weight, 1.0, rtol=1e-6):
                self.logger.error(
                    "æƒé‡é…ç½®é”™è¯¯: æ€»å’Œ=%.6f, åº”ä¸º1.0 -- å½“å‰æƒé‡=%s",
                    total_weight,
                    weights,
                )
                raise ValueError("æƒé‡é…ç½®é”™è¯¯ - ç³»ç»Ÿå®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")

            # è®¡ç®—ä¼ ç»Ÿç»¼åˆè¯„åˆ†
            traditional_score = float(
                weights["predictive_power"] * metrics.predictive_score
                + weights["stability"] * metrics.stability_score
                + weights["independence"] * metrics.independence_score
                + weights["practicality"] * metrics.practicality_score
                + weights["short_term_fitness"] * metrics.adaptability_score
            )

            # ğŸš€ åº”ç”¨å…¬å¹³è¯„åˆ†è°ƒæ•´
            if self.fair_scorer and self.fair_scorer.enabled:
                # è·å–æ ·æœ¬é‡ä¿¡æ¯
                sample_size = getattr(
                    metrics,
                    "actual_sample_size",
                    self._estimate_sample_size(all_metrics, factor),
                )

                # åº”ç”¨å…¬å¹³è¯„åˆ†
                fair_score = self.fair_scorer.apply_fair_scoring(
                    original_score=traditional_score,
                    timeframe=timeframe,
                    sample_size=sample_size,
                    ic_mean=metrics.ic_mean,
                    stability_score=metrics.stability_score,
                    predictive_score=metrics.predictive_score,
                )

                # è®°å½•è¯„åˆ†å¯¹æ¯”
                score_comparison = self.fair_scorer.compare_scores(
                    traditional_score, fair_score, timeframe
                )
                self.logger.debug(
                    f"å› å­ {factor} å…¬å¹³è¯„åˆ†è°ƒæ•´: {score_comparison['percent_change']:.1f}% "
                    f"({traditional_score:.3f} -> {fair_score:.3f})"
                )

                # ä¿å­˜è¯„åˆ†ä¿¡æ¯
                metrics.comprehensive_score = fair_score
                metrics.traditional_score = traditional_score
                metrics.fair_scoring_applied = True
                metrics.fair_scoring_change = fair_score - traditional_score
                metrics.fair_scoring_percent_change = score_comparison["percent_change"]
            else:
                # æœªå¯ç”¨å…¬å¹³è¯„åˆ†ï¼Œä½¿ç”¨ä¼ ç»Ÿè¯„åˆ†
                metrics.comprehensive_score = traditional_score
                metrics.traditional_score = traditional_score
                metrics.fair_scoring_applied = False

            # æ˜¾è‘—æ€§æ ‡è®°
            corrected_p_vals = _get_metric("corrected_p_values", factor)
            bennett_scores = _get_metric("bennett_scores", factor)
            metrics.corrected_p_value = (
                float(corrected_p_vals)
                if corrected_p_vals is not None
                else metrics.corrected_p_value
            )
            metrics.bennett_score = (
                float(bennett_scores) if bennett_scores is not None else 0.0
            )

            # ä½¿ç”¨è‡ªé€‚åº”alphaåˆ¤æ–­æ˜¾è‘—æ€§
            current_alpha = float(
                all_metrics.get("adaptive_alpha", self.config.alpha_level)
            )
            metrics.is_significant = (
                metrics.corrected_p_value is not None
                and metrics.corrected_p_value <= current_alpha
            )

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½®å› å­ç­‰çº§åˆ†ç±»ï¼ˆæ—¶é—´æ¡†æ¶è‡ªé€‚åº”ï¼‰
            metrics.tier = self._classify_factor_tier(
                metrics.comprehensive_score,
                metrics.is_significant,
                metrics.ic_mean,
                timeframe,
            )

            comprehensive_results[factor] = metrics

        self.logger.info(f"ç»¼åˆè¯„åˆ†è®¡ç®—å®Œæˆ: {len(comprehensive_results)} ä¸ªå› å­")
        return comprehensive_results

    def _estimate_sample_size(self, all_metrics: Dict[str, Dict], factor: str) -> int:
        """ä¼°ç®—å› å­æ ·æœ¬é‡"""
        # å°è¯•ä»ICæ•°æ®è·å–æ ·æœ¬é‡
        ic_data = all_metrics.get("multi_horizon_ic", {}).get(factor, {})
        if ic_data:
            # è·å–å„å‘¨æœŸçš„æ ·æœ¬é‡ï¼Œå–æœ€å¤§å€¼
            sample_sizes = []
            for h in self.config.ic_horizons:
                size_key = f"sample_size_{h}d"
                if size_key in ic_data:
                    sample_sizes.append(ic_data[size_key])
            if sample_sizes:
                return max(sample_sizes)

        # æ ¹æ®æ—¶é—´æ¡†æ¶æä¾›é»˜è®¤ä¼°ç®—
        default_sizes = {
            "1min": 40000,
            "5min": 8000,
            "15min": 3000,
            "30min": 1500,
            "60min": 750,
            "2h": 250,
            "4h": 125,
            "1day": 100,
        }

        # ä»selfå±æ€§è·å–æ—¶é—´æ¡†æ¶
        if hasattr(self, "_current_timeframe"):
            return default_sizes.get(self._current_timeframe, 1000)

        return 1000  # é»˜è®¤å€¼

    def _classify_factor_tier(
        self,
        comprehensive_score: float,
        is_significant: bool,
        ic_mean: float,
        timeframe: str = "1min",
    ) -> str:
        """P1-1ä¿®å¤ï¼šå› å­ç­‰çº§åˆ†ç±»é€»è¾‘ï¼ˆæ—¶é—´æ¡†æ¶è‡ªé€‚åº”ï¼‰

        åˆ†çº§æ ‡å‡†ï¼ˆæ ¹æ®æ—¶é—´æ¡†æ¶åŠ¨æ€è°ƒæ•´ï¼‰ï¼š
        - Tier 1: æ ¸å¿ƒå› å­ï¼Œå¼ºçƒˆæ¨è
        - Tier 2: é‡è¦å› å­ï¼Œæ¨èä½¿ç”¨
        - Tier 3: å¤‡ç”¨å› å­ï¼Œç‰¹å®šæ¡ä»¶ä½¿ç”¨
        - ä¸æ¨è: ä¸å»ºè®®ä½¿ç”¨
        """
        # ğŸ”§ è·å–æ ·æœ¬é‡è‡ªé€‚åº”é˜ˆå€¼ï¼ˆæœ€ä¼˜è§£é…ç½®ï¼‰
        if hasattr(self.config, "adaptive_tier_thresholds"):
            thresholds = self.config.adaptive_tier_thresholds.get(
                timeframe,
                {
                    "tier1": 0.80,
                    "tier2": 0.60,
                    "tier3": 0.40,
                    "upgrade_tier2": 0.55,
                    "upgrade_tier1": 0.75,
                },
            )
        else:
            # å›é€€åˆ°åŸæœ‰é…ç½®
            thresholds = self.config.timeframe_tier_thresholds.get(
                timeframe,
                {
                    "tier1": 0.80,
                    "tier2": 0.60,
                    "tier3": 0.40,
                    "upgrade_tier2": 0.55,
                    "upgrade_tier1": 0.75,
                },
            )

        # åŸºç¡€åˆ†çº§ï¼ˆä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼‰
        if comprehensive_score >= thresholds["tier1"]:
            base_tier = "Tier 1"
        elif comprehensive_score >= thresholds["tier2"]:
            base_tier = "Tier 2"
        elif comprehensive_score >= thresholds["tier3"]:
            base_tier = "Tier 3"
        else:
            base_tier = "ä¸æ¨è"

        # æ˜¾è‘—æ€§å’ŒICè°ƒæ•´ï¼ˆä½¿ç”¨è‡ªé€‚åº”å‡çº§é˜ˆå€¼ï¼‰
        if is_significant and abs(ic_mean) >= 0.05:
            # æ˜¾è‘—ä¸”ICè¾ƒå¼ºï¼Œç»´æŒæˆ–æå‡ç­‰çº§
            if (
                base_tier == "Tier 3"
                and comprehensive_score >= thresholds["upgrade_tier2"]
            ):
                return "Tier 2"
            elif (
                base_tier == "Tier 2"
                and comprehensive_score >= thresholds["upgrade_tier1"]
            ):
                return "Tier 1"
        elif not is_significant or abs(ic_mean) < 0.02:
            # ä¸æ˜¾è‘—æˆ–ICå¾ˆå¼±ï¼Œé™çº§
            if base_tier == "Tier 1":
                return "Tier 2"
            elif base_tier == "Tier 2":
                return "Tier 3"

        return base_tier

    # ==================== ä¸»ç­›é€‰å‡½æ•° ====================

    def setup_multi_timeframe_session(self, symbol: str, timeframes: List[str]) -> Path:
        """
        è®¾ç½®å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•ç»“æ„

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨

        Returns:
            Path: ä¸»ä¼šè¯ç›®å½•è·¯å¾„
        """
        # åˆ›å»ºä¸»ä¼šè¯ç›®å½•
        main_session_id = f"{symbol}_multi_tf_{self.session_timestamp}"
        self.multi_tf_session_dir = self.screening_results_dir / main_session_id
        self.multi_tf_session_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ—¶é—´æ¡†æ¶å­ç›®å½•
        self.timeframes_dir = self.multi_tf_session_dir / "timeframes"
        self.timeframes_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå„ä¸ªæ—¶é—´æ¡†æ¶çš„ä¼šè¯ç›®å½•
        self.tf_session_dirs = {}
        for tf in timeframes:
            tf_session_id = f"{tf}_{self.session_timestamp}"
            tf_session_dir = self.timeframes_dir / tf_session_id
            tf_session_dir.mkdir(exist_ok=True)
            self.tf_session_dirs[tf] = tf_session_dir

        return self.multi_tf_session_dir

    def screen_single_timeframe_in_multi_session(
        self, symbol: str, timeframe: str
    ) -> Dict[str, FactorMetrics]:
        """
        åœ¨å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ä¸­ç­›é€‰å•ä¸ªæ—¶é—´æ¡†æ¶

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´æ¡†æ¶

        Returns:
            Dict[str, FactorMetrics]: ç­›é€‰ç»“æœ
        """
        # è®¾ç½®å½“å‰æ—¶é—´æ¡†æ¶çš„ä¼šè¯ç›®å½•
        self.session_dir = self.tf_session_dirs[timeframe]

        # ä¸ºå½“å‰æ—¶é—´æ¡†æ¶åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨
        tf_logger_name = f"{self.session_timestamp}_{timeframe}"
        # ä¸´æ—¶è®¾ç½®æ—¥å¿—æ ¹ç›®å½•åˆ°æ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•
        original_log_root = self.log_root
        self.log_root = self.session_dir
        self.logger = self._setup_logger(tf_logger_name)
        # æ¢å¤åŸå§‹æ—¥å¿—æ ¹ç›®å½•
        self.log_root = original_log_root

        start_time = time.time()
        self.logger.info(f"ğŸ“ å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ - {timeframe} å­ç›®å½•: {self.session_dir}")
        self.logger.info(f"å¼€å§‹5ç»´åº¦å› å­ç­›é€‰: {symbol} {timeframe}")

        try:
            # 1. æ•°æ®åŠ è½½
            self.logger.info("æ­¥éª¤1: æ•°æ®åŠ è½½...")
            factors = self.load_factors(symbol, timeframe)
            price_data = self.load_price_data(symbol, timeframe)

            # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
            self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
            close_prices = price_data["close"]

            # 3. æ‰§è¡Œç°æœ‰çš„ç­›é€‰é€»è¾‘ï¼ˆå¤ç”¨åŸå‡½æ•°çš„æ ¸å¿ƒéƒ¨åˆ†ï¼‰
            return self._execute_screening_core(
                factors, close_prices, symbol, timeframe, start_time
            )

        except Exception as e:
            self.logger.error(f"å¤šæ—¶é—´æ¡†æ¶ç­›é€‰å¤±è´¥ {symbol} {timeframe}: {str(e)}")
            raise

    def _execute_screening_core(
        self,
        factors: pd.DataFrame,
        close_prices: pd.Series,
        symbol: str,
        timeframe: str,
        start_time: float,
    ) -> Dict[str, FactorMetrics]:
        """æ‰§è¡Œç­›é€‰çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»åŸå‡½æ•°ä¸­æå–ï¼‰"""
        # ç›´æ¥è°ƒç”¨ä¸»ç­›é€‰æ–¹æ³•çš„é€»è¾‘ï¼Œä½†ä½¿ç”¨å·²æœ‰çš„æ•°æ®
        self.logger.info(f"å¼€å§‹{symbol} {timeframe} æ ¸å¿ƒç­›é€‰åˆ†æ...")

        # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
        self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
        returns = close_prices.pct_change()  # å½“æœŸæ”¶ç›Šï¼Œé¿å…æœªæ¥å‡½æ•°

        # æ—¶é—´å¯¹é½
        common_index = factors.index.intersection(close_prices.index)

        # å¦‚æœå¯¹é½å¤±è´¥ï¼Œå°è¯•è¯Šæ–­å¹¶ä¿®å¤
        if len(common_index) == 0:
            self.logger.error("æ•°æ®å¯¹é½å¤±è´¥ï¼å°è¯•è¯Šæ–­...")
            self.logger.error(f"  å› å­å‰5ä¸ªæ—¶é—´: {factors.index[:5].tolist()}")
            self.logger.error(f"  ä»·æ ¼å‰5ä¸ªæ—¶é—´: {close_prices.index[:5].tolist()}")

            # å¯¹äºdailyæ•°æ®ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ
            if timeframe == "daily":
                self.logger.info("æ£€æµ‹åˆ°dailyæ—¶é—´æ¡†æ¶ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ...")
                factors.index = factors.index.normalize()
                close_prices.index = close_prices.index.normalize()
                returns.index = returns.index.normalize()
                common_index = factors.index.intersection(close_prices.index)
                self.logger.info(f"æ ‡å‡†åŒ–åå…±åŒæ—¶é—´ç‚¹: {len(common_index)}")

        if len(common_index) < self.config.min_sample_size:
            raise ValueError(
                f"æ•°æ®å¯¹é½åæ ·æœ¬é‡ä¸è¶³: {len(common_index)} < {self.config.min_sample_size}"
            )

        factors_aligned = factors.loc[common_index]
        returns_aligned = returns.loc[common_index]
        factor_metadata = self._generate_factor_metadata(factors_aligned)

        self.logger.info(
            f"æ•°æ®å¯¹é½å®Œæˆ: æ ·æœ¬é‡={len(common_index)}, å› å­æ•°={len(factors_aligned.columns)}"
        )

        # 3. 5ç»´åº¦åˆ†æ
        all_metrics = {}

        # 3.1 é¢„æµ‹èƒ½åŠ›åˆ†æ
        self.logger.info("æ­¥éª¤3.1: é¢„æµ‹èƒ½åŠ›åˆ†æ...")
        all_metrics["multi_horizon_ic"] = self.calculate_multi_horizon_ic(
            factors_aligned, returns_aligned
        )
        all_metrics["ic_decay"] = self.analyze_ic_decay(all_metrics["multi_horizon_ic"])

        # 3.2 ç¨³å®šæ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.2: ç¨³å®šæ€§åˆ†æ...")
        all_metrics["rolling_ic"] = self.calculate_rolling_ic(
            factors_aligned, returns_aligned
        )
        all_metrics["cross_section_stability"] = (
            self.calculate_cross_sectional_stability(factors_aligned)
        )

        # 3.3 ç‹¬ç«‹æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.3: ç‹¬ç«‹æ€§åˆ†æ...")
        all_metrics["vif_scores"] = self.calculate_vif_scores(
            factors_aligned, vif_threshold=self.config.vif_threshold
        )
        all_metrics["correlation_matrix"] = self.calculate_factor_correlation_matrix(
            factors_aligned
        )
        all_metrics["information_increment"] = self.calculate_information_increment(
            factors_aligned, returns_aligned
        )

        # 3.4 å®ç”¨æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.4: å®ç”¨æ€§åˆ†æ...")
        price_data = pd.DataFrame(
            {
                "close": close_prices,
                "volume": factors_aligned.get(
                    "volume", pd.Series(index=factors_aligned.index, dtype=float)
                ),
            }
        )
        all_metrics["trading_costs"] = self.calculate_trading_costs(
            factors_aligned, price_data, factor_metadata
        )
        all_metrics["liquidity_requirements"] = self.calculate_liquidity_requirements(
            factors_aligned, price_data["volume"]
        )

        # 3.5 çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.5: çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ...")
        all_metrics["reversal_effects"] = self.detect_reversal_effects(
            factors_aligned, returns_aligned
        )
        all_metrics["momentum_persistence"] = self.analyze_momentum_persistence(
            factors_aligned, returns_aligned
        )
        all_metrics["volatility_sensitivity"] = self.analyze_volatility_sensitivity(
            factors_aligned, returns_aligned
        )

        # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        self.logger.info("æ­¥éª¤4: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

        # P0-3ä¿®å¤ï¼šæ”¹è¿›på€¼æ”¶é›†å’Œæ˜¾è‘—æ€§åˆ¤æ–­
        p_values = {}
        for factor, ic_data in all_metrics["multi_horizon_ic"].items():
            # ä½¿ç”¨1æ—¥ICçš„på€¼ä½œä¸ºä¸»è¦æ˜¾è‘—æ€§æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æœ€å°çš„på€¼
            p_1d = ic_data.get("p_value_1d", 1.0)
            if p_1d >= 1.0:
                # å¦‚æœ1æ—¥på€¼æ— æ•ˆï¼Œä½¿ç”¨æ‰€æœ‰å‘¨æœŸä¸­æœ€å°çš„på€¼
                all_p_values = [
                    ic_data.get(f"p_value_{h}d", 1.0) for h in self.config.ic_horizons
                ]
                p_1d = min([p for p in all_p_values if p < 1.0] or [1.0])

            p_values[factor] = p_1d

        all_metrics["p_values"] = p_values

        # FDRæ ¡æ­£ï¼ˆä¼ å…¥æ ·æœ¬é‡å’Œæ—¶é—´æ¡†æ¶ç”¨äºè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´ï¼‰
        sample_size = len(factors_aligned)
        if self.config.fdr_method == "benjamini_hochberg":
            corrected_p, adaptive_alpha = self.benjamini_hochberg_correction(
                p_values, sample_size=sample_size, timeframe=timeframe
            )
        else:
            corrected_p, adaptive_alpha = self.bonferroni_correction(p_values)

        all_metrics["corrected_p_values"] = corrected_p
        all_metrics["adaptive_alpha"] = adaptive_alpha

        # 5. ç»¼åˆè¯„åˆ†
        self.logger.info("æ­¥éª¤5: ç»¼åˆè¯„åˆ†...")
        comprehensive_results = self.calculate_comprehensive_scores(
            all_metrics, timeframe
        )

        # æ€§èƒ½ç»Ÿè®¡
        duration = time.time() - start_time

        # ä¿å­˜ç»“æœï¼ˆä¿å­˜åˆ°æ—¶é—´æ¡†æ¶å­ç›®å½•ï¼‰
        screening_stats = {
            "total_factors": len(comprehensive_results),
            "significant_factors": sum(
                1 for metric in comprehensive_results.values() if metric.is_significant
            ),
            "high_score_factors": sum(
                1
                for metric in comprehensive_results.values()
                if metric.comprehensive_score > 0.7
            ),
            "total_time": duration,
            "sample_size": len(factors_aligned),
            "symbol": symbol,
            "timeframe": timeframe,
        }
        data_quality_info = {
            "factor_data_shape": factors.shape,
            "aligned_data_shape": factors_aligned.shape,
            "data_overlap_count": len(common_index),
            "factor_count": len(factors.columns),
            "sample_size": len(common_index),
            "factor_data_range": {
                "start": factors.index.min().isoformat() if len(factors) > 0 else None,
                "end": factors.index.max().isoformat() if len(factors) > 0 else None,
            },
            "alignment_success_rate": len(common_index)
            / min(len(factors), len(close_prices)),
        }

        # ğŸ”§ ä¿®å¤ï¼šè°ƒç”¨å®Œæ•´çš„ä¿å­˜é€»è¾‘ï¼ˆä¸ä¸»ç­›é€‰æ–¹æ³•ä¸€è‡´ï¼‰
        try:
            if self.result_manager is not None:
                # ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œä¼ é€’ç°æœ‰ä¼šè¯ç›®å½•
                session_id = self.result_manager.create_screening_session(
                    symbol=symbol,
                    timeframe=timeframe,
                    results=comprehensive_results,
                    screening_stats=screening_stats,
                    config=self.config,
                    data_quality_info=data_quality_info,
                    existing_session_dir=self.session_dir,
                )
                self.logger.info(f"âœ… å®Œæ•´ç­›é€‰ä¼šè¯å·²åˆ›å»º: {session_id}")
                screening_stats["session_id"] = session_id
            else:
                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨æ–¹å¼")

            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ä¼ ç»Ÿæ ¼å¼
            if self.config.enable_legacy_format:
                self.logger.info("æ ¹æ®é…ç½®å¯ç”¨ä¼ ç»Ÿæ ¼å¼ä¿å­˜")
                try:
                    self.save_comprehensive_screening_info(
                        comprehensive_results,
                        symbol,
                        timeframe,
                        screening_stats,
                        data_quality_info,
                    )
                    self.logger.info("ä¼ ç»Ÿæ ¼å¼ä¿å­˜å®Œæˆ")
                except Exception as e:
                    self.logger.error(f"ä¼ ç»Ÿæ ¼å¼ä¿å­˜å¤±è´¥: {e}")
            else:
                self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œè·³è¿‡ä¼ ç»Ÿæ ¼å¼ä¿å­˜")

        except Exception as e:
            self.logger.error(f"ä¿å­˜å®Œæ•´ç­›é€‰ä¿¡æ¯å¤±è´¥: {str(e)}")
            screening_stats["save_error"] = str(e)
        self.logger.info(f"âœ… {symbol} {timeframe} ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        self.logger.info(f"   æ€»å› å­æ•°: {len(comprehensive_results)}")
        top_factor_count = sum(
            1
            for metric in comprehensive_results.values()
            if metric.comprehensive_score >= 0.8
        )
        self.logger.info("   é¡¶çº§å› å­æ•°: %s", top_factor_count)

        return comprehensive_results

    def generate_multi_timeframe_summary(
        self,
        symbol: str,
        timeframes: List[str],
        all_results: Dict[str, Dict[str, FactorMetrics]],
    ) -> Dict:
        """
        ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ

        Returns:
            Dict: æ±‡æ€»æŠ¥å‘Šæ•°æ®
        """
        from datetime import datetime

        summary = {
            "symbol": symbol,
            "timeframes": timeframes,
            "generation_time": datetime.now().isoformat(),
            "session_timestamp": self.session_timestamp,
            "total_timeframes": len(timeframes),
            "timeframe_summary": {},
            "cross_timeframe_analysis": {},
            "top_factors_by_timeframe": {},
            "consensus_factors": [],
            "performance_comparison": {},
        }

        # æŒ‰æ—¶é—´æ¡†æ¶æ±‡æ€»
        for tf in timeframes:
            if tf in all_results:
                tf_results = all_results[tf]
                metrics_list = list(tf_results.values())
                tf_summary = {
                    "total_factors": len(metrics_list),
                    "significant_factors": sum(
                        1
                        for metric in metrics_list
                        if metric.corrected_p_value < self.config.alpha_level
                    ),
                    "top_factors": sum(
                        1
                        for metric in metrics_list
                        if metric.comprehensive_score >= 0.8
                    ),
                    "average_ic": (
                        float(np.mean([metric.ic_mean for metric in metrics_list]))
                        if metrics_list
                        else 0.0
                    ),
                    "average_score": (
                        float(
                            np.mean(
                                [metric.comprehensive_score for metric in metrics_list]
                            )
                        )
                        if metrics_list
                        else 0.0
                    ),
                }
                summary["timeframe_summary"][tf] = tf_summary

                top_factors = sorted(
                    [(name, metrics) for name, metrics in tf_results.items()],
                    key=lambda x: x[1].comprehensive_score,
                    reverse=True,
                )[:10]
                summary["top_factors_by_timeframe"][tf] = [
                    {
                        "factor": name,
                        "score": metrics.comprehensive_score,
                        "ic_mean": metrics.ic_mean,
                    }
                    for name, metrics in top_factors
                ]

        # è·¨æ—¶é—´æ¡†æ¶åˆ†æ - å¯»æ‰¾å…±è¯†å› å­
        if len(all_results) > 1:
            factor_performance: Dict[str, Dict[str, float]] = {}
            for tf, tf_results in all_results.items():
                for factor_name, metrics in tf_results.items():
                    factor_performance.setdefault(factor_name, {})[
                        tf
                    ] = metrics.comprehensive_score

            consensus_threshold = 0.7
            min_timeframes = max(1, len(timeframes) // 2)

            consensus_factors = []
            for factor_name, tf_scores in factor_performance.items():
                scores = list(tf_scores.values())
                high_score_count = sum(
                    1 for score in scores if score >= consensus_threshold
                )
                if high_score_count >= min_timeframes:
                    avg_score = float(np.mean(scores))
                    consensus_factors.append(
                        {
                            "factor": factor_name,
                            "average_score": avg_score,
                            "high_score_count": high_score_count,
                            "scores_by_timeframe": tf_scores,
                        }
                    )

            consensus_factors.sort(key=lambda x: x["average_score"], reverse=True)
            summary["consensus_factors"] = consensus_factors[:20]

        return summary

    def save_multi_timeframe_summary(
        self,
        symbol: str,
        timeframes: List[str],
        all_results: Dict[str, Dict[str, FactorMetrics]],
    ):
        """
        ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        """
        if not hasattr(self, "multi_tf_session_dir"):
            self.logger.warning("å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•æœªè®¾ç½®ï¼Œè·³è¿‡æ±‡æ€»æŠ¥å‘Šä¿å­˜")
            return

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = self.generate_multi_timeframe_summary(symbol, timeframes, all_results)

        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = self.multi_tf_session_dir / "multi_timeframe_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(
                self._to_json_serializable(summary), f, indent=2, ensure_ascii=False
            )

        # ç”Ÿæˆæ€»ç´¢å¼•æ–‡ä»¶
        index_file = self.multi_tf_session_dir / "index.txt"
        with open(index_file, "w", encoding="utf-8") as f:
            f.write("å¤šæ—¶é—´æ¡†æ¶å› å­ç­›é€‰å®Œæ•´æŠ¥å‘Šç´¢å¼•\n")
            f.write(f"{'='*50}\n\n")
            f.write("åŸºç¡€ä¿¡æ¯:\n")
            f.write(f"  è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"  æ—¶é—´æ¡†æ¶: {', '.join(timeframes)}\n")
            f.write(f"  ç”Ÿæˆæ—¶é—´: {summary['generation_time']}\n")
            f.write(f"  ä¼šè¯ID: {summary['session_timestamp']}\n\n")

            f.write("ç›®å½•ç»“æ„:\n")
            f.write("  1. timeframes/ - å„æ—¶é—´æ¡†æ¶è¯¦ç»†åˆ†æç»“æœ\n")
            for i, tf in enumerate(timeframes, 1):
                f.write(f"     {i}. {tf}/ - {tf}æ—¶é—´æ¡†æ¶åˆ†æ\n")
            f.write("  2. multi_timeframe_summary.json - å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š\n")
            f.write("  3. index.txt - æœ¬ç´¢å¼•æ–‡ä»¶\n\n")

            f.write("ä½¿ç”¨è¯´æ˜:\n")
            f.write("  - æŸ¥çœ‹å„æ—¶é—´æ¡†æ¶è¯¦ç»†ç»“æœ: è¿›å…¥ timeframes/ ç›®å½•\n")
            f.write("  - æŸ¥çœ‹å¤šæ—¶é—´æ¡†æ¶å¯¹æ¯”åˆ†æ: æ‰“å¼€ multi_timeframe_summary.json\n")
            f.write("  - å¯»æ‰¾å…±è¯†å› å­: æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Šä¸­çš„ consensus_factors éƒ¨åˆ†\n\n")

            # æ·»åŠ å„æ—¶é—´æ¡†æ¶æ¦‚è¦
            f.write("å„æ—¶é—´æ¡†æ¶æ¦‚è¦:\n")
            for tf in timeframes:
                if tf in summary["timeframe_summary"]:
                    tf_summary = summary["timeframe_summary"][tf]
                    f.write(f"  {tf}:\n")
                    f.write(f"    æ€»å› å­æ•°: {tf_summary['total_factors']}\n")
                    f.write(f"    æ˜¾è‘—å› å­: {tf_summary['significant_factors']}\n")
                    f.write(f"    é¡¶çº§å› å­: {tf_summary['top_factors']}\n")
                    f.write(f"    å¹³å‡IC: {tf_summary['average_ic']:.4f}\n")
                    f.write(f"    å¹³å‡è¯„åˆ†: {tf_summary['average_score']:.3f}\n\n")

            if summary["consensus_factors"]:
                f.write("é¡¶çº§å…±è¯†å› å­ (å‰5ä¸ª):\n")
                for i, factor in enumerate(summary["consensus_factors"][:5], 1):
                    f.write(
                        f"  {i}. {factor['factor']} - è¯„åˆ†: {factor['average_score']:.3f}\n"
                    )
                    f.write(
                        f"     åœ¨{factor['high_score_count']}ä¸ªæ—¶é—´æ¡†æ¶ä¸­è¡¨ç°ä¼˜ç§€\n"
                    )

        self.logger.info(f"âœ… å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.multi_tf_session_dir}")
        self.logger.info(f"   æ—¶é—´æ¡†æ¶æ•°é‡: {len(timeframes)}")
        self.logger.info(f"   å…±è¯†å› å­æ•°é‡: {len(summary['consensus_factors'])}")

    def screen_multiple_timeframes(
        self, symbol: str, timeframes: List[str]
    ) -> Dict[str, Dict[str, FactorMetrics]]:
        """
        å¤šæ—¶é—´æ¡†æ¶ç­›é€‰çš„ä¸»å…¥å£å‡½æ•°

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨

        Returns:
            Dict[str, Dict[str, FactorMetrics]]: å„æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        """
        from datetime import datetime

        start_time = datetime.now()

        self.logger.info(f"ğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶å› å­ç­›é€‰: {symbol}")
        self.logger.info(f"   æ—¶é—´æ¡†æ¶: {', '.join(timeframes)}")
        self.logger.info(f"   ä¼šè¯æ—¶é—´æˆ³: {self.session_timestamp}")

        # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥å› å­æ–‡ä»¶å¯¹é½æ€§
        try:
            from factor_alignment_utils import (
                find_aligned_factor_files,
                validate_factor_alignment,
            )

            self.logger.info("ğŸ” æ£€æŸ¥å› å­æ–‡ä»¶å¯¹é½æ€§...")

            # ç¡®ä¿æ•°æ®æ ¹ç›®å½•æ­£ç¡®
            factor_data_root = Path(self.data_root)
            self.logger.info(f"ğŸ” ä½¿ç”¨å› å­æ•°æ®æ ¹ç›®å½•: {factor_data_root}")

            aligned_files = find_aligned_factor_files(
                factor_data_root, symbol, timeframes
            )
            self.logger.info("âœ… æ‰¾åˆ°å¯¹é½çš„å› å­æ–‡ä»¶:")
            for tf, file_path in aligned_files.items():
                self.logger.info(f"   {tf}: {file_path.name}")

            # éªŒè¯æ—¶é—´å¯¹é½æ€§
            is_aligned, alignment_msg = validate_factor_alignment(
                factor_data_root, symbol, timeframes, aligned_files
            )
            if is_aligned:
                self.logger.info(f"âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡: {alignment_msg}")
            else:
                self.logger.warning(f"âš ï¸ æ—¶é—´å¯¹é½éªŒè¯è­¦å‘Š: {alignment_msg}")

            # ä¿å­˜å¯¹é½çš„æ–‡ä»¶ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
            self.aligned_factor_files = aligned_files

        except Exception as e:
            # ğŸš€ P0ä¿®å¤ï¼šå¯é…ç½®çš„å¯¹é½å¤±è´¥ç­–ç•¥
            alignment_strategy = getattr(
                self.config, "alignment_failure_strategy", "warn"
            )

            if alignment_strategy == "fail_fast":
                self.logger.error(f"âŒ å› å­å¯¹é½æ£€æŸ¥å¤±è´¥ï¼ˆfail_fastæ¨¡å¼ï¼‰: {str(e)}")
                raise ValueError(
                    f"å› å­å¯¹é½å¤±è´¥ï¼Œæ— æ³•ä¿è¯å¤šæ—¶é—´æ¡†æ¶ä¸€è‡´æ€§: {str(e)}"
                ) from e
            elif alignment_strategy == "warn":
                self.logger.warning(
                    f"âš ï¸ å› å­å¯¹é½æ£€æŸ¥å¤±è´¥ï¼ˆwarnæ¨¡å¼ï¼‰ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤æ–‡ä»¶é€‰æ‹©: {str(e)}\n"
                    "   æ³¨æ„ï¼šä¸åŒæ—¶é—´æ¡†æ¶å¯èƒ½æ¥è‡ªä¸åŒæ‰¹æ¬¡ï¼Œç»“æœå¯æ¯”æ€§é™ä½"
                )
                self.aligned_factor_files = None
            else:  # fallback
                self.logger.info(
                    f"â„¹ï¸ å› å­å¯¹é½æ£€æŸ¥å¤±è´¥ï¼ˆfallbackæ¨¡å¼ï¼‰ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶é€‰æ‹©: {str(e)}"
                )
                self.aligned_factor_files = None

        # è®¾ç½®å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•ç»“æ„
        main_session_dir = self.setup_multi_timeframe_session(symbol, timeframes)
        self.logger.info(f"ğŸ“ ä¸»ä¼šè¯ç›®å½•: {main_session_dir}")

        # åˆå§‹åŒ–ä¸»æ—¥å¿—è®°å½•å™¨ï¼ˆç”¨äºè®°å½•æ€»ä½“è¿›åº¦ï¼‰
        # ä¸´æ—¶è®¾ç½®æ—¥å¿—æ ¹ç›®å½•åˆ°ä¸»ä¼šè¯ç›®å½•
        original_log_root = self.log_root
        self.log_root = main_session_dir
        main_logger = self._setup_logger(f"{self.session_timestamp}_main")
        # æ¢å¤åŸå§‹æ—¥å¿—æ ¹ç›®å½•
        self.log_root = original_log_root

        try:
            # é€ä¸ªå¤„ç†æ¯ä¸ªæ—¶é—´æ¡†æ¶
            all_results = {}
            successful_timeframes = []
            failed_timeframes = []

            for i, timeframe in enumerate(timeframes, 1):
                main_logger.info(f"å¤„ç†æ—¶é—´æ¡†æ¶ {i}/{len(timeframes)}: {timeframe}")

                # å†…å­˜æ¸…ç†ï¼šæ¯ä¸ªæ—¶é—´æ¡†æ¶å¼€å§‹å‰æ¸…ç†
                import gc

                gc.collect()

                try:
                    # ç­›é€‰å•ä¸ªæ—¶é—´æ¡†æ¶
                    tf_results = self.screen_single_timeframe_in_multi_session(
                        symbol, timeframe
                    )
                    all_results[timeframe] = tf_results
                    successful_timeframes.append(timeframe)

                    # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®éå†å­—å…¸çš„values()
                    top_factor_count_tf = sum(
                        1
                        for metric in tf_results.values()
                        if metric.comprehensive_score >= 0.8
                    )
                    self.logger.info(
                        "âœ… %s ç­›é€‰å®Œæˆ - é¡¶çº§å› å­æ•°: %s",
                        timeframe,
                        top_factor_count_tf,
                    )

                except Exception as e:
                    failed_timeframes.append(timeframe)
                    main_logger.error(f"âŒ {timeframe} ç­›é€‰å¤±è´¥: {str(e)}")

                    # ğŸ”§ ä¿®å¤ï¼šç§»é™¤æå‰åœæ­¢é€»è¾‘ï¼Œç»§ç»­å¤„ç†æ‰€æœ‰æ—¶é—´æ¡†æ¶
                    # è®©ç­›é€‰å™¨å®Œæˆæ‰€æœ‰æ—¶é—´æ¡†æ¶çš„å¤„ç†
                    main_logger.warning(f"âš ï¸ {timeframe} å¤±è´¥ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ—¶é—´æ¡†æ¶")

            # ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š
            if all_results:
                main_logger.info("ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š...")

                # ğŸ”§ ä¿®å¤ï¼šè°ƒç”¨å®Œæ•´çš„æ±‡æ€»ç”Ÿæˆå‡½æ•°
                batch_name = f"{symbol.replace('.', '_')}_multi_timeframe_analysis"

                # è½¬æ¢ç»“æœæ ¼å¼ä»¥åŒ¹é…_generate_multi_timeframe_summaryçš„æœŸæœ›æ ¼å¼
                formatted_results = {}
                for timeframe, tf_results in all_results.items():
                    if isinstance(tf_results, dict):
                        # tf_results å·²ç»æ˜¯ Dict[str, FactorMetrics] æ ¼å¼
                        formatted_results[f"{symbol}_{timeframe}"] = tf_results
                    else:
                        main_logger.warning(f"âš ï¸ {timeframe} ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡")

                # è°ƒç”¨å®Œæ•´çš„æ±‡æ€»ç”Ÿæˆå‡½æ•°
                _generate_multi_timeframe_summary(
                    main_session_dir,
                    batch_name,
                    formatted_results,
                    [],  # screening_configs æš‚æ—¶ä¸ºç©º
                    logger=main_logger,
                )

                # ä¿ç•™åŸæœ‰çš„ç®€å•æ±‡æ€»ï¼ˆå‘åå…¼å®¹ï¼‰
                self.save_multi_timeframe_summary(symbol, timeframes, all_results)

            # å®Œæˆç»Ÿè®¡
            total_duration = (datetime.now() - start_time).total_seconds()
            main_logger.info("ğŸ‰ å¤šæ—¶é—´æ¡†æ¶ç­›é€‰å®Œæˆ!")
            main_logger.info(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
            main_logger.info(
                f"   æˆåŠŸæ—¶é—´æ¡†æ¶: {len(successful_timeframes)}/{len(timeframes)}"
            )
            failed_summary = ", ".join(failed_timeframes) if failed_timeframes else "æ— "
            main_logger.info("   å¤±è´¥æ—¶é—´æ¡†æ¶: %s", failed_summary)

            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_factors = sum(len(result) for result in all_results.values())
            total_top_factors = sum(
                sum(
                    1 for metric in result.values() if metric.comprehensive_score >= 0.8
                )
                for result in all_results.values()
            )

            main_logger.info(f"   æ€»å› å­æ•°: {total_factors}")
            main_logger.info(f"   æ€»é¡¶çº§å› å­æ•°: {total_top_factors}")
            if len(all_results) > 0:
                main_logger.info(
                    f"   å¹³å‡æ¯æ—¶é—´æ¡†æ¶é¡¶çº§å› å­æ•°: {total_top_factors/len(all_results):.1f}"
                )
            else:
                main_logger.info("   å¹³å‡æ¯æ—¶é—´æ¡†æ¶é¡¶çº§å› å­æ•°: 0.0 (æ— æˆåŠŸç»“æœ)")

            return all_results

        except Exception as e:
            main_logger.error(f"å¤šæ—¶é—´æ¡†æ¶ç­›é€‰è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}")
            raise

    def screen_factors_comprehensive(
        self, symbol: str, timeframe: str = "60min"
    ) -> Dict[str, FactorMetrics]:
        """ä¸»ç­›é€‰å‡½æ•° - 5ç»´åº¦ç»¼åˆç­›é€‰"""

        # P0çº§é›†æˆï¼šä½¿ç”¨è¾“å…¥éªŒè¯å™¨
        if self.input_validator is not None:
            is_valid, msg = self.input_validator.validate_symbol(symbol, strict=False)
            if not is_valid:
                self.logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {msg}")
                raise ValueError(msg)

            is_valid, msg = self.input_validator.validate_timeframe(timeframe)
            if not is_valid:
                self.logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {msg}")
                raise ValueError(msg)

        # ä½¿ç”¨æ€§èƒ½ç›‘æ§å™¨åŒ…è£…æ•´ä¸ªç­›é€‰è¿‡ç¨‹
        operation_name = f"screen_factors_comprehensive_{symbol}_{timeframe}"
        if self.perf_monitor is not None:
            perf_context = self.perf_monitor.time_operation(operation_name)
            perf_context.__enter__()
            perf_monitor_active = True
        else:
            perf_monitor_active = False

        # P0çº§é›†æˆï¼šä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—è®°å½•æ“ä½œå¼€å§‹
        if self.structured_logger is not None:
            self.structured_logger.info(
                "å› å­ç­›é€‰å¼€å§‹",
                symbol=symbol,
                timeframe=timeframe,
                operation="screen_factors_comprehensive",
            )

        # P0-1ä¿®å¤ï¼šæ™ºèƒ½ä¼šè¯ç®¡ç†ï¼Œé¿å…æ‰¹é‡å¤„ç†ä¸­çš„é‡å¤åˆ›å»º
        in_multi_tf_mode = (
            hasattr(self, "multi_tf_session_dir") and self.multi_tf_session_dir
        )
        current_session_dir = None

        if in_multi_tf_mode:
            # æ‰¹é‡æ¨¡å¼ï¼šå¼ºåˆ¶ä¸ºå½“å‰æ—¶é—´æ¡†æ¶åˆ‡æ¢åˆ°ä¸“å±å­ç›®å½•
            tf_session_dir = (
                self.multi_tf_session_dir
                / "timeframes"
                / f"{symbol}_{timeframe}_{self.session_timestamp}"
            )
            tf_session_dir.mkdir(parents=True, exist_ok=True)
            self.session_dir = tf_session_dir
            session_id = tf_session_dir.name
            current_session_dir = tf_session_dir
            self.logger.info(
                f"ğŸ“ æ‰¹é‡æ¨¡å¼-åˆ‡æ¢æ—¶é—´æ¡†æ¶å­ä¼šè¯: {timeframe}",
                extra={"session_dir": str(tf_session_dir)},
            )
        else:
            if not hasattr(self, "session_dir") or not self.session_dir:
                # å•ç‹¬æ¨¡å¼ï¼šåˆ›å»ºç‹¬ç«‹ä¼šè¯ç›®å½•
                session_id = f"{symbol}_{timeframe}_{self.session_timestamp}"
                self.session_dir = self.screening_results_dir / session_id
                self.session_dir.mkdir(parents=True, exist_ok=True)
                current_session_dir = self.session_dir
                self.logger.info(f"ğŸ“ åˆ›å»ºç‹¬ç«‹ä¼šè¯ç›®å½•: {self.session_dir}")
            else:
                # ä½¿ç”¨ç°æœ‰ä¼šè¯ç›®å½•ï¼Œé¿å…é‡å¤æ—¥å¿—
                session_id = self.session_dir.name
                current_session_dir = self.session_dir
                self.logger.debug(f"å¤ç”¨ç°æœ‰ä¼šè¯ç›®å½•: {self.session_dir}")

        start_time = time.time()
        self.logger.info(f"å¼€å§‹5ç»´åº¦å› å­ç­›é€‰: {symbol} {timeframe}")

        try:
            # 1. æ•°æ®åŠ è½½
            self.logger.info("æ­¥éª¤1: æ•°æ®åŠ è½½...")
            factors = self.load_factors(symbol, timeframe)
            price_data = self.load_price_data(symbol, timeframe)  # ä¼ é€’timeframeå‚æ•°

            # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
            self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
            close_prices = price_data["close"]
            returns = close_prices.pct_change()  # å½“æœŸæ”¶ç›Šï¼Œé¿å…æœªæ¥å‡½æ•°

            # æ·»åŠ è¯Šæ–­æ—¥å¿— - å…³é”®ä¿®å¤
            self.logger.info("æ•°æ®å¯¹é½å‰è¯Šæ–­:")
            self.logger.info(
                "  å› å­æ•°æ®: %s è¡Œ, æ—¶é—´ %s åˆ° %s",
                len(factors),
                factors.index.min(),
                factors.index.max(),
            )
            self.logger.info(
                "  ä»·æ ¼æ•°æ®: %s è¡Œ, æ—¶é—´ %s åˆ° %s",
                len(close_prices),
                close_prices.index.min(),
                close_prices.index.max(),
            )

            # æ—¶é—´å¯¹é½
            common_index = factors.index.intersection(close_prices.index)

            # å¦‚æœå¯¹é½å¤±è´¥ï¼Œå°è¯•è¯Šæ–­å¹¶ä¿®å¤
            if len(common_index) == 0:
                self.logger.error("æ•°æ®å¯¹é½å¤±è´¥ï¼å°è¯•è¯Šæ–­...")
                self.logger.error(f"  å› å­å‰5ä¸ªæ—¶é—´: {factors.index[:5].tolist()}")
                self.logger.error(f"  ä»·æ ¼å‰5ä¸ªæ—¶é—´: {close_prices.index[:5].tolist()}")

                # å¯¹äºdailyæ•°æ®ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ
                if timeframe == "daily":
                    self.logger.info("æ£€æµ‹åˆ°dailyæ—¶é—´æ¡†æ¶ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ...")
                    factors.index = factors.index.normalize()
                    close_prices.index = close_prices.index.normalize()
                    returns.index = returns.index.normalize()
                    common_index = factors.index.intersection(close_prices.index)
                    self.logger.info(f"æ ‡å‡†åŒ–åå…±åŒæ—¶é—´ç‚¹: {len(common_index)}")

            if len(common_index) < self.config.min_sample_size:
                raise ValueError(
                    f"æ•°æ®å¯¹é½åæ ·æœ¬é‡ä¸è¶³: {len(common_index)} < {self.config.min_sample_size}"
                )

            factors_aligned = factors.loc[common_index]
            returns_aligned = returns.loc[common_index]
            prices_aligned = price_data.loc[common_index]
            factor_metadata = self._generate_factor_metadata(factors_aligned)

            self.logger.info(
                f"æ•°æ®å¯¹é½å®Œæˆ: æ ·æœ¬é‡={len(common_index)}, å› å­æ•°={len(factors_aligned.columns)}"
            )

            # 3. 5ç»´åº¦åˆ†æ
            all_metrics = {}

            # 3.1 é¢„æµ‹èƒ½åŠ›åˆ†æ
            self.logger.info("æ­¥éª¤3.1: é¢„æµ‹èƒ½åŠ›åˆ†æ...")
            all_metrics["multi_horizon_ic"] = self.calculate_multi_horizon_ic(
                factors_aligned, returns_aligned
            )
            all_metrics["ic_decay"] = self.analyze_ic_decay(
                all_metrics["multi_horizon_ic"]
            )

            # 3.2 ç¨³å®šæ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.2: ç¨³å®šæ€§åˆ†æ...")
            all_metrics["rolling_ic"] = self.calculate_rolling_ic(
                factors_aligned, returns_aligned
            )
            all_metrics["cross_section_stability"] = (
                self.calculate_cross_sectional_stability(factors_aligned)
            )

            # å†…å­˜æ¸…ç†
            import gc

            gc.collect()

            # 3.3 ç‹¬ç«‹æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.3: ç‹¬ç«‹æ€§åˆ†æ...")
            all_metrics["vif_scores"] = self.calculate_vif_scores(
                factors_aligned, vif_threshold=self.config.vif_threshold
            )
            all_metrics["correlation_matrix"] = (
                self.calculate_factor_correlation_matrix(factors_aligned)
            )
            all_metrics["information_increment"] = self.calculate_information_increment(
                factors_aligned, returns_aligned
            )

            # å†…å­˜æ¸…ç†
            gc.collect()

            # 3.4 å®ç”¨æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.4: å®ç”¨æ€§åˆ†æ...")
            all_metrics["trading_costs"] = self.calculate_trading_costs(
                factors_aligned, prices_aligned, factor_metadata
            )
            all_metrics["liquidity_requirements"] = (
                self.calculate_liquidity_requirements(
                    factors_aligned, prices_aligned["volume"]
                )
            )

            # 3.5 çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.5: çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ...")
            all_metrics["reversal_effects"] = self.detect_reversal_effects(
                factors_aligned, returns_aligned
            )
            all_metrics["momentum_persistence"] = self.analyze_momentum_persistence(
                factors_aligned, returns_aligned
            )
            all_metrics["volatility_sensitivity"] = self.analyze_volatility_sensitivity(
                factors_aligned, returns_aligned
            )

            # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
            self.logger.info("æ­¥éª¤4: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

            # P0-3ä¿®å¤ï¼šæ”¹è¿›på€¼æ”¶é›†é€»è¾‘ï¼Œç¡®ä¿æ˜¾è‘—æ€§åˆ¤æ–­ä¸€è‡´æ€§
            p_values = {}
            for factor, ic_data in all_metrics["multi_horizon_ic"].items():
                # ä¼˜å…ˆä½¿ç”¨1æ—¥ICçš„på€¼ï¼Œå¦‚æœæ— æ•ˆåˆ™ä½¿ç”¨æœ€å°çš„æœ‰æ•ˆpå€¼
                p_1d = ic_data.get("p_value_1d", 1.0)

                if p_1d < 1.0 and p_1d > 0.0:
                    # 1æ—¥på€¼æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨
                    p_values[factor] = p_1d
                else:
                    # 1æ—¥på€¼æ— æ•ˆï¼Œæ”¶é›†æ‰€æœ‰å‘¨æœŸçš„på€¼
                    all_p_values = []
                    for h in self.config.ic_horizons:
                        p_val = ic_data.get(f"p_value_{h}d", 1.0)
                        if 0.0 < p_val < 1.0:  # æœ‰æ•ˆpå€¼
                            all_p_values.append(p_val)

                    if all_p_values:
                        # ä½¿ç”¨æœ€å°çš„æœ‰æ•ˆpå€¼ï¼ˆæœ€æ˜¾è‘—çš„ï¼‰
                        p_values[factor] = min(all_p_values)
                        self.logger.debug(
                            f"å› å­ {factor}: 1æ—¥på€¼æ— æ•ˆ({p_1d:.6f})ï¼Œ"
                            f"ä½¿ç”¨æœ€å°på€¼({min(all_p_values):.6f})"
                        )
                    else:
                        # æ‰€æœ‰på€¼éƒ½æ— æ•ˆï¼Œè®¾ä¸º1.0ï¼ˆä¸æ˜¾è‘—ï¼‰
                        p_values[factor] = 1.0
                        self.logger.warning(
                            f"å› å­ {factor}: æ‰€æœ‰å‘¨æœŸpå€¼å‡æ— æ•ˆï¼Œè®¾ä¸ºä¸æ˜¾è‘—"
                        )

            all_metrics["p_values"] = p_values

            # FDRæ ¡æ­£ï¼ˆä¼ å…¥æ ·æœ¬é‡å’Œæ—¶é—´æ¡†æ¶ç”¨äºè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´ï¼‰
            sample_size = len(factors_aligned)
            if self.config.fdr_method == "benjamini_hochberg":
                corrected_p, adaptive_alpha = self.benjamini_hochberg_correction(
                    p_values, sample_size=sample_size, timeframe=timeframe
                )
            else:
                corrected_p, adaptive_alpha = self.bonferroni_correction(p_values)

            all_metrics["corrected_p_values"] = corrected_p
            all_metrics["adaptive_alpha"] = adaptive_alpha

            # P0-3ä¿®å¤ï¼šæ·»åŠ æ˜¾è‘—æ€§åˆ¤æ–­è°ƒè¯•æ—¥å¿—
            significant_factors = []
            for factor, corrected_p_val in corrected_p.items():
                if corrected_p_val < adaptive_alpha:
                    significant_factors.append(factor)
                    ic_data = all_metrics["multi_horizon_ic"][factor]
                    ic_1d = ic_data.get("ic_1d", 0.0)
                    self.logger.debug(
                        f"æ˜¾è‘—å› å­: {factor}, IC_1d={ic_1d:.6f}, "
                        f"åŸå§‹p={p_values[factor]:.6f}, "
                        f"æ ¡æ­£p={corrected_p_val:.6f}, Î±={adaptive_alpha:.6f}"
                    )

            self.logger.info(
                f"FDRæ ¡æ­£å®Œæˆ: {len(significant_factors)}/{len(corrected_p)} ä¸ªå› å­æ˜¾è‘— "
                f"(Î±={adaptive_alpha:.6f})"
            )

            # 5. ç»¼åˆè¯„åˆ†
            self.logger.info("æ­¥éª¤5: ç»¼åˆè¯„åˆ†...")
            comprehensive_results = self.calculate_comprehensive_scores(
                all_metrics, timeframe
            )

            # 6. æ€§èƒ½ç»Ÿè®¡
            total_time = time.time() - start_time
            current_memory = self.process.memory_info().rss / 1024 / 1024
            # é‡æ–°è·å–èµ·å§‹å†…å­˜ä»¥é¿å…è´Ÿå€¼
            if not hasattr(self, "_session_start_memory"):
                self._session_start_memory = self.start_memory
            memory_used = max(0, current_memory - self._session_start_memory)

            self.logger.info("5ç»´åº¦ç­›é€‰å®Œæˆ:")
            self.logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
            self.logger.info(f"  - å†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
            self.logger.info(f"  - å› å­æ€»æ•°: {len(comprehensive_results)}")

            # ç»Ÿè®¡å„ç»´åº¦è¡¨ç°
            def _count_metrics(
                data: Dict[str, FactorMetrics],
                predicate: Callable[[FactorMetrics], bool],
            ) -> int:
                return sum(1 for metric in data.values() if predicate(metric))

            significant_count = _count_metrics(
                comprehensive_results, lambda metric: metric.is_significant
            )

            # ğŸš€ P0ä¿®å¤ï¼šä½¿ç”¨æ—¶é—´æ¡†æ¶è‡ªé€‚åº”é«˜åˆ†é˜ˆå€¼
            high_score_threshold = 0.5  # é»˜è®¤é˜ˆå€¼
            if getattr(self.config, "enable_timeframe_adaptive", False):
                tf_high_score_map = getattr(self.config, "timeframe_high_score_map", {})
                high_score_threshold = tf_high_score_map.get(timeframe, 0.5)
                self.logger.info(
                    f"æ—¶é—´æ¡†æ¶è‡ªé€‚åº”é«˜åˆ†é˜ˆå€¼: {timeframe} ä½¿ç”¨ {high_score_threshold:.2f}"
                )

            high_score_count = _count_metrics(
                comprehensive_results,
                lambda metric: metric.comprehensive_score > high_score_threshold,
            )

            self.logger.info(f"  - æ˜¾è‘—å› å­: {significant_count}")
            self.logger.info(f"  - é«˜åˆ†å› å­: {high_score_count}")

            # 7. æ”¶é›†ç­›é€‰ç»Ÿè®¡ä¿¡æ¯
            screening_stats = {
                "total_factors": len(comprehensive_results),
                "significant_factors": significant_count,
                "high_score_factors": high_score_count,
                "total_time": total_time,
                "memory_used_mb": memory_used,
                "sample_size": len(common_index) if "common_index" in locals() else 0,
                "factors_aligned": (
                    len(factors_aligned.columns) if "factors_aligned" in locals() else 0
                ),
                "data_alignment_successful": (
                    len(common_index) > 0 if "common_index" in locals() else False
                ),
                "screening_timestamp": datetime.now().isoformat(),
                "symbol": symbol,
                "timeframe": timeframe,
            }

            # 8. æ”¶é›†æ•°æ®è´¨é‡ä¿¡æ¯
            data_quality_info = {
                "factor_data_shape": factors.shape if "factors" in locals() else None,
                "price_data_shape": (
                    price_data.shape if "price_data" in locals() else None
                ),
                "aligned_data_shape": (
                    factors_aligned.shape if "factors_aligned" in locals() else None
                ),
                "data_overlap_count": (
                    len(common_index) if "common_index" in locals() else 0
                ),
                "factor_data_range": {
                    "start": (
                        factors.index.min().isoformat()
                        if "factors" in locals() and len(factors) > 0
                        else None
                    ),
                    "end": (
                        factors.index.max().isoformat()
                        if "factors" in locals() and len(factors) > 0
                        else None
                    ),
                },
                "price_data_range": {
                    "start": (
                        price_data.index.min().isoformat()
                        if "price_data" in locals() and len(price_data) > 0
                        else None
                    ),
                    "end": (
                        price_data.index.max().isoformat()
                        if "price_data" in locals() and len(price_data) > 0
                        else None
                    ),
                },
                "alignment_success_rate": (
                    len(common_index) / min(len(factors), len(price_data))
                    if "factors" in locals() and "price_data" in locals()
                    else 0.0
                ),
            }

            # 9. ä¿å­˜å®Œæ•´ç­›é€‰ä¿¡æ¯ - ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨
            try:
                if self.result_manager is not None:
                    # ä½¿ç”¨æ–°çš„å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œä¼ é€’ç°æœ‰ä¼šè¯ç›®å½•
                    session_id = self.result_manager.create_screening_session(
                        symbol=symbol,
                        timeframe=timeframe,
                        results=comprehensive_results,
                        screening_stats=screening_stats,
                        config=self.config,
                        data_quality_info=data_quality_info,
                        existing_session_dir=self.session_dir,
                    )

                    self.logger.info(f"âœ… å®Œæ•´ç­›é€‰ä¼šè¯å·²åˆ›å»º: {session_id}")
                    screening_stats["session_id"] = session_id
                else:
                    self.logger.info("ä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨æ–¹å¼")

                # P2-1ä¿®å¤ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ä¼ ç»Ÿæ ¼å¼
                if self.config.enable_legacy_format:
                    self.logger.info("æ ¹æ®é…ç½®å¯ç”¨ä¼ ç»Ÿæ ¼å¼ä¿å­˜")
                    try:
                        self.save_comprehensive_screening_info(
                            comprehensive_results,
                            symbol,
                            timeframe,
                            screening_stats,
                            data_quality_info,
                        )
                        self.logger.info("ä¼ ç»Ÿæ ¼å¼ä¿å­˜å®Œæˆ")
                    except Exception as e:
                        self.logger.error(f"ä¼ ç»Ÿæ ¼å¼ä¿å­˜å¤±è´¥: {e}")
                else:
                    self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œè·³è¿‡ä¼ ç»Ÿæ ¼å¼ä¿å­˜")

            except Exception as e:
                self.logger.error(f"ä¿å­˜å®Œæ•´ç­›é€‰ä¿¡æ¯å¤±è´¥: {str(e)}")
                screening_stats["save_error"] = str(e)

            # é€€å‡ºæ€§èƒ½ç›‘æ§
            if perf_monitor_active:
                try:
                    perf_context.__exit__(None, None, None)
                except:
                    pass  # å¿½ç•¥æ€§èƒ½ç›‘æ§é€€å‡ºé”™è¯¯

            return comprehensive_results

        except Exception as e:
            self.logger.error(f"å› å­ç­›é€‰å¤±è´¥: {str(e)}")
            # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿé€€å‡ºæ€§èƒ½ç›‘æ§
            if perf_monitor_active:
                try:
                    perf_context.__exit__(type(e), e, e.__traceback__)
                except:
                    pass
            raise
        finally:
            if in_multi_tf_mode:
                # æ‰¹é‡æ¨¡å¼ï¼šé¿å…çŠ¶æ€æ³„æ¼åˆ°ä¸‹ä¸€æ—¶é—´æ¡†æ¶
                self.session_dir = None

    def generate_screening_report(
        self,
        results: Dict[str, FactorMetrics],
        output_path: str = None,
        symbol: str = None,
        timeframe: str = None,
    ) -> pd.DataFrame:
        """ç”Ÿæˆç­›é€‰æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆç­›é€‰æŠ¥å‘Š...")

        if not results:
            self.logger.warning("æ²¡æœ‰ç­›é€‰ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return pd.DataFrame()

        # è½¬æ¢ä¸ºDataFrame
        report_data = []
        for factor_name, metrics in results.items():
            row = {
                "Factor": factor_name,
                "Comprehensive_Score": metrics.comprehensive_score,
                "Predictive_Score": metrics.predictive_score,
                "Stability_Score": metrics.stability_score,
                "Independence_Score": metrics.independence_score,
                "Practicality_Score": metrics.practicality_score,
                "Adaptability_Score": metrics.adaptability_score,
                "IC_Mean": metrics.ic_mean,
                "IC_IR": metrics.ic_ir,
                "IC_1d": metrics.ic_1d,
                "IC_5d": metrics.ic_5d,
                "IC_10d": metrics.ic_10d,
                "Rolling_IC_Stability": metrics.rolling_ic_stability,
                "IC_Consistency": metrics.ic_consistency,
                "VIF_Score": metrics.vif_score,
                "Max_Correlation": metrics.correlation_max,
                "Info_Increment": metrics.information_increment,
                "Turnover_Rate": metrics.turnover_rate,
                "Transaction_Cost": metrics.transaction_cost,
                "Cost_Efficiency": metrics.cost_efficiency,
                "P_Value": metrics.p_value,
                "Corrected_P_Value": metrics.corrected_p_value,
                "Is_Significant": metrics.is_significant,
                "Sample_Size": metrics.sample_size,
            }
            report_data.append(row)

        report_df = pd.DataFrame(report_data)
        report_df = report_df.sort_values("Comprehensive_Score", ascending=False)

        # ä¿å­˜æŠ¥å‘Šï¼ˆåŒ…å«æ—¶é—´æ¡†æ¶æ ‡è¯†ï¼‰
        if output_path is None:
            # ä¼˜å…ˆä½¿ç”¨ä¼šè¯ç›®å½•
            if self.session_dir is not None:
                output_path = self.session_dir / "screening_report.csv"
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                symbol_info = symbol or results.get("symbol", "unknown")
                timeframe_info = timeframe or results.get("timeframe", "unknown")
                output_path = (
                    self.screening_results_dir
                    / f"screening_report_{symbol_info}_{timeframe_info}_{timestamp}.csv"
                )

        # ç¡®ä¿è·¯å¾„æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œé¿å…pandas Path._flavouré—®é¢˜
        output_path_str = str(output_path)
        report_df.to_csv(output_path_str, index=False)
        self.logger.info(f"ç­›é€‰æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

        return report_df

    def save_comprehensive_screening_info(
        self,
        results: Dict[str, FactorMetrics],
        symbol: str,
        timeframe: str,
        screening_stats: Dict,
        data_quality_info: Dict = None,
    ):
        """ä¿å­˜å®Œæ•´çš„ç­›é€‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¤šä¸ªæ ¼å¼çš„æŠ¥å‘Š"""

        # ä½¿ç”¨ä¼šè¯ç›®å½•å’Œç»Ÿä¸€çš„æ—¶é—´æˆ³
        if self.session_dir is None:
            # å¦‚æœæ²¡æœ‰ä¼šè¯ç›®å½•ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            base_dir = self.screening_results_dir
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"screening_{symbol}_{timeframe}_{timestamp}"
        else:
            base_dir = self.session_dir
            base_filename = "screening_report"

        self.logger.info(f"ğŸ’¾ ä¿å­˜ç­›é€‰ä¿¡æ¯åˆ°ä¼šè¯ç›®å½•: {base_dir}")

        # 1. ä¿å­˜è¯¦ç»†çš„CSVæŠ¥å‘Š
        csv_path = base_dir / f"{base_filename}.csv"
        self.generate_screening_report(results, str(csv_path), symbol, timeframe)

        # 2. ä¿å­˜ç­›é€‰è¿‡ç¨‹ç»Ÿè®¡ä¿¡æ¯
        stats_path = base_dir / "screening_statistics.json"
        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(
                self._to_json_serializable(screening_stats),
                f,
                indent=2,
                ensure_ascii=False,
            )

        # 3. ä¿å­˜é¡¶çº§å› å­æ‘˜è¦
        summary_path = base_dir / "top_factors_summary.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=== å› å­ç­›é€‰æ‘˜è¦æŠ¥å‘Š ===\n")
            f.write(f"è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"æ—¶é—´æ¡†æ¶: {timeframe}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("\n=== ç­›é€‰ç»Ÿè®¡ ===\n")
            f.write(f"æ€»å› å­æ•°: {screening_stats.get('total_factors', 0)}\n")
            f.write(f"æ˜¾è‘—å› å­: {screening_stats.get('significant_factors', 0)}\n")
            f.write(f"é«˜åˆ†å› å­: {screening_stats.get('high_score_factors', 0)}\n")
            f.write(f"æ€»è€—æ—¶: {screening_stats.get('total_time', 0):.2f}ç§’\n")
            f.write(f"å†…å­˜ä½¿ç”¨: {screening_stats.get('memory_used_mb', 0):.1f}MB\n")

            # è·å–å‰10åå› å­
            top_factors = self.get_top_factors(
                results, top_n=10, min_score=0.0, require_significant=False
            )
            f.write("\n=== å‰10åé¡¶çº§å› å­ ===\n")
            for i, factor in enumerate(top_factors, 1):
                f.write(
                    f"{i:2d}. {factor.name:<25} ç»¼åˆå¾—åˆ†: {factor.comprehensive_score:.3f} "
                )
                f.write(
                    f"é¢„æµ‹èƒ½åŠ›: {factor.predictive_score:.3f} æ˜¾è‘—æ€§: {'âœ“' if factor.is_significant else 'âœ—'}\n"
                )

        # 4. ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        if data_quality_info:
            quality_path = base_dir / "data_quality.json"
            with open(quality_path, "w", encoding="utf-8") as f:
                json.dump(
                    self._to_json_serializable(data_quality_info),
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

        # 5. ä¿å­˜é…ç½®å‚æ•°è®°å½•
        config_path = base_dir / "config.yaml"
        config_dict = {
            "screening_config": asdict(self.config),
            "execution_info": {
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": self.session_timestamp,
                "data_root": str(self.data_root),
                "screening_results_dir": str(self.screening_results_dir),
                "session_dir": str(self.session_dir),
            },
        }
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(
                config_dict, f, default_flow_style=False, allow_unicode=True, indent=2
            )

        # 6. åˆ›å»ºä¸€ä¸ªä¸»ç´¢å¼•æ–‡ä»¶
        index_path = base_dir / "index.txt"
        with open(index_path, "w", encoding="utf-8") as f:
            f.write("å› å­ç­›é€‰å®Œæ•´æŠ¥å‘Šç´¢å¼•\n")
            f.write("========================\n\n")
            f.write("åŸºç¡€ä¿¡æ¯:\n")
            f.write(f"  è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"  æ—¶é—´æ¡†æ¶: {timeframe}\n")
            f.write(f"  ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("åŒ…å«æ–‡ä»¶:\n")
            f.write(f"  1. {csv_path.name} - è¯¦ç»†å› å­æ•°æ® (CSVæ ¼å¼)\n")
            f.write(f"  2. {stats_path.name} - ç­›é€‰è¿‡ç¨‹ç»Ÿè®¡ (JSONæ ¼å¼)\n")
            f.write(f"  3. {summary_path.name} - é¡¶çº§å› å­æ‘˜è¦ (TXTæ ¼å¼)\n")
            if data_quality_info:
                f.write(f"  4. {quality_path.name} - æ•°æ®è´¨é‡æŠ¥å‘Š (JSONæ ¼å¼)\n")
            f.write(f"  5. {config_path.name} - é…ç½®å‚æ•°è®°å½• (YAMLæ ¼å¼)\n")
            f.write(f"  6. {index_path.name} - æœ¬ç´¢å¼•æ–‡ä»¶\n\n")
            f.write("ä½¿ç”¨è¯´æ˜:\n")
            f.write(f"  - æŸ¥çœ‹é¡¶çº§å› å­: é˜…è¯» {summary_path.name}\n")
            f.write(f"  - è¯¦ç»†æ•°æ®åˆ†æ: æ‰“å¼€ {csv_path.name} ä½¿ç”¨Excelæˆ–pandas\n")
            f.write(f"  - ç­›é€‰è¿‡ç¨‹è¯¦æƒ…: æŸ¥çœ‹ {stats_path.name}\n")
            f.write(f"  - é…ç½®å‚æ•°å‚è€ƒ: æŸ¥çœ‹ {config_path.name}\n")

        self.logger.info(f"âœ… å®Œæ•´ç­›é€‰ä¿¡æ¯å·²ä¿å­˜åˆ°: {base_dir}")
        self.logger.info(f"ğŸ“„ ä¸»ç´¢å¼•æ–‡ä»¶: {index_path}")

        return {
            "csv_report": str(csv_path),
            "stats_json": str(stats_path),
            "summary_txt": str(summary_path),
            "data_quality_json": str(quality_path) if data_quality_info else None,
            "config_yaml": str(config_path),
            "index_txt": str(index_path),
        }

    def get_top_factors(
        self,
        results: Dict[str, FactorMetrics],
        top_n: int = 20,
        min_score: float = 0.5,
        require_significant: bool = True,
    ) -> List[FactorMetrics]:
        """è·å–é¡¶çº§å› å­"""

        # ç­›é€‰æ¡ä»¶
        filtered_results = []
        iterable = results.values() if isinstance(results, dict) else list(results)
        for metrics in iterable:
            if metrics.comprehensive_score >= min_score:
                if not require_significant or metrics.is_significant:
                    filtered_results.append(metrics)

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        filtered_results.sort(key=lambda x: x.comprehensive_score, reverse=True)

        return filtered_results[:top_n]


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œæ‰¹é‡é…ç½®"""
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="ä¸“ä¸šçº§å› å­ç­›é€‰ç³»ç»Ÿ")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--symbol", type=str, default="0700.HK", help="è‚¡ç¥¨ä»£ç ")
    parser.add_argument("--timeframe", type=str, default="60min", help="æ—¶é—´æ¡†æ¶")

    args = parser.parse_args()

    if args.config:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶
        try:
            from config_manager import ConfigManager

            manager = ConfigManager()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡é…ç½®
            import yaml

            with open(args.config, "r", encoding="utf-8") as f:
                config_data = yaml.safe_load(f)

            if "batch_name" in config_data:
                # æ‰¹é‡é…ç½® - ç›´æ¥å¤„ç†æ‰€æœ‰å­é…ç½®
                import yaml

                with open(args.config, "r", encoding="utf-8") as f:
                    batch_config = yaml.safe_load(f)

                print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç­›é€‰: {batch_config['batch_name']}")
                print(f"ğŸ“Š å­ä»»åŠ¡æ•°é‡: {len(batch_config['screening_configs'])}")
                print("=" * 80)

                successful_tasks = 0
                failed_tasks = 0
                all_results = {}  # æ”¶é›†æ‰€æœ‰ç»“æœ

                print(f"\nğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶æ‰¹é‡åˆ†æ: {batch_config['batch_name']}")
                print(f"ğŸ“Š åˆ†ææ—¶é—´æ¡†æ¶æ•°é‡: {len(batch_config['screening_configs'])}")
                print("=" * 80)

                # è·å–ç¬¬ä¸€ä¸ªé…ç½®çš„æ•°æ®æ ¹å’Œè¾“å‡ºç›®å½•
                first_config = batch_config["screening_configs"][0]
                data_root = first_config.get("data_root", "../factor_output")
                output_dir = first_config.get("output_dir", "./screening_results")

                print(f"ğŸ“ æ•°æ®ç›®å½•: {data_root}")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

                # P2-2ä¿®å¤ï¼šå¢å¼ºæ‰¹é‡å¤„ç†ä¿¡æ¯é€æ˜åº¦
                print("\nğŸ“‹ æ‰¹é‡å¤„ç†è¯¦ç»†ä¿¡æ¯:")
                print(
                    f"  - é¢„è®¡å¤„ç†æ—¶é—´: ~{len(batch_config['screening_configs']) * 2}åˆ†é’Ÿ"
                )
                print("  - å†…å­˜ä½¿ç”¨é¢„ä¼°: ~500MB")
                print(
                    f"  - å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if batch_config.get('enable_parallel', True) else 'ç¦ç”¨'}"
                )
                print(f"  - å·¥ä½œè¿›ç¨‹æ•°: {batch_config.get('max_workers', 4)}")
                print("=" * 80)

                # åˆ›å»ºç»Ÿä¸€çš„æ‰¹é‡ç­›é€‰å™¨
                batch_screener = ProfessionalFactorScreener(data_root=data_root)
                batch_screener.screening_results_dir = Path(output_dir)

                # P0-1ä¿®å¤ï¼šåˆ›å»ºç»Ÿä¸€çš„æ‰¹é‡ä¼šè¯ç›®å½•ï¼Œé¿å…é‡å¤åˆ›å»º
                from datetime import datetime

                batch_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                batch_session_id = (
                    f"{batch_config['batch_name']}_multi_timeframe_{batch_timestamp}"
                )

                # è®¾ç½®å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•ï¼ˆå…³é”®ä¿®å¤ï¼‰
                batch_screener.multi_tf_session_dir = (
                    batch_screener.screening_results_dir / batch_session_id
                )
                batch_screener.multi_tf_session_dir.mkdir(parents=True, exist_ok=True)
                batch_screener.session_timestamp = batch_timestamp

                # åˆ›å»ºæ—¶é—´æ¡†æ¶å­ç›®å½•ç»“æ„
                timeframes_dir = batch_screener.multi_tf_session_dir / "timeframes"
                timeframes_dir.mkdir(exist_ok=True)

                print(f"ğŸ“ æ‰¹é‡ä¼šè¯ç›®å½•: {batch_screener.multi_tf_session_dir}")
                print(f"ğŸ“ æ—¶é—´æ¡†æ¶å­ç›®å½•: {timeframes_dir}")

                for i, sub_config in enumerate(batch_config["screening_configs"], 1):
                    try:
                        # P2-2ä¿®å¤ï¼šå¢å¼ºä¸­é—´æ­¥éª¤å¯è§‚æµ‹æ€§
                        start_time = time.time()
                        print(
                            f"\nğŸ“Š [{i}/{len(batch_config['screening_configs'])}] å¤„ç†: {sub_config['name']}"
                        )
                        print(f"   è‚¡ç¥¨: {sub_config['symbols'][0]}")
                        print(f"   æ—¶é—´æ¡†æ¶: {sub_config['timeframes'][0]}")
                        from datetime import datetime, timedelta

                        print(f"   å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
                        print(
                            "   é¢„è®¡å®Œæˆæ—¶é—´: "
                            f"{(datetime.now() + timedelta(minutes=2)).strftime('%H:%M:%S')}"
                        )
                        print("   " + "-" * 50)

                        # ä½¿ç”¨åŒä¸€ä¸ªç­›é€‰å™¨æ‰§è¡Œç­›é€‰ï¼ˆå¤ç”¨ä¼šè¯ç›®å½•ï¼‰
                        result = batch_screener.screen_factors_comprehensive(
                            symbol=sub_config["symbols"][0],
                            timeframe=sub_config["timeframes"][0],
                        )

                        # æ”¶é›†ç»“æœ - result æ˜¯ Dict[str, FactorMetrics]
                        tf_key = (
                            f"{sub_config['symbols'][0]}_{sub_config['timeframes'][0]}"
                        )
                        all_results[tf_key] = result

                        # P2-2ä¿®å¤ï¼šè¯¦ç»†å®ŒæˆæŠ¥å‘Š
                        end_time = time.time()
                        duration = end_time - start_time
                        significant_count = sum(
                            metric.is_significant for metric in result.values()
                        )
                        high_score_count = sum(
                            metric.comprehensive_score >= 0.6
                            for metric in result.values()
                        )

                        successful_tasks += 1
                        print(f"   âœ… å®Œæˆ: è€—æ—¶ {duration:.1f}ç§’")
                        print(f"      - æ€»å› å­: {len(result)}")
                        print(f"      - æ˜¾è‘—å› å­: {significant_count}")
                        print(f"      - é«˜åˆ†å› å­: {high_score_count}")
                        print(
                            f"      - å®Œæˆæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"
                        )

                        # è¿›åº¦æ¡æ˜¾ç¤º
                        progress = i / len(batch_config["screening_configs"]) * 100
                        total_msg = (
                            "   ğŸ“ˆ æ€»ä½“è¿›åº¦: "
                            f"{progress:.1f}% ({i}/{len(batch_config['screening_configs'])})"
                        )
                        self.logger.info(total_msg)

                    except Exception as e:
                        failed_tasks += 1
                        print(f"   âŒ å¤±è´¥: {str(e)}")
                        continue

                # ç”Ÿæˆç»Ÿä¸€çš„å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š
                print("\nğŸ“ˆ ç”Ÿæˆç»Ÿä¸€æ±‡æ€»æŠ¥å‘Š...")
                if all_results:
                    _generate_multi_timeframe_summary(
                        batch_screener.multi_tf_session_dir,
                        batch_config["batch_name"],
                        all_results,
                        batch_config["screening_configs"],
                        logger=batch_screener.logger,
                    )

                print("\nâœ… å¤šæ—¶é—´æ¡†æ¶åˆ†æå®Œæˆ:")
                print(f"   æ€»ä»»åŠ¡: {len(batch_config['screening_configs'])}")
                print(f"   æˆåŠŸ: {successful_tasks}")
                print(f"   å¤±è´¥: {failed_tasks}")
                if all_results:
                    print(
                        f"   ğŸ“Š æ±‡æ€»æŠ¥å‘Š: å·²ç”Ÿæˆåˆ°ç»Ÿä¸€ä¼šè¯ç›®å½•: {batch_screener.session_dir}"
                    )
                    print(
                        "   ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼Œæ— éœ€æŸ¥æ‰¾å¤šä¸ªæ—¶é—´æ¡†æ¶å­ç›®å½•"
                    )

                return
            else:
                # å•ä¸ªé…ç½®
                config = manager.load_config(args.config, config_type="screening")
                symbol = config.symbols[0] if config.symbols else args.symbol
                timeframe = (
                    config.timeframes[0] if config.timeframes else args.timeframe
                )
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    else:
        # æ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶æ—¶ï¼Œå°è¯•åŠ è½½é»˜è®¤é…ç½®
        default_config_path = (
            Path(__file__).parent / "configs" / "0700_multi_timeframe_config.yaml"
        )
        if default_config_path.exists():
            try:
                import yaml

                with open(default_config_path, "r", encoding="utf-8") as f:
                    config_data = yaml.safe_load(f)

                # å¦‚æœæ˜¯æ‰¹é‡é…ç½®ï¼ŒåŠ è½½ç¬¬ä¸€ä¸ªå­é…ç½®
                if "batch_name" in config_data and "screening_configs" in config_data:
                    first_sub_config = config_data["screening_configs"][0]
                    from config_manager import ScreeningConfig

                    config = ScreeningConfig(**first_sub_config)
                    print(f"âœ… è‡ªåŠ¨åŠ è½½é»˜è®¤é…ç½®: {default_config_path}")
                    print(f"ğŸ“ æ•°æ®ç›®å½•: {config.data_root}")
                    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
                else:
                    from config_manager import ScreeningConfig

                    config = ScreeningConfig(**config_data)
            except Exception as e:
                print(f"âš ï¸ é»˜è®¤é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®é…ç½®: {e}")
                from config_manager import ScreeningConfig
            config = ScreeningConfig(
                data_root="../factor_output",
                output_dir="./screening_results",
                ic_horizons=[1, 3, 5, 10, 20],
                min_sample_size=100,
                alpha_level=0.05,
                fdr_method="benjamini_hochberg",
                min_ic_threshold=0.02,
                min_ir_threshold=0.5,
            )
        symbol = args.symbol
        timeframe = args.timeframe

    # å•ä¸ªç­›é€‰ä»»åŠ¡
    screener = ProfessionalFactorScreener(config=config)

    print(f"å¼€å§‹ä¸“ä¸šçº§å› å­ç­›é€‰: {symbol} {timeframe}")
    print("=" * 80)

    try:
        # 5ç»´åº¦ç»¼åˆç­›é€‰
        results = screener.screen_factors_comprehensive(symbol, timeframe)

        # ç”ŸæˆæŠ¥å‘Š
        report_df = screener.generate_screening_report(results)

        # è·å–é¡¶çº§å› å­
        top_factors = screener.get_top_factors(results, top_n=10, min_score=0.6)

        # è¾“å‡ºç»“æœ
        print("\n5ç»´åº¦å› å­ç­›é€‰ç»“æœ:")
        print("=" * 80)
        print(f"æ€»å› å­æ•°é‡: {len(results)}")
        print(f"æ˜¾è‘—å› å­æ•°é‡: {sum(1 for m in results.values() if m.is_significant)}")
        print(
            f"é«˜åˆ†å› å­æ•°é‡ (>0.6): {sum(1 for m in results.values() if m.comprehensive_score > 0.6)}"
        )
        print(f"é¡¶çº§å› å­æ•°é‡: {len(top_factors)}")

        print("\nå‰10åé¡¶çº§å› å­:")
        print("-" * 120)
        print(
            f"{'æ’å':<4} {'å› å­åç§°':<20} {'ç»¼åˆå¾—åˆ†':<8} {'é¢„æµ‹':<6} {'ç¨³å®š':<6} {'ç‹¬ç«‹':<6} {'å®ç”¨':<6} {'é€‚åº”':<6} {'ICå‡å€¼':<8} {'æ˜¾è‘—æ€§':<6}"
        )
        print("-" * 120)

        for i, metrics in enumerate(top_factors[:10]):
            significance = (
                "***"
                if metrics.corrected_p_value < 0.001
                else (
                    "**"
                    if metrics.corrected_p_value < 0.01
                    else "*" if metrics.corrected_p_value < 0.05 else ""
                )
            )

            print(
                f"{i+1:<4} {metrics.name:<20} {metrics.comprehensive_score:.3f}    "
                f"{metrics.predictive_score:.3f}  {metrics.stability_score:.3f}  "
                f"{metrics.independence_score:.3f}  {metrics.practicality_score:.3f}  "
                f"{metrics.adaptability_score:.3f}  {metrics.ic_mean:+.4f}  {significance:<6}"
            )

        print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_df}")

    except Exception as e:
        print(f"ç­›é€‰å¤±è´¥: {str(e)}")
        raise


def _generate_multi_timeframe_summary(
    session_dir,
    batch_name: str,
    all_results: Dict,
    screening_configs: List[Dict],
    logger: Optional[logging.Logger] = None,
) -> None:
    """ç”Ÿæˆç»Ÿä¸€çš„å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

    Args:
        session_dir: ä¼šè¯ç›®å½•è·¯å¾„
        batch_name: æ‰¹é‡å¤„ç†åç§°
        all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        screening_configs: ç­›é€‰é…ç½®åˆ—è¡¨
    """
    from datetime import datetime

    summary_logger = logger or logging.getLogger(__name__)
    summary_logger.info("ğŸ“Š ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š...")

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
    if not all_results:
        summary_logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        return

    # åˆ›å»ºæ±‡æ€»æ•°æ®ç»“æ„
    summary_data = []
    best_factors_overall = []

    for tf_key, result in all_results.items():
        # result æ˜¯ Dict[str, FactorMetrics]
        if result and isinstance(result, dict):
            # è·å–æ—¶é—´æ¡†æ¶ä¿¡æ¯
            parts = tf_key.split("_")
            symbol = parts[0]
            timeframe = "_".join(parts[1:]) if len(parts) > 1 else "unknown"

            # ğŸ”§ ç­›é€‰æœ€ä½³å› å­ - å¿…é¡»æ»¡è¶³ç»Ÿè®¡æ˜¾è‘—æ€§ OR Tier 1/2
            sorted_factors = sorted(
                result.values(), key=lambda x: x.comprehensive_score, reverse=True
            )

            # æ ¸å¿ƒç­›é€‰é€»è¾‘ï¼šåªä¿ç•™ä¼˜ç§€å› å­ï¼ˆä¿®å¤ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³ç»Ÿè®¡æ˜¾è‘—å’ŒTierè¦æ±‚ï¼‰
            top_factors = [
                f
                for f in sorted_factors
                if (
                    f.is_significant  # å¿…é¡»ç»Ÿè®¡æ˜¾è‘—
                    and getattr(f, "tier", "N/A") in ["Tier 1", "Tier 2"]
                )  # ä¸”Tier 1/2
            ][
                :20
            ]  # æœ€å¤šå–20ä¸ªä¼˜ç§€å› å­

            for factor_metrics in top_factors:
                summary_item = {
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "factor_name": factor_metrics.name,
                    "comprehensive_score": factor_metrics.comprehensive_score,
                    "tier": getattr(factor_metrics, "tier", "N/A"),
                    "predictive_power": factor_metrics.predictive_score,
                    "stability": factor_metrics.stability_score,
                    "independence": factor_metrics.independence_score,
                    "practicality": factor_metrics.practicality_score,
                    "short_term_fitness": factor_metrics.adaptability_score,
                    "ic_mean": factor_metrics.ic_mean,
                    "ic_ir": factor_metrics.ic_ir,
                    "ic_win_rate": getattr(factor_metrics, "ic_win_rate", 0),
                    "rank_ic_mean": getattr(factor_metrics, "rank_ic_mean", 0),
                    "rank_ic_ir": getattr(factor_metrics, "rank_ic_ir", 0),
                    "turnover": getattr(factor_metrics, "turnover_rate", 0),
                    "p_value": factor_metrics.p_value,
                    "significant": factor_metrics.is_significant,
                }
                summary_data.append(summary_item)
                best_factors_overall.append(summary_item)

    if not summary_data:
        summary_logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        return

    # åˆ›å»ºæ±‡æ€»DataFrame
    import pandas as pd

    summary_df = pd.DataFrame(summary_data)

    # ä¿å­˜ç»Ÿä¸€æ±‡æ€»æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_filename = f"{batch_name}_multi_timeframe_summary_{timestamp}.csv"
    summary_path = session_dir / summary_filename

    summary_df.to_csv(summary_path, index=False, encoding="utf-8")
    summary_logger.info(
        "âœ… å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜", extra={"summary_path": str(summary_path)}
    )

    # ç”Ÿæˆæœ€ä½³å› å­ç»¼åˆæ’è¡Œ
    if best_factors_overall:
        best_df = pd.DataFrame(best_factors_overall)

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        best_df_sorted = best_df.sort_values("comprehensive_score", ascending=False)

        # ä¿å­˜æœ€ä½³å› å­æ’è¡Œ
        best_filename = f"{batch_name}_best_factors_overall_{timestamp}.csv"
        best_path = session_dir / best_filename
        best_df_sorted.to_csv(best_path, index=False, encoding="utf-8")
        summary_logger.info(
            "âœ… æœ€ä½³å› å­ç»¼åˆæ’è¡Œå·²ä¿å­˜", extra={"best_factors_path": str(best_path)}
        )

        # è¾“å‡ºTop 10æœ€ä½³å› å­åˆ°æ§åˆ¶å°ï¼ˆå…¨å±€ï¼‰
        print("\nğŸ† Top 10 æœ€ä½³å› å­ (è·¨æ‰€æœ‰æ—¶é—´æ¡†æ¶):")
        print("=" * 120)
        top_10 = best_df_sorted.head(10)
        for i, (_, factor) in enumerate(top_10.iterrows(), 1):
            print(
                f"{i:2d}. {factor['factor_name']:<25} | "
                f"{factor['symbol']}-{factor['timeframe']:<10} | "
                f"è¯„åˆ†: {factor['comprehensive_score']:.3f} | "
                f"ç­‰çº§: {factor['tier']:<2} | "
                f"IC: {factor['ic_mean']:.3f} | "
                f"èƒœç‡: {factor['ic_win_rate']:.1%}"
            )

        # ğŸ†• è¾“å‡ºå„æ—¶é—´æ¡†æ¶Top 5ï¼ˆåˆ†å±‚å±•ç¤ºï¼Œé¿å…é«˜é¢‘ç»Ÿæ²»ï¼‰
        print("\nğŸ“Š å„æ—¶é—´æ¡†æ¶ Top 5 æœ€ä½³å› å­ï¼ˆåˆ†å±‚å¯¹æ¯”ï¼‰:")
        print("=" * 120)
        all_timeframes = best_df_sorted["timeframe"].unique()
        for tf in all_timeframes:
            tf_data = best_df_sorted[best_df_sorted["timeframe"] == tf]
            tier2_count = (tf_data["tier"] == "Tier 2").sum()
            tier1_count = (tf_data["tier"] == "Tier 1").sum()

            print(
                f"\nã€{tf}ã€‘ (Tier1: {tier1_count}, Tier2: {tier2_count}, æ€»è®¡: {len(tf_data)})"
            )
            print("-" * 120)
            top_5 = tf_data.head(5)
            for i, (_, factor) in enumerate(top_5.iterrows(), 1):
                print(
                    f"  {i}. {factor['factor_name']:<25} | "
                    f"è¯„åˆ†: {factor['comprehensive_score']:.3f} | "
                    f"ç­‰çº§: {factor['tier']:<8} | "
                    f"IC: {factor['ic_mean']:>6.3f} | "
                    f"èƒœç‡: {factor['ic_win_rate']:>5.1%}"
                )

    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    _generate_batch_statistics(
        session_dir,
        batch_name,
        all_results,
        timestamp,
        logger=logger,
    )


def _generate_batch_statistics(
    session_dir,
    batch_name: str,
    all_results: Dict[str, Dict[str, FactorMetrics]],
    timestamp: str,
    logger: Optional[logging.Logger] = None,
) -> None:
    """ç”Ÿæˆæ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦"""

    from datetime import datetime

    import pandas as pd

    stats_logger = logger or logging.getLogger(__name__)
    stats_logger.info("ğŸ“ˆ ç”Ÿæˆæ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦...", extra={"batch_name": batch_name})

    stats_data: List[Dict[str, Any]] = []
    overall_totals = Counter()

    for tf_key, tf_results in all_results.items():
        if not tf_results:
            continue

        symbol = tf_key.split("_")[0]
        timeframe = tf_key[len(symbol) + 1 :] if "_" in tf_key else "unknown"

        total_count = len(tf_results)
        if total_count == 0:
            continue

        tier_counter = Counter(
            (metrics.tier or "æœªåˆ†çº§") for metrics in tf_results.values()
        )
        significant_count = sum(
            metrics.is_significant for metrics in tf_results.values()
        )
        top_count = sum(
            metrics.comprehensive_score >= 0.8 for metrics in tf_results.values()
        )
        avg_score = float(
            np.mean([metrics.comprehensive_score for metrics in tf_results.values()])
        )

        stats_item = {
            "symbol": symbol,
            "timeframe": timeframe,
            "total_factors": total_count,
            "significant_factors": significant_count,
            "tier1_factors": tier_counter.get("Tier 1", 0),
            "tier2_factors": tier_counter.get("Tier 2", 0),
            "tier3_factors": tier_counter.get("Tier 3", 0),
            "not_recommended": tier_counter.get("ä¸æ¨è", 0),
            "top_factors": top_count,
            "average_score": round(avg_score, 6),
        }
        stats_data.append(stats_item)

        overall_totals.update(
            {
                "total_factors": total_count,
                "significant_factors": significant_count,
                "tier1_factors": stats_item["tier1_factors"],
                "tier2_factors": stats_item["tier2_factors"],
                "tier3_factors": stats_item["tier3_factors"],
                "not_recommended": stats_item["not_recommended"],
                "top_factors": top_count,
            }
        )

        stats_logger.info("æ—¶é—´æ¡†æ¶ç»Ÿè®¡", extra={**stats_item})

    if not stats_data:
        stats_logger.warning("âš ï¸ æ‰¹é‡ç»Ÿè®¡æ‘˜è¦æ— æ•°æ®")
        return

    stats_df = pd.DataFrame(stats_data)

    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦ CSV
    stats_filename = f"{batch_name}_batch_statistics_{timestamp}.csv"
    stats_path = session_dir / stats_filename
    stats_df.to_csv(stats_path, index=False, encoding="utf-8")
    stats_logger.info(
        "âœ… æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜",
        extra={"batch_statistics_path": str(stats_path)},
    )

    # ä¿å­˜ç»Ÿè®¡æ‘˜è¦ JSON
    summary_payload = {
        "batch_name": batch_name,
        "generated_at": datetime.now().isoformat(),
        "timeframes": stats_data,
        "overall": dict(overall_totals),
    }
    stats_json_path = session_dir / f"{batch_name}_batch_statistics_{timestamp}.json"
    with open(stats_json_path, "w", encoding="utf-8") as fp:
        json.dump(
            summary_payload, fp, ensure_ascii=False, indent=2, cls=NumpyJSONEncoder
        )
    stats_logger.info(
        "âœ… æ‰¹é‡ç»Ÿè®¡JSONå·²ä¿å­˜",
        extra={"batch_statistics_json": str(stats_json_path)},
    )

    # æ§åˆ¶å°è¾“å‡ºï¼ˆä¿æŒåŸæœ‰äººæœºæç¤ºï¼‰
    print("\nğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦:")
    print("=" * 80)
    print(f"å¤„ç†æ—¶é—´æ¡†æ¶æ•°é‡: {len(stats_data)}")
    print(f"æ€»å¤„ç†å› å­æ•°: {overall_totals['total_factors']}")
    print(f"æ€»æ˜¾è‘—å› å­æ•°: {overall_totals['significant_factors']}")
    print(f"æ€»Tier 1å› å­æ•°: {overall_totals['tier1_factors']}")
    print(f"æ€»Tier 2å› å­æ•°: {overall_totals['tier2_factors']}")

    total_factors = overall_totals["total_factors"] or 1
    overall_tier1_ratio = overall_totals["tier1_factors"] / total_factors
    overall_tier2_ratio = overall_totals["tier2_factors"] / total_factors

    print(f"æ•´ä½“Tier 1æ¯”ä¾‹: {overall_tier1_ratio:.1%}")
    print(f"æ•´ä½“Tier 2æ¯”ä¾‹: {overall_tier2_ratio:.1%}")

    stats_logger.info(
        "æ•´ä½“Tieræ¯”ä¾‹",
        extra={
            "overall_tier1_ratio": overall_tier1_ratio,
            "overall_tier2_ratio": overall_tier2_ratio,
        },
    )

    print("\nå„æ—¶é—´æ¡†æ¶è¯¦ç»†ç»Ÿè®¡:")
    for stats_item in stats_data:
        print(
            f"  {stats_item['symbol']}-{stats_item['timeframe']:>8}: "
            f"æ€»è®¡ {stats_item['total_factors']:>3} ä¸ª | "
            f"æ˜¾è‘— {stats_item['significant_factors']:>3} ä¸ª | "
            f"Tier1 {stats_item['tier1_factors']:>2} ä¸ª | "
            f"Tier2 {stats_item['tier2_factors']:>2} ä¸ª"
        )

    stats_logger.info(
        "âœ… æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦ç”Ÿæˆå®Œæˆ",
        extra={
            "timeframe_count": len(stats_data),
            "total_factors": overall_totals["total_factors"],
            "significant_factors": overall_totals["significant_factors"],
        },
    )


class ProfessionalFactorScreenerEnhanced(ProfessionalFactorScreener):
    """ğŸ”§ å¢å¼ºç‰ˆç­›é€‰å™¨ï¼šé›†æˆdata_loader_patchæ”¹è¿›"""

    def load_factors_v2(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """
        ğŸ”§ é›†æˆç‰ˆï¼šåŠ è½½å› å­æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨data_loader_patchçš„æ”¹è¿›ç‰ˆæœ¬
        """
        # ğŸ”§ ä¼˜å…ˆå°è¯•ä½¿ç”¨data_loader_patchçš„æ”¹è¿›ç‰ˆæœ¬
        try:
            from data_loader_patch import load_factors_v2 as patch_load_factors

            return patch_load_factors(self, symbol, timeframe)
        except (ImportError, NameError):
            # å¦‚æœè¡¥ä¸ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
            self.logger.warning("data_loader_patchä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹å› å­åŠ è½½æ–¹æ³•")
            return super().load_factors(symbol, timeframe)

    def load_price_data_v2(self, symbol: str, timeframe: str = None) -> pd.DataFrame:
        """
        ğŸ”§ é›†æˆç‰ˆï¼šåŠ è½½ä»·æ ¼æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨data_loader_patchçš„æ”¹è¿›ç‰ˆæœ¬
        """
        # ğŸ”§ ä¼˜å…ˆå°è¯•ä½¿ç”¨data_loader_patchçš„æ”¹è¿›ç‰ˆæœ¬
        try:
            from data_loader_patch import load_price_data_v2 as patch_load_price_data

            return patch_load_price_data(self, symbol, timeframe)
        except (ImportError, NameError):
            # å¦‚æœè¡¥ä¸ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
            self.logger.warning("data_loader_patchä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ä»·æ ¼åŠ è½½æ–¹æ³•")
            return super().load_price_data(symbol, timeframe)


# ä¸ºäº†å‘åå…¼å®¹ï¼Œåˆ›å»ºå·¥å‚å‡½æ•°
def create_enhanced_screener(
    data_root: Optional[str] = None, config: Optional[ScreeningConfig] = None
):
    """
    åˆ›å»ºå¢å¼ºç‰ˆç­›é€‰å™¨å®ä¾‹

    Returns:
        é›†æˆäº†data_loader_patchæ”¹è¿›çš„ç­›é€‰å™¨å®ä¾‹
    """
    return ProfessionalFactorScreenerEnhanced(data_root=data_root, config=config)


if __name__ == "__main__":
    main()
