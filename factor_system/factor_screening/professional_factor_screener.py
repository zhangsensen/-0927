#!/usr/bin/env python3
"""
ä¸“ä¸šçº§é‡åŒ–äº¤æ˜“å› å­ç­›é€‰ç³»ç»Ÿ - 5ç»´åº¦ç­›é€‰æ¡†æ¶
ä½œè€…ï¼šé‡åŒ–é¦–å¸­å·¥ç¨‹å¸ˆ
ç‰ˆæœ¬ï¼š2.0.0
æ—¥æœŸï¼š2025-09-29

æ ¸å¿ƒç‰¹æ€§ï¼š
1. 5ç»´åº¦ç­›é€‰æ¡†æ¶ï¼šé¢„æµ‹èƒ½åŠ›ã€ç¨³å®šæ€§ã€ç‹¬ç«‹æ€§ã€å®ç”¨æ€§ã€çŸ­å‘¨æœŸé€‚åº”æ€§
2. å¤šå‘¨æœŸICåˆ†æï¼š1æ—¥ã€3æ—¥ã€5æ—¥ã€10æ—¥ã€20æ—¥é¢„æµ‹èƒ½åŠ›è¯„ä¼°
3. ä¸¥æ ¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼šBenjamini-Hochberg FDRæ ¡æ­£
4. VIFæ£€æµ‹å’Œä¿¡æ¯å¢é‡åˆ†æ
5. äº¤æ˜“æˆæœ¬å’ŒæµåŠ¨æ€§è¯„ä¼°
6. ç”Ÿäº§çº§æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
"""

import json
import logging
import time
import warnings
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# å¯¼å…¥é…ç½®ç±»
from config_manager import ScreeningConfig  # type: ignore

import numpy as np
import pandas as pd
import psutil
import yaml
from scipy import stats

# å¯¼å…¥å› å­å¯¹é½å·¥å…·
try:
    from utils import (  # type: ignore
        FactorFileAligner,
        find_aligned_factor_files,
        validate_factor_alignment,
    )
except ImportError as e:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæä¾›å›é€€æ–¹æ¡ˆ
    import logging
    logging.getLogger(__name__).warning(f"å› å­å¯¹é½å·¥å…·å¯¼å…¥å¤±è´¥: {e}")
    FactorFileAligner = None

    def find_aligned_factor_files(*args: Any, **kwargs: Any) -> None:
        raise ImportError("å› å­å¯¹é½å·¥å…·ä¸å¯ç”¨")

    def validate_factor_alignment(*args: Any, **kwargs: Any) -> Tuple[bool, str]:
        return True, "å¯¹é½éªŒè¯å·¥å…·ä¸å¯ç”¨"

try:
    from utils.temporal_validator import TemporalValidationError, TemporalValidator
except ImportError as e:  # pragma: no cover - è¿è¡Œç¯å¢ƒç¼ºå¤±
    TemporalValidator = None  # type: ignore

    class TemporalValidationError(Exception):
        """æ—¶é—´åºåˆ—éªŒè¯å™¨ä¸å¯ç”¨æ—¶çš„åå¤‡å¼‚å¸¸"""

        pass


warnings.filterwarnings("ignore")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class FactorMetrics:
    """å› å­ç»¼åˆæŒ‡æ ‡"""

    name: str

    # é¢„æµ‹èƒ½åŠ›æŒ‡æ ‡
    ic_1d: float = 0.0
    ic_3d: float = 0.0
    ic_5d: float = 0.0
    ic_10d: float = 0.0
    ic_20d: float = 0.0
    ic_mean: float = 0.0
    ic_std: float = 0.0
    ic_ir: float = 0.0
    ic_decay_rate: float = 0.0
    ic_longevity: int = 0
    predictive_power_mean_ic: float = 0.0  # æ·»åŠ ç¼ºå¤±å­—æ®µ

    # ç¨³å®šæ€§æŒ‡æ ‡
    rolling_ic_mean: float = 0.0
    rolling_ic_std: float = 0.0
    rolling_ic_stability: float = 0.0
    ic_consistency: float = 0.0
    cross_section_stability: float = 0.0

    # ç‹¬ç«‹æ€§æŒ‡æ ‡
    vif_score: float = 0.0
    correlation_max: float = 0.0
    information_increment: float = 0.0
    redundancy_score: float = 0.0

    # å®ç”¨æ€§æŒ‡æ ‡
    turnover_rate: float = 0.0
    transaction_cost: float = 0.0
    cost_efficiency: float = 0.0
    liquidity_demand: float = 0.0
    capacity_score: float = 0.0

    # çŸ­å‘¨æœŸé€‚åº”æ€§æŒ‡æ ‡
    reversal_effect: float = 0.0
    momentum_persistence: float = 0.0
    volatility_sensitivity: float = 0.0
    regime_adaptability: float = 0.0

    # ç»Ÿè®¡æ˜¾è‘—æ€§
    p_value: float = 1.0
    corrected_p_value: float = 1.0
    is_significant: bool = False

    # ç»¼åˆè¯„åˆ†
    predictive_score: float = 0.0
    stability_score: float = 0.0
    independence_score: float = 0.0
    practicality_score: float = 0.0
    adaptability_score: float = 0.0
    comprehensive_score: float = 0.0

    # å…ƒæ•°æ®
    sample_size: int = 0
    calculation_time: float = 0.0
    data_quality_score: float = 0.0

    # å› å­åˆ†ç±»ä¿¡æ¯
    tier: str = ""
    type: str = ""
    description: str = ""


# ScreeningConfigç±»å·²ç§»è‡³config_manager.pyï¼Œé¿å…é‡å¤å®šä¹‰


class ProfessionalFactorScreener:
    """ä¸“ä¸šçº§å› å­ç­›é€‰å™¨ - 5ç»´åº¦ç­›é€‰æ¡†æ¶"""

    @staticmethod
    def _to_json_serializable(obj: Any) -> Any:
        """è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼ - æç®€å®ç°"""
        if isinstance(obj, dict):
            return {
                k: ProfessionalFactorScreener._to_json_serializable(v)
                for k, v in obj.items()
            }
        elif isinstance(obj, (list, tuple)):
            return [
                ProfessionalFactorScreener._to_json_serializable(item) for item in obj
            ]
        elif isinstance(obj, (np.integer, np.floating, np.bool_)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, "shape"):  # pandaså¯¹è±¡
            return str(obj)
        return obj

    def __init__(self, data_root: Optional[str] = None, config: Optional[ScreeningConfig] = None):
        """åˆå§‹åŒ–ç­›é€‰å™¨

        Args:
            data_root: å‘åå…¼å®¹å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨configä¸­çš„è·¯å¾„é…ç½®
            config: ç­›é€‰é…ç½®å¯¹è±¡
        """
        self.config = config or ScreeningConfig()

        # è·¯å¾„ä¼˜å…ˆçº§: config.data_root > data_rootå‚æ•° > é»˜è®¤å€¼
        if hasattr(self.config, "data_root") and self.config.data_root:
            self.data_root = Path(self.config.data_root)
        elif data_root:
            self.data_root = Path(data_root)
        else:
            self.data_root = Path("../å› å­è¾“å‡º")  # é»˜è®¤å› å­æ•°æ®ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰

        # è®¾ç½®æ—¥å¿—å’Œç¼“å­˜è·¯å¾„
        self.log_root = Path(getattr(self.config, "log_root", "./logs/screening"))
        self.cache_dir = Path(
            getattr(self.config, "cache_root", self.data_root / "cache")
        )

        # è®¾ç½®ç­›é€‰æŠ¥å‘Šä¸“ç”¨ç›®å½•
        if hasattr(self.config, "output_dir") and self.config.output_dir:
            self.screening_results_dir = Path(self.config.output_dir)
        else:
            self.screening_results_dir = Path("./å› å­ç­›é€‰")
        self.screening_results_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä¼šè¯ç›¸å…³å˜é‡ï¼ˆç¨ååœ¨screen_factors_comprehensiveä¸­åˆ›å»ºï¼‰
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = None

        # è®¾ç½®æ—¥å¿—å’Œç¼“å­˜è·¯å¾„ï¼ˆå…ˆä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        self.log_root = Path(getattr(self.config, "log_root", "./logs/screening"))
        self.cache_dir = Path(
            getattr(self.config, "cache_root", self.data_root / "cache")
        )

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.log_root.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger(self.session_timestamp)

        # åˆå§‹åŒ–æ—¶é—´åºåˆ—éªŒè¯å™¨ï¼Œç¡®ä¿æ—§ä»£ç åŒæ ·å—é˜²æŠ¤
        self.temporal_validator = None
        if TemporalValidator is not None:
            try:
                self.temporal_validator = TemporalValidator(strict_mode=True)
                self.logger.info("âœ… æ—¶é—´åºåˆ—éªŒè¯å™¨å·²å¯ç”¨")
            except Exception as validator_error:
                self.logger.warning(
                    "æ—¶é—´åºåˆ—éªŒè¯å™¨åˆå§‹åŒ–å¤±è´¥: %s", validator_error
                )
                self.temporal_validator = None
        else:
            self.logger.warning("æ—¶é—´åºåˆ—éªŒè¯å™¨æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡è¿è¡Œæ—¶æ ¡éªŒ")

        # åˆå§‹åŒ–å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨
        try:
            from enhanced_result_manager import EnhancedResultManager  # type: ignore

            self.result_manager = EnhancedResultManager(str(self.screening_results_dir))
            self.logger.info("âœ… å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            self.result_manager = None
            self.logger.warning(f"å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨å¯¼å…¥å¤±è´¥: {e}")
            self.logger.info("å°†ä½¿ç”¨ä¼ ç»Ÿæ–‡ä»¶ä¿å­˜æ–¹å¼")

        # æ€§èƒ½ç›‘æ§
        self.process = psutil.Process()
        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        self.logger.info("ä¸“ä¸šçº§å› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(
            f"é…ç½®: ICå‘¨æœŸ={self.config.ic_horizons}, æœ€å°æ ·æœ¬={self.config.min_sample_size}"
        )
        self.logger.info(
            f"æ˜¾è‘—æ€§æ°´å¹³={self.config.alpha_level}, FDRæ–¹æ³•={self.config.fdr_method}"
        )

    def _setup_logger(self, session_timestamp: Optional[str] = None) -> logging.Logger:
        """è®¾ç½®ä¸“ä¸šçº§æ—¥å¿—ç³»ç»Ÿ - æ”¹è¿›ç‰ˆ"""
        logger = logging.getLogger(f"{__name__}.{id(self)}")
        logger.setLevel(logging.INFO)

        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # ä½¿ç”¨æ—¥å¿—è½®è½¬ - å…³é”®ä¿®å¤
        from logging.handlers import RotatingFileHandler

        # æ”¯æŒä¼šè¯æ—¶é—´æˆ³æˆ–æ—¥æœŸå‘½å
        if session_timestamp:
            log_filename = f"professional_screener_{session_timestamp}.log"
        else:
            today = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_filename = f"professional_screener_{today}.log"

        log_file = self.log_root / log_filename

        # ä¿å­˜å½“å‰æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œæ–¹ä¾¿å…¶ä»–åœ°æ–¹è®¿é—®
        self.current_log_file = str(log_file)

        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10 * 1024 * 1024,  # 10MB
            backupCount=5,  # ä¿ç•™5ä¸ªå¤‡ä»½
            encoding="utf-8",
        )
        file_handler.setLevel(logging.DEBUG)

        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s"
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        return logger

    def _generate_factor_metadata(self, factors_df: pd.DataFrame) -> dict:
        """ç”Ÿæˆå› å­å…ƒæ•°æ®"""
        metadata = {}

        for factor_name in factors_df.columns:
            meta = {
                "name": factor_name,
                "type": self._infer_factor_type(factor_name),
                "warmup_period": self._infer_warmup_period(factor_name),
                "description": self._generate_factor_description(factor_name),
            }

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            factor_data = factors_df[factor_name]
            meta.update(
                {
                    "total_periods": len(factor_data),
                    "missing_periods": factor_data.isnull().sum(),
                    "missing_ratio": factor_data.isnull().sum() / len(factor_data),
                    "first_valid_index": self._find_first_non_missing_index(
                        factor_data
                    ),
                    "valid_ratio": 1 - (factor_data.isnull().sum() / len(factor_data)),
                }
            )

            metadata[factor_name] = meta

        return metadata

    def _infer_factor_type(self, factor_name: str) -> str:
        """æ ¹æ®å› å­åç§°æ¨æ–­ç±»å‹"""
        name_lower = factor_name.lower()

        if any(
            indicator in name_lower
            for indicator in [
                "ma",
                "ema",
                "sma",
                "wma",
                "dema",
                "tema",
                "trima",
                "kama",
                "t3",
            ]
        ):
            return "trend"
        elif any(
            indicator in name_lower
            for indicator in ["rsi", "stoch", "cci", "willr", "mfi", "roc", "mom"]
        ):
            return "momentum"
        elif any(
            indicator in name_lower
            for indicator in ["bb", "bollinger", "atr", "std", "mstd"]
        ):
            return "volatility"
        elif any(indicator in name_lower for indicator in ["volume", "obv", "vwap"]):
            return "volume"
        elif any(
            indicator in name_lower
            for indicator in ["macd", "signal", "histogram", "hist"]
        ):
            return "momentum"
        elif any(indicator in name_lower for indicator in ["cdl", "pattern"]):
            return "pattern"
        else:
            return "unknown"

    def _infer_warmup_period(self, factor_name: str) -> int:
        """æ ¹æ®å› å­åç§°æ¨æ–­é¢„çƒ­æœŸ"""
        name_lower = factor_name.lower()

        # ç§»åŠ¨å¹³å‡ç±» - ç²¾ç¡®åŒ¹é…æ•°å­—
        import re

        ma_match = re.search(r"ma(\d+)", name_lower)
        if ma_match:
            return int(ma_match.group(1))

        sma_match = re.search(r"sma_?(\d+)", name_lower)
        if sma_match:
            return int(sma_match.group(1))

        ema_match = re.search(r"ema_?(\d+)", name_lower)
        if ema_match:
            return int(ema_match.group(1))

        # RSIç±»
        rsi_match = re.search(r"rsi(\d+)", name_lower)
        if rsi_match:
            return int(rsi_match.group(1))
        elif "rsi" in name_lower:
            return 14

        # å¸ƒæ—å¸¦ç±»
        bb_match = re.search(r"bb_(\d+)", name_lower)
        if bb_match:
            return int(bb_match.group(1))
        elif "bb" in name_lower or "bollinger" in name_lower:
            return 20

        # MACDç±»
        if "macd" in name_lower:
            return 26

        # CCIç±»
        cci_match = re.search(r"cci(\d+)", name_lower)
        if cci_match:
            return int(cci_match.group(1))
        elif "cci" in name_lower:
            return 20

        # WILLRç±»
        willr_match = re.search(r"willr(\d+)", name_lower)
        if willr_match:
            return int(willr_match.group(1))
        elif "willr" in name_lower:
            return 14

        # ATRç±»
        atr_match = re.search(r"atr(\d+)", name_lower)
        if atr_match:
            return int(atr_match.group(1))
        elif "atr" in name_lower:
            return 14

        # é»˜è®¤é¢„çƒ­æœŸ
        return 20

    def _generate_factor_description(self, factor_name: str) -> str:
        """ç”Ÿæˆå› å­æè¿°"""
        name_lower = factor_name.lower()

        if "ma" in name_lower:
            return f"ç§»åŠ¨å¹³å‡çº¿æŒ‡æ ‡ - {factor_name}"
        elif "rsi" in name_lower:
            return f"ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ - {factor_name}"
        elif "macd" in name_lower:
            return f"ç§»åŠ¨å¹³å‡æ”¶æ•›æ•£åº¦æŒ‡æ ‡ - {factor_name}"
        elif "bb" in name_lower:
            return f"å¸ƒæ—å¸¦æŒ‡æ ‡ - {factor_name}"
        elif "volume" in name_lower:
            return f"æˆäº¤é‡æŒ‡æ ‡ - {factor_name}"
        else:
            return f"æŠ€æœ¯æŒ‡æ ‡ - {factor_name}"

    def _find_first_non_missing_index(self, series: pd.Series) -> int:
        """æ‰¾åˆ°ç¬¬ä¸€ä¸ªéç¼ºå¤±å€¼çš„ç´¢å¼•ä½ç½®"""
        non_null_mask = series.notna()
        if non_null_mask.any():
            return non_null_mask.idxmax()
        return len(series)

    def _smart_forward_fill(self, series: pd.Series) -> pd.Series:
        """æ™ºèƒ½å‰å‘å¡«å……"""
        result = series.copy()

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
        first_valid_idx = series.first_valid_index()

        if first_valid_idx is not None:
            # ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……å‰é¢çš„ç¼ºå¤±å€¼
            first_valid_value = series.loc[first_valid_idx]
            result = result.bfill(limit=1)
            result = result.fillna(first_valid_value)

            # å‰å‘å¡«å……å‰©ä½™çš„ç¼ºå¤±å€¼
            result = result.ffill()

        return result

    def _smart_interpolation(self, series: pd.Series) -> pd.Series:
        """æ™ºèƒ½æ’å€¼"""
        result = series.copy()

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
        first_valid_idx = series.first_valid_index()

        if first_valid_idx is not None:
            # ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……å‰é¢çš„ç¼ºå¤±å€¼
            first_valid_value = series.loc[first_valid_idx]
            result.loc[:first_valid_idx] = result.loc[:first_valid_idx].fillna(
                first_valid_value
            )

            # å¯¹å‰©ä½™ç¼ºå¤±å€¼è¿›è¡Œçº¿æ€§æ’å€¼
            result = result.interpolate(method="linear")

            # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨å‰å‘å¡«å……
            result = result.ffill()
            result = result.bfill()

        return result

    def smart_missing_value_handling(
        self, factors_df: pd.DataFrame, factor_metadata: dict = None
    ) -> tuple:
        """
        æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†ï¼ŒåŒºåˆ†æ­£å¸¸é¢„çƒ­æœŸç¼ºå¤±å’Œé—®é¢˜æ•°æ®

        Args:
            factors_df: å› å­æ•°æ®DataFrame
            factor_metadata: å› å­å…ƒæ•°æ®ï¼ŒåŒ…å«é¢„çƒ­æœŸä¿¡æ¯

        Returns:
            tuple: (cleaned_df, handling_report)
        """
        if factor_metadata is None:
            factor_metadata = self._generate_factor_metadata(factors_df)

        handling_report = {
            "total_factors": len(factors_df.columns),
            "removed_factors": [],
            "handled_factors": [],
            "forward_filled_factors": [],
            "interpolated_factors": [],
            "dropped_factors": [],
        }

        # ç›´æ¥åœ¨åŸDataFrameä¸Šæ“ä½œï¼Œé¿å…ä¸å¿…è¦çš„å†…å­˜å¤åˆ¶
        cleaned_df = factors_df

        for factor_name in factors_df.columns:
            factor_data = factors_df[factor_name]
            missing_count = factor_data.isnull().sum()

            if missing_count == 0:
                handling_report["handled_factors"].append(factor_name)
                continue

            missing_ratio = missing_count / len(factor_data)

            # è·å–å› å­å…ƒæ•°æ®
            meta = factor_metadata.get(factor_name, {})
            warmup_period = meta.get("warmup_period", 20)
            factor_type = meta.get("type", "unknown")

            # åˆ¤æ–­ç¼ºå¤±å€¼æ¨¡å¼
            first_valid_idx = factor_data.first_valid_index()
            if first_valid_idx is not None:
                first_valid_pos = factor_data.index.get_loc(first_valid_idx)
            else:
                first_valid_pos = len(factor_data)

            # å†³ç­–é€»è¾‘
            if first_valid_pos <= warmup_period * 1.5:  # å…è®¸1.5å€çš„é¢„çƒ­æœŸå®¹å¿åº¦
                # æ­£å¸¸é¢„çƒ­æœŸç¼ºå¤±ï¼Œè¿›è¡Œæ™ºèƒ½å¡«å……
                if factor_type in ["momentum", "trend", "volatility"]:
                    # æŠ€æœ¯æŒ‡æ ‡ç±»å› å­ä½¿ç”¨å‰å‘å¡«å……
                    cleaned_df[factor_name] = self._smart_forward_fill(factor_data)
                    handling_report["forward_filled_factors"].append(factor_name)
                else:
                    # å…¶ä»–ç±»å‹å› å­ä½¿ç”¨æ’å€¼
                    cleaned_df[factor_name] = self._smart_interpolation(factor_data)
                    handling_report["interpolated_factors"].append(factor_name)

                handling_report["handled_factors"].append(factor_name)

            elif missing_ratio > self.config.max_missing_ratio:
                # ç¼ºå¤±æ¯”ä¾‹è¿‡é«˜ï¼Œåˆ é™¤
                cleaned_df = cleaned_df.drop(columns=[factor_name])
                handling_report["dropped_factors"].append(factor_name)
                handling_report["removed_factors"].append(factor_name)

            else:
                # éšæœºç¼ºå¤±ï¼Œä½¿ç”¨æ’å€¼
                cleaned_df[factor_name] = self._smart_interpolation(factor_data)
                handling_report["interpolated_factors"].append(factor_name)
                handling_report["handled_factors"].append(factor_name)

        return cleaned_df, handling_report

    def validate_factor_data_quality(
        self, factors_df: pd.DataFrame, symbol: str, timeframe: str
    ) -> pd.DataFrame:
        """æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯ - è§£å†³æ ¹æœ¬é—®é¢˜è€Œä¸æ˜¯ç²—æš´åˆ é™¤"""
        self.logger.info(f"å¼€å§‹æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯: {symbol} {timeframe}")

        original_shape = factors_df.shape
        issues_found = []

        # 1. æ£€æŸ¥éæ•°å€¼åˆ—ï¼ˆæ ¹æœ¬é—®é¢˜ï¼‰
        non_numeric_cols = factors_df.select_dtypes(
            exclude=[np.number, "datetime64[ns]"]
        ).columns.tolist()
        if non_numeric_cols:
            self.logger.warning(f"å‘ç°éæ•°å€¼åˆ—: {non_numeric_cols}")
            factors_df = factors_df.drop(columns=non_numeric_cols)
            issues_found.append(f"ç§»é™¤éæ•°å€¼åˆ—: {non_numeric_cols}")

        # 2. æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç† - æ ¸å¿ƒæ”¹è¿›
        if factors_df.isnull().any().any():
            self.logger.info("å¼€å§‹æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†...")
            factor_metadata = self._generate_factor_metadata(factors_df)
            factors_df, handling_report = self.smart_missing_value_handling(
                factors_df, factor_metadata
            )

            self.logger.info("æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†å®Œæˆ:")
            self.logger.info(f"  - æ€»å› å­æ•°: {handling_report['total_factors']}")
            self.logger.info(
                f"  - å‰å‘å¡«å……å› å­æ•°: {len(handling_report['forward_filled_factors'])}"
            )
            self.logger.info(
                f"  - æ’å€¼å¡«å……å› å­æ•°: {len(handling_report['interpolated_factors'])}"
            )
            self.logger.info(
                f"  - åˆ é™¤å› å­æ•°: {len(handling_report['removed_factors'])}"
            )

            if handling_report["removed_factors"]:
                issues_found.append(
                    f"æ™ºèƒ½å¤„ç†ç§»é™¤å› å­: {handling_report['removed_factors']}"
                )

        # 3. æ£€æŸ¥æ— ç©·å€¼ - ä¿®å¤è€Œä¸æ˜¯åˆ é™¤
        inf_cols = []
        for col in factors_df.select_dtypes(include=[np.number]).columns:
            if np.isinf(factors_df[col]).any():
                inf_cols.append(col)
                # ç”¨æå€¼æ›¿æ¢æ— ç©·å€¼
                factors_df[col] = factors_df[col].replace(
                    [np.inf, -np.inf],
                    [factors_df[col].quantile(0.99), factors_df[col].quantile(0.01)],
                )

        if inf_cols:
            self.logger.info(f"ä¿®å¤æ— ç©·å€¼åˆ—: {inf_cols}")
            issues_found.append(f"ä¿®å¤æ— ç©·å€¼åˆ—: {inf_cols}")

        # 4. æ£€æŸ¥å¸¸é‡åˆ—ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰
        constant_cols = []
        for col in factors_df.select_dtypes(include=[np.number]).columns:
            if factors_df[col].std() < 1e-6:  # åˆç†çš„å¸¸é‡æ£€æµ‹é˜ˆå€¼
                constant_cols.append(col)

        if constant_cols:
            self.logger.warning(f"å‘ç°å¸¸é‡åˆ—: {constant_cols}")
            factors_df = factors_df.drop(columns=constant_cols)
            issues_found.append(f"ç§»é™¤å¸¸é‡åˆ—: {constant_cols}")

        # 5. æ£€æŸ¥é‡å¤åˆ—
        duplicate_cols = factors_df.columns[factors_df.columns.duplicated()].tolist()
        if duplicate_cols:
            self.logger.warning(f"å‘ç°é‡å¤åˆ—: {duplicate_cols}")
            factors_df = factors_df.loc[:, ~factors_df.columns.duplicated()]
            issues_found.append(f"ç§»é™¤é‡å¤åˆ—: {duplicate_cols}")

        final_shape = factors_df.shape

        # æŠ¥å‘ŠéªŒè¯ç»“æœ
        retention_rate = final_shape[1] / original_shape[1]
        self.logger.info("æ™ºèƒ½æ•°æ®è´¨é‡éªŒè¯å®Œæˆ:")
        self.logger.info(f"  - åŸå§‹å½¢çŠ¶: {original_shape}")
        self.logger.info(f"  - æœ€ç»ˆå½¢çŠ¶: {final_shape}")
        self.logger.info(f"  - å› å­ä¿ç•™ç‡: {retention_rate:.1%}")

        if issues_found:
            for issue in issues_found:
                self.logger.info(f"  - {issue}")

        # ç¡®ä¿è¿˜æœ‰è¶³å¤Ÿçš„å› å­æ•°æ®
        if len(factors_df.columns) < 10:
            raise ValueError(f"éªŒè¯åå› å­æ•°é‡è¿‡å°‘: {len(factors_df.columns)} < 10")

        return factors_df

    def load_factors(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """åŠ è½½å› å­æ•°æ® - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå› å­æ–‡ä»¶å¯¹é½"""
        start_time = time.time()
        self.logger.info(f"åŠ è½½å› å­æ•°æ®: {symbol} {timeframe}")

        # ğŸ¯ ä¼˜å…ˆä½¿ç”¨å¯¹é½çš„å› å­æ–‡ä»¶
        if hasattr(self, "aligned_factor_files") and self.aligned_factor_files:
            if timeframe in self.aligned_factor_files:
                selected_file = self.aligned_factor_files[timeframe]
                self.logger.info(f"âœ… ä½¿ç”¨å¯¹é½çš„å› å­æ–‡ä»¶: {selected_file.name}")

                try:
                    # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨columnså‚æ•°é€‰æ‹©æ€§åŠ è½½
                    # ä»…è¯»å–æ•°å€¼åˆ—ï¼Œå‡å°‘å†…å­˜å ç”¨
                    factors = pd.read_parquet(
                        selected_file,
                        # æš‚ä¸æŒ‡å®šcolumnsï¼Œå› ä¸ºéœ€è¦å…ˆè¯»å–æ‰çŸ¥é“åˆ—å
                        # åç»­é€šè¿‡dropnaå’Œæ•°æ®ç±»å‹ç­›é€‰ä¼˜åŒ–
                    )

                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    if factors.empty:
                        self.logger.warning(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")
                        raise ValueError(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")

                    # å†…å­˜ä¼˜åŒ–ï¼šç«‹å³ç§»é™¤å…¨éƒ¨ä¸ºNaNçš„åˆ—
                    factors = factors.dropna(axis=1, how="all")

                    # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢æ•°æ®ç±»å‹ä»¥å‡å°‘å†…å­˜å ç”¨
                    for col in factors.select_dtypes(include=["float64"]).columns:
                        factors[col] = factors[col].astype("float32")

                    # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                    if not isinstance(factors.index, pd.DatetimeIndex):
                        factors.index = pd.to_datetime(factors.index)

                    # Linuså¼æ•°æ®è´¨é‡éªŒè¯
                    factors = self.validate_factor_data_quality(
                        factors, symbol, timeframe
                    )

                    initial_memory = factors.memory_usage(deep=True).sum() / 1024 / 1024
                    self.logger.info(
                        f"å› å­æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={factors.shape}, å†…å­˜={initial_memory:.1f}MB"
                    )
                    self.logger.info(
                        f"æ—¶é—´èŒƒå›´: {factors.index.min()} åˆ° {factors.index.max()}"
                    )
                    self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                    return factors

                except Exception as e:
                    self.logger.warning(f"åŠ è½½å¯¹é½æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤æœç´¢: {str(e)}")

        # å¤„ç†symbolæ ¼å¼
        clean_symbol = symbol.replace(".HK", "")

        # æœç´¢ç­–ç•¥ï¼šæŒ‰ä¼˜å…ˆçº§æœç´¢ä¸åŒæ ¼å¼çš„æ–‡ä»¶
        search_patterns = [
            # æ–°æ ¼å¼ï¼štimeframeå­ç›®å½• (å¸¦.HKåç¼€)
            (
                self.data_root / timeframe,
                f"{clean_symbol}.HK_{timeframe}_factors_*.parquet",
            ),
            (
                self.data_root / timeframe,
                f"{clean_symbol}HK_{timeframe}_factors_*.parquet",
            ),
            (
                self.data_root / timeframe,
                f"{clean_symbol}_{timeframe}_factors_*.parquet",
            ),
            # multi_tfæ ¼å¼
            (self.data_root, f"aligned_multi_tf_factors_{clean_symbol}*.parquet"),
            # æ ¹ç›®å½•æ ¼å¼
            (self.data_root, f"{clean_symbol}*_{timeframe}_factors_*.parquet"),
        ]

        for search_dir, pattern in search_patterns:
            if search_dir.exists():
                factor_files = list(search_dir.glob(pattern))
                if factor_files:
                    selected_file = factor_files[-1]  # é€‰æ‹©æœ€æ–°æ–‡ä»¶
                    self.logger.info(f"æ‰¾åˆ°å› å­æ–‡ä»¶: {selected_file}")

                    try:
                        # å†…å­˜ä¼˜åŒ–ï¼šé€‰æ‹©æ€§åŠ è½½
                        factors = pd.read_parquet(selected_file)

                        # å†…å­˜ä¼˜åŒ–ï¼šç«‹å³ç§»é™¤å…¨éƒ¨ä¸ºNaNçš„åˆ—å’Œè½¬æ¢æ•°æ®ç±»å‹
                        factors = factors.dropna(axis=1, how="all")
                        for col in factors.select_dtypes(include=["float64"]).columns:
                            factors[col] = factors[col].astype("float32")

                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        if factors.empty:
                            self.logger.warning(f"å› å­æ–‡ä»¶ä¸ºç©º: {selected_file}")
                            continue

                        # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
                        if not isinstance(factors.index, pd.DatetimeIndex):
                            factors.index = pd.to_datetime(factors.index)

                        # Linuså¼æ•°æ®è´¨é‡éªŒè¯ - è§£å†³æ ¹æœ¬é—®é¢˜
                        factors = self.validate_factor_data_quality(
                            factors, symbol, timeframe
                        )

                        self.logger.info(f"å› å­æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={factors.shape}")
                        self.logger.info(
                            f"æ—¶é—´èŒƒå›´: {factors.index.min()} åˆ° {factors.index.max()}"
                        )
                        self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                        return factors

                    except Exception as e:
                        self.logger.error(f"åŠ è½½å› å­æ–‡ä»¶å¤±è´¥ {selected_file}: {str(e)}")
                        continue

        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        self.logger.error("æœªæ‰¾åˆ°å› å­æ•°æ®:")
        self.logger.error(f"æœç´¢è·¯å¾„: {self.data_root}")
        self.logger.error(f"æœç´¢ç¬¦å·: {clean_symbol}")
        self.logger.error(f"æ—¶é—´æ¡†æ¶: {timeframe}")

        available_dirs = [d.name for d in self.data_root.iterdir() if d.is_dir()]
        self.logger.error(f"å¯ç”¨ç›®å½•: {available_dirs}")

        raise FileNotFoundError(f"No factor data found for {symbol} {timeframe}")

    def load_price_data(self, symbol: str, timeframe: str = None) -> pd.DataFrame:
        """åŠ è½½ä»·æ ¼æ•°æ® - æ™ºèƒ½åŒ¹é…æ—¶é—´æ¡†æ¶ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        start_time = time.time()
        self.logger.info(f"åŠ è½½ä»·æ ¼æ•°æ®: {symbol} (æ—¶é—´æ¡†æ¶: {timeframe})")

        # å¤„ç†symbolæ ¼å¼
        if symbol.endswith(".HK"):
            clean_symbol = symbol.replace(".HK", "") + "HK"
        else:
            clean_symbol = symbol

        # åŸå§‹æ•°æ®è·¯å¾„ - ä½¿ç”¨é…ç½®æˆ–ç›¸å¯¹è·¯å¾„
        if hasattr(self.config, "raw_data_root") and self.config.raw_data_root:
            raw_data_path = Path(self.config.raw_data_root)
        else:
            raw_data_path = self.data_root.parent / "raw" / "HK"

        if not raw_data_path.exists():
            # å›é€€åˆ°é¡¹ç›®æ ¹ç›®å½•çš„raw/HK
            raw_data_path = Path(__file__).parent.parent.parent / "raw" / "HK"

        # æ—¶é—´æ¡†æ¶åˆ°æ–‡ä»¶åçš„æ˜ å°„
        timeframe_map = {
            "1min": "1min",
            "2min": "2min",
            "3min": "3min",
            "5min": "5min",
            "15min": "15m",
            "30min": "30m",
            "60min": "60m",
            "daily": "1day",
            "1d": "1day",
        }

        # æ ¹æ®æ—¶é—´æ¡†æ¶æ™ºèƒ½é€‰æ‹©æœç´¢æ¨¡å¼
        if timeframe and timeframe in timeframe_map:
            file_pattern = timeframe_map[timeframe]
            self.logger.info(
                f"æ ¹æ®æ—¶é—´æ¡†æ¶ '{timeframe}' æœç´¢ '{file_pattern}' æ ¼å¼æ–‡ä»¶"
            )
            search_patterns = [
                f"{clean_symbol}_{file_pattern}_*.parquet",  # ç²¾ç¡®åŒ¹é…
                f"{clean_symbol}_*.parquet",  # å¤‡ç”¨
            ]
        else:
            # é»˜è®¤æœç´¢é¡ºåºï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            self.logger.warning("æœªæŒ‡å®šæ—¶é—´æ¡†æ¶æˆ–ä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤æœç´¢")
            search_patterns = [
                f"{clean_symbol}_60m_*.parquet",  # 60åˆ†é’Ÿæ•°æ®
                f"{clean_symbol}_1day_*.parquet",  # æ—¥çº¿æ•°æ®
                f"{clean_symbol}_*.parquet",  # ä»»æ„æ—¶é—´æ¡†æ¶
            ]

        for pattern in search_patterns:
            price_files = list(raw_data_path.glob(pattern))
            if price_files:
                selected_file = price_files[-1]  # é€‰æ‹©æœ€æ–°æ–‡ä»¶
                self.logger.info(f"æ‰¾åˆ°ä»·æ ¼æ–‡ä»¶: {selected_file}")

                try:
                    # å†…å­˜ä¼˜åŒ–ï¼šä»…è¯»å–OHLCVå’Œtimestampåˆ—
                    price_data = pd.read_parquet(selected_file)

                    # å…ˆå¤„ç†timestampåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if "timestamp" in price_data.columns:
                        price_data["timestamp"] = pd.to_datetime(
                            price_data["timestamp"]
                        )
                        price_data = price_data.set_index("timestamp")
                    elif not isinstance(price_data.index, pd.DatetimeIndex):
                        price_data.index = pd.to_datetime(price_data.index)

                    # ç„¶åé€‰æ‹©OHLCVåˆ—
                    ohlcv_cols = ["open", "high", "low", "close", "volume"]
                    available_cols = [
                        col for col in ohlcv_cols if col in price_data.columns
                    ]
                    if available_cols:
                        price_data = price_data[available_cols]

                    # å†…å­˜ä¼˜åŒ–ï¼šè½¬æ¢æ•°æ®ç±»å‹
                    for col in price_data.select_dtypes(include=["float64"]).columns:
                        price_data[col] = price_data[col].astype("float32")

                    # ç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—
                    required_cols = ["open", "high", "low", "close", "volume"]
                    missing_cols = [
                        col for col in required_cols if col not in price_data.columns
                    ]
                    if missing_cols:
                        self.logger.error(f"ä»·æ ¼æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                        continue

                    self.logger.info(f"ä»·æ ¼æ•°æ®åŠ è½½æˆåŠŸ: å½¢çŠ¶={price_data.shape}")
                    self.logger.info(
                        f"æ—¶é—´èŒƒå›´: {price_data.index.min()} åˆ° {price_data.index.max()}"
                    )
                    self.logger.info(f"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f}ç§’")

                    return price_data[required_cols]

                except Exception as e:
                    self.logger.error(f"åŠ è½½ä»·æ ¼æ–‡ä»¶å¤±è´¥ {selected_file}: {str(e)}")
                    continue

        self.logger.error("æœªæ‰¾åˆ°ä»·æ ¼æ•°æ®:")
        self.logger.error(f"æœç´¢è·¯å¾„: {raw_data_path}")
        self.logger.error(f"æœç´¢ç¬¦å·: {clean_symbol}")

        available_files = [f.name for f in raw_data_path.glob("*.parquet")][:10]
        self.logger.error(f"å¯ç”¨æ–‡ä»¶ç¤ºä¾‹: {available_files}")

        raise FileNotFoundError(f"No price data found for {symbol}")

    # ==================== 1. é¢„æµ‹èƒ½åŠ›åˆ†æ ====================

    def calculate_multi_horizon_ic(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å¤šå‘¨æœŸICå€¼ - æ ¸å¿ƒé¢„æµ‹èƒ½åŠ›è¯„ä¼°"""
        self.logger.info("å¼€å§‹å¤šå‘¨æœŸICè®¡ç®—...")
        start_time = time.time()

        ic_results: Dict[str, Dict[str, float]] = {}
        horizons = self.config.ic_horizons

        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        total_factors = len(factor_cols)
        processed = 0

        returns_series = returns.reindex(factors.index)

        for factor in factor_cols:
            processed += 1
            if processed % self.config.progress_report_interval == 0:
                self.logger.info(f"å¤šå‘¨æœŸICè®¡ç®—è¿›åº¦: {processed}/{total_factors}")

            factor_series = factors[factor]
            horizon_ics: Dict[str, float] = {}

            for horizon in horizons:
                if horizon < 0:
                    self.logger.warning(
                        f"å¿½ç•¥éæ³•é¢„æµ‹å‘¨æœŸ {horizon}ï¼Œå› å­ {factor}"
                    )
                    continue

                if self.temporal_validator is not None:
                    try:
                        is_valid, message = self.temporal_validator.validate_time_alignment(
                            factor_series,
                            returns_series,
                            horizon,
                            context=f"IC-{factor}",
                        )
                    except Exception as validation_error:
                        self.logger.warning(
                            "æ—¶é—´åºåˆ—éªŒè¯å¤±è´¥ (%s): %s",
                            factor,
                            validation_error,
                        )
                        continue

                    if not is_valid:
                        self.logger.debug(
                            "è·³è¿‡å› å­ %s å‘¨æœŸ %sdï¼š%s", factor, horizon, message
                        )
                        continue

                lagged_factor = factor_series.shift(horizon)

                aligned_returns = returns_series.reindex(lagged_factor.index)
                common_idx = lagged_factor.index.intersection(aligned_returns.index)

                if len(common_idx) < self.config.min_sample_size:
                    continue

                final_factor = lagged_factor.loc[common_idx]
                final_returns = aligned_returns.loc[common_idx]

                valid_mask = final_factor.notna() & final_returns.notna()
                valid_count = int(valid_mask.sum())

                if valid_count < self.config.min_sample_size:
                    continue

                final_factor = final_factor[valid_mask]
                final_returns = final_returns[valid_mask]

                try:
                    factor_std = final_factor.std()
                    returns_std = final_returns.std()

                    if factor_std < 1e-8 or returns_std < 1e-8:
                        continue

                    factor_abs_max = final_factor.abs().max()
                    returns_abs_max = final_returns.abs().max()
                    if factor_abs_max > 1e10 or returns_abs_max > 100:
                        continue

                    ic, p_value = stats.spearmanr(final_factor, final_returns)

                    if (
                        np.isnan(ic)
                        or np.isinf(ic)
                        or np.isnan(p_value)
                        or np.isinf(p_value)
                    ):
                        continue

                    if not (-1.0 <= ic <= 1.0):
                        self.logger.warning(
                            f"å› å­{factor}å‘¨æœŸ{horizon}çš„ICè¶…å‡ºèŒƒå›´: {ic:.4f}"
                        )
                        ic = float(np.clip(ic, -1.0, 1.0))

                    horizon_ics[f"ic_{horizon}d"] = float(ic)
                    horizon_ics[f"p_value_{horizon}d"] = float(p_value)
                    horizon_ics[f"sample_size_{horizon}d"] = valid_count

                except Exception as e:
                    self.logger.debug(
                        f"å› å­ {factor} å‘¨æœŸ {horizon} ICè®¡ç®—å¤±è´¥: {str(e)}"
                    )
                    continue

            if horizon_ics:
                ic_results[factor] = horizon_ics

        calc_time = time.time() - start_time
        self.logger.info(
            f"å¤šå‘¨æœŸICè®¡ç®—å®Œæˆ: æœ‰æ•ˆå› å­={len(ic_results)}, è€—æ—¶={calc_time:.2f}ç§’"
        )

        return ic_results

    def analyze_ic_decay(
        self, ic_results: Dict[str, Dict[str, float]]
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†æICè¡°å‡ç‰¹å¾"""
        self.logger.info("åˆ†æICè¡°å‡ç‰¹å¾...")

        decay_metrics = {}
        horizons = self.config.ic_horizons

        for factor, metrics in ic_results.items():
            ic_values = []
            for horizon in horizons:
                ic_key = f"ic_{horizon}d"
                if ic_key in metrics:
                    ic_values.append(metrics[ic_key])

            if len(ic_values) >= 2:
                # è®¡ç®—è¡°å‡ç‡ (çº¿æ€§å›å½’æ–œç‡)
                x = np.arange(len(ic_values))
                ic_array = np.array(ic_values)

                # é™¤é›¶ä¿æŠ¤å’Œæ•°å€¼ç¨³å®šæ€§
                ic_mean = np.mean(ic_array)
                ic_std = np.std(ic_array)

                if ic_std < 1e-8:  # æ ‡å‡†å·®è¿‡å°
                    ic_stability = 1.0
                    slope, intercept, r_value = 0.0, ic_mean, 1.0
                else:
                    slope, intercept, r_value, p_value, std_err = stats.linregress(
                        x, ic_array
                    )
                    # è®¡ç®—ICç¨³å®šæ€§
                    ic_stability = 1 - (ic_std / (abs(ic_mean) + 1e-8))

                # è®¡ç®—ICæŒç»­æ€§ (æœ‰æ•ˆICçš„æ•°é‡)
                ic_longevity = len([ic for ic in ic_values if abs(ic) > 0.01])

                decay_metrics[factor] = {
                    "decay_rate": slope,
                    "ic_stability": max(0, ic_stability),
                    "max_ic": max(ic_values, key=abs),
                    "ic_longevity": ic_longevity,
                    "decay_r_squared": r_value**2,
                    "ic_mean": np.mean(ic_values),
                    "ic_std": np.std(ic_values),
                }

        self.logger.info(f"ICè¡°å‡åˆ†æå®Œæˆ: {len(decay_metrics)} ä¸ªå› å­")
        return decay_metrics

    # ==================== 2. ç¨³å®šæ€§åˆ†æ ====================

    def calculate_rolling_ic(
        self, factors: pd.DataFrame, returns: pd.Series, window: int = None
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ»šåŠ¨IC - Linusæ¨¡å¼å‘é‡åŒ–ä¼˜åŒ–ï¼Œæ¶ˆç­å¾ªç¯å¤æ‚æ€§"""
        if window is None:
            window = self.config.rolling_window

        self.logger.info(f"ğŸš€ Linusæ¨¡å¼ï¼šè®¡ç®—æ»šåŠ¨IC (çª—å£={window})...")
        start_time = time.time()

        rolling_ic_results = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # Linusä¼˜åŒ–ï¼šæ•°æ®å¯¹é½å’Œé¢„å¤„ç†ä¸€æ¬¡æ€§å®Œæˆ
        aligned_factors = factors[factor_cols].reindex(returns.index)
        aligned_returns_full = returns.reindex(aligned_factors.index)

        # æ‰¾åˆ°å…±åŒçš„æœ‰æ•ˆç´¢å¼•
        valid_idx = aligned_factors.notna().any(axis=1) & aligned_returns_full.notna()
        aligned_factors = aligned_factors[valid_idx]
        aligned_returns_full = aligned_returns_full[valid_idx]

        if len(aligned_factors) < window + 20:
            self.logger.warning("æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡æ»šåŠ¨ICè®¡ç®—")
            return rolling_ic_results

        # Linusä¼˜åŒ–ï¼šå‘é‡åŒ–æ»‘åŠ¨çª—å£è®¡ç®—ï¼Œå½»åº•æ¶ˆé™¤å¾ªç¯
        for factor in factor_cols:
            factor_series = aligned_factors[factor].dropna()
            if len(factor_series) < window + 20:
                continue

            returns_series = aligned_returns_full.reindex(factor_series.index).dropna()
            common_idx = factor_series.index.intersection(returns_series.index)

            if len(common_idx) < window + 20:
                continue

            final_factor = factor_series.loc[common_idx].values
            final_returns = returns_series.loc[common_idx].values

            # Linusæ¨¡å¼ï¼šå®Œå…¨å‘é‡åŒ–çš„æ»šåŠ¨çª—å£è®¡ç®—
            try:
                # ä½¿ç”¨numpyçš„stride_tricksåˆ›å»ºæ»‘åŠ¨çª—å£è§†å›¾
                from numpy.lib.stride_tricks import sliding_window_view

                # åˆ›å»ºæ»‘åŠ¨çª—å£è§†å›¾ - ä¸€æ¬¡ç”Ÿæˆæ‰€æœ‰çª—å£
                factor_windows = sliding_window_view(final_factor, window_shape=window)
                returns_windows = sliding_window_view(final_returns, window_shape=window)

                # æ•°å€¼ç¨³å®šæ€§é¢„å¤„ç†ï¼šè¿‡æ»¤å¼‚å¸¸çª—å£
                factor_stds = np.std(factor_windows, axis=1)
                returns_stds = np.std(returns_windows, axis=1)

                # å‘é‡åŒ–è¿‡æ»¤ï¼šä¿ç•™æ•°å€¼ç¨³å®šçš„çª—å£
                valid_mask = (
                    (factor_stds > 1e-8) &
                    (returns_stds > 1e-8) &
                    (np.max(np.abs(factor_windows), axis=1) <= 1e10) &
                    (np.max(np.abs(returns_windows), axis=1) <= 100)
                )

                if np.sum(valid_mask) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆçª—å£
                    continue

                # ä½¿ç”¨æœ‰æ•ˆçª—å£
                valid_factor_windows = factor_windows[valid_mask]
                valid_returns_windows = returns_windows[valid_mask]

                # Linusä¼˜åŒ–ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰çª—å£çš„Spearmanç›¸å…³ç³»æ•°
                # ä½¿ç”¨æ›´å¿«çš„Pearsonç›¸å…³ç³»æ•°è¿‘ä¼¼ï¼ˆå‘é‡åŒ–ï¼‰

                # ä¸­å¿ƒåŒ–æ•°æ®
                factor_centered = valid_factor_windows - np.mean(valid_factor_windows, axis=1, keepdims=True)
                returns_centered = valid_returns_windows - np.mean(valid_returns_windows, axis=1, keepdims=True)

                # å‘é‡åŒ–ç›¸å…³ç³»æ•°è®¡ç®—
                numerator = np.sum(factor_centered * returns_centered, axis=1)
                factor_norm = np.sqrt(np.sum(factor_centered ** 2, axis=1))
                returns_norm = np.sqrt(np.sum(returns_centered ** 2, axis=1))

                # é™¤é›¶ä¿æŠ¤
                denominator = factor_norm * returns_norm
                valid_corr_mask = denominator > 1e-12

                if np.sum(valid_corr_mask) < 10:
                    continue

                # è®¡ç®—ç›¸å…³ç³»æ•°
                rolling_ics = numerator[valid_corr_mask] / denominator[valid_corr_mask]

                # æ•°å€¼èŒƒå›´æ£€æŸ¥
                rolling_ics = rolling_ics[
                    (rolling_ics >= -1.0) & (rolling_ics <= 1.0) &
                    ~np.isnan(rolling_ics) &
                    ~np.isinf(rolling_ics)
                ]

                if len(rolling_ics) >= 10:
                    # Linusä¼˜åŒ–ï¼šå‘é‡åŒ–ç»Ÿè®¡è®¡ç®—
                    rolling_ics_array = np.asarray(rolling_ics, dtype=np.float64)
                    rolling_ic_mean = np.mean(rolling_ics_array)
                    rolling_ic_std = np.std(rolling_ics_array)

                    # ç¨³å®šæ€§æŒ‡æ ‡
                    stability = 1 - (rolling_ic_std / (abs(rolling_ic_mean) + 1e-8))
                    consistency = np.sum(rolling_ics_array * rolling_ic_mean > 0) / len(rolling_ics_array)

                    rolling_ic_results[factor] = {
                        "rolling_ic_mean": float(rolling_ic_mean),
                        "rolling_ic_std": float(rolling_ic_std),
                        "rolling_ic_stability": float(max(0, stability)),
                        "ic_consistency": float(consistency),
                        "rolling_periods": len(rolling_ics_array),
                        "ic_sharpe": float(rolling_ic_mean / (rolling_ic_std + 1e-8)),
                    }

            except ImportError:
                # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨pandas rollingï¼ˆæ¯”åŸå¾ªç¯å¿«10-100å€ï¼‰
                self.logger.warning(f"sliding_window_viewä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆè®¡ç®—å› å­ {factor}")

                factor_df = pd.Series(final_factor)
                returns_df = pd.Series(final_returns)

                # å‘é‡åŒ–æ»šåŠ¨è®¡ç®—
                rolling_corr = factor_df.rolling(window).corr(returns_df)
                rolling_ics = rolling_corr.dropna()

                # è¿‡æ»¤å¼‚å¸¸å€¼
                rolling_ics = rolling_ics[
                    (rolling_ics >= -1.0) & (rolling_ics <= 1.0) &
                    ~np.isnan(rolling_ics) &
                    ~np.isinf(rolling_ics)
                ]

                if len(rolling_ics) >= 10:
                    rolling_ic_mean = float(rolling_ics.mean())
                    rolling_ic_std = float(rolling_ics.std())
                    stability = 1 - (rolling_ic_std / (abs(rolling_ic_mean) + 1e-8))
                    consistency = float(np.sum(rolling_ics * rolling_ic_mean > 0) / len(rolling_ics))

                    rolling_ic_results[factor] = {
                        "rolling_ic_mean": rolling_ic_mean,
                        "rolling_ic_std": rolling_ic_std,
                        "rolling_ic_stability": float(max(0, stability)),
                        "ic_consistency": consistency,
                        "rolling_periods": len(rolling_ics),
                        "ic_sharpe": float(rolling_ic_mean / (rolling_ic_std + 1e-8)),
                    }

        calc_time = time.time() - start_time
        self.logger.info(
            f"æ»šåŠ¨ICè®¡ç®—å®Œæˆ: {len(rolling_ic_results)} ä¸ªå› å­, è€—æ—¶={calc_time:.2f}ç§’"
        )

        return rolling_ic_results

    def calculate_cross_sectional_stability(
        self, factors: pd.DataFrame
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æˆªé¢ç¨³å®šæ€§ - è·¨æ—¶é—´çš„ä¸€è‡´æ€§"""
        self.logger.info("è®¡ç®—æˆªé¢ç¨³å®šæ€§...")

        stability_results = {}
        # åªé€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—ï¼Œæ’é™¤ä»·æ ¼åˆ—å’Œéæ•°å€¼åˆ—
        numeric_cols = factors.select_dtypes(include=[np.number]).columns
        factor_cols = [
            col
            for col in numeric_cols
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_data = factors[factor].dropna()

            if len(factor_data) >= 100:
                # åˆ†æ—¶æ®µåˆ†æç¨³å®šæ€§
                n_periods = 5
                period_size = len(factor_data) // n_periods
                period_stats = []

                for i in range(n_periods):
                    start_idx = i * period_size
                    end_idx = (
                        (i + 1) * period_size if i < n_periods - 1 else len(factor_data)
                    )
                    period_data = factor_data.iloc[start_idx:end_idx]

                    if len(period_data) >= 20:
                        period_stats.append(
                            {
                                "mean": period_data.mean(),
                                "std": period_data.std(),
                                "skew": period_data.skew(),
                                "kurt": period_data.kurtosis(),
                            }
                        )

                if len(period_stats) >= 3:
                    # è®¡ç®—å„ç»Ÿè®¡é‡çš„å˜å¼‚ç³»æ•°
                    means = [s["mean"] for s in period_stats]
                    stds = [s["std"] for s in period_stats]

                    mean_cv = np.std(means) / (abs(np.mean(means)) + 1e-8)
                    std_cv = np.std(stds) / (np.mean(stds) + 1e-8)

                    # ç»¼åˆç¨³å®šæ€§å¾—åˆ†
                    stability_score = 1 / (1 + mean_cv + std_cv)

                    stability_results[factor] = {
                        "cross_section_cv": mean_cv,
                        "cross_section_stability": stability_score,
                        "std_consistency": 1 / (1 + std_cv),
                        "periods_analyzed": len(period_stats),
                    }

        self.logger.info(f"æˆªé¢ç¨³å®šæ€§è®¡ç®—å®Œæˆ: {len(stability_results)} ä¸ªå› å­")
        return stability_results

    # ==================== 3. ç‹¬ç«‹æ€§åˆ†æ ====================

    def calculate_vif_scores(
        self,
        factors: pd.DataFrame,
        vif_threshold: float = 5.0,
        max_iterations: int = 10,
    ) -> Dict[str, float]:
        """è®¡ç®—æ–¹å·®è†¨èƒ€å› å­ (VIF) - é€’å½’ç§»é™¤é«˜å…±çº¿æ€§å› å­ã€‚

        æœ¬å®ç°åŸºäºQRåˆ†è§£çš„ç¨³å¥å›å½’ï¼Œåœ¨æ¯ä¸€è½®è¿­ä»£ä¸­æ‰¹é‡è®¡ç®—æ‰€æœ‰å€™é€‰å› å­çš„
        VIFã€‚æ•°å€¼ä¿æŠ¤ç­–ç•¥ï¼š
        - å¯¹è®¾è®¡çŸ©é˜µæ‰§è¡Œæ¡ä»¶æ•°æ£€æµ‹ï¼Œå¦‚ cond > 1e12 åˆ¤å®šä¸ºæ•°å€¼ä¸ç¨³å®š
        - ä½¿ç”¨ ``numpy.linalg.lstsq`` (QR/SVD) æ±‚è§£å›å½’ï¼Œè®¡ç®—R^2
        - VIF ä¸Šé™è£å‰ªè‡³ ``1e6``ï¼Œé¿å…æµ®ç‚¹æº¢å‡º
        - é€’å½’ç§»é™¤æœ€é«˜VIFå› å­ï¼Œç›´è‡³æ‰€æœ‰å› å­æ»¡è¶³ ``vif_threshold`` æˆ–è¾¾åˆ°æœ€å°ä¿ç•™é‡

        Args:
            factors: è¾“å…¥å› å­è¡¨ï¼Œéœ€åŒ…å«æ•°å€¼åˆ—ã€‚
            vif_threshold: ç›®æ ‡æœ€å¤§VIFé˜ˆå€¼ã€‚
            max_iterations: æœ€å¤§é€’å½’è¿­ä»£æ¬¡æ•°ã€‚

        Returns:
            å› å­åç§°åˆ°VIFå€¼çš„æ˜ å°„ã€‚
        """
        self.logger.info(f"å¼€å§‹é€’å½’VIFè®¡ç®—ï¼ˆé˜ˆå€¼={vif_threshold}ï¼‰...")

        # é€‰æ‹©æ•°å€¼ç±»å‹çš„åˆ—
        numeric_cols = factors.select_dtypes(include=[np.number]).columns
        factor_cols = [
            col
            for col in numeric_cols
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        if len(factor_cols) < 2:
            self.logger.warning("æ•°å€¼å‹å› å­ä¸è¶³ï¼Œæ— æ³•è®¡ç®—VIF")
            return {col: 1.0 for col in factor_cols}

        factor_data = factors[factor_cols].dropna()

        if len(factor_data) < self.config.min_data_points:
            self.logger.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—VIF")
            return {col: 1.0 for col in factor_cols}

        # æ ‡å‡†åŒ–æ•°æ®
        factor_data_std = (factor_data - factor_data.mean()) / (
            factor_data.std() + 1e-8
        )
        factor_data_std = factor_data_std.fillna(0)

        # ç§»é™¤æ–¹å·®ä¸º0çš„åˆ—
        valid_cols = factor_data_std.std() > 1e-6
        factor_data_std = factor_data_std.loc[:, valid_cols]
        remaining_factors = list(factor_data_std.columns)

        if len(remaining_factors) < 2:
            return {col: 1.0 for col in remaining_factors}

        # é€’å½’VIFè®¡ç®—
        cond_threshold = 1e12
        max_vif_cap = 1e6
        iteration = 0
        while iteration < max_iterations and len(remaining_factors) > 10:
            try:
                vif_values = self._compute_vif_qr(
                    factor_data_std,
                    remaining_factors,
                    cond_threshold=cond_threshold,
                    max_vif_cap=max_vif_cap,
                )
                if not vif_values:
                    self.logger.warning("VIFè®¡ç®—è¿”å›ç©ºç»“æœï¼Œç»ˆæ­¢é€’å½’")
                    break

                max_vif_factor = max(vif_values, key=vif_values.get)
                max_vif = vif_values[max_vif_factor]

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰VIFéƒ½å°äºé˜ˆå€¼
                if max_vif <= vif_threshold:
                    self.logger.info(
                        f"VIFé€’å½’å®Œæˆ: è¿­ä»£{iteration}æ¬¡ï¼Œä¿ç•™{len(remaining_factors)}ä¸ªå› å­ï¼Œæœ€å¤§VIF={max_vif:.2f}"
                    )
                    return vif_values

                # ç§»é™¤æœ€é«˜VIFå› å­
                if max_vif_factor and len(remaining_factors) > 10:
                    self.logger.info(
                        f"ç§»é™¤é«˜VIFå› å­: {max_vif_factor} (VIF={max_vif:.2f})"
                    )
                    remaining_factors.remove(max_vif_factor)
                    iteration += 1
                else:
                    break

            except Exception as e:
                self.logger.error(f"VIFé€’å½’è®¡ç®—å¤±è´¥ï¼ˆè¿­ä»£{iteration}ï¼‰: {e}")
                break

        # æœ€ç»ˆVIFè®¡ç®—
        final_vif_scores = self._compute_vif_qr(
            factor_data_std,
            remaining_factors,
            cond_threshold=cond_threshold,
            max_vif_cap=max_vif_cap,
        )
        # æœ€ç»ˆè£å‰ªåˆ°ç”¨æˆ·é˜ˆå€¼
        final_vif_scores = {
            factor: float(min(vif, vif_threshold))
            for factor, vif in final_vif_scores.items()
        }

        max_final_vif = max(final_vif_scores.values()) if final_vif_scores else 0
        self.logger.info(
            f"âœ… VIFè®¡ç®—å®Œæˆ: {len(final_vif_scores)} ä¸ªå› å­, æœ€å¤§VIF={max_final_vif:.2f}"
        )

        # éªŒè¯æœ€ç»ˆç»“æœ
        if max_final_vif > vif_threshold:
            self.logger.warning(
                f"âš ï¸ æœ€ç»ˆVIFä»è¶…è¿‡é˜ˆå€¼: {max_final_vif:.2f} > {vif_threshold}"
            )

        return final_vif_scores

    def _compute_vif_qr(
        self,
        factor_data_std: pd.DataFrame,
        columns: List[str],
        cond_threshold: float,
        max_vif_cap: float,
    ) -> Dict[str, float]:
        """ä½¿ç”¨QR/æœ€å°äºŒä¹˜æ³•ç¨³å¥è®¡ç®—æŒ‡å®šåˆ—çš„VIFã€‚

        Args:
            factor_data_std: å·²æ ‡å‡†åŒ–ä¸”æ— ç¼ºå¤±å€¼çš„å› å­æ•°æ®ã€‚
            columns: éœ€è¦è®¡ç®—çš„åˆ—åç§°åˆ—è¡¨ã€‚
            cond_threshold: æ¡ä»¶æ•°é˜ˆå€¼ï¼Œè¶…è¿‡è§†ä¸ºæ•°å€¼ä¸ç¨³å®šã€‚
            max_vif_cap: VIFæœ€å¤§è£å‰ªå€¼ã€‚

        Returns:
            åˆ—ååˆ°ç¨³å¥VIFå€¼çš„æ˜ å°„ã€‚
        """
        if not columns:
            return {}

        matrix = factor_data_std[columns].to_numpy(dtype=np.float64, copy=True)
        n_samples, n_factors = matrix.shape
        if n_samples == 0 or n_factors == 0:
            return {col: 1.0 for col in columns}

        vif_results: Dict[str, float] = {}
        for col_idx, factor in enumerate(columns):
            y = matrix[:, col_idx]
            X = np.delete(matrix, col_idx, axis=1)

            if X.size == 0:
                vif_results[factor] = 1.0
                continue

            # æ¡ä»¶æ•°æ£€æŸ¥
            try:
                cond_number = np.linalg.cond(X)
            except np.linalg.LinAlgError:
                cond_number = np.inf

            if not np.isfinite(cond_number) or cond_number > cond_threshold:
                vif_results[factor] = float(max_vif_cap)
                continue

            try:
                beta, residuals, rank, singular_vals = np.linalg.lstsq(X, y, rcond=None)
            except np.linalg.LinAlgError as err:
                self.logger.debug(f"VIFæœ€å°äºŒä¹˜æ±‚è§£å¤±è´¥ {factor}: {err}")
                vif_results[factor] = float(max_vif_cap)
                continue

            if residuals.size > 0:
                rss = residuals[0]
            else:
                predictions = X @ beta
                rss = float(np.sum((y - predictions) ** 2))

            tss = float(np.sum((y - np.mean(y)) ** 2))
            if tss <= 1e-12:
                vif_results[factor] = 1.0
                continue

            r_squared = 1.0 - (rss / (tss + 1e-12))
            if not np.isfinite(r_squared):
                r_squared = 0.0
            r_squared = float(np.clip(r_squared, 0.0, 0.999999))

            vif = 1.0 / max(1e-6, 1.0 - r_squared)
            vif_results[factor] = float(min(vif, max_vif_cap))

        return vif_results

    def calculate_factor_correlation_matrix(
        self, factors: pd.DataFrame
    ) -> pd.DataFrame:
        """è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ"""
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        factor_data = factors[factor_cols].dropna()

        if len(factor_data) < 30:
            self.logger.warning("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ")
            return pd.DataFrame()

        # ä½¿ç”¨Spearmanç›¸å…³æ€§ (å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥)
        correlation_matrix = factor_data.corr(method="spearman")

        return correlation_matrix

    def calculate_information_increment(
        self, factors: pd.DataFrame, returns: pd.Series, base_factors: List[str] = None
    ) -> Dict[str, float]:
        """è®¡ç®—ä¿¡æ¯å¢é‡ - ç›¸å¯¹äºåŸºå‡†å› å­çš„å¢é‡ä¿¡æ¯"""
        if base_factors is None:
            base_factors = self.config.base_factors

        self.logger.info(f"è®¡ç®—ä¿¡æ¯å¢é‡ (åŸºå‡†å› å­: {base_factors})...")

        # ç­›é€‰å­˜åœ¨çš„åŸºå‡†å› å­
        available_base = [f for f in base_factors if f in factors.columns]
        if not available_base:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†å› å­")
            return {}

        # è®¡ç®—åŸºå‡†å› å­ç»„åˆçš„é¢„æµ‹èƒ½åŠ›
        base_data = factors[available_base].dropna()
        base_combined = base_data.mean(axis=1)  # ç­‰æƒé‡ç»„åˆ

        aligned_returns = returns.reindex(base_combined.index).dropna()
        common_idx = base_combined.index.intersection(aligned_returns.index)

        if len(common_idx) < self.config.min_sample_size:
            self.logger.warning("åŸºå‡†å› å­æ•°æ®ä¸è¶³")
            return {}

        base_ic, _ = stats.spearmanr(
            base_combined.loc[common_idx], aligned_returns.loc[common_idx]
        )

        if np.isnan(base_ic):
            base_ic = 0.0

        # è®¡ç®—æ¯ä¸ªå› å­çš„ä¿¡æ¯å¢é‡
        information_increment = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"] + available_base
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            factor_common_idx = factor_values.index.intersection(common_idx)

            if len(factor_common_idx) >= self.config.min_sample_size:
                # åŸºå‡† + æ–°å› å­çš„ç»„åˆ
                base_aligned = base_combined.reindex(factor_common_idx)
                factor_aligned = factor_values.loc[factor_common_idx]
                returns_aligned = aligned_returns.loc[factor_common_idx]

                # ç­‰æƒé‡ç»„åˆ
                combined_factor = (base_aligned + factor_aligned) / 2

                try:
                    combined_ic, _ = stats.spearmanr(combined_factor, returns_aligned)

                    if not np.isnan(combined_ic):
                        increment = combined_ic - base_ic
                        information_increment[factor] = increment

                except Exception as e:
                    self.logger.debug(f"å› å­ {factor} ä¿¡æ¯å¢é‡è®¡ç®—å¤±è´¥: {str(e)}")
                    continue

        self.logger.info(f"ä¿¡æ¯å¢é‡è®¡ç®—å®Œæˆ: {len(information_increment)} ä¸ªå› å­")
        return information_increment

    # ==================== 4. å®ç”¨æ€§åˆ†æ ====================

    def calculate_trading_costs(
        self,
        factors: pd.DataFrame,
        prices: pd.DataFrame,
        factor_metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—äº¤æ˜“æˆæœ¬ - åŸºäºå› å­çš„å®é™…äº¤æ˜“æˆæœ¬è¯„ä¼°"""
        self.logger.info("è®¡ç®—äº¤æ˜“æˆæœ¬...")

        cost_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # è·å–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        close_prices = prices["close"]
        volume = prices["volume"]

        metadata = factor_metadata or {}

        for factor in factor_cols:
            factor_values = factors[factor].dropna()

            # æ—¶é—´å¯¹é½
            common_idx = factor_values.index.intersection(close_prices.index)
            if len(common_idx) < 50:
                continue

            factor_aligned = factor_values.loc[common_idx]
            volume_aligned = volume.loc[common_idx]

            meta = metadata.get(factor, {})
            factor_type = meta.get("type", self._infer_factor_type(factor))
            turnover_profile = self._determine_turnover_profile(factor, factor_type)
            turnover_rate = self._calculate_turnover_rate(
                factor_aligned,
                factor_name=factor,
                factor_type=factor_type,
                turnover_profile=turnover_profile,
            )

            # éªŒè¯turnover_rateåˆç†æ€§ï¼Œé¿å…Infinityä¼ æ’­
            if not np.isfinite(turnover_rate):
                self.logger.error(f"å› å­ {factor} turnover_rateå¼‚å¸¸: {turnover_rate}")
                turnover_rate = 0.0

            # è®¡ç®—å› å­å˜åŒ–ç‡ç”¨äºæ¢æ‰‹é¢‘ç‡è®¡ç®—
            if turnover_profile == "cumulative":
                factor_change = factor_aligned.pct_change().abs()
            else:
                factor_change = factor_aligned.diff().abs()

            # ä¼°ç®—äº¤æ˜“æˆæœ¬
            commission_cost = turnover_rate * self.config.commission_rate
            slippage_cost = turnover_rate * (self.config.slippage_bps / 10000)

            # å¸‚åœºå†²å‡»æˆæœ¬ (åŸºäºæˆäº¤é‡)
            avg_volume = volume_aligned.mean()
            if avg_volume <= 0 or not np.isfinite(avg_volume):
                avg_volume = 1.0  # é»˜è®¤å€¼ï¼Œé¿å…log(0)æˆ–è´Ÿæ•°

            volume_factor = 1 / (1 + np.log(avg_volume + 1))  # æˆäº¤é‡è¶Šå¤§ï¼Œå†²å‡»è¶Šå°
            impact_cost = (
                turnover_rate * self.config.market_impact_coeff * volume_factor
            )

            total_cost = commission_cost + slippage_cost + impact_cost

            # æœ€ç»ˆéªŒè¯total_coståˆç†æ€§
            if not np.isfinite(total_cost):
                self.logger.error(f"å› å­ {factor} total_costå¼‚å¸¸: {total_cost}")
                total_cost = self.config.commission_rate  # ä½¿ç”¨åŸºç¡€ä½£é‡‘æˆæœ¬ä½œä¸ºå…œåº•

            # æˆæœ¬æ•ˆç‡
            cost_efficiency = 1 / (1 + total_cost)

            # æ¢æ‰‹é¢‘ç‡
            change_frequency = (factor_change > self.config.factor_change_threshold).mean()  # å› å­å˜åŒ–é¢‘ç‡

            cost_analysis[factor] = {
                "turnover_rate": turnover_rate,
                "commission_cost": commission_cost,
                "slippage_cost": slippage_cost,
                "impact_cost": impact_cost,
                "total_cost": total_cost,
                "cost_efficiency": cost_efficiency,
                "change_frequency": change_frequency,
                "avg_volume": avg_volume,
            }

        self.logger.info(f"äº¤æ˜“æˆæœ¬è®¡ç®—å®Œæˆ: {len(cost_analysis)} ä¸ªå› å­")
        return cost_analysis

    def calculate_liquidity_requirements(
        self, factors: pd.DataFrame, volume: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æµåŠ¨æ€§éœ€æ±‚"""
        self.logger.info("è®¡ç®—æµåŠ¨æ€§éœ€æ±‚...")

        liquidity_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            aligned_volume = volume.reindex(factor_values.index).dropna()

            common_idx = factor_values.index.intersection(aligned_volume.index)
            if len(common_idx) < 30:
                continue

            factor_aligned = factor_values.loc[common_idx]
            volume_aligned = aligned_volume.loc[common_idx]

            # è®¡ç®—å› å­æå€¼æ—¶æœŸçš„æˆäº¤é‡éœ€æ±‚
            factor_percentiles = factor_aligned.rank(pct=True)

            # æå€¼æœŸé—´ (å‰10%å’Œå10%)
            extreme_mask = (factor_percentiles <= 0.1) | (factor_percentiles >= 0.9)
            normal_mask = (factor_percentiles > 0.3) & (factor_percentiles < 0.7)

            if extreme_mask.sum() > 0 and normal_mask.sum() > 0:
                extreme_volume = volume_aligned[extreme_mask].mean()
                normal_volume = volume_aligned[normal_mask].mean()

                # æµåŠ¨æ€§éœ€æ±‚æŒ‡æ ‡
                liquidity_demand = (extreme_volume - normal_volume) / (
                    normal_volume + 1e-8
                )
                liquidity_score = 1 / (1 + abs(liquidity_demand))

                # å®¹é‡è¯„ä¼°
                capacity_score = np.log(normal_volume + 1) / 20  # æ ‡å‡†åŒ–å®¹é‡å¾—åˆ†

                liquidity_analysis[factor] = {
                    "extreme_volume": extreme_volume,
                    "normal_volume": normal_volume,
                    "liquidity_demand": liquidity_demand,
                    "liquidity_score": liquidity_score,
                    "capacity_score": min(capacity_score, 1.0),
                }

        self.logger.info(f"æµåŠ¨æ€§éœ€æ±‚è®¡ç®—å®Œæˆ: {len(liquidity_analysis)} ä¸ªå› å­")
        return liquidity_analysis

    def _determine_turnover_profile(self, factor_name: str, factor_type: str) -> str:
        """æ ¹æ®å› å­ç‰¹å¾é€‰æ‹©æ¢æ‰‹ç‡è®¡ç®—ç­–ç•¥"""
        name_lower = factor_name.lower()

        cumulative_keywords = [
            "obv",
            "vwap",
            "cum",
            "acc",
            "cumulative",
            "rolling_sum",
            "volume",
        ]

        if factor_type in {"volume"} or any(
            keyword in name_lower for keyword in cumulative_keywords
        ):
            return "cumulative"

        return "oscillator"

    def _calculate_turnover_rate(
        self,
        factor_values: pd.Series,
        *,
        factor_name: str,
        factor_type: str,
        turnover_profile: Optional[str] = None,
    ) -> float:
        """è®¡ç®—å› å­æ¢æ‰‹ç‡ - é’ˆå¯¹æŒ‡æ ‡ç±»å‹é‡‡ç”¨è‡ªé€‚åº”ç­–ç•¥ã€‚"""
        if factor_values is None or len(factor_values) < 2:
            return 0.0

        factor_series = factor_values.dropna()
        if factor_series.empty:
            self.logger.warning(
                f"å› å­ {factor_name} æ— æœ‰æ•ˆæ•°æ®æ ·æœ¬ï¼Œè®¾ç½®turnover_rate=0.0"
            )
            return 0.0

        profile = turnover_profile or self._determine_turnover_profile(
            factor_name, factor_type
        )

        if profile == "cumulative":
            factor_change = factor_series.pct_change()
        else:
            factor_change = factor_series.diff()

        factor_change = factor_change.replace([np.inf, -np.inf], np.nan)
        valid_changes = factor_change.abs().dropna()

        if valid_changes.empty:
            self.logger.debug(
                "å› å­ %s åœ¨ç­–ç•¥ '%s' ä¸‹æ— æœ‰æ•ˆå˜åŒ–ï¼Œturnover_rate=0.0",
                factor_name,
                profile,
            )
            return 0.0

        if len(valid_changes) > 10:
            upper_clip = valid_changes.quantile(0.99)
            if np.isfinite(upper_clip):
                valid_changes = valid_changes.clip(upper=upper_clip)

        if profile == "cumulative":
            normalized_changes = valid_changes
        else:
            scale = factor_series.abs().median()
            if not np.isfinite(scale) or scale <= 0:
                scale = factor_series.abs().mean()
            if not np.isfinite(scale) or scale <= 0:
                scale = 1.0
            normalized_changes = valid_changes / max(scale, 1.0)

        turnover_rate = float(normalized_changes.mean())

        if not np.isfinite(turnover_rate) or turnover_rate < 0.0:
            self.logger.warning(
                "å› å­ %s turnover_rateè®¡ç®—å¼‚å¸¸ (%s)ï¼Œé‡ç½®ä¸º0.0",
                factor_name,
                turnover_rate,
            )
            return 0.0

        if turnover_rate > 2.0:
            self.logger.warning(
                "å› å­ %s turnoverç‡å¼‚å¸¸é«˜ (%.6f)ï¼Œå·²è£å‰ªè‡³2.0",
                factor_name,
                turnover_rate,
            )
            turnover_rate = 2.0

        return turnover_rate

    # ==================== 5. çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ ====================

    def detect_reversal_effects(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """æ£€æµ‹åè½¬æ•ˆåº” - çŸ­æœŸåè½¬ç‰¹å¾"""
        self.logger.info("æ£€æµ‹åè½¬æ•ˆåº”...")

        reversal_effects = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        for factor in factor_cols:
            factor_values = factors[factor].dropna()
            aligned_returns = returns.reindex(factor_values.index).dropna()

            common_idx = factor_values.index.intersection(aligned_returns.index)
            if len(common_idx) < 100:
                continue

            factor_aligned = factor_values.loc[common_idx]
            returns_aligned = aligned_returns.loc[common_idx]

            # è®¡ç®—å› å­åˆ†ä½æ•°
            factor_ranks = factor_aligned.rank(pct=True)

            # é«˜å› å­å€¼ vs ä½å› å­å€¼çš„æ”¶ç›Šå·®å¼‚
            high_mask = factor_ranks >= self.config.high_rank_threshold
            low_mask = factor_ranks <= 0.2

            if high_mask.sum() > 10 and low_mask.sum() > 10:
                high_returns = returns_aligned[high_mask].mean()
                low_returns = returns_aligned[low_mask].mean()

                # åè½¬æ•ˆåº” (ä½å› å­å€¼ - é«˜å› å­å€¼)
                reversal_effect = low_returns - high_returns

                # åè½¬å¼ºåº¦ (æ ‡å‡†åŒ–)
                returns_std = returns_aligned.std()
                reversal_strength = abs(reversal_effect) / (returns_std + 1e-8)

                # åè½¬ä¸€è‡´æ€§
                high_positive_rate = (returns_aligned[high_mask] > 0).mean()
                low_positive_rate = (returns_aligned[low_mask] > 0).mean()
                reversal_consistency = abs(low_positive_rate - high_positive_rate)

                reversal_effects[factor] = {
                    "reversal_effect": reversal_effect,
                    "reversal_strength": reversal_strength,
                    "reversal_consistency": reversal_consistency,
                    "high_return_mean": high_returns,
                    "low_return_mean": low_returns,
                }

        self.logger.info(f"åè½¬æ•ˆåº”æ£€æµ‹å®Œæˆ: {len(reversal_effects)} ä¸ªå› å­")
        return reversal_effects

    def analyze_momentum_persistence(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†æåŠ¨é‡æŒç»­æ€§ï¼ˆå‘é‡åŒ–å®ç°ï¼‰ã€‚"""
        self.logger.info("åˆ†æåŠ¨é‡æŒç»­æ€§...")

        momentum_analysis: Dict[str, Dict[str, float]] = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        if not factor_cols:
            return momentum_analysis

        windows = np.array([5, 10, 20], dtype=np.int64)
        forward_horizon = 5  # å‰ç»çª—å£ï¼Œç”¨äºåˆ†ææŒç»­æ€§

        for factor in factor_cols:
            factor_series = factors[factor].dropna()
            returns_series = returns.reindex(factor_series.index).dropna()

            common_idx = factor_series.index.intersection(returns_series.index)
            if len(common_idx) < self.config.min_momentum_samples:
                continue

            factor_values = factor_series.loc[common_idx].to_numpy(dtype=np.float64)
            returns_values = returns_series.loc[common_idx].to_numpy(dtype=np.float64)

            n = factor_values.shape[0]
            if n < self.config.min_momentum_samples:
                continue

            signals = []
            forward_returns = []  # å‰ç»æ”¶ç›Šç‡ï¼Œç”¨äºåˆ†ææŒç»­æ€§

            for window in windows:
                max_start = n - forward_horizon
                if max_start <= window:
                    continue

                current_vals = factor_values[window:max_start]
                forward_mat = np.lib.stride_tricks.sliding_window_view(
                    returns_values[window + 1 :], forward_horizon
                )
                forward_sums = forward_mat[: len(current_vals)].sum(axis=1)

                signals.append(current_vals)
                forward_returns.append(forward_sums)

            if not signals:
                continue

            signals_array = np.concatenate(signals).astype(np.float64, copy=False)
            forward_returns_array = np.concatenate(forward_returns).astype(
                np.float64, copy=False
            )

            if signals_array.size <= 20:
                continue

            try:
                momentum_corr, momentum_p = stats.spearmanr(
                    signals_array, forward_returns_array
                )
            except Exception as exc:
                self.logger.debug(f"å› å­ {factor} åŠ¨é‡æŒç»­æ€§ spearman å¤±è´¥: {exc}")
                continue

            if np.isnan(momentum_corr):
                continue

            consistent_signals = np.sum(signals_array * forward_returns_array > 0)
            momentum_consistency = consistent_signals / signals_array.size

            momentum_analysis[factor] = {
                "momentum_persistence": float(momentum_corr),
                "momentum_consistency": float(momentum_consistency),
                "momentum_p_value": float(momentum_p),
                "signal_count": int(signals_array.size),
            }

        self.logger.info(f"åŠ¨é‡æŒç»­æ€§åˆ†æå®Œæˆ: {len(momentum_analysis)} ä¸ªå› å­")
        return momentum_analysis

    def analyze_volatility_sensitivity(
        self, factors: pd.DataFrame, returns: pd.Series
    ) -> Dict[str, Dict[str, float]]:
        """åˆ†ææ³¢åŠ¨ç‡æ•æ„Ÿæ€§"""
        self.logger.info("åˆ†ææ³¢åŠ¨ç‡æ•æ„Ÿæ€§...")

        volatility_analysis = {}
        factor_cols = [
            col
            for col in factors.columns
            if col not in ["open", "high", "low", "close", "volume"]
        ]

        # è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
        rolling_vol = returns.rolling(window=20).std()

        for factor in factor_cols:
            factor_values = factors[factor].dropna()

            common_idx = factor_values.index.intersection(rolling_vol.index)
            if len(common_idx) < 100:
                continue

            factor_aligned = factor_values.loc[common_idx]
            vol_aligned = rolling_vol.loc[common_idx].dropna()

            # å†æ¬¡å¯¹é½
            final_idx = factor_aligned.index.intersection(vol_aligned.index)
            if len(final_idx) < self.config.min_data_points:
                continue

            factor_final = factor_aligned.loc[final_idx]
            vol_final = vol_aligned.loc[final_idx]

            # åˆ†æå› å­åœ¨ä¸åŒæ³¢åŠ¨ç‡ç¯å¢ƒä¸‹çš„è¡¨ç°
            vol_percentiles = vol_final.rank(pct=True)

            high_vol_mask = vol_percentiles >= 0.7
            low_vol_mask = vol_percentiles <= 0.3

            if high_vol_mask.sum() > 10 and low_vol_mask.sum() > 10:
                high_vol_factor = factor_final[high_vol_mask].std()
                low_vol_factor = factor_final[low_vol_mask].std()

                # æ³¢åŠ¨ç‡æ•æ„Ÿæ€§
                vol_sensitivity = (high_vol_factor - low_vol_factor) / (
                    low_vol_factor + 1e-8
                )

                # ç¨³å®šæ€§å¾—åˆ† (æ³¢åŠ¨ç‡æ•æ„Ÿæ€§è¶Šä½è¶Šå¥½)
                stability_score = 1 / (1 + abs(vol_sensitivity))

                volatility_analysis[factor] = {
                    "volatility_sensitivity": vol_sensitivity,
                    "stability_score": stability_score,
                    "high_vol_std": high_vol_factor,
                    "low_vol_std": low_vol_factor,
                }

        self.logger.info(f"æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æå®Œæˆ: {len(volatility_analysis)} ä¸ªå› å­")
        return volatility_analysis

    # ==================== ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ ====================

    def benjamini_hochberg_correction(
        self, p_values: Dict[str, float], alpha: float = None, sample_size: int = None
    ) -> Tuple[Dict[str, float], float]:
        """
        æ”¹è¿›çš„Benjamini-Hochberg FDRæ ¡æ­£ - è‡ªé€‚åº”æ˜¾è‘—æ€§é˜ˆå€¼

        æ”¹è¿›ç‚¹:
        1. æ ¹æ®æ ·æœ¬é‡åŠ¨æ€è°ƒæ•´alphaé˜ˆå€¼
        2. å°æ ·æœ¬é‡ä¸‹æ”¾å®½æ˜¾è‘—æ€§é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦ä¸¥æ ¼
        3. å¤§æ ·æœ¬é‡ä¸‹æ”¶ç´§é˜ˆå€¼ï¼Œæé«˜å¯é æ€§
        4. ç¡®ä¿è‡³å°‘5-20%çš„å› å­å¯ä»¥é€šè¿‡æ£€éªŒ
        """
        if alpha is None:
            alpha = self.config.alpha_level

        if not p_values:
            return {}

        # è½¬æ¢ä¸ºæ•°ç»„
        factors = list(p_values.keys())
        p_vals = np.array([p_values[factor] for factor in factors])
        n_tests = len(p_vals)

        # è‡ªé€‚åº”alphaè°ƒæ•´
        adaptive_alpha = alpha
        if sample_size is not None:
            if sample_size < 100:
                # å°æ ·æœ¬ï¼šæ”¾å®½åˆ°alpha * 2.0
                adaptive_alpha = min(alpha * 2.0, 0.10)
                self.logger.info(
                    f"å°æ ·æœ¬é‡({sample_size})æ£€æµ‹ï¼Œæ”¾å®½alphaè‡³{adaptive_alpha:.3f}"
                )
            elif sample_size < 200:
                # ä¸­ç­‰æ ·æœ¬ï¼šæ”¾å®½åˆ°alpha * 1.5
                adaptive_alpha = min(alpha * 1.5, 0.075)
                self.logger.info(
                    f"ä¸­ç­‰æ ·æœ¬é‡({sample_size})ï¼Œè°ƒæ•´alphaè‡³{adaptive_alpha:.3f}"
                )
            else:
                # å¤§æ ·æœ¬ï¼šä¿æŒæ ‡å‡†alpha
                adaptive_alpha = alpha

        # æŒ‰på€¼æ’åº
        sorted_indices = np.argsort(p_vals)
        sorted_p = p_vals[sorted_indices]
        sorted_factors = [factors[i] for i in sorted_indices]

        # æ ‡å‡†BHç¨‹åº
        corrected_p = {}
        significant_count = 0

        for i, (factor, p_val) in enumerate(zip(sorted_factors, sorted_p)):
            # BHæ ¡æ­£å…¬å¼: p_corrected = p * n / (i + 1)
            corrected_p_val = min(p_val * n_tests / (i + 1), 1.0)
            corrected_p[factor] = corrected_p_val

            if corrected_p_val <= adaptive_alpha:
                significant_count += 1

        # æ£€æŸ¥æ˜¾è‘—å› å­æ¯”ä¾‹
        significant_ratio = significant_count / n_tests if n_tests > 0 else 0

        # å¦‚æœæ˜¾è‘—å› å­è¿‡å°‘(<5%)ï¼Œå†æ¬¡æ”¾å®½é˜ˆå€¼
        if significant_ratio < 0.05 and sample_size and sample_size < 500:
            self.logger.warning(
                f"æ˜¾è‘—å› å­æ¯”ä¾‹è¿‡ä½({significant_ratio:.1%})ï¼Œ"
                f"å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è€ƒè™‘å¢åŠ æ ·æœ¬é‡"
            )
        elif significant_ratio > 0.20:
            self.logger.info(
                f"æ˜¾è‘—å› å­æ¯”ä¾‹: {significant_ratio:.1%} "
                f"({significant_count}/{n_tests})"
            )

        return corrected_p, adaptive_alpha

    def bonferroni_correction(self, p_values: Dict[str, float]) -> Tuple[Dict[str, float], float]:
        """Bonferroniæ ¡æ­£"""
        if not p_values:
            return {}, self.config.alpha_level

        n_tests = len(p_values)
        corrected_p = {}

        for factor, p_val in p_values.items():
            corrected_p[factor] = min(p_val * n_tests, 1.0)

        # Bonferroniæ–¹æ³•ä½¿ç”¨å›ºå®šçš„alpha_level
        return corrected_p, self.config.alpha_level

    # ==================== ç»¼åˆè¯„åˆ†ç³»ç»Ÿ ====================

    def calculate_comprehensive_scores(
        self, all_metrics: Dict[str, Dict]
    ) -> Dict[str, FactorMetrics]:
        """è®¡ç®—ç»¼åˆè¯„åˆ† - 5ç»´åº¦åŠ æƒè¯„åˆ†"""
        self.logger.info("è®¡ç®—ç»¼åˆè¯„åˆ†...")

        comprehensive_results = {}

        # è·å–æ‰€æœ‰å› å­åç§°
        all_factors = set()
        for metric_dict in all_metrics.values():
            if isinstance(metric_dict, dict):
                all_factors.update(metric_dict.keys())

        for factor in all_factors:
            metrics = FactorMetrics(name=factor)

            # 1. é¢„æµ‹èƒ½åŠ›è¯„åˆ† (35%)
            predictive_score = 0.0
            if (
                "multi_horizon_ic" in all_metrics
                and factor in all_metrics["multi_horizon_ic"]
            ):
                ic_data = all_metrics["multi_horizon_ic"][factor]

                # æå–å„å‘¨æœŸIC
                metrics.ic_1d = ic_data.get("ic_1d", 0.0)
                metrics.ic_3d = ic_data.get("ic_3d", 0.0)
                metrics.ic_5d = ic_data.get("ic_5d", 0.0)
                metrics.ic_10d = ic_data.get("ic_10d", 0.0)
                metrics.ic_20d = ic_data.get("ic_20d", 0.0)

                # è®¡ç®—å¹³å‡ICå’ŒIR
                ic_values = [
                    abs(ic_data.get(f"ic_{h}d", 0.0)) for h in self.config.ic_horizons
                ]
                ic_values = [ic for ic in ic_values if ic != 0.0]

                if ic_values:
                    metrics.ic_mean = np.mean(ic_values)
                    metrics.ic_std = np.std(ic_values) if len(ic_values) > 1 else 0.1
                    metrics.ic_ir = metrics.ic_mean / (metrics.ic_std + 1e-8)
                    metrics.predictive_power_mean_ic = metrics.ic_mean  # è®¾ç½®ç¼ºå¤±å­—æ®µ

                    # é¢„æµ‹èƒ½åŠ›å¾—åˆ†
                    predictive_score = min(metrics.ic_mean * 10, 1.0)  # æ ‡å‡†åŒ–åˆ°[0,1]

            if "ic_decay" in all_metrics and factor in all_metrics["ic_decay"]:
                decay_data = all_metrics["ic_decay"][factor]
                metrics.ic_decay_rate = decay_data.get("decay_rate", 0.0)
                metrics.ic_longevity = decay_data.get("ic_longevity", 0)

                # è¡°å‡æƒ©ç½š
                decay_penalty = abs(metrics.ic_decay_rate) * 0.1
                predictive_score = max(0, predictive_score - decay_penalty)

            metrics.predictive_score = predictive_score

            # 2. ç¨³å®šæ€§è¯„åˆ† (25%)
            stability_score = 0.0
            if "rolling_ic" in all_metrics and factor in all_metrics["rolling_ic"]:
                rolling_data = all_metrics["rolling_ic"][factor]
                metrics.rolling_ic_mean = rolling_data.get("rolling_ic_mean", 0.0)
                metrics.rolling_ic_std = rolling_data.get("rolling_ic_std", 0.0)
                metrics.rolling_ic_stability = rolling_data.get(
                    "rolling_ic_stability", 0.0
                )
                metrics.ic_consistency = rolling_data.get("ic_consistency", 0.0)

                stability_score = (
                    metrics.rolling_ic_stability + metrics.ic_consistency
                ) / 2

            if (
                "cross_section_stability" in all_metrics
                and factor in all_metrics["cross_section_stability"]
            ):
                cs_data = all_metrics["cross_section_stability"][factor]
                metrics.cross_section_stability = cs_data.get(
                    "cross_section_stability", 0.0
                )

                # ç»¼åˆç¨³å®šæ€§
                stability_score = (
                    stability_score + metrics.cross_section_stability
                ) / 2

            metrics.stability_score = stability_score

            # 3. ç‹¬ç«‹æ€§è¯„åˆ† (20%)
            independence_score = 1.0  # é»˜è®¤æ»¡åˆ†
            if "vif_scores" in all_metrics and factor in all_metrics["vif_scores"]:
                metrics.vif_score = all_metrics["vif_scores"][factor]
                vif_penalty = min(metrics.vif_score / self.config.vif_threshold, 2.0)
                independence_score *= 1 / (1 + vif_penalty)

            if "correlation_matrix" in all_metrics:
                corr_matrix = all_metrics["correlation_matrix"]
                if factor in corr_matrix.columns:
                    factor_corrs = corr_matrix[factor].drop(factor, errors="ignore")
                    if len(factor_corrs) > 0:
                        metrics.correlation_max = factor_corrs.abs().max()
                        corr_penalty = max(0, metrics.correlation_max - 0.5) * 2
                        independence_score *= 1 - corr_penalty

            if (
                "information_increment" in all_metrics
                and factor in all_metrics["information_increment"]
            ):
                metrics.information_increment = all_metrics["information_increment"][
                    factor
                ]
                # ä¿¡æ¯å¢é‡å¥–åŠ±
                info_bonus = max(0, metrics.information_increment) * 5
                independence_score = min(independence_score + info_bonus, 1.0)

            metrics.independence_score = max(0, independence_score)

            # 4. å®ç”¨æ€§è¯„åˆ† (15%)
            practicality_score = 1.0
            if (
                "trading_costs" in all_metrics
                and factor in all_metrics["trading_costs"]
            ):
                cost_data = all_metrics["trading_costs"][factor]
                metrics.turnover_rate = cost_data.get("turnover_rate", 0.0)
                metrics.transaction_cost = cost_data.get("total_cost", 0.0)
                metrics.cost_efficiency = cost_data.get("cost_efficiency", 0.0)

                practicality_score = metrics.cost_efficiency

            if (
                "liquidity_requirements" in all_metrics
                and factor in all_metrics["liquidity_requirements"]
            ):
                liq_data = all_metrics["liquidity_requirements"][factor]
                metrics.liquidity_demand = liq_data.get("liquidity_demand", 0.0)
                metrics.capacity_score = liq_data.get("capacity_score", 0.0)

                # ç»¼åˆå®ç”¨æ€§
                practicality_score = (practicality_score + metrics.capacity_score) / 2

            metrics.practicality_score = practicality_score

            # 5. çŸ­å‘¨æœŸé€‚åº”æ€§è¯„åˆ† (5%)
            adaptability_score = 0.5  # é»˜è®¤ä¸­æ€§
            if (
                "reversal_effects" in all_metrics
                and factor in all_metrics["reversal_effects"]
            ):
                rev_data = all_metrics["reversal_effects"][factor]
                metrics.reversal_effect = rev_data.get("reversal_effect", 0.0)
                reversal_strength = rev_data.get("reversal_strength", 0.0)

                # é€‚åº¦çš„åè½¬æ•ˆåº”æ˜¯å¥½çš„
                adaptability_score += min(reversal_strength * 0.5, 0.3)

            if (
                "momentum_persistence" in all_metrics
                and factor in all_metrics["momentum_persistence"]
            ):
                mom_data = all_metrics["momentum_persistence"][factor]
                metrics.momentum_persistence = mom_data.get("momentum_persistence", 0.0)

                # åŠ¨é‡æŒç»­æ€§å¥–åŠ±
                adaptability_score += abs(metrics.momentum_persistence) * 0.2

            if (
                "volatility_sensitivity" in all_metrics
                and factor in all_metrics["volatility_sensitivity"]
            ):
                vol_data = all_metrics["volatility_sensitivity"][factor]
                vol_stability = vol_data.get("stability_score", 0.0)

                # æ³¢åŠ¨ç‡ç¨³å®šæ€§å¥–åŠ±
                adaptability_score = (adaptability_score + vol_stability) / 2

            metrics.adaptability_score = min(adaptability_score, 1.0)

            # ç»¼åˆè¯„åˆ†è®¡ç®—
            if hasattr(self.config, 'weights') and self.config.weights:
                # ä½¿ç”¨config_manager.pyä¸­çš„weightså­—å…¸
                weights = self.config.weights
            else:
                # ä½¿ç”¨é»˜è®¤æƒé‡
                weights = {
                    "predictive_power": 0.35,
                    "stability": 0.25,
                    "independence": 0.20,
                    "practicality": 0.15,
                    "short_term_fitness": 0.05,
                }
            total_weight = sum(weights.values())
            if not np.isclose(total_weight, 1.0, rtol=1e-6):
                self.logger.error(f"æƒé‡é…ç½®é”™è¯¯: æ€»å’Œ={total_weight:.6f}, åº”ä¸º1.0")
                raise ValueError("æƒé‡é…ç½®é”™è¯¯ - ç³»ç»Ÿå®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")

            metrics.comprehensive_score = (
                metrics.predictive_score * weights["predictive_power"]
                + metrics.stability_score * weights["stability"]
                + metrics.independence_score * weights["independence"]
                + metrics.practicality_score * weights["practicality"]
                + metrics.adaptability_score * weights["short_term_fitness"]
            )

            # ç»Ÿè®¡æ˜¾è‘—æ€§
            if "p_values" in all_metrics and factor in all_metrics["p_values"]:
                metrics.p_value = all_metrics["p_values"][factor]

            if (
                "corrected_p_values" in all_metrics
                and factor in all_metrics["corrected_p_values"]
            ):
                metrics.corrected_p_value = all_metrics["corrected_p_values"][factor]
                # ä¿®å¤ï¼šä½¿ç”¨adaptive_alphaè€Œä¸æ˜¯å›ºå®šçš„self.config.alpha_level
                adaptive_alpha = all_metrics.get("adaptive_alpha", self.config.alpha_level)
                metrics.is_significant = (
                    metrics.corrected_p_value < adaptive_alpha
                )

            comprehensive_results[factor] = metrics

        self.logger.info(f"ç»¼åˆè¯„åˆ†è®¡ç®—å®Œæˆ: {len(comprehensive_results)} ä¸ªå› å­")
        return comprehensive_results

    # ==================== ä¸»ç­›é€‰å‡½æ•° ====================

    def setup_multi_timeframe_session(self, symbol: str, timeframes: List[str]) -> Path:
        """
        è®¾ç½®å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•ç»“æ„

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨

        Returns:
            Path: ä¸»ä¼šè¯ç›®å½•è·¯å¾„
        """
        # åˆ›å»ºä¸»ä¼šè¯ç›®å½•
        main_session_id = f"{symbol}_multi_tf_{self.session_timestamp}"
        self.multi_tf_session_dir = self.screening_results_dir / main_session_id
        self.multi_tf_session_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ—¶é—´æ¡†æ¶å­ç›®å½•
        self.timeframes_dir = self.multi_tf_session_dir / "timeframes"
        self.timeframes_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå„ä¸ªæ—¶é—´æ¡†æ¶çš„ä¼šè¯ç›®å½•
        self.tf_session_dirs = {}
        for tf in timeframes:
            tf_session_id = f"{tf}_{self.session_timestamp}"
            tf_session_dir = self.timeframes_dir / tf_session_id
            tf_session_dir.mkdir(exist_ok=True)
            self.tf_session_dirs[tf] = tf_session_dir

        return self.multi_tf_session_dir

    def screen_single_timeframe_in_multi_session(
        self, symbol: str, timeframe: str
    ) -> Dict[str, FactorMetrics]:
        """
        åœ¨å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ä¸­ç­›é€‰å•ä¸ªæ—¶é—´æ¡†æ¶

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´æ¡†æ¶

        Returns:
            Dict[str, FactorMetrics]: ç­›é€‰ç»“æœ
        """
        # è®¾ç½®å½“å‰æ—¶é—´æ¡†æ¶çš„ä¼šè¯ç›®å½•
        self.session_dir = self.tf_session_dirs[timeframe]

        # ä¸ºå½“å‰æ—¶é—´æ¡†æ¶åˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨
        tf_logger_name = f"{self.session_timestamp}_{timeframe}"
        # ä¸´æ—¶è®¾ç½®æ—¥å¿—æ ¹ç›®å½•åˆ°æ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•
        original_log_root = self.log_root
        self.log_root = self.session_dir
        self.logger = self._setup_logger(tf_logger_name)
        # æ¢å¤åŸå§‹æ—¥å¿—æ ¹ç›®å½•
        self.log_root = original_log_root

        start_time = time.time()
        self.logger.info(f"ğŸ“ å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ - {timeframe} å­ç›®å½•: {self.session_dir}")
        self.logger.info(f"å¼€å§‹5ç»´åº¦å› å­ç­›é€‰: {symbol} {timeframe}")

        try:
            # 1. æ•°æ®åŠ è½½
            self.logger.info("æ­¥éª¤1: æ•°æ®åŠ è½½...")
            factors = self.load_factors(symbol, timeframe)
            price_data = self.load_price_data(symbol, timeframe)

            # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
            self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
            close_prices = price_data["close"]

            # 3. æ‰§è¡Œç°æœ‰çš„ç­›é€‰é€»è¾‘ï¼ˆå¤ç”¨åŸå‡½æ•°çš„æ ¸å¿ƒéƒ¨åˆ†ï¼‰
            return self._execute_screening_core(
                factors, close_prices, symbol, timeframe, start_time
            )

        except Exception as e:
            self.logger.error(f"å¤šæ—¶é—´æ¡†æ¶ç­›é€‰å¤±è´¥ {symbol} {timeframe}: {str(e)}")
            raise

    def _execute_screening_core(
        self,
        factors: pd.DataFrame,
        close_prices: pd.Series,
        symbol: str,
        timeframe: str,
        start_time: float,
    ) -> Dict[str, FactorMetrics]:
        """æ‰§è¡Œç­›é€‰çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä»åŸå‡½æ•°ä¸­æå–ï¼‰"""
        # ç›´æ¥è°ƒç”¨ä¸»ç­›é€‰æ–¹æ³•çš„é€»è¾‘ï¼Œä½†ä½¿ç”¨å·²æœ‰çš„æ•°æ®
        self.logger.info(f"å¼€å§‹{symbol} {timeframe} æ ¸å¿ƒç­›é€‰åˆ†æ...")

        # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
        self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
        returns = close_prices.pct_change()  # å½“æœŸæ”¶ç›Šï¼Œé¿å…æœªæ¥å‡½æ•°

        # æ—¶é—´å¯¹é½
        common_index = factors.index.intersection(close_prices.index)

        # å¦‚æœå¯¹é½å¤±è´¥ï¼Œå°è¯•è¯Šæ–­å¹¶ä¿®å¤
        if len(common_index) == 0:
            self.logger.error("æ•°æ®å¯¹é½å¤±è´¥ï¼å°è¯•è¯Šæ–­...")
            self.logger.error(f"  å› å­å‰5ä¸ªæ—¶é—´: {factors.index[:5].tolist()}")
            self.logger.error(f"  ä»·æ ¼å‰5ä¸ªæ—¶é—´: {close_prices.index[:5].tolist()}")

            # å¯¹äºdailyæ•°æ®ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ
            if timeframe == "daily":
                self.logger.info("æ£€æµ‹åˆ°dailyæ—¶é—´æ¡†æ¶ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ...")
                factors.index = factors.index.normalize()
                close_prices.index = close_prices.index.normalize()
                returns.index = returns.index.normalize()
                common_index = factors.index.intersection(close_prices.index)
                self.logger.info(f"æ ‡å‡†åŒ–åå…±åŒæ—¶é—´ç‚¹: {len(common_index)}")

        if len(common_index) < self.config.min_sample_size:
            raise ValueError(
                f"æ•°æ®å¯¹é½åæ ·æœ¬é‡ä¸è¶³: {len(common_index)} < {self.config.min_sample_size}"
            )

        factors_aligned = factors.loc[common_index]
        returns_aligned = returns.loc[common_index]
        factor_metadata = self._generate_factor_metadata(factors_aligned)

        self.logger.info(
            f"æ•°æ®å¯¹é½å®Œæˆ: æ ·æœ¬é‡={len(common_index)}, å› å­æ•°={len(factors_aligned.columns)}"
        )

        # 3. 5ç»´åº¦åˆ†æ
        all_metrics = {}

        # 3.1 é¢„æµ‹èƒ½åŠ›åˆ†æ
        self.logger.info("æ­¥éª¤3.1: é¢„æµ‹èƒ½åŠ›åˆ†æ...")
        all_metrics["multi_horizon_ic"] = self.calculate_multi_horizon_ic(
            factors_aligned, returns_aligned
        )
        all_metrics["ic_decay"] = self.analyze_ic_decay(all_metrics["multi_horizon_ic"])

        # 3.2 ç¨³å®šæ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.2: ç¨³å®šæ€§åˆ†æ...")
        all_metrics["rolling_ic"] = self.calculate_rolling_ic(
            factors_aligned, returns_aligned
        )
        all_metrics["cross_section_stability"] = (
            self.calculate_cross_sectional_stability(factors_aligned)
        )

        # 3.3 ç‹¬ç«‹æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.3: ç‹¬ç«‹æ€§åˆ†æ...")
        all_metrics["vif_scores"] = self.calculate_vif_scores(
            factors_aligned, vif_threshold=self.config.vif_threshold
        )
        all_metrics["correlation_matrix"] = self.calculate_factor_correlation_matrix(
            factors_aligned
        )
        all_metrics["information_increment"] = self.calculate_information_increment(
            factors_aligned, returns_aligned
        )

        # 3.4 å®ç”¨æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.4: å®ç”¨æ€§åˆ†æ...")
        price_data = pd.DataFrame(
            {
                "close": close_prices,
                "volume": factors_aligned.get(
                    "volume", pd.Series(index=factors_aligned.index, dtype=float)
                ),
            }
        )
        all_metrics["trading_costs"] = self.calculate_trading_costs(
            factors_aligned, price_data, factor_metadata
        )
        all_metrics["liquidity_requirements"] = self.calculate_liquidity_requirements(
            factors_aligned, price_data["volume"]
        )

        # 3.5 çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ
        self.logger.info("æ­¥éª¤3.5: çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ...")
        all_metrics["reversal_effects"] = self.detect_reversal_effects(
            factors_aligned, returns_aligned
        )
        all_metrics["momentum_persistence"] = self.analyze_momentum_persistence(
            factors_aligned, returns_aligned
        )
        all_metrics["volatility_sensitivity"] = self.analyze_volatility_sensitivity(
            factors_aligned, returns_aligned
        )

        # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        self.logger.info("æ­¥éª¤4: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

        # æ”¶é›†på€¼
        p_values = {}
        for factor, ic_data in all_metrics["multi_horizon_ic"].items():
            # ä½¿ç”¨1æ—¥ICçš„på€¼ä½œä¸ºä¸»è¦æ˜¾è‘—æ€§æŒ‡æ ‡
            p_values[factor] = ic_data.get("p_value_1d", 1.0)

        all_metrics["p_values"] = p_values

        # FDRæ ¡æ­£ï¼ˆä¼ å…¥æ ·æœ¬é‡ç”¨äºè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´ï¼‰
        sample_size = len(factors_aligned)
        if self.config.fdr_method == "benjamini_hochberg":
            corrected_p, adaptive_alpha = self.benjamini_hochberg_correction(
                p_values, sample_size=sample_size
            )
        else:
            corrected_p, adaptive_alpha = self.bonferroni_correction(p_values)

        all_metrics["corrected_p_values"] = corrected_p
        all_metrics["adaptive_alpha"] = adaptive_alpha

        # 5. ç»¼åˆè¯„åˆ†
        self.logger.info("æ­¥éª¤5: ç»¼åˆè¯„åˆ†...")
        comprehensive_results = self.calculate_comprehensive_scores(all_metrics)

        # æ€§èƒ½ç»Ÿè®¡
        duration = time.time() - start_time

        # ä¿å­˜ç»“æœï¼ˆä¿å­˜åˆ°æ—¶é—´æ¡†æ¶å­ç›®å½•ï¼‰
        screening_stats = {
            "total_factors": len(comprehensive_results),
            "significant_factors": sum(
                1 for m in comprehensive_results.values() if m.is_significant
            ),
            "high_score_factors": sum(
                1 for m in comprehensive_results.values() if m.comprehensive_score > 0.7
            ),
            "total_time": duration,
            "sample_size": len(factors_aligned),
            "symbol": symbol,
            "timeframe": timeframe,
        }
        data_quality_info = {
            "factor_data_shape": factors.shape,
            "aligned_data_shape": factors_aligned.shape,
            "data_overlap_count": len(common_index),
        }
        self.save_comprehensive_screening_info(
            comprehensive_results, symbol, timeframe, screening_stats, data_quality_info
        )
        self.logger.info(f"âœ… {symbol} {timeframe} ç­›é€‰å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        self.logger.info(f"   æ€»å› å­æ•°: {len(comprehensive_results)}")
        self.logger.info(
            f"   é¡¶çº§å› å­æ•°: {sum(1 for m in comprehensive_results.values() if m.comprehensive_score >= 0.8)}"
        )

        return comprehensive_results

    def generate_multi_timeframe_summary(
        self,
        symbol: str,
        timeframes: List[str],
        all_results: Dict[str, Dict[str, FactorMetrics]],
    ) -> Dict:
        """
        ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ

        Returns:
            Dict: æ±‡æ€»æŠ¥å‘Šæ•°æ®
        """
        from datetime import datetime

        summary = {
            "symbol": symbol,
            "timeframes": timeframes,
            "generation_time": datetime.now().isoformat(),
            "session_timestamp": self.session_timestamp,
            "total_timeframes": len(timeframes),
            "timeframe_summary": {},
            "cross_timeframe_analysis": {},
            "top_factors_by_timeframe": {},
            "consensus_factors": [],
            "performance_comparison": {},
        }

        # æŒ‰æ—¶é—´æ¡†æ¶æ±‡æ€»
        for tf in timeframes:
            if tf in all_results:
                tf_results = all_results[tf]
                tf_summary = {
                    "total_factors": len(tf_results),
                    "significant_factors": sum(
                        1 for m in tf_results.values() if m.p_value < 0.05
                    ),
                    "top_factors": sum(
                        1 for m in tf_results.values() if m.comprehensive_score >= 0.8
                    ),
                    "average_ic": (
                        sum(m.ic_mean for m in tf_results.values()) / len(tf_results)
                        if tf_results
                        else 0
                    ),
                    "average_score": (
                        sum(m.comprehensive_score for m in tf_results.values())
                        / len(tf_results)
                        if tf_results
                        else 0
                    ),
                }
                summary["timeframe_summary"][tf] = tf_summary

                # é¡¶çº§å› å­åˆ—è¡¨
                top_factors = sorted(
                    [(name, metrics) for name, metrics in tf_results.items()],
                    key=lambda x: x[1].comprehensive_score,
                    reverse=True,
                )[
                    :10
                ]  # å–å‰10ä¸ª
                summary["top_factors_by_timeframe"][tf] = [
                    {
                        "factor": name,
                        "score": metrics.comprehensive_score,
                        "ic_mean": metrics.ic_mean,
                    }
                    for name, metrics in top_factors
                ]

        # è·¨æ—¶é—´æ¡†æ¶åˆ†æ - å¯»æ‰¾å…±è¯†å› å­
        if len(all_results) > 1:
            # æ‰¾å‡ºåœ¨å¤šä¸ªæ—¶é—´æ¡†æ¶ä¸­éƒ½è¡¨ç°ä¼˜ç§€çš„å› å­
            factor_performance = {}
            for tf, tf_results in all_results.items():
                for factor_name, metrics in tf_results.items():
                    if factor_name not in factor_performance:
                        factor_performance[factor_name] = {}
                    factor_performance[factor_name][tf] = metrics.comprehensive_score

            # è®¡ç®—å…±è¯†å› å­ï¼ˆåœ¨è¶…è¿‡ä¸€åŠçš„æ—¶é—´æ¡†æ¶ä¸­å¾—åˆ†>=0.7çš„å› å­ï¼‰
            consensus_threshold = 0.7
            min_timeframes = max(1, len(timeframes) // 2)

            consensus_factors = []
            for factor_name, tf_scores in factor_performance.items():
                high_score_count = sum(
                    1 for score in tf_scores.values() if score >= consensus_threshold
                )
                if high_score_count >= min_timeframes:
                    avg_score = sum(tf_scores.values()) / len(tf_scores)
                    consensus_factors.append(
                        {
                            "factor": factor_name,
                            "average_score": avg_score,
                            "high_score_count": high_score_count,
                            "scores_by_timeframe": tf_scores,
                        }
                    )

            # æŒ‰å¹³å‡åˆ†æ•°æ’åº
            consensus_factors.sort(key=lambda x: x["average_score"], reverse=True)
            summary["consensus_factors"] = consensus_factors[:20]  # å–å‰20ä¸ªå…±è¯†å› å­

        return summary

    def save_multi_timeframe_summary(
        self,
        symbol: str,
        timeframes: List[str],
        all_results: Dict[str, Dict[str, FactorMetrics]],
    ):
        """
        ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨
            all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        """
        if not hasattr(self, "multi_tf_session_dir"):
            self.logger.warning("å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•æœªè®¾ç½®ï¼Œè·³è¿‡æ±‡æ€»æŠ¥å‘Šä¿å­˜")
            return

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = self.generate_multi_timeframe_summary(symbol, timeframes, all_results)

        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = self.multi_tf_session_dir / "multi_timeframe_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(
                self._to_json_serializable(summary), f, indent=2, ensure_ascii=False
            )

        # ç”Ÿæˆæ€»ç´¢å¼•æ–‡ä»¶
        index_file = self.multi_tf_session_dir / "index.txt"
        with open(index_file, "w", encoding="utf-8") as f:
            f.write("å¤šæ—¶é—´æ¡†æ¶å› å­ç­›é€‰å®Œæ•´æŠ¥å‘Šç´¢å¼•\n")
            f.write(f"{'='*50}\n\n")
            f.write("åŸºç¡€ä¿¡æ¯:\n")
            f.write(f"  è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"  æ—¶é—´æ¡†æ¶: {', '.join(timeframes)}\n")
            f.write(f"  ç”Ÿæˆæ—¶é—´: {summary['generation_time']}\n")
            f.write(f"  ä¼šè¯ID: {summary['session_timestamp']}\n\n")

            f.write("ç›®å½•ç»“æ„:\n")
            f.write("  1. timeframes/ - å„æ—¶é—´æ¡†æ¶è¯¦ç»†åˆ†æç»“æœ\n")
            for i, tf in enumerate(timeframes, 1):
                f.write(f"     {i}. {tf}/ - {tf}æ—¶é—´æ¡†æ¶åˆ†æ\n")
            f.write("  2. multi_timeframe_summary.json - å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š\n")
            f.write("  3. index.txt - æœ¬ç´¢å¼•æ–‡ä»¶\n\n")

            f.write("ä½¿ç”¨è¯´æ˜:\n")
            f.write("  - æŸ¥çœ‹å„æ—¶é—´æ¡†æ¶è¯¦ç»†ç»“æœ: è¿›å…¥ timeframes/ ç›®å½•\n")
            f.write("  - æŸ¥çœ‹å¤šæ—¶é—´æ¡†æ¶å¯¹æ¯”åˆ†æ: æ‰“å¼€ multi_timeframe_summary.json\n")
            f.write("  - å¯»æ‰¾å…±è¯†å› å­: æŸ¥çœ‹æ±‡æ€»æŠ¥å‘Šä¸­çš„ consensus_factors éƒ¨åˆ†\n\n")

            # æ·»åŠ å„æ—¶é—´æ¡†æ¶æ¦‚è¦
            f.write("å„æ—¶é—´æ¡†æ¶æ¦‚è¦:\n")
            for tf in timeframes:
                if tf in summary["timeframe_summary"]:
                    tf_summary = summary["timeframe_summary"][tf]
                    f.write(f"  {tf}:\n")
                    f.write(f"    æ€»å› å­æ•°: {tf_summary['total_factors']}\n")
                    f.write(f"    æ˜¾è‘—å› å­: {tf_summary['significant_factors']}\n")
                    f.write(f"    é¡¶çº§å› å­: {tf_summary['top_factors']}\n")
                    f.write(f"    å¹³å‡IC: {tf_summary['average_ic']:.4f}\n")
                    f.write(f"    å¹³å‡è¯„åˆ†: {tf_summary['average_score']:.3f}\n\n")

            if summary["consensus_factors"]:
                f.write("é¡¶çº§å…±è¯†å› å­ (å‰5ä¸ª):\n")
                for i, factor in enumerate(summary["consensus_factors"][:5], 1):
                    f.write(
                        f"  {i}. {factor['factor']} - è¯„åˆ†: {factor['average_score']:.3f}\n"
                    )
                    f.write(
                        f"     åœ¨{factor['high_score_count']}ä¸ªæ—¶é—´æ¡†æ¶ä¸­è¡¨ç°ä¼˜ç§€\n"
                    )

        self.logger.info(f"âœ… å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.multi_tf_session_dir}")
        self.logger.info(f"   æ—¶é—´æ¡†æ¶æ•°é‡: {len(timeframes)}")
        self.logger.info(f"   å…±è¯†å› å­æ•°é‡: {len(summary['consensus_factors'])}")

    def screen_multiple_timeframes(
        self, symbol: str, timeframes: List[str]
    ) -> Dict[str, Dict[str, FactorMetrics]]:
        """
        å¤šæ—¶é—´æ¡†æ¶ç­›é€‰çš„ä¸»å…¥å£å‡½æ•°

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframes: æ—¶é—´æ¡†æ¶åˆ—è¡¨

        Returns:
            Dict[str, Dict[str, FactorMetrics]]: å„æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        """
        from datetime import datetime

        start_time = datetime.now()

        self.logger.info(f"ğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶å› å­ç­›é€‰: {symbol}")
        self.logger.info(f"   æ—¶é—´æ¡†æ¶: {', '.join(timeframes)}")
        self.logger.info(f"   ä¼šè¯æ—¶é—´æˆ³: {self.session_timestamp}")

        # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥å› å­æ–‡ä»¶å¯¹é½æ€§
        try:
            from utils import (
                find_aligned_factor_files,
                validate_factor_alignment,
            )

            self.logger.info("ğŸ” æ£€æŸ¥å› å­æ–‡ä»¶å¯¹é½æ€§...")

            # ç¡®ä¿æ•°æ®æ ¹ç›®å½•æ­£ç¡®
            factor_data_root = Path(self.data_root)
            self.logger.info(f"ğŸ” ä½¿ç”¨å› å­æ•°æ®æ ¹ç›®å½•: {factor_data_root}")

            aligned_files = find_aligned_factor_files(
                factor_data_root, symbol, timeframes
            )
            self.logger.info("âœ… æ‰¾åˆ°å¯¹é½çš„å› å­æ–‡ä»¶:")
            for tf, file_path in aligned_files.items():
                self.logger.info(f"   {tf}: {file_path.name}")

            # éªŒè¯æ—¶é—´å¯¹é½æ€§
            is_aligned, alignment_msg = validate_factor_alignment(
                factor_data_root, symbol, timeframes, aligned_files
            )
            if is_aligned:
                self.logger.info(f"âœ… æ—¶é—´å¯¹é½éªŒè¯é€šè¿‡: {alignment_msg}")
            else:
                self.logger.warning(f"âš ï¸ æ—¶é—´å¯¹é½éªŒè¯è­¦å‘Š: {alignment_msg}")

            # ä¿å­˜å¯¹é½çš„æ–‡ä»¶ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
            self.aligned_factor_files = aligned_files

        except Exception as e:
            self.logger.warning(f"âš ï¸ å› å­å¯¹é½æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶é€‰æ‹©: {str(e)}")
            self.aligned_factor_files = None

        # è®¾ç½®å¤šæ—¶é—´æ¡†æ¶ä¼šè¯ç›®å½•ç»“æ„
        main_session_dir = self.setup_multi_timeframe_session(symbol, timeframes)
        self.logger.info(f"ğŸ“ ä¸»ä¼šè¯ç›®å½•: {main_session_dir}")

        # åˆå§‹åŒ–ä¸»æ—¥å¿—è®°å½•å™¨ï¼ˆç”¨äºè®°å½•æ€»ä½“è¿›åº¦ï¼‰
        # ä¸´æ—¶è®¾ç½®æ—¥å¿—æ ¹ç›®å½•åˆ°ä¸»ä¼šè¯ç›®å½•
        original_log_root = self.log_root
        self.log_root = main_session_dir
        main_logger = self._setup_logger(f"{self.session_timestamp}_main")
        # æ¢å¤åŸå§‹æ—¥å¿—æ ¹ç›®å½•
        self.log_root = original_log_root

        try:
            # é€ä¸ªå¤„ç†æ¯ä¸ªæ—¶é—´æ¡†æ¶
            all_results = {}
            successful_timeframes = []
            failed_timeframes = []

            for i, timeframe in enumerate(timeframes, 1):
                main_logger.info(f"å¤„ç†æ—¶é—´æ¡†æ¶ {i}/{len(timeframes)}: {timeframe}")

                # å†…å­˜æ¸…ç†ï¼šæ¯ä¸ªæ—¶é—´æ¡†æ¶å¼€å§‹å‰æ¸…ç†
                import gc

                gc.collect()

                try:
                    # ç­›é€‰å•ä¸ªæ—¶é—´æ¡†æ¶
                    tf_results = self.screen_single_timeframe_in_multi_session(
                        symbol, timeframe
                    )
                    all_results[timeframe] = tf_results
                    successful_timeframes.append(timeframe)

                    main_logger.info(
                        f"âœ… {timeframe} ç­›é€‰å®Œæˆ - é¡¶çº§å› å­æ•°: {sum(1 for m in tf_results.values() if m.comprehensive_score >= 0.8)}"
                    )

                except Exception as e:
                    failed_timeframes.append(timeframe)
                    main_logger.error(f"âŒ {timeframe} ç­›é€‰å¤±è´¥: {str(e)}")

                    if (
                        len(failed_timeframes) > len(timeframes) // 2
                    ):  # å¦‚æœå¤±è´¥è¶…è¿‡ä¸€åŠï¼Œåœæ­¢æ‰§è¡Œ
                        main_logger.error("å¤±è´¥æ—¶é—´æ¡†æ¶è¿‡å¤šï¼Œåœæ­¢æ‰§è¡Œ")
                        break

            # ä¿å­˜å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š
            if all_results:
                main_logger.info("ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š...")
                self.save_multi_timeframe_summary(symbol, timeframes, all_results)

            # å®Œæˆç»Ÿè®¡
            total_duration = (datetime.now() - start_time).total_seconds()
            main_logger.info("ğŸ‰ å¤šæ—¶é—´æ¡†æ¶ç­›é€‰å®Œæˆ!")
            main_logger.info(f"   æ€»è€—æ—¶: {total_duration:.2f}ç§’")
            main_logger.info(
                f"   æˆåŠŸæ—¶é—´æ¡†æ¶: {len(successful_timeframes)}/{len(timeframes)}"
            )
            main_logger.info(
                f"   å¤±è´¥æ—¶é—´æ¡†æ¶: {', '.join(failed_timeframes) if failed_timeframes else 'æ— '}"
            )

            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_factors = sum(len(results) for results in all_results.values())
            total_top_factors = sum(
                sum(1 for m in results.values() if m.comprehensive_score >= 0.8)
                for results in all_results.values()
            )

            main_logger.info(f"   æ€»å› å­æ•°: {total_factors}")
            main_logger.info(f"   æ€»é¡¶çº§å› å­æ•°: {total_top_factors}")
            if len(all_results) > 0:
                main_logger.info(
                    f"   å¹³å‡æ¯æ—¶é—´æ¡†æ¶é¡¶çº§å› å­æ•°: {total_top_factors/len(all_results):.1f}"
                )
            else:
                main_logger.info("   å¹³å‡æ¯æ—¶é—´æ¡†æ¶é¡¶çº§å› å­æ•°: 0.0 (æ— æˆåŠŸç»“æœ)")

            return all_results

        except Exception as e:
            main_logger.error(f"å¤šæ—¶é—´æ¡†æ¶ç­›é€‰è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}")
            raise

    def screen_factors_comprehensive(
        self, symbol: str, timeframe: str = "60min"
    ) -> Dict[str, FactorMetrics]:
        """ä¸»ç­›é€‰å‡½æ•° - 5ç»´åº¦ç»¼åˆç­›é€‰"""

        # åˆ›å»ºä¼šè¯ç›®å½•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
        if not hasattr(self, "session_dir") or not self.session_dir:
            session_id = f"{symbol}_{timeframe}_{self.session_timestamp}"
            self.session_dir = self.screening_results_dir / session_id
            self.session_dir.mkdir(parents=True, exist_ok=True)
        else:
            # ä»ç°æœ‰çš„session_diræå–session_id
            session_id = self.session_dir.name

        start_time = time.time()
        self.logger.info(f"ğŸ“ åˆ›å»ºä¼šè¯ç›®å½•: {self.session_dir}")
        self.logger.info(f"å¼€å§‹5ç»´åº¦å› å­ç­›é€‰: {symbol} {timeframe}")

        try:
            # 1. æ•°æ®åŠ è½½
            self.logger.info("æ­¥éª¤1: æ•°æ®åŠ è½½...")
            factors = self.load_factors(symbol, timeframe)
            price_data = self.load_price_data(symbol, timeframe)  # ä¼ é€’timeframeå‚æ•°

            # 2. æ•°æ®é¢„å¤„ç†å’Œå¯¹é½
            self.logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç†...")
            close_prices = price_data["close"]
            returns = close_prices.pct_change()  # å½“æœŸæ”¶ç›Šï¼Œé¿å…æœªæ¥å‡½æ•°

            # æ·»åŠ è¯Šæ–­æ—¥å¿— - å…³é”®ä¿®å¤
            self.logger.info("æ•°æ®å¯¹é½å‰è¯Šæ–­:")
            self.logger.info(
                f"  å› å­æ•°æ®: {len(factors)} è¡Œ, æ—¶é—´ {factors.index.min()} åˆ° {factors.index.max()}"
            )
            self.logger.info(
                f"  ä»·æ ¼æ•°æ®: {len(close_prices)} è¡Œ, æ—¶é—´ {close_prices.index.min()} åˆ° {close_prices.index.max()}"
            )

            # æ—¶é—´å¯¹é½
            common_index = factors.index.intersection(close_prices.index)

            # å¦‚æœå¯¹é½å¤±è´¥ï¼Œå°è¯•è¯Šæ–­å¹¶ä¿®å¤
            if len(common_index) == 0:
                self.logger.error("æ•°æ®å¯¹é½å¤±è´¥ï¼å°è¯•è¯Šæ–­...")
                self.logger.error(f"  å› å­å‰5ä¸ªæ—¶é—´: {factors.index[:5].tolist()}")
                self.logger.error(f"  ä»·æ ¼å‰5ä¸ªæ—¶é—´: {close_prices.index[:5].tolist()}")

                # å¯¹äºdailyæ•°æ®ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ
                if timeframe == "daily":
                    self.logger.info("æ£€æµ‹åˆ°dailyæ—¶é—´æ¡†æ¶ï¼Œå°è¯•æ ‡å‡†åŒ–åˆ°æ—¥æœŸ...")
                    factors.index = factors.index.normalize()
                    close_prices.index = close_prices.index.normalize()
                    returns.index = returns.index.normalize()
                    common_index = factors.index.intersection(close_prices.index)
                    self.logger.info(f"æ ‡å‡†åŒ–åå…±åŒæ—¶é—´ç‚¹: {len(common_index)}")

            if len(common_index) < self.config.min_sample_size:
                raise ValueError(
                    f"æ•°æ®å¯¹é½åæ ·æœ¬é‡ä¸è¶³: {len(common_index)} < {self.config.min_sample_size}"
                )

            factors_aligned = factors.loc[common_index]
            returns_aligned = returns.loc[common_index]
            prices_aligned = price_data.loc[common_index]
            factor_metadata = self._generate_factor_metadata(factors_aligned)

            self.logger.info(
                f"æ•°æ®å¯¹é½å®Œæˆ: æ ·æœ¬é‡={len(common_index)}, å› å­æ•°={len(factors_aligned.columns)}"
            )

            # 3. 5ç»´åº¦åˆ†æ
            all_metrics = {}

            # 3.1 é¢„æµ‹èƒ½åŠ›åˆ†æ
            self.logger.info("æ­¥éª¤3.1: é¢„æµ‹èƒ½åŠ›åˆ†æ...")
            all_metrics["multi_horizon_ic"] = self.calculate_multi_horizon_ic(
                factors_aligned, returns_aligned
            )
            all_metrics["ic_decay"] = self.analyze_ic_decay(
                all_metrics["multi_horizon_ic"]
            )

            # 3.2 ç¨³å®šæ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.2: ç¨³å®šæ€§åˆ†æ...")
            all_metrics["rolling_ic"] = self.calculate_rolling_ic(
                factors_aligned, returns_aligned
            )
            all_metrics["cross_section_stability"] = (
                self.calculate_cross_sectional_stability(factors_aligned)
            )

            # å†…å­˜æ¸…ç†
            import gc

            gc.collect()

            # 3.3 ç‹¬ç«‹æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.3: ç‹¬ç«‹æ€§åˆ†æ...")
            all_metrics["vif_scores"] = self.calculate_vif_scores(
                factors_aligned, vif_threshold=self.config.vif_threshold
            )
            all_metrics["correlation_matrix"] = (
                self.calculate_factor_correlation_matrix(factors_aligned)
            )
            all_metrics["information_increment"] = self.calculate_information_increment(
                factors_aligned, returns_aligned
            )

            # å†…å­˜æ¸…ç†
            gc.collect()

            # 3.4 å®ç”¨æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.4: å®ç”¨æ€§åˆ†æ...")
            all_metrics["trading_costs"] = self.calculate_trading_costs(
                factors_aligned, prices_aligned, factor_metadata
            )
            all_metrics["liquidity_requirements"] = (
                self.calculate_liquidity_requirements(
                    factors_aligned, prices_aligned["volume"]
                )
            )

            # 3.5 çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ
            self.logger.info("æ­¥éª¤3.5: çŸ­å‘¨æœŸé€‚åº”æ€§åˆ†æ...")
            all_metrics["reversal_effects"] = self.detect_reversal_effects(
                factors_aligned, returns_aligned
            )
            all_metrics["momentum_persistence"] = self.analyze_momentum_persistence(
                factors_aligned, returns_aligned
            )
            all_metrics["volatility_sensitivity"] = self.analyze_volatility_sensitivity(
                factors_aligned, returns_aligned
            )

            # 4. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
            self.logger.info("æ­¥éª¤4: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")

            # æ”¶é›†på€¼
            p_values = {}
            for factor, ic_data in all_metrics["multi_horizon_ic"].items():
                # ä½¿ç”¨1æ—¥ICçš„på€¼ä½œä¸ºä¸»è¦æ˜¾è‘—æ€§æŒ‡æ ‡
                p_values[factor] = ic_data.get("p_value_1d", 1.0)

            all_metrics["p_values"] = p_values

            # FDRæ ¡æ­£ï¼ˆä¼ å…¥æ ·æœ¬é‡ç”¨äºè‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´ï¼‰
            sample_size = len(factors_aligned)
            if self.config.fdr_method == "benjamini_hochberg":
                corrected_p, adaptive_alpha = self.benjamini_hochberg_correction(
                    p_values, sample_size=sample_size
                )
            else:
                corrected_p, adaptive_alpha = self.bonferroni_correction(p_values)

            all_metrics["corrected_p_values"] = corrected_p
            all_metrics["adaptive_alpha"] = adaptive_alpha

            # 5. ç»¼åˆè¯„åˆ†
            self.logger.info("æ­¥éª¤5: ç»¼åˆè¯„åˆ†...")
            comprehensive_results = self.calculate_comprehensive_scores(all_metrics)

            # 6. æ€§èƒ½ç»Ÿè®¡
            total_time = time.time() - start_time
            current_memory = self.process.memory_info().rss / 1024 / 1024
            # é‡æ–°è·å–èµ·å§‹å†…å­˜ä»¥é¿å…è´Ÿå€¼
            if not hasattr(self, "_session_start_memory"):
                self._session_start_memory = self.start_memory
            memory_used = max(0, current_memory - self._session_start_memory)

            self.logger.info("5ç»´åº¦ç­›é€‰å®Œæˆ:")
            self.logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
            self.logger.info(f"  - å†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
            self.logger.info(f"  - å› å­æ€»æ•°: {len(comprehensive_results)}")

            # ç»Ÿè®¡å„ç»´åº¦è¡¨ç°
            significant_count = sum(
                1 for m in comprehensive_results.values() if m.is_significant
            )
            high_score_count = sum(
                1 for m in comprehensive_results.values() if m.comprehensive_score > 0.7
            )

            self.logger.info(f"  - æ˜¾è‘—å› å­: {significant_count}")
            self.logger.info(f"  - é«˜åˆ†å› å­: {high_score_count}")

            # 7. æ”¶é›†ç­›é€‰ç»Ÿè®¡ä¿¡æ¯
            screening_stats = {
                "total_factors": len(comprehensive_results),
                "significant_factors": significant_count,
                "high_score_factors": high_score_count,
                "total_time": total_time,
                "memory_used_mb": memory_used,
                "sample_size": len(common_index) if "common_index" in locals() else 0,
                "factors_aligned": (
                    len(factors_aligned.columns) if "factors_aligned" in locals() else 0
                ),
                "data_alignment_successful": (
                    len(common_index) > 0 if "common_index" in locals() else False
                ),
                "screening_timestamp": datetime.now().isoformat(),
                "symbol": symbol,
                "timeframe": timeframe,
            }

            # 8. æ”¶é›†æ•°æ®è´¨é‡ä¿¡æ¯
            data_quality_info = {
                "factor_data_shape": factors.shape if "factors" in locals() else None,
                "price_data_shape": (
                    price_data.shape if "price_data" in locals() else None
                ),
                "aligned_data_shape": (
                    factors_aligned.shape if "factors_aligned" in locals() else None
                ),
                "data_overlap_count": (
                    len(common_index) if "common_index" in locals() else 0
                ),
                "factor_data_range": {
                    "start": (
                        factors.index.min().isoformat()
                        if "factors" in locals() and len(factors) > 0
                        else None
                    ),
                    "end": (
                        factors.index.max().isoformat()
                        if "factors" in locals() and len(factors) > 0
                        else None
                    ),
                },
                "price_data_range": {
                    "start": (
                        price_data.index.min().isoformat()
                        if "price_data" in locals() and len(price_data) > 0
                        else None
                    ),
                    "end": (
                        price_data.index.max().isoformat()
                        if "price_data" in locals() and len(price_data) > 0
                        else None
                    ),
                },
                "alignment_success_rate": (
                    len(common_index) / min(len(factors), len(price_data))
                    if "factors" in locals() and "price_data" in locals()
                    else 0.0
                ),
            }

            # 9. ä¿å­˜å®Œæ•´ç­›é€‰ä¿¡æ¯ - ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨
            try:
                if self.result_manager is not None:
                    # ä½¿ç”¨æ–°çš„å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œä¼ é€’ç°æœ‰ä¼šè¯ç›®å½•
                    session_id = self.result_manager.create_screening_session(
                        symbol=symbol,
                        timeframe=timeframe,
                        results=comprehensive_results,
                        screening_stats=screening_stats,
                        config=self.config,
                        data_quality_info=data_quality_info,
                        existing_session_dir=self.session_dir,
                    )

                    self.logger.info(f"âœ… å®Œæ•´ç­›é€‰ä¼šè¯å·²åˆ›å»º: {session_id}")
                    screening_stats["session_id"] = session_id
                else:
                    self.logger.info("ä½¿ç”¨ä¼ ç»Ÿå­˜å‚¨æ–¹å¼")

                # è·³è¿‡ä¼ ç»Ÿæ ¼å¼ä¿å­˜ï¼Œé¿å…é‡å¤ç›®å½•åˆ›å»º
                self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ï¼Œè·³è¿‡ä¼ ç»Ÿæ ¼å¼ä¿å­˜")

            except Exception as e:
                self.logger.error(f"ä¿å­˜å®Œæ•´ç­›é€‰ä¿¡æ¯å¤±è´¥: {str(e)}")
                screening_stats["save_error"] = str(e)

            return comprehensive_results

        except Exception as e:
            self.logger.error(f"å› å­ç­›é€‰å¤±è´¥: {str(e)}")
            raise

    def generate_screening_report(
        self,
        results: Dict[str, FactorMetrics],
        output_path: str = None,
        symbol: str = None,
        timeframe: str = None,
    ) -> pd.DataFrame:
        """ç”Ÿæˆç­›é€‰æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆç­›é€‰æŠ¥å‘Š...")

        if not results:
            self.logger.warning("æ²¡æœ‰ç­›é€‰ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return pd.DataFrame()

        # è½¬æ¢ä¸ºDataFrame
        report_data = []
        for factor_name, metrics in results.items():
            row = {
                "Factor": factor_name,
                "Comprehensive_Score": metrics.comprehensive_score,
                "Predictive_Score": metrics.predictive_score,
                "Stability_Score": metrics.stability_score,
                "Independence_Score": metrics.independence_score,
                "Practicality_Score": metrics.practicality_score,
                "Adaptability_Score": metrics.adaptability_score,
                "IC_Mean": metrics.ic_mean,
                "IC_IR": metrics.ic_ir,
                "IC_1d": metrics.ic_1d,
                "IC_5d": metrics.ic_5d,
                "IC_10d": metrics.ic_10d,
                "Rolling_IC_Stability": metrics.rolling_ic_stability,
                "IC_Consistency": metrics.ic_consistency,
                "VIF_Score": metrics.vif_score,
                "Max_Correlation": metrics.correlation_max,
                "Info_Increment": metrics.information_increment,
                "Turnover_Rate": metrics.turnover_rate,
                "Transaction_Cost": metrics.transaction_cost,
                "Cost_Efficiency": metrics.cost_efficiency,
                "P_Value": metrics.p_value,
                "Corrected_P_Value": metrics.corrected_p_value,
                "Is_Significant": metrics.is_significant,
                "Sample_Size": metrics.sample_size,
            }
            report_data.append(row)

        report_df = pd.DataFrame(report_data)
        report_df = report_df.sort_values("Comprehensive_Score", ascending=False)

        # ä¿å­˜æŠ¥å‘Šï¼ˆåŒ…å«æ—¶é—´æ¡†æ¶æ ‡è¯†ï¼‰
        if output_path is None:
            # ä¼˜å…ˆä½¿ç”¨ä¼šè¯ç›®å½•
            if self.session_dir is not None:
                output_path = self.session_dir / "screening_report.csv"
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                symbol_info = symbol or results.get("symbol", "unknown")
                timeframe_info = timeframe or results.get("timeframe", "unknown")
                output_path = (
                    self.screening_results_dir
                    / f"screening_report_{symbol_info}_{timeframe_info}_{timestamp}.csv"
                )

        # ç¡®ä¿è·¯å¾„æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œé¿å…pandas Path._flavouré—®é¢˜
        output_path_str = str(output_path)
        report_df.to_csv(output_path_str, index=False)
        self.logger.info(f"ç­›é€‰æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

        return report_df

    def save_comprehensive_screening_info(
        self,
        results: Dict[str, FactorMetrics],
        symbol: str,
        timeframe: str,
        screening_stats: Dict,
        data_quality_info: Dict = None,
    ):
        """ä¿å­˜å®Œæ•´çš„ç­›é€‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¤šä¸ªæ ¼å¼çš„æŠ¥å‘Š"""

        # ä½¿ç”¨ä¼šè¯ç›®å½•å’Œç»Ÿä¸€çš„æ—¶é—´æˆ³
        if self.session_dir is None:
            # å¦‚æœæ²¡æœ‰ä¼šè¯ç›®å½•ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            base_dir = self.screening_results_dir
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = f"screening_{symbol}_{timeframe}_{timestamp}"
        else:
            base_dir = self.session_dir
            base_filename = "screening_report"

        self.logger.info(f"ğŸ’¾ ä¿å­˜ç­›é€‰ä¿¡æ¯åˆ°ä¼šè¯ç›®å½•: {base_dir}")

        # 1. ä¿å­˜è¯¦ç»†çš„CSVæŠ¥å‘Š
        csv_path = base_dir / f"{base_filename}.csv"
        self.generate_screening_report(
            results, str(csv_path), symbol, timeframe
        )

        # 2. ä¿å­˜ç­›é€‰è¿‡ç¨‹ç»Ÿè®¡ä¿¡æ¯
        stats_path = base_dir / "screening_statistics.json"
        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(
                self._to_json_serializable(screening_stats),
                f,
                indent=2,
                ensure_ascii=False,
            )

        # 3. ä¿å­˜é¡¶çº§å› å­æ‘˜è¦
        summary_path = base_dir / "top_factors_summary.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("=== å› å­ç­›é€‰æ‘˜è¦æŠ¥å‘Š ===\n")
            f.write(f"è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"æ—¶é—´æ¡†æ¶: {timeframe}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("\n=== ç­›é€‰ç»Ÿè®¡ ===\n")
            f.write(f"æ€»å› å­æ•°: {screening_stats.get('total_factors', 0)}\n")
            f.write(f"æ˜¾è‘—å› å­: {screening_stats.get('significant_factors', 0)}\n")
            f.write(f"é«˜åˆ†å› å­: {screening_stats.get('high_score_factors', 0)}\n")
            f.write(f"æ€»è€—æ—¶: {screening_stats.get('total_time', 0):.2f}ç§’\n")
            f.write(f"å†…å­˜ä½¿ç”¨: {screening_stats.get('memory_used_mb', 0):.1f}MB\n")

            # è·å–å‰10åå› å­
            top_factors = self.get_top_factors(
                results, top_n=10, min_score=0.0, require_significant=False
            )
            f.write("\n=== å‰10åé¡¶çº§å› å­ ===\n")
            for i, factor in enumerate(top_factors, 1):
                f.write(
                    f"{i:2d}. {factor.name:<25} ç»¼åˆå¾—åˆ†: {factor.comprehensive_score:.3f} "
                )
                f.write(
                    f"é¢„æµ‹èƒ½åŠ›: {factor.predictive_score:.3f} æ˜¾è‘—æ€§: {'âœ“' if factor.is_significant else 'âœ—'}\n"
                )

        # 4. ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
        if data_quality_info:
            quality_path = base_dir / "data_quality.json"
            with open(quality_path, "w", encoding="utf-8") as f:
                json.dump(
                    self._to_json_serializable(data_quality_info),
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

        # 5. ä¿å­˜é…ç½®å‚æ•°è®°å½•
        config_path = base_dir / "config.yaml"
        config_dict = {
            "screening_config": asdict(self.config),
            "execution_info": {
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": self.session_timestamp,
                "data_root": str(self.data_root),
                "screening_results_dir": str(self.screening_results_dir),
                "session_dir": str(self.session_dir),
            },
        }
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(
                config_dict, f, default_flow_style=False, allow_unicode=True, indent=2
            )

        # 6. åˆ›å»ºä¸€ä¸ªä¸»ç´¢å¼•æ–‡ä»¶
        index_path = base_dir / "index.txt"
        with open(index_path, "w", encoding="utf-8") as f:
            f.write("å› å­ç­›é€‰å®Œæ•´æŠ¥å‘Šç´¢å¼•\n")
            f.write("========================\n\n")
            f.write("åŸºç¡€ä¿¡æ¯:\n")
            f.write(f"  è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"  æ—¶é—´æ¡†æ¶: {timeframe}\n")
            f.write(f"  ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("åŒ…å«æ–‡ä»¶:\n")
            f.write(f"  1. {csv_path.name} - è¯¦ç»†å› å­æ•°æ® (CSVæ ¼å¼)\n")
            f.write(f"  2. {stats_path.name} - ç­›é€‰è¿‡ç¨‹ç»Ÿè®¡ (JSONæ ¼å¼)\n")
            f.write(f"  3. {summary_path.name} - é¡¶çº§å› å­æ‘˜è¦ (TXTæ ¼å¼)\n")
            if data_quality_info:
                f.write(f"  4. {quality_path.name} - æ•°æ®è´¨é‡æŠ¥å‘Š (JSONæ ¼å¼)\n")
            f.write(f"  5. {config_path.name} - é…ç½®å‚æ•°è®°å½• (YAMLæ ¼å¼)\n")
            f.write(f"  6. {index_path.name} - æœ¬ç´¢å¼•æ–‡ä»¶\n\n")
            f.write("ä½¿ç”¨è¯´æ˜:\n")
            f.write(f"  - æŸ¥çœ‹é¡¶çº§å› å­: é˜…è¯» {summary_path.name}\n")
            f.write(f"  - è¯¦ç»†æ•°æ®åˆ†æ: æ‰“å¼€ {csv_path.name} ä½¿ç”¨Excelæˆ–pandas\n")
            f.write(f"  - ç­›é€‰è¿‡ç¨‹è¯¦æƒ…: æŸ¥çœ‹ {stats_path.name}\n")
            f.write(f"  - é…ç½®å‚æ•°å‚è€ƒ: æŸ¥çœ‹ {config_path.name}\n")

        self.logger.info(f"âœ… å®Œæ•´ç­›é€‰ä¿¡æ¯å·²ä¿å­˜åˆ°: {base_dir}")
        self.logger.info(f"ğŸ“„ ä¸»ç´¢å¼•æ–‡ä»¶: {index_path}")

        return {
            "csv_report": str(csv_path),
            "stats_json": str(stats_path),
            "summary_txt": str(summary_path),
            "data_quality_json": str(quality_path) if data_quality_info else None,
            "config_yaml": str(config_path),
            "index_txt": str(index_path),
        }

    def get_top_factors(
        self,
        results: Dict[str, FactorMetrics],
        top_n: int = 20,
        min_score: float = 0.5,
        require_significant: bool = True,
    ) -> List[FactorMetrics]:
        """è·å–é¡¶çº§å› å­"""

        # ç­›é€‰æ¡ä»¶
        filtered_results = []
        for metrics in results.values():
            if metrics.comprehensive_score >= min_score:
                if not require_significant or metrics.is_significant:
                    filtered_results.append(metrics)

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        filtered_results.sort(key=lambda x: x.comprehensive_score, reverse=True)

        return filtered_results[:top_n]


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œæ‰¹é‡é…ç½®"""
    import argparse
    import sys

    parser = argparse.ArgumentParser(description="ä¸“ä¸šçº§å› å­ç­›é€‰ç³»ç»Ÿ")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--symbol", type=str, default="0700.HK", help="è‚¡ç¥¨ä»£ç ")
    parser.add_argument("--timeframe", type=str, default="60min", help="æ—¶é—´æ¡†æ¶")

    args = parser.parse_args()

    if args.config:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶
        try:
            from config_manager import ConfigManager

            manager = ConfigManager()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡é…ç½®
            import yaml

            with open(args.config, "r", encoding="utf-8") as f:
                config_data = yaml.safe_load(f)

            if "batch_name" in config_data:
                # æ‰¹é‡é…ç½® - ç›´æ¥å¤„ç†æ‰€æœ‰å­é…ç½®
                import yaml

                with open(args.config, "r", encoding="utf-8") as f:
                    batch_config = yaml.safe_load(f)

                print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç­›é€‰: {batch_config['batch_name']}")
                print(f"ğŸ“Š å­ä»»åŠ¡æ•°é‡: {len(batch_config['screening_configs'])}")
                print("=" * 80)

                successful_tasks = 0
                failed_tasks = 0
                all_results = {}  # æ”¶é›†æ‰€æœ‰ç»“æœ

                print(f"\nğŸš€ å¼€å§‹å¤šæ—¶é—´æ¡†æ¶æ‰¹é‡åˆ†æ: {batch_config['batch_name']}")
                print(f"ğŸ“Š åˆ†ææ—¶é—´æ¡†æ¶æ•°é‡: {len(batch_config['screening_configs'])}")
                print("=" * 80)

                # è·å–ç¬¬ä¸€ä¸ªé…ç½®çš„æ•°æ®æ ¹å’Œè¾“å‡ºç›®å½•
                first_config = batch_config["screening_configs"][0]
                data_root = first_config.get("data_root", "../å› å­è¾“å‡º")
                output_dir = first_config.get("output_dir", "./å› å­ç­›é€‰")

                print(f"ğŸ“ æ•°æ®ç›®å½•: {data_root}")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

                # åˆ›å»ºç»Ÿä¸€çš„æ‰¹é‡ç­›é€‰å™¨
                batch_screener = ProfessionalFactorScreener(data_root=data_root)
                batch_screener.screening_results_dir = Path(output_dir)

                # åˆ›å»ºç»Ÿä¸€çš„æ‰¹é‡ä¼šè¯ç›®å½•
                from datetime import datetime

                batch_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                batch_session_id = (
                    f"{batch_config['batch_name']}_multi_timeframe_{batch_timestamp}"
                )
                batch_screener.session_dir = (
                    batch_screener.screening_results_dir / batch_session_id
                )
                batch_screener.session_dir.mkdir(parents=True, exist_ok=True)
                batch_screener.session_timestamp = batch_timestamp

                print(f"ğŸ“ æ‰¹é‡ä¼šè¯ç›®å½•: {batch_screener.session_dir}")

                for i, sub_config in enumerate(batch_config["screening_configs"], 1):
                    try:
                        print(
                            f"\nğŸ“Š [{i}/{len(batch_config['screening_configs'])}] å¤„ç†: {sub_config['name']}"
                        )
                        print(f"   è‚¡ç¥¨: {sub_config['symbols'][0]}")
                        print(f"   æ—¶é—´æ¡†æ¶: {sub_config['timeframes'][0]}")

                        # ä½¿ç”¨åŒä¸€ä¸ªç­›é€‰å™¨æ‰§è¡Œç­›é€‰ï¼ˆå¤ç”¨ä¼šè¯ç›®å½•ï¼‰
                        result = batch_screener.screen_factors_comprehensive(
                            symbol=sub_config["symbols"][0],
                            timeframe=sub_config["timeframes"][0],
                        )

                        # æ”¶é›†ç»“æœ - result æ˜¯ Dict[str, FactorMetrics]
                        tf_key = (
                            f"{sub_config['symbols'][0]}_{sub_config['timeframes'][0]}"
                        )
                        all_results[tf_key] = result

                        # ç»Ÿè®¡æ˜¾è‘—å› å­æ•°é‡
                        significant_count = sum(
                            1 for m in result.values() if m.is_significant
                        )
                        successful_tasks += 1
                        print(f"   âœ… å®Œæˆ: {significant_count} ä¸ªæ˜¾è‘—å› å­")

                    except Exception as e:
                        failed_tasks += 1
                        print(f"   âŒ å¤±è´¥: {str(e)}")
                        continue

                # ç”Ÿæˆç»Ÿä¸€çš„å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š
                print("\nğŸ“ˆ ç”Ÿæˆç»Ÿä¸€æ±‡æ€»æŠ¥å‘Š...")
                if all_results:
                    _generate_multi_timeframe_summary(
                        batch_screener.session_dir,
                        batch_config["batch_name"],
                        all_results,
                        batch_config["screening_configs"],
                    )

                print("\nâœ… å¤šæ—¶é—´æ¡†æ¶åˆ†æå®Œæˆ:")
                print(f"   æ€»ä»»åŠ¡: {len(batch_config['screening_configs'])}")
                print(f"   æˆåŠŸ: {successful_tasks}")
                print(f"   å¤±è´¥: {failed_tasks}")
                if all_results:
                    print(
                        f"   ğŸ“Š æ±‡æ€»æŠ¥å‘Š: å·²ç”Ÿæˆåˆ°ç»Ÿä¸€ä¼šè¯ç›®å½•: {batch_screener.session_dir}"
                    )
                    print(
                        "   ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼Œæ— éœ€æŸ¥æ‰¾å¤šä¸ªæ—¶é—´æ¡†æ¶å­ç›®å½•"
                    )

                return
            else:
                # å•ä¸ªé…ç½®
                config = manager.load_config(args.config, config_type="screening")
                symbol = config.symbols[0] if config.symbols else args.symbol
                timeframe = (
                    config.timeframes[0] if config.timeframes else args.timeframe
                )
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    else:
        # æ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶æ—¶ï¼Œå°è¯•åŠ è½½é»˜è®¤é…ç½®
        default_config_path = (
            Path(__file__).parent / "configs" / "0700_multi_timeframe_config.yaml"
        )
        if default_config_path.exists():
            try:
                import yaml

                with open(default_config_path, "r", encoding="utf-8") as f:
                    config_data = yaml.safe_load(f)

                # å¦‚æœæ˜¯æ‰¹é‡é…ç½®ï¼ŒåŠ è½½ç¬¬ä¸€ä¸ªå­é…ç½®
                if "batch_name" in config_data and "screening_configs" in config_data:
                    first_sub_config = config_data["screening_configs"][0]
                    from config_manager import ScreeningConfig

                    config = ScreeningConfig(**first_sub_config)
                    print(f"âœ… è‡ªåŠ¨åŠ è½½é»˜è®¤é…ç½®: {default_config_path}")
                    print(f"ğŸ“ æ•°æ®ç›®å½•: {config.data_root}")
                    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
                else:
                    from config_manager import ScreeningConfig

                    config = ScreeningConfig(**config_data)
            except Exception as e:
                print(f"âš ï¸ é»˜è®¤é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®é…ç½®: {e}")
                from config_manager import ScreeningConfig
            config = ScreeningConfig(
                data_root="../å› å­è¾“å‡º",
                output_dir="./å› å­ç­›é€‰",
                ic_horizons=[1, 3, 5, 10, 20],
                min_sample_size=100,
                alpha_level=0.05,
                fdr_method="benjamini_hochberg",
                min_ic_threshold=0.02,
                min_ir_threshold=0.5,
            )
        symbol = args.symbol
        timeframe = args.timeframe

    # å•ä¸ªç­›é€‰ä»»åŠ¡
    screener = ProfessionalFactorScreener(config=config)

    print(f"å¼€å§‹ä¸“ä¸šçº§å› å­ç­›é€‰: {symbol} {timeframe}")
    print("=" * 80)

    try:
        # 5ç»´åº¦ç»¼åˆç­›é€‰
        results = screener.screen_factors_comprehensive(symbol, timeframe)

        # ç”ŸæˆæŠ¥å‘Š
        report_df = screener.generate_screening_report(results)

        # è·å–é¡¶çº§å› å­
        top_factors = screener.get_top_factors(results, top_n=10, min_score=0.6)

        # è¾“å‡ºç»“æœ
        print("\n5ç»´åº¦å› å­ç­›é€‰ç»“æœ:")
        print("=" * 80)
        print(f"æ€»å› å­æ•°é‡: {len(results)}")
        print(f"æ˜¾è‘—å› å­æ•°é‡: {sum(1 for m in results.values() if m.is_significant)}")
        print(
            f"é«˜åˆ†å› å­æ•°é‡ (>0.6): {sum(1 for m in results.values() if m.comprehensive_score > 0.6)}"
        )
        print(f"é¡¶çº§å› å­æ•°é‡: {len(top_factors)}")

        print("\nå‰10åé¡¶çº§å› å­:")
        print("-" * 120)
        print(
            f"{'æ’å':<4} {'å› å­åç§°':<20} {'ç»¼åˆå¾—åˆ†':<8} {'é¢„æµ‹':<6} {'ç¨³å®š':<6} {'ç‹¬ç«‹':<6} {'å®ç”¨':<6} {'é€‚åº”':<6} {'ICå‡å€¼':<8} {'æ˜¾è‘—æ€§':<6}"
        )
        print("-" * 120)

        for i, metrics in enumerate(top_factors[:10]):
            significance = (
                "***"
                if metrics.corrected_p_value < 0.001
                else (
                    "**"
                    if metrics.corrected_p_value < 0.01
                    else "*" if metrics.corrected_p_value < 0.05 else ""
                )
            )

            print(
                f"{i+1:<4} {metrics.name:<20} {metrics.comprehensive_score:.3f}    "
                f"{metrics.predictive_score:.3f}  {metrics.stability_score:.3f}  "
                f"{metrics.independence_score:.3f}  {metrics.practicality_score:.3f}  "
                f"{metrics.adaptability_score:.3f}  {metrics.ic_mean:+.4f}  {significance:<6}"
            )

        print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_df}")

    except Exception as e:
        print(f"ç­›é€‰å¤±è´¥: {str(e)}")
        raise


def _generate_multi_timeframe_summary(
    session_dir, batch_name: str, all_results: Dict, screening_configs: List[Dict]
) -> None:
    """ç”Ÿæˆç»Ÿä¸€çš„å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š

    Args:
        session_dir: ä¼šè¯ç›®å½•è·¯å¾„
        batch_name: æ‰¹é‡å¤„ç†åç§°
        all_results: æ‰€æœ‰æ—¶é—´æ¡†æ¶çš„ç­›é€‰ç»“æœ
        screening_configs: ç­›é€‰é…ç½®åˆ—è¡¨
    """
    from datetime import datetime

    print("ğŸ“Š ç”Ÿæˆå¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Š...")

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
    if not all_results:
        print("âš ï¸ æ²¡æœ‰æ•°æ®ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        return

    # åˆ›å»ºæ±‡æ€»æ•°æ®ç»“æ„
    summary_data = []
    best_factors_overall = []

    for tf_key, result in all_results.items():
        # result æ˜¯ Dict[str, FactorMetrics]
        if result and isinstance(result, dict):
            # è·å–æ—¶é—´æ¡†æ¶ä¿¡æ¯
            parts = tf_key.split("_")
            symbol = parts[0]
            timeframe = "_".join(parts[1:]) if len(parts) > 1 else "unknown"

            # è·å–æœ€ä½³å› å­ - æŒ‰ç»¼åˆå¾—åˆ†æ’åº
            sorted_factors = sorted(
                result.values(), key=lambda x: x.comprehensive_score, reverse=True
            )
            top_factors = sorted_factors[:10]  # å–å‰10ä¸ªå› å­

            for factor_metrics in top_factors:
                summary_item = {
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "factor_name": factor_metrics.name,
                    "comprehensive_score": factor_metrics.comprehensive_score,
                    "tier": getattr(factor_metrics, "tier", "N/A"),
                    "predictive_power": factor_metrics.predictive_score,
                    "stability": factor_metrics.stability_score,
                    "independence": factor_metrics.independence_score,
                    "practicality": factor_metrics.practicality_score,
                    "short_term_fitness": factor_metrics.adaptability_score,
                    "ic_mean": factor_metrics.ic_mean,
                    "ic_ir": factor_metrics.ic_ir,
                    "ic_win_rate": getattr(factor_metrics, "ic_win_rate", 0),
                    "rank_ic_mean": getattr(factor_metrics, "rank_ic_mean", 0),
                    "rank_ic_ir": getattr(factor_metrics, "rank_ic_ir", 0),
                    "turnover": getattr(factor_metrics, "turnover_rate", 0),
                    "p_value": factor_metrics.p_value,
                    "significant": factor_metrics.is_significant,
                }
                summary_data.append(summary_item)
                best_factors_overall.append(summary_item)

    if not summary_data:
        print("âš ï¸ æ²¡æœ‰æ•°æ®ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š")
        return

    # åˆ›å»ºæ±‡æ€»DataFrame
    import pandas as pd

    summary_df = pd.DataFrame(summary_data)

    # ä¿å­˜ç»Ÿä¸€æ±‡æ€»æŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_filename = f"{batch_name}_multi_timeframe_summary_{timestamp}.csv"
    summary_path = session_dir / summary_filename

    summary_df.to_csv(summary_path, index=False, encoding="utf-8")
    print(f"âœ… å¤šæ—¶é—´æ¡†æ¶æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

    # ç”Ÿæˆæœ€ä½³å› å­ç»¼åˆæ’è¡Œ
    if best_factors_overall:
        best_df = pd.DataFrame(best_factors_overall)

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        best_df_sorted = best_df.sort_values("comprehensive_score", ascending=False)

        # ä¿å­˜æœ€ä½³å› å­æ’è¡Œ
        best_filename = f"{batch_name}_best_factors_overall_{timestamp}.csv"
        best_path = session_dir / best_filename
        best_df_sorted.to_csv(best_path, index=False, encoding="utf-8")
        print(f"âœ… æœ€ä½³å› å­ç»¼åˆæ’è¡Œå·²ä¿å­˜: {best_path}")

        # è¾“å‡ºTop 10æœ€ä½³å› å­åˆ°æ§åˆ¶å°
        print("\nğŸ† Top 10 æœ€ä½³å› å­ (è·¨æ‰€æœ‰æ—¶é—´æ¡†æ¶):")
        print("=" * 120)
        top_10 = best_df_sorted.head(10)
        for i, (_, factor) in enumerate(top_10.iterrows(), 1):
            print(
                f"{i:2d}. {factor['factor_name']:<25} | "
                f"{factor['symbol']}-{factor['timeframe']:<10} | "
                f"è¯„åˆ†: {factor['comprehensive_score']:.3f} | "
                f"ç­‰çº§: {factor['tier']:<2} | "
                f"IC: {factor['ic_mean']:.3f} | "
                f"èƒœç‡: {factor['ic_win_rate']:.1%}"
            )

    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    _generate_batch_statistics(session_dir, batch_name, all_results, timestamp)


def _generate_batch_statistics(
    session_dir, batch_name: str, all_results: Dict, timestamp: str
) -> None:
    """ç”Ÿæˆæ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦"""

    import pandas as pd

    print("ğŸ“ˆ ç”Ÿæˆæ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦...")

    stats_data = []
    total_factors_processed = 0
    total_tier1_factors = 0
    total_tier2_factors = 0

    for tf_key, result in all_results.items():
        if result and "session_info" in result:
            symbol = tf_key.split("_")[0]
            timeframe = tf_key.split("_")[1] if "_" in tf_key else "unknown"

            session_info = result["session_info"]
            screening_results = result.get("screening_results", {})

            # ç»Ÿè®¡å„ç­‰çº§å› å­æ•°é‡
            tier_counts = screening_results.get("tier_counts", {})
            tier1_count = tier_counts.get("Tier 1 (â‰¥0.8)", 0)
            tier2_count = tier_counts.get("Tier 2 (0.6-0.8)", 0)
            total_count = screening_results.get("total_factors", 0)

            total_factors_processed += total_count
            total_tier1_factors += tier1_count
            total_tier2_factors += tier2_count

            stats_item = {
                "symbol": symbol,
                "timeframe": timeframe,
                "total_factors": total_count,
                "tier1_factors": tier1_count,
                "tier2_factors": tier2_count,
                "tier1_ratio": tier1_count / total_count if total_count > 0 else 0,
                "start_time": session_info.get("start_time", ""),
                "status": session_info.get("status", "unknown"),
            }
            stats_data.append(stats_item)

    if stats_data:
        stats_df = pd.DataFrame(stats_data)

        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
        stats_filename = f"{batch_name}_batch_statistics_{timestamp}.csv"
        stats_path = session_dir / stats_filename
        stats_df.to_csv(stats_path, index=False, encoding="utf-8")
        print(f"âœ… æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {stats_path}")

        # è¾“å‡ºç»Ÿè®¡æ‘˜è¦åˆ°æ§åˆ¶å°
        print("\nğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡æ‘˜è¦:")
        print("=" * 80)
        print(f"å¤„ç†æ—¶é—´æ¡†æ¶æ•°é‡: {len(stats_data)}")
        print(f"æ€»å¤„ç†å› å­æ•°: {total_factors_processed}")
        print(f"æ€»Tier 1å› å­æ•°: {total_tier1_factors}")
        print(f"æ€»Tier 2å› å­æ•°: {total_tier2_factors}")

        if total_factors_processed > 0:
            overall_tier1_ratio = total_tier1_factors / total_factors_processed
            overall_tier2_ratio = total_tier2_factors / total_factors_processed
            print(f"æ•´ä½“Tier 1æ¯”ä¾‹: {overall_tier1_ratio:.1%}")
            print(f"æ•´ä½“Tier 2æ¯”ä¾‹: {overall_tier2_ratio:.1%}")

            # æŒ‰æ—¶é—´æ¡†æ¶ç»Ÿè®¡
            print("\nå„æ—¶é—´æ¡†æ¶è¯¦ç»†ç»Ÿè®¡:")
            for _, stats in stats_df.iterrows():
                print(
                    f"  {stats['symbol']}-{stats['timeframe']:>8}: "
                    f"æ€»è®¡ {stats['total_factors']:>3} ä¸ª | "
                    f"Tier1 {stats['tier1_factors']:>2} ä¸ª ({stats['tier1_ratio']:.1%}) | "
                    f"Tier2 {stats['tier2_factors']:>2} ä¸ª"
                )


if __name__ == "__main__":
    main()
