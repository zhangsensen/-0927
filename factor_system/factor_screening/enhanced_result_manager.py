#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆç­›é€‰ç»“æœç®¡ç†å™¨
åŸºäºæ—¶é—´æˆ³æ–‡ä»¶å¤¹çš„å®Œæ•´ä¿¡æ¯å­˜å‚¨ç³»ç»Ÿ
"""

import json
import logging
import platform
import sys
from dataclasses import asdict, dataclass, is_dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import yaml

# é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼Œé¿å…matplotlibè­¦å‘Š
matplotlib.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
matplotlib.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

logger = logging.getLogger(__name__)


@dataclass
class ScreeningSession:
    """ç­›é€‰ä¼šè¯ä¿¡æ¯"""

    session_id: str
    timestamp: str
    symbol: str
    timeframe: str
    config_hash: str
    total_factors: int
    significant_factors: int
    high_score_factors: int
    total_time_seconds: float
    memory_used_mb: float
    sample_size: int
    data_quality_score: float
    top_factor_name: str
    top_factor_score: float


class EnhancedResultManager:
    """å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨ - åŸºäºæ—¶é—´æˆ³æ–‡ä»¶å¤¹çš„å®Œæ•´å­˜å‚¨"""

    def __init__(self, base_output_dir: str = "./å› å­ç­›é€‰"):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºä¼šè¯ç´¢å¼•æ–‡ä»¶
        self.sessions_index_file = (
            self.base_output_dir / "screening_sessions_index.json"
        )
        self.sessions_index = self._load_sessions_index()

    def _load_sessions_index(self) -> List[Dict[str, Any]]:
        """åŠ è½½ä¼šè¯ç´¢å¼•"""
        if self.sessions_index_file.exists():
            try:
                with open(self.sessions_index_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # ç¡®ä¿è¿”å›ç±»å‹æ­£ç¡®
                    if isinstance(data, list):
                        return data
                    elif isinstance(data, dict):
                        return [data]
                    else:
                        logger.warning(
                            f"ä¼šè¯ç´¢å¼•æ–‡ä»¶æ ¼å¼å¼‚å¸¸ï¼ŒæœŸæœ›listæˆ–dictï¼Œå¾—åˆ°{type(data)}"
                        )
                        return []
            except FileNotFoundError:
                logger.warning(f"ä¼šè¯ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.sessions_index_file}")
                return []
            except json.JSONDecodeError as e:
                logger.error(f"ä¼šè¯ç´¢å¼•æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                return []
            except Exception as e:
                logger.error(f"æœªçŸ¥é”™è¯¯åŠ è½½ä¼šè¯ç´¢å¼•: {str(e)}", exc_info=True)
                return []
        return []

    def _save_sessions_index(self) -> None:
        """ä¿å­˜ä¼šè¯ç´¢å¼•"""
        try:
            serialized = self._serialize_sessions_index()
        except Exception as serialization_error:  # pragma: no cover - ç†è®ºä¸Šä¸åº”å‡ºç°
            logger.error(
                "ä¼šè¯ç´¢å¼•åºåˆ—åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ä¿å­˜: %s",
                serialization_error,
                exc_info=True,
            )
            serialized = self.sessions_index

        try:
            with open(self.sessions_index_file, "w", encoding="utf-8") as f:
                json.dump(serialized, f, indent=2, ensure_ascii=False)
        except OSError as file_error:
            logger.error(f"å†™å…¥ä¼šè¯ç´¢å¼•æ–‡ä»¶å¤±è´¥: {file_error}")

    def _serialize_sessions_index(self) -> List[Any]:
        """å°†ä¼šè¯ç´¢å¼•è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–ç»“æ„ï¼Œé¿å…å¾ªç¯å¯¼å…¥"""

        def _serialize(obj: Any) -> Any:
            if is_dataclass(obj):
                return asdict(obj)
            if isinstance(obj, dict):
                return {key: _serialize(value) for key, value in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [_serialize(item) for item in obj]
            if isinstance(obj, (np.integer, np.floating, np.bool_)):
                return obj.item()
            if isinstance(obj, pd.Timestamp):
                return obj.isoformat()
            if isinstance(obj, datetime):
                return obj.isoformat()
            return obj

        return [_serialize(item) for item in self.sessions_index]

    def create_screening_session(
        self,
        symbol: str,
        timeframe: str,
        results: Dict[str, Any],
        screening_stats: Dict[str, Any],
        config: Any,
        data_quality_info: Optional[Dict[str, Any]] = None,
        existing_session_dir: Optional[Path] = None,
    ) -> str:
        """åˆ›å»ºå®Œæ•´çš„ç­›é€‰ä¼šè¯å­˜å‚¨"""

        # 1. ä½¿ç”¨ç°æœ‰ä¼šè¯ç›®å½•æˆ–åˆ›å»ºæ–°çš„
        if existing_session_dir and existing_session_dir.exists():
            session_dir = existing_session_dir
            session_id = session_dir.name
            logger.info(f"ä½¿ç”¨ç°æœ‰ç­›é€‰ä¼šè¯: {session_id}")
        else:
            timestamp = datetime.now()
            session_id = f"{symbol}_{timeframe}_{timestamp.strftime('%Y%m%d_%H%M%S')}"
            session_dir = self.base_output_dir / session_id
            session_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"åˆ›å»ºç­›é€‰ä¼šè¯: {session_id}")

        logger.info(f"ä¼šè¯ç›®å½•: {session_dir}")

        # 2. ä¿å­˜æ ¸å¿ƒç­›é€‰æ•°æ®
        self._save_core_screening_data(
            session_dir, symbol, timeframe, results, screening_stats
        )

        # 3. ä¿å­˜é…ç½®å’Œå…ƒæ•°æ®
        self._save_configuration_data(session_dir, config, data_quality_info)

        # 4. ä¿å­˜åˆ†ææŠ¥å‘Š
        self._save_analysis_reports(
            session_dir, symbol, timeframe, results, screening_stats
        )

        # 5. ä¿å­˜å¯è§†åŒ–å›¾è¡¨
        self._save_visualization_charts(session_dir, results, screening_stats)

        # 6. ä¿å­˜å› å­ç›¸å…³æ€§åˆ†æ
        self._save_factor_correlation_analysis(session_dir, results)

        # 7. ä¿å­˜ICæ—¶é—´åºåˆ—åˆ†æ
        self._save_ic_time_series_analysis(session_dir, results)

        # 8. ç”Ÿæˆä¼šè¯æ‘˜è¦
        session_summary = self._generate_session_summary(
            session_dir, symbol, timeframe, results, screening_stats, config
        )

        # 9. æ›´æ–°ä¼šè¯ç´¢å¼•
        self._update_sessions_index(session_id, session_summary)

        # 10. ç”ŸæˆREADMEæ–‡ä»¶
        self._generate_session_readme(session_dir, session_summary)

        logger.info(f"âœ… ç­›é€‰ä¼šè¯åˆ›å»ºå®Œæˆ: {session_id}")
        return session_id

    def _save_core_screening_data(
        self,
        session_dir: Path,
        symbol: str,
        timeframe: str,
        results: Dict[str, Any],
        screening_stats: Dict[str, Any],
    ) -> None:
        """ä¿å­˜æ ¸å¿ƒç­›é€‰æ•°æ®"""

        # 1. è¯¦ç»†å› å­ç­›é€‰æŠ¥å‘Š (CSV)
        report_data = []
        for factor_name, metrics in results.items():
            row = {
                "Factor": factor_name,
                "Comprehensive_Score": metrics.comprehensive_score,
                "Predictive_Power_Mean_IC": metrics.predictive_power_mean_ic,
                "Predictive_Power_IC_IR": metrics.ic_ir,
                "Stability_Rolling_IC_Mean": metrics.rolling_ic_mean,
                "Stability_Rolling_IC_Std": metrics.rolling_ic_std,
                "Independence_VIF": metrics.vif_score,
                "Independence_Information_Increment": metrics.information_increment,
                "Practicality_Turnover_Rate": metrics.turnover_rate,
                "Practicality_Transaction_Cost": metrics.transaction_cost,
                "Short_Term_Adaptability_Reversal_Effect": metrics.reversal_effect,
                "Short_Term_Adaptability_Momentum_Persistence": metrics.momentum_persistence,
                "P_Value": metrics.p_value,
                "FDR_P_Value": metrics.corrected_p_value,
                "Is_Significant": metrics.is_significant,
                "Tier": metrics.tier,
                "Type": metrics.type,
                "Description": metrics.description,
            }
            report_data.append(row)

        report_df = pd.DataFrame(report_data).sort_values(
            "Comprehensive_Score", ascending=False
        )
        report_df.to_csv(
            session_dir / "detailed_factor_report.csv", index=False, encoding="utf-8"
        )

        # 2. ç­›é€‰ç»Ÿè®¡ä¿¡æ¯ (JSON)
        enhanced_stats = {
            **screening_stats,
            "session_metadata": {
                "symbol": symbol,
                "timeframe": timeframe,
                "screening_timestamp": datetime.now().isoformat(),
                "total_factors_processed": len(results),
                "factors_by_tier": self._count_factors_by_tier(results),
                "score_distribution": self._calculate_score_distribution(results),
            },
        }

        from professional_factor_screener import ProfessionalFactorScreener

        with open(
            session_dir / "screening_statistics.json", "w", encoding="utf-8"
        ) as f:
            json.dump(
                ProfessionalFactorScreener._to_json_serializable(enhanced_stats),
                f,
                indent=2,
                ensure_ascii=False,
            )

        # 3. é¡¶çº§å› å­è¯¦ç»†ä¿¡æ¯ (JSON)
        top_factors = sorted(
            results.to_numpy()(), key=lambda x: x.comprehensive_score, reverse=True
        )[:20]
        top_factors_data = []

        for i, factor in enumerate(top_factors, 1):
            factor_info = {
                "rank": i,
                "name": factor.name,
                "comprehensive_score": round(factor.comprehensive_score, 4),
                "predictive_score": round(factor.predictive_score, 4),
                "stability_score": round(factor.stability_score, 4),
                "independence_score": round(factor.independence_score, 4),
                "practicality_score": round(factor.practicality_score, 4),
                "adaptability_score": round(factor.adaptability_score, 4),
                "is_significant": factor.is_significant,
                "tier": factor.tier,
                "type": factor.type,
                "description": factor.description,
                "key_metrics": {
                    "mean_ic": round(factor.predictive_power_mean_ic, 4),
                    "ic_ir": round(factor.ic_ir, 4),
                    "rolling_ic_mean": round(factor.rolling_ic_mean, 4),
                    "vi": round(factor.vif_score, 4) if factor.vif_score else None,
                    "turnover_rate": round(factor.turnover_rate, 4),
                    "transaction_cost": round(factor.transaction_cost, 4),
                },
            }
            top_factors_data.append(factor_info)

        from professional_factor_screener import ProfessionalFactorScreener

        with open(
            session_dir / "top_factors_detailed.json", "w", encoding="utf-8"
        ) as f:
            json.dump(
                ProfessionalFactorScreener._to_json_serializable(top_factors_data),
                f,
                indent=2,
                ensure_ascii=False,
            )

    def _save_configuration_data(
        self,
        session_dir: Path,
        config: Any,
        data_quality_info: Optional[Dict[str, Any]],
    ) -> None:
        """ä¿å­˜é…ç½®å’Œæ•°æ®è´¨é‡ä¿¡æ¯"""

        # 1. ç­›é€‰é…ç½® (YAML)
        config_dict = {
            "screening_parameters": (
                asdict(config) if hasattr(config, "__dict__") else str(config)
            ),
            "execution_info": {
                "timestamp": datetime.now().isoformat(),
                "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                "system_info": {
                    "platform": platform.system(),
                    "architecture": platform.architecture()[0],
                    "processor": platform.processor(),
                },
            },
        }

        with open(session_dir / "screening_config.yaml", "w", encoding="utf-8") as f:
            yaml.dump(
                config_dict, f, default_flow_style=False, allow_unicode=True, indent=2
            )

        # 2. æ•°æ®è´¨é‡æŠ¥å‘Š (JSON)
        if data_quality_info:
            enhanced_quality_info = {
                **data_quality_info,
                "quality_assessment": {
                    "overall_score": self._calculate_overall_quality_score(
                        data_quality_info
                    ),
                    "recommendations": self._generate_quality_recommendations(
                        data_quality_info
                    ),
                },
            }

            from professional_factor_screener import ProfessionalFactorScreener

            with open(
                session_dir / "data_quality_report.json", "w", encoding="utf-8"
            ) as f:
                json.dump(
                    ProfessionalFactorScreener._to_json_serializable(
                        enhanced_quality_info
                    ),
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

    def _save_analysis_reports(
        self,
        session_dir: Path,
        symbol: str,
        timeframe: str,
        results: Dict[str, Any],
        screening_stats: Dict[str, Any],
    ) -> None:
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""

        # 1. æ‰§è¡Œæ‘˜è¦ (TXT)
        summary_path = session_dir / "executive_summary.txt"
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("ğŸ¯ å› å­ç­›é€‰æ‰§è¡Œæ‘˜è¦\n")
            f.write(f"{'='*50}\n\n")
            f.write("ğŸ“Š åŸºæœ¬ä¿¡æ¯\n")
            f.write(f"è‚¡ç¥¨ä»£ç : {symbol}\n")
            f.write(f"æ—¶é—´æ¡†æ¶: {timeframe}\n")
            f.write(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è€—æ—¶: {screening_stats.get('total_time', 0):.2f}ç§’\n")
            f.write(f"å†…å­˜ä½¿ç”¨: {screening_stats.get('memory_used_mb', 0):.1f}MB\n\n")

            f.write("ğŸ“ˆ ç­›é€‰ç»“æœç»Ÿè®¡\n")
            f.write(f"æ€»å› å­æ•°: {screening_stats.get('total_factors', 0)}\n")
            f.write(f"æ˜¾è‘—å› å­: {screening_stats.get('significant_factors', 0)}\n")
            f.write(
                f"é«˜åˆ†å› å­ (>0.6): {screening_stats.get('high_score_factors', 0)}\n"
            )
            f.write(f"æ ·æœ¬é‡: {screening_stats.get('sample_size', 0)}\n\n")

            # é¡¶çº§å› å­åˆ—è¡¨
            top_factors = sorted(
                results.to_numpy()(), key=lambda x: x.comprehensive_score, reverse=True
            )[:10]
            f.write("ğŸ† å‰10åé¡¶çº§å› å­\n")
            for i, factor in enumerate(top_factors, 1):
                f.write(
                    f"{i:2d}. {factor.name:<25} å¾—åˆ†:{factor.comprehensive_score:.3f} "
                )
                f.write(f"IC:{factor.predictive_power_mean_ic:.3f} ")
                f.write(f"æ˜¾è‘—æ€§:{'âœ“' if factor.is_significant else 'âœ—'}\n")

            f.write("\nğŸ“‹ å› å­åˆ†å±‚ç»Ÿè®¡\n")
            tier_counts = self._count_factors_by_tier(results)
            for tier, count in tier_counts.items():
                f.write(f"{tier}: {count} ä¸ª\n")

        # 2. è¯¦ç»†åˆ†ææŠ¥å‘Š (Markdown)
        analysis_path = session_dir / "detailed_analysis.md"
        with open(analysis_path, "w", encoding="utf-8") as f:
            f.write("# å› å­ç­›é€‰è¯¦ç»†åˆ†ææŠ¥å‘Š\n\n")
            f.write("## åŸºæœ¬ä¿¡æ¯\n")
            f.write(f"- **è‚¡ç¥¨ä»£ç **: {symbol}\n")
            f.write(f"- **æ—¶é—´æ¡†æ¶**: {timeframe}\n")
            f.write(
                f"- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            )

            f.write("## ç­›é€‰ç»“æœæ¦‚è§ˆ\n")
            f.write("| æŒ‡æ ‡ | æ•°å€¼ |\n")
            f.write("|------|------|\n")
            f.write(f"| æ€»å› å­æ•° | {screening_stats.get('total_factors', 0)} |\n")
            f.write(
                f"| æ˜¾è‘—å› å­æ•° | {screening_stats.get('significant_factors', 0)} |\n"
            )
            f.write(
                f"| é«˜åˆ†å› å­æ•° | {screening_stats.get('high_score_factors', 0)} |\n"
            )
            f.write(f"| å¤„ç†æ—¶é—´ | {screening_stats.get('total_time', 0):.2f}ç§’ |\n")
            f.write(
                f"| å†…å­˜ä½¿ç”¨ | {screening_stats.get('memory_used_mb', 0):.1f}MB |\n\n"
            )

            f.write("## é¡¶çº§å› å­åˆ†æ\n")
            top_factors = sorted(
                results.to_numpy()(), key=lambda x: x.comprehensive_score, reverse=True
            )[:10]
            f.write(
                "| æ’å | å› å­åç§° | ç»¼åˆå¾—åˆ† | é¢„æµ‹èƒ½åŠ› | ç¨³å®šæ€§ | ç‹¬ç«‹æ€§ | å®ç”¨æ€§ |\n"
            )
            f.write(
                "|------|----------|----------|----------|--------|--------|--------|\n"
            )
            for i, factor in enumerate(top_factors, 1):
                f.write(f"| {i} | {factor.name} | {factor.comprehensive_score:.3f} | ")
                f.write(
                    f"{factor.predictive_score:.3f} | {factor.stability_score:.3f} | "
                )
                f.write(
                    f"{factor.independence_score:.3f} | {factor.practicality_score:.3f} |\n"
                )

    def _save_visualization_charts(
        self,
        session_dir: Path,
        results: Dict[str, Any],
        screening_stats: Dict[str, Any],
    ) -> None:
        """ä¿å­˜å¯è§†åŒ–å›¾è¡¨ï¼ˆä¸²è¡Œç‰ˆï¼Œmatplotlibçº¿ç¨‹ä¸å®‰å…¨ï¼‰"""
        charts_dir = session_dir / "charts"
        charts_dir.mkdir(exist_ok=True)

        # æ³¨æ„ï¼šmatplotlibåœ¨macOSä¸Šä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½¿ç”¨ä¸²è¡Œç”Ÿæˆ
        # from concurrent.futures import ThreadPoolExecutor, as_completed

        def generate_score_distribution():
            """ç”Ÿæˆå› å­å¾—åˆ†åˆ†å¸ƒå›¾"""
            try:
                scores = [factor.comprehensive_score for factor in results.to_numpy()()]
                fig = plt.figure(figsize=(10, 6))
                plt.hist(scores, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
                plt.title("å› å­ç»¼åˆå¾—åˆ†åˆ†å¸ƒ")
                plt.xlabel("ç»¼åˆå¾—åˆ†")
                plt.ylabel("å› å­æ•°é‡")
                plt.grid(True, alpha=0.3)
                plt.savefig(
                    charts_dir / "score_distribution.png", dpi=300, bbox_inches="tight"
                )
                plt.close(fig)
                return "score_distribution.png"
            except Exception as e:
                logger.warning(f"ç”Ÿæˆå¾—åˆ†åˆ†å¸ƒå›¾å¤±è´¥: {e}")
                return None

        def generate_radar_chart():
            """ç”Ÿæˆé›·è¾¾å›¾"""
            try:
                top_factors = sorted(
                    results.to_numpy()(),
                    key=lambda x: x.comprehensive_score,
                    reverse=True,
                )[:5]

                fig, ax = plt.subplots(
                    figsize=(10, 8), subplot_kw=dict(projection="polar")
                )

                categories = ["é¢„æµ‹èƒ½åŠ›", "ç¨³å®šæ€§", "ç‹¬ç«‹æ€§", "å®ç”¨æ€§", "é€‚åº”æ€§"]
                angles = np.linspace(
                    0, 2 * np.pi, len(categories), endpoint=False
                ).tolist()
                angles += angles[:1]  # é—­åˆ

                for i, factor in enumerate(top_factors):
                    values = [
                        factor.predictive_score,
                        factor.stability_score,
                        factor.independence_score,
                        factor.practicality_score,
                        factor.adaptability_score,
                    ]
                    values += values[:1]  # é—­åˆ

                    ax.plot(angles, values, "o-", linewidth=2, label=factor.name)
                    ax.fill(angles, values, alpha=0.25)

                ax.set_xticks(angles[:-1])
                ax.set_xticklabels(categories)
                ax.set_ylim(0, 1)
                ax.set_title("é¡¶çº§å› å­äº”ç»´åº¦è¯„åˆ†å¯¹æ¯”", pad=20)
                ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.0))

                plt.savefig(
                    charts_dir / "top_factors_radar.png", dpi=300, bbox_inches="tight"
                )
                plt.close(fig)
                return "top_factors_radar.png"
            except Exception as e:
                logger.warning(f"ç”Ÿæˆé›·è¾¾å›¾å¤±è´¥: {e}")
                return None

        def generate_pie_chart():
            """ç”Ÿæˆé¥¼å›¾"""
            try:
                factor_types: Dict[str, int] = {}
                for factor in results.to_numpy()():
                    factor_type = factor.type or "Unknown"
                    factor_types[factor_type] = factor_types.get(factor_type, 0) + 1

                fig = plt.figure(figsize=(8, 8))
                plt.pie(
                    list(factor_types.to_numpy()()),
                    labels=list(factor_types.keys()),
                    autopct="%1.1f%%",
                )
                plt.title("å› å­ç±»å‹åˆ†å¸ƒ")
                plt.savefig(
                    charts_dir / "factor_types_distribution.png",
                    dpi=300,
                    bbox_inches="tight",
                )
                plt.close(fig)
                return "factor_types_distribution.png"
            except Exception as e:
                logger.warning(f"ç”Ÿæˆé¥¼å›¾å¤±è´¥: {e}")
                return None

        # ä¸²è¡Œç”Ÿæˆæ‰€æœ‰å›¾è¡¨ï¼ˆmatplotlibçº¿ç¨‹ä¸å®‰å…¨ï¼‰
        try:
            generate_score_distribution()
            generate_radar_chart()
            generate_pie_chart()
        except Exception as e:
            logger.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")

    def _save_factor_correlation_analysis(
        self, session_dir: Path, results: Dict[str, Any]
    ) -> None:
        """ä¿å­˜å› å­ç›¸å…³æ€§åˆ†æ"""
        try:
            # æå–é¡¶çº§å› å­çš„å…³é”®æŒ‡æ ‡
            top_factors = sorted(
                results.to_numpy()(), key=lambda x: x.comprehensive_score, reverse=True
            )[:20]

            correlation_data = []
            for factor in top_factors:
                correlation_data.append(
                    {
                        "name": factor.name,
                        "comprehensive_score": factor.comprehensive_score,
                        "predictive_score": factor.predictive_score,
                        "stability_score": factor.stability_score,
                        "independence_score": factor.independence_score,
                        "practicality_score": factor.practicality_score,
                        "mean_ic": factor.predictive_power_mean_ic,
                        "ic_ir": factor.ic_ir,
                        "rolling_ic_mean": factor.rolling_ic_mean,
                    }
                )

            correlation_df = pd.DataFrame(correlation_data)
            correlation_df = correlation_df.set_index("name")

            # ä¿å­˜ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = correlation_df.corr()
            correlation_matrix.to_csv(
                session_dir / "factor_correlation_matrix.csv", encoding="utf-8"
            )

            # ç”Ÿæˆç›¸å…³æ€§çƒ­åŠ›å›¾
            charts_dir = session_dir / "charts"
            charts_dir.mkdir(exist_ok=True)

            plt.figure(figsize=(12, 10))
            sns.heatmap(
                correlation_matrix,
                annot=True,
                cmap="coolwarm",
                center=0,
                square=True,
                fmt=".2",
            )
            plt.title("é¡¶çº§å› å­æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾")
            plt.tight_layout()
            plt.savefig(
                charts_dir / "correlation_heatmap.png", dpi=300, bbox_inches="tight"
            )
            plt.close()

        except Exception as e:
            logger.warning(f"ç”Ÿæˆå› å­ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")

    def _save_ic_time_series_analysis(
        self, session_dir: Path, results: Dict[str, Any]
    ) -> None:
        """ä¿å­˜ICæ—¶é—´åºåˆ—åˆ†æ"""
        try:
            # è¿™é‡Œéœ€è¦ä»resultsä¸­æå–ICæ—¶é—´åºåˆ—æ•°æ®
            # ç”±äºå½“å‰FactorMetricså¯èƒ½æ²¡æœ‰å­˜å‚¨å®Œæ•´çš„ICæ—¶é—´åºåˆ—ï¼Œ
            # æˆ‘ä»¬å…ˆä¿å­˜ä¸€ä¸ªå ä½ç¬¦æ–‡ä»¶ï¼Œåç»­å¯ä»¥æ‰©å±•

            ic_analysis = {
                "note": "ICæ—¶é—´åºåˆ—åˆ†æéœ€è¦åœ¨å› å­ç­›é€‰è¿‡ç¨‹ä¸­æ”¶é›†æ›´è¯¦ç»†çš„æ—¶é—´åºåˆ—æ•°æ®",
                "available_metrics": {
                    "mean_ic_values": {
                        factor.name: factor.predictive_power_mean_ic
                        for factor in results.to_numpy()()
                    },
                    "ic_ir_values": {
                        factor.name: factor.ic_ir for factor in results.to_numpy()()
                    },
                    "rolling_ic_means": {
                        factor.name: factor.rolling_ic_mean
                        for factor in results.to_numpy()()
                    },
                },
            }

            from professional_factor_screener import ProfessionalFactorScreener

            with open(
                session_dir / "ic_time_series_analysis.json", "w", encoding="utf-8"
            ) as f:
                json.dump(
                    ProfessionalFactorScreener._to_json_serializable(ic_analysis),
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

        except Exception as e:
            logger.warning(f"ç”ŸæˆICæ—¶é—´åºåˆ—åˆ†æå¤±è´¥: {e}")

    def _generate_session_summary(
        self,
        session_dir: Path,
        symbol: str,
        timeframe: str,
        results: Dict[str, Any],
        screening_stats: Dict[str, Any],
        config: Any,
    ) -> ScreeningSession:
        """ç”Ÿæˆä¼šè¯æ‘˜è¦"""

        top_factor = max(results.to_numpy()(), key=lambda x: x.comprehensive_score)

        return ScreeningSession(
            session_id=session_dir.name,
            timestamp=datetime.now().isoformat(),
            symbol=symbol,
            timeframe=timeframe,
            config_hash=str(hash(str(config)))[:8],
            total_factors=len(results),
            significant_factors=sum(
                1 for f in results.to_numpy()() if f.is_significant
            ),
            high_score_factors=sum(
                1 for f in results.to_numpy()() if f.comprehensive_score > 0.6
            ),
            total_time_seconds=screening_stats.get("total_time", 0),
            memory_used_mb=screening_stats.get("memory_used_mb", 0),
            sample_size=screening_stats.get("sample_size", 0),
            data_quality_score=0.85,  # å ä½ç¬¦ï¼Œéœ€è¦å®é™…è®¡ç®—
            top_factor_name=top_factor.name,
            top_factor_score=top_factor.comprehensive_score,
        )

    def _update_sessions_index(
        self, session_id: str, session_summary: ScreeningSession
    ) -> None:
        """æ›´æ–°ä¼šè¯ç´¢å¼•"""
        self.sessions_index.append(asdict(session_summary))
        self._save_sessions_index()

    def _generate_session_readme(
        self, session_dir: Path, session_summary: ScreeningSession
    ) -> None:
        """ç”Ÿæˆä¼šè¯READMEæ–‡ä»¶"""
        readme_path = session_dir / "README.md"

        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# å› å­ç­›é€‰ä¼šè¯: {session_summary.session_id}\n\n")
            f.write("## ä¼šè¯ä¿¡æ¯\n")
            f.write(f"- **ä¼šè¯ID**: {session_summary.session_id}\n")
            f.write(f"- **è‚¡ç¥¨ä»£ç **: {session_summary.symbol}\n")
            f.write(f"- **æ—¶é—´æ¡†æ¶**: {session_summary.timeframe}\n")
            f.write(f"- **æ‰§è¡Œæ—¶é—´**: {session_summary.timestamp}\n")
            f.write(f"- **é…ç½®å“ˆå¸Œ**: {session_summary.config_hash}\n\n")

            f.write("## ç­›é€‰ç»“æœ\n")
            f.write(f"- **æ€»å› å­æ•°**: {session_summary.total_factors}\n")
            f.write(f"- **æ˜¾è‘—å› å­æ•°**: {session_summary.significant_factors}\n")
            f.write(f"- **é«˜åˆ†å› å­æ•°**: {session_summary.high_score_factors}\n")
            f.write(
                f"- **é¡¶çº§å› å­**: {session_summary.top_factor_name} (å¾—åˆ†: {session_summary.top_factor_score:.3f})\n\n"
            )

            f.write("## æ€§èƒ½æŒ‡æ ‡\n")
            f.write(f"- **æ‰§è¡Œæ—¶é—´**: {session_summary.total_time_seconds:.2f}ç§’\n")
            f.write(f"- **å†…å­˜ä½¿ç”¨**: {session_summary.memory_used_mb:.1f}MB\n")
            f.write(f"- **æ ·æœ¬é‡**: {session_summary.sample_size}\n\n")

            f.write("## æ–‡ä»¶è¯´æ˜\n")
            f.write("- `detailed_factor_report.csv` - è¯¦ç»†å› å­ç­›é€‰æŠ¥å‘Š\n")
            f.write("- `screening_statistics.json` - ç­›é€‰è¿‡ç¨‹ç»Ÿè®¡ä¿¡æ¯\n")
            f.write("- `top_factors_detailed.json` - é¡¶çº§å› å­è¯¦ç»†ä¿¡æ¯\n")
            f.write("- `screening_config.yaml` - ç­›é€‰é…ç½®å‚æ•°\n")
            f.write("- `data_quality_report.json` - æ•°æ®è´¨é‡æŠ¥å‘Š\n")
            f.write("- `executive_summary.txt` - æ‰§è¡Œæ‘˜è¦\n")
            f.write("- `detailed_analysis.md` - è¯¦ç»†åˆ†ææŠ¥å‘Š\n")
            f.write("- `charts/` - å¯è§†åŒ–å›¾è¡¨ç›®å½•\n")
            f.write("- `factor_correlation_matrix.csv` - å› å­ç›¸å…³æ€§çŸ©é˜µ\n")
            f.write("- `ic_time_series_analysis.json` - ICæ—¶é—´åºåˆ—åˆ†æ\n\n")

            f.write("## ä½¿ç”¨å»ºè®®\n")
            f.write("1. æŸ¥çœ‹ `executive_summary.txt` è·å–å¿«é€Ÿæ¦‚è§ˆ\n")
            f.write("2. åˆ†æ `detailed_factor_report.csv` äº†è§£æ‰€æœ‰å› å­è¯¦æƒ…\n")
            f.write("3. æŸ¥çœ‹ `charts/` ç›®å½•ä¸­çš„å¯è§†åŒ–å›¾è¡¨\n")
            f.write("4. å‚è€ƒ `detailed_analysis.md` è¿›è¡Œæ·±å…¥åˆ†æ\n")

    # è¾…åŠ©æ–¹æ³•
    def _count_factors_by_tier(self, results: Dict[str, Any]) -> Dict[str, int]:
        """ç»Ÿè®¡å„å±‚çº§å› å­æ•°é‡"""
        tier_counts: Dict[str, int] = {}
        for factor in results.to_numpy()():
            tier = factor.tier or "Unknown"
            tier_counts[tier] = tier_counts.get(tier, 0) + 1
        return tier_counts

    def _calculate_score_distribution(self, results: Dict[str, Any]) -> Dict[str, int]:
        """è®¡ç®—å¾—åˆ†åˆ†å¸ƒ"""
        distribution = {
            "excellent (>0.8)": 0,
            "good (0.6-0.8)": 0,
            "average (0.4-0.6)": 0,
            "poor (<0.4)": 0,
        }

        for factor in results.to_numpy()():
            score = factor.comprehensive_score
            if score > 0.8:
                distribution["excellent (>0.8)"] += 1
            elif score > 0.6:
                distribution["good (0.6-0.8)"] += 1
            elif score > 0.4:
                distribution["average (0.4-0.6)"] += 1
            else:
                distribution["poor (<0.4)"] += 1

        return distribution

    def _calculate_overall_quality_score(self, data_quality_info: Dict) -> float:
        """è®¡ç®—æ€»ä½“æ•°æ®è´¨é‡å¾—åˆ†"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®æ•°æ®è´¨é‡ä¿¡æ¯è®¡ç®—ä¸€ä¸ªç»¼åˆå¾—åˆ†
        return 0.85  # å ä½ç¬¦

    def _generate_quality_recommendations(self, data_quality_info: Dict) -> List[str]:
        """ç”Ÿæˆæ•°æ®è´¨é‡å»ºè®®"""
        recommendations = []
        # æ ¹æ®æ•°æ®è´¨é‡ä¿¡æ¯ç”Ÿæˆå…·ä½“å»ºè®®
        recommendations.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œå› å­ç­›é€‰")
        return recommendations

    def get_session_history(
        self,
        symbol: Optional[str] = None,
        timeframe: Optional[str] = None,
        limit: int = 10,
    ) -> List[ScreeningSession]:
        """è·å–ä¼šè¯å†å²"""
        sessions = []
        for session_data in self.sessions_index:
            session = ScreeningSession(**session_data)

            # è¿‡æ»¤æ¡ä»¶
            if symbol and session.symbol != symbol:
                continue
            if timeframe and session.timeframe != timeframe:
                continue

            sessions.append(session)

        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        sessions.sort(key=lambda x: x.timestamp, reverse=True)
        return sessions[:limit]

    def cleanup_old_sessions(self, keep_days: int = 30) -> None:
        """æ¸…ç†æ—§ä¼šè¯"""
        cutoff_date = datetime.now() - timedelta(days=keep_days)

        sessions_to_remove = []
        for i, session_data in enumerate(self.sessions_index):
            session_time = datetime.fromisoformat(session_data["timestamp"])
            if session_time < cutoff_date:
                # åˆ é™¤ä¼šè¯ç›®å½•
                session_dir = self.base_output_dir / session_data["session_id"]
                if session_dir.exists():
                    import shutil

                    shutil.rmtree(session_dir)
                sessions_to_remove.append(i)

        # ä»ç´¢å¼•ä¸­ç§»é™¤
        for i in reversed(sessions_to_remove):
            del self.sessions_index[i]

        self._save_sessions_index()
        logger.info(f"æ¸…ç†äº† {len(sessions_to_remove)} ä¸ªæ—§ä¼šè¯")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    manager = EnhancedResultManager()
    print("å¢å¼ºç‰ˆç»“æœç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
