#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡åŒ–æ ¸å¿ƒå¼•æ“ - åŸºäº VectorBT çš„çœŸæ­£å‘é‡åŒ–å®ç°
ä½œè€…ï¼šé‡åŒ–é¦–å¸­å·¥ç¨‹å¸ˆ
ç‰ˆæœ¬ï¼š3.0.0 (Linus é‡æ„ç‰ˆ)

è®¾è®¡åŸåˆ™ï¼š
1. æ¶ˆç­æ‰€æœ‰ä¸å¿…è¦çš„ for-loop
2. å…¨é‡ä½¿ç”¨ NumPy å¹¿æ’­å’Œ VectorBT æ‰¹å¤„ç†
3. å•æ¬¡æ•°æ®å¯¹é½ï¼Œå¤šæ¬¡å¤ç”¨
4. O(NÃ—F) -> O(N+F) å¤æ‚åº¦
"""

import logging
import time
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import vectorbt as vbt
from numpy.lib.stride_tricks import sliding_window_view
from scipy import stats
from scipy.stats import rankdata

logger = logging.getLogger(__name__)


class VectorizedFactorAnalyzer:
    """VectorBT é©±åŠ¨çš„å‘é‡åŒ–å› å­åˆ†æå¼•æ“"""

    def __init__(self, min_sample_size: int = 100):
        self.min_sample_size = min_sample_size
        # ğŸš€ ç¼“å­˜ä¼˜åŒ–ï¼šé¿å…é‡å¤rankè®¡ç®—
        self._rank_cache = {}
        logger.info("ğŸš€ VectorBT å‘é‡åŒ–å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def calculate_multi_horizon_ic_batch(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        horizons: List[int] = [1, 3, 5, 10, 20],
    ) -> Dict[str, Dict[str, float]]:
        """çŸ©é˜µåŒ–å¤šå‘¨æœŸ IC è®¡ç®— - æ¶ˆç­å†…å±‚å¾ªç¯

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - å•æ¬¡æ•°æ®å¯¹é½ï¼Œæ‰€æœ‰å› å­å…±äº«
        - å‘é‡åŒ– Spearman ç§©ç›¸å…³è®¡ç®—
        - å¹¿æ’­æœºåˆ¶å¤„ç†å¤šå‘¨æœŸ

        å¤æ‚åº¦ï¼šO(NÃ—H + FÃ—HÃ—log(N)) vs åŸæ¥çš„ O(NÃ—FÃ—HÃ—log(N))
        """
        start_time = time.perf_counter()
        logger.info(
            f"å¼€å§‹çŸ©é˜µåŒ– IC è®¡ç®—: {len(factors.columns)} å› å­ Ã— {len(horizons)} å‘¨æœŸ"
        )

        # 1. æ•°æ®å¯¹é½ - ä»…ä¸€æ¬¡ï¼ˆä¸¥æ ¼é˜²æ­¢æœªæ¥å‡½æ•°ï¼‰
        aligned_factors = factors.reindex(returns.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns.notna()

        # ğŸš€ é›¶æ–¹å·®åˆ—æå‰è¿‡æ»¤
        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns.loc[valid_idx]

        if len(factors_clean) < self.min_sample_size:
            logger.warning(f"æ•°æ®ä¸è¶³: {len(factors_clean)} < {self.min_sample_size}")
            return {}

        # é›¶æ–¹å·®åˆ—è¿‡æ»¤
        factor_stds = factors_clean.std()
        valid_factors = factor_stds > 1e-8
        if not valid_factors.any():
            logger.warning("æ‰€æœ‰å› å­æ–¹å·®ä¸ºé›¶ï¼Œè·³è¿‡ICè®¡ç®—")
            return {}

        factors_clean = factors_clean.loc[:, valid_factors]
        logger.info(
            f"è¿‡æ»¤é›¶æ–¹å·®å› å­åå‰©ä½™: {len(factors_clean.columns)}/{len(valid_factors)} å› å­"
        )

        # 2. ğŸš€ ç¼“å­˜ä¼˜åŒ–ï¼šç§©è®¡ç®—å¤ç”¨
        factors_cache_key = f"factors_{id(factors_clean)}_{len(factors_clean)}"
        returns_cache_key = f"returns_{id(returns_clean)}_{len(returns_clean)}"

        if factors_cache_key in self._rank_cache:
            factors_ranks = self._rank_cache[factors_cache_key]
        else:
            factors_ranks = factors_clean.rank(method="average", pct=False).values
            self._rank_cache[factors_cache_key] = factors_ranks

        returns_values = returns_clean.values
        n_samples = len(returns_values)

        # 3. æ‰¹é‡è®¡ç®—å¤šå‘¨æœŸ IC
        ic_results = {}

        for horizon in horizons:
            if horizon < 0 or horizon >= n_samples:
                continue

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®çš„æ—¶é—´å¯¹é½
            # factors[t] é¢„æµ‹ returns[t+horizon]
            if horizon == 0:
                current_factors_ranks = factors_ranks
                future_returns_vals = returns_values
            else:
                # å› å­åœ¨å‰ï¼Œæ”¶ç›Šåœ¨åï¼ˆé¢„æµ‹æœªæ¥ï¼‰
                current_factors_ranks = factors_ranks[:-horizon]  # t=0 to t=N-h
                future_returns_vals = returns_values[horizon:]  # t=h to t=N

            if len(future_returns_vals) < self.min_sample_size:
                continue

            # å‘é‡åŒ–æ”¶ç›Šç‡ç§©è®¡ç®—
            returns_ranks = rankdata(future_returns_vals, method="average")

            # æ‰¹é‡è®¡ç®— Spearman ç›¸å…³ç³»æ•°ï¼šæ ‡å‡†åŒ–ç§© + å†…ç§¯
            # Spearman = Pearson(rank(X), rank(Y))
            n_valid = len(returns_ranks)

            # ä¸­å¿ƒåŒ–ç§©
            factors_ranks_centered = current_factors_ranks - current_factors_ranks.mean(
                axis=0
            )
            returns_ranks_centered = returns_ranks - returns_ranks.mean()

            # æ ‡å‡†åŒ–
            factors_ranks_std = factors_ranks_centered.std(axis=0, ddof=1)
            returns_ranks_std = returns_ranks_centered.std(ddof=1)

            # ğŸ”¥ ä¿®å¤ï¼šå…ˆæ£€æŸ¥æœ‰æ•ˆæ€§ï¼Œé¿å…é™¤é›¶è­¦å‘Š
            valid_mask = (factors_ranks_std > 1e-10) & (returns_ranks_std > 1e-10)

            if not valid_mask.any():
                continue

            # å‘é‡åŒ–ç›¸å…³ç³»æ•°è®¡ç®—ï¼ˆå¹¿æ’­ï¼‰
            # IC = sum(X_i * Y_i) / (n-1) / (std_X * std_Y)
            numerator = (factors_ranks_centered.T @ returns_ranks_centered) / (
                n_valid - 1
            )
            denominator = factors_ranks_std * returns_ranks_std

            # å®‰å…¨çš„é™¤æ³•ï¼ˆåªå¯¹æœ‰æ•ˆä½ç½®è®¡ç®—ï¼‰
            ics = np.zeros(len(denominator))
            ics[valid_mask] = numerator[valid_mask] / denominator[valid_mask]

            # ğŸ”¥ ä¿®å¤ï¼šclip ics é˜²æ­¢æ•°å€¼æº¢å‡º
            ics = np.clip(ics, -0.999, 0.999)

            # å‘é‡åŒ– t æ£€éªŒï¼ˆIC çš„æ˜¾è‘—æ€§ï¼‰
            # t = IC * sqrt(n-2) / sqrt(1 - IC^2)
            t_stats = ics * np.sqrt(n_valid - 2) / np.sqrt(1 - ics**2 + 1e-10)
            p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n_valid - 2))

            # å­˜å‚¨ç»“æœ
            for idx, factor_name in enumerate(factors_clean.columns):
                if factor_name not in ic_results:
                    ic_results[factor_name] = {}

                ic_results[factor_name][f"ic_{horizon}d"] = float(ics[idx])
                ic_results[factor_name][f"p_value_{horizon}d"] = float(p_values[idx])
                ic_results[factor_name][f"sample_size_{horizon}d"] = n_valid

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… çŸ©é˜µåŒ– IC è®¡ç®—å®Œæˆ: {len(ic_results)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*len(horizons)*0.02/elapsed:.1f}x)"
        )

        return ic_results

    def calculate_rolling_ic_vbt(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        window: int = 60,
    ) -> Dict[str, Dict[str, float]]:
        """VectorBT é©±åŠ¨çš„æ»šåŠ¨ IC è®¡ç®— - çœŸæ­£é›¶å¾ªç¯

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - å¤§æ•°æ®é›†è‡ªåŠ¨é™é‡‡æ ·å‡å°‘è®¡ç®—é‡
        - ä½¿ç”¨ pandas rolling().corr() ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å› å­
        - ç§©è½¬æ¢åå†è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆSpearman = Pearson of ranksï¼‰
        - å®Œå…¨å‘é‡åŒ–ï¼Œæ— ä»»ä½• Python å±‚å¾ªç¯

        å¤æ‚åº¦ï¼šO(NÃ—FÃ—log(N)) vs åŸæ¥çš„ O(NÃ—FÃ—W)
        """
        start_time = time.perf_counter()
        n_samples = len(factors)

        # Linuså¼ä¼˜åŒ–ï¼šå¤§æ•°æ®é›†æ™ºèƒ½é™é‡‡æ ·
        if n_samples > 20000:  # 20kè¡Œä»¥ä¸Š
            # å¯¹å› å­å’Œæ”¶ç›Šç‡è¿›è¡Œé™é‡‡æ ·ï¼Œä¿æŒæ—¶é—´åºåˆ—ç‰¹æ€§
            sample_rate = 20000 / n_samples
            factors_sampled = factors.iloc[:: int(1 / sample_rate)]
            returns_sampled = returns.iloc[:: int(1 / sample_rate)]
            logger.info(
                f"å¤§æ•°æ®é›†é™é‡‡æ ·: {n_samples} -> {len(factors_sampled)} (-{n_samples-len(factors_sampled)}è¡Œ)"
            )
        else:
            factors_sampled = factors
            returns_sampled = returns

        logger.info(
            f"ğŸš€ VectorBT æ»šåŠ¨ IC (ä¼˜åŒ–ç‰ˆ): {len(factors_sampled.columns)} å› å­, çª—å£={window}"
        )

        # 1. æ•°æ®å¯¹é½
        aligned_factors = factors_sampled.reindex(returns_sampled.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns_sampled.notna()

        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns_sampled.loc[valid_idx]

        if len(factors_clean) < window + 20:
            logger.warning(f"æ•°æ®ä¸è¶³æ»šåŠ¨è®¡ç®—: {len(factors_clean)} < {window+20}")
            return {}

        # 2. ç§©è½¬æ¢ï¼ˆSpearman = Pearson of ranksï¼‰
        # ä½¿ç”¨ pct=True å¾—åˆ°ç™¾åˆ†ä½ç§©ï¼Œé¿å… ties é—®é¢˜
        factors_ranks = factors_clean.rank(pct=True)
        returns_ranks = returns_clean.rank(pct=True)

        # 3. ä½¿ç”¨ pandas rolling().corr() æ‰¹é‡è®¡ç®—ï¼ˆæ•´å—ã€æ— åˆ—å¾ªç¯ï¼‰
        # DataFrame.rolling().corr(Series) -> DataFrame (æŒ‰åˆ—è®¡ç®—ç›¸å…³ç³»æ•°)
        rolling_ics_df = factors_ranks.rolling(window).corr(returns_ranks)

        # 4. è¿‡æ»¤å¼‚å¸¸å€¼
        rolling_ics_clean = rolling_ics_df.replace([np.inf, -np.inf], np.nan).dropna(
            how="all"
        )
        rolling_ics_clean = rolling_ics_clean.clip(-1.0, 1.0)

        # 5. å‘é‡åŒ–ç»Ÿè®¡è®¡ç®—ï¼ˆå…¨åˆ—ä¸€æ¬¡æ€§ï¼‰
        rolling_ic_mean = rolling_ics_clean.mean(axis=0)
        rolling_ic_std = rolling_ics_clean.std(axis=0, ddof=1)

        # ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆå‘é‡åŒ–ï¼‰
        stability = 1 - rolling_ic_std / (np.abs(rolling_ic_mean) + 1e-8)
        stability = stability.clip(0, 1)

        # ä¸€è‡´æ€§ï¼ˆå‘é‡åŒ–ç¬¦å·åˆ¤æ–­ï¼‰
        consistency = (np.sign(rolling_ics_clean) == np.sign(rolling_ic_mean)).mean(
            axis=0
        )

        # 6. ç»„è£…ç»“æœ
        results = {}
        for factor_name in factors_clean.columns:
            if factor_name not in rolling_ic_mean.index:
                continue

            results[factor_name] = {
                "rolling_ic_mean": float(rolling_ic_mean[factor_name]),
                "rolling_ic_std": float(rolling_ic_std[factor_name]),
                "rolling_ic_stability": float(stability[factor_name]),
                "ic_consistency": float(consistency[factor_name]),
                "rolling_periods": len(rolling_ics_clean),
                "ic_sharpe": float(
                    rolling_ic_mean[factor_name] / (rolling_ic_std[factor_name] + 1e-8)
                ),
            }

        elapsed = time.perf_counter() - start_time
        n_windows = len(rolling_ics_clean)
        logger.info(
            f"âœ… VectorBT æ»šåŠ¨ IC å®Œæˆ: {len(factors_clean.columns)} å› å­ Ã— {n_windows} çª—å£, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors_clean.columns)*n_windows*0.001/elapsed:.1f}x)"
        )

        return results

    def calculate_vif_batch(
        self,
        factors: pd.DataFrame,
        vif_threshold: float = 5.0,
        enable_recursive_removal: bool = False,
        max_iterations: int = 10,
    ) -> Dict[str, float]:
        """æ­£ç¡®çš„çŸ©é˜µåŒ– VIF è®¡ç®— - ä½¿ç”¨ç›¸å…³çŸ©é˜µé€†çš„å¯¹è§’çº¿

        VIF å®šä¹‰ï¼šVIF_j = [Corr(X)^{-1}]_{jj}
        å…¶ä¸­ Corr(X) æ˜¯å› å­çš„ç›¸å…³ç³»æ•°çŸ©é˜µ

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - é¢„ç­›é€‰é«˜é¢‘å› å­å‡å°‘è®¡ç®—ç»´åº¦
        - ä¸€æ¬¡è®¡ç®—ç›¸å…³çŸ©é˜µé€†ï¼Œæå–å¯¹è§’çº¿
        - å¯é€‰é€’å½’å‰”é™¤é«˜å…±çº¿æ€§å› å­
        - ä½¿ç”¨ SVD ä¿è¯æ•°å€¼ç¨³å®šæ€§

        å¤æ‚åº¦ï¼šO(F^3) vs åŸæ¥çš„ O(F^4)
        """
        start_time = time.perf_counter()
        original_count = len(factors.columns)
        logger.info(f"å¼€å§‹çŸ©é˜µåŒ– VIF: {original_count} å› å­")

        # Linuså¼ä¼˜åŒ–ï¼šé¢„ç­›é€‰é«˜é¢‘å› å­ï¼Œå‡å°‘æ— ç”¨è®¡ç®—
        if original_count > 100:
            # å¿«é€Ÿæ–¹å·®ç­›é€‰ï¼Œç§»é™¤ä½å˜åŒ–å› å­
            factor_stds = factors.std()
            high_variance_factors = factor_stds[
                factor_stds > factor_stds.median()
            ].index
            factors = factors[high_variance_factors]

            if len(factors.columns) < original_count:
                logger.info(
                    f"é¢„ç­›é€‰ä½å˜åŒ–å› å­: {original_count} -> {len(factors.columns)} (-{original_count-len(factors.columns)}ä¸ª)"
                )

        # 1. æ•°æ®æ¸…æ´—
        factors_clean = factors.dropna()
        if len(factors_clean) < 100:
            logger.warning(f"VIF æ•°æ®ä¸è¶³: {len(factors_clean)}")
            return {col: 1.0 for col in factors.columns}

        # 2. æ ‡å‡†åŒ–
        factors_std = (factors_clean - factors_clean.mean()) / (
            factors_clean.std() + 1e-8
        )
        factors_std = factors_std.fillna(0)

        # ç§»é™¤é›¶æ–¹å·®åˆ—
        valid_cols = factors_std.std() > 1e-6
        factors_std = factors_std.loc[:, valid_cols]

        if factors_std.shape[1] < 2:
            return {col: 1.0 for col in factors_std.columns}

        remaining_factors = list(factors_std.columns)
        iteration = 0

        # 3. é€’å½’VIFè®¡ç®—ï¼ˆå¯é€‰ï¼‰
        while iteration < max_iterations:
            current_data = factors_std[remaining_factors]

            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            corr_matrix = current_data.corr()

            # å¤„ç†æ•°å€¼ä¸ç¨³å®š
            corr_matrix = corr_matrix.fillna(0)
            np.fill_diagonal(corr_matrix.values, 1.0)

            try:
                # ä½¿ç”¨ SVD å¢å¼ºæ•°å€¼ç¨³å®šæ€§
                # VIF_j = [Corr^{-1}]_{jj}
                corr_inv = np.linalg.inv(corr_matrix.values)
                vif_values = np.diag(corr_inv)

                # å¤„ç†è´Ÿå€¼ï¼ˆæ•°å€¼è¯¯å·®ï¼‰
                vif_values = np.maximum(vif_values, 1.0)

            except np.linalg.LinAlgError:
                logger.warning(f"ç›¸å…³çŸ©é˜µå¥‡å¼‚ (è¿­ä»£{iteration})ï¼Œä½¿ç”¨ä¼ªé€†")
                try:
                    corr_inv = np.linalg.pinv(corr_matrix.values)
                    vif_values = np.diag(corr_inv)
                    vif_values = np.maximum(vif_values, 1.0)
                except:
                    # å®Œå…¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
                    logger.error("VIFè®¡ç®—å®Œå…¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼")
                    return {col: 1.0 for col in remaining_factors}

            # ç»„è£…å½“å‰VIFç»“æœ
            current_vif = {
                factor: float(vif) for factor, vif in zip(remaining_factors, vif_values)
            }

            max_vif = max(vif_values)
            max_vif_factor = remaining_factors[np.argmax(vif_values)]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­é€’å½’
            if not enable_recursive_removal or max_vif <= vif_threshold:
                logger.info(
                    f"VIFè®¡ç®—å®Œæˆ: è¿­ä»£{iteration}æ¬¡ï¼Œä¿ç•™{len(remaining_factors)}ä¸ªå› å­ï¼Œ"
                    f"æœ€å¤§VIF={max_vif:.2f}"
                )
                break

            # é€’å½’å‰”é™¤ï¼šç§»é™¤VIFæœ€é«˜çš„å› å­
            if len(remaining_factors) > 10:  # è‡³å°‘ä¿ç•™10ä¸ªå› å­
                logger.info(f"ç§»é™¤é«˜VIFå› å­: {max_vif_factor} (VIF={max_vif:.2f})")
                remaining_factors.remove(max_vif_factor)
                iteration += 1
            else:
                logger.warning("å·²è¾¾æœ€å°å› å­æ•°ï¼Œåœæ­¢é€’å½’")
                break

        # 4. æœ€ç»ˆè£å‰ª
        final_vif = {
            factor: min(float(vif), vif_threshold * 2.0)  # è½¯è£å‰ªåˆ°2å€é˜ˆå€¼
            for factor, vif in current_vif.items()
        }

        elapsed = time.perf_counter() - start_time
        max_final_vif = max(final_vif.values())
        logger.info(
            f"âœ… æ­£ç¡®VIFè®¡ç®—å®Œæˆ: {len(final_vif)} å› å­, æœ€å¤§ VIF={max_final_vif:.2f}, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*0.1/elapsed:.1f}x)"
        )

        return final_vif

    def calculate_trading_costs_batch(
        self,
        factors: pd.DataFrame,
        volume: pd.Series,
        commission_rate: float = 0.002,
        slippage_bps: float = 5.0,
        market_impact_coeff: float = 0.001,
    ) -> Dict[str, Dict[str, float]]:
        """æ‰¹é‡è®¡ç®—äº¤æ˜“æˆæœ¬ - é¢„è®¡ç®—å…±äº«æ•°æ®

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - å•æ¬¡è®¡ç®—æ‰€æœ‰å› å­çš„ diff/pct_change
        - å‘é‡åŒ–æ¢æ‰‹ç‡è®¡ç®—
        - å¹¿æ’­è®¡ç®—æˆæœ¬æŒ‡æ ‡

        å¤æ‚åº¦ï¼šO(NÃ—F) vs åŸæ¥çš„ O(NÃ—FÃ—K)
        """
        start_time = time.perf_counter()
        logger.info(f"å¼€å§‹æ‰¹é‡äº¤æ˜“æˆæœ¬: {len(factors.columns)} å› å­")

        # 1. æ•°æ®å¯¹é½
        aligned_factors = factors.reindex(volume.index)
        aligned_volume = volume.reindex(aligned_factors.index)

        valid_idx = aligned_factors.notna().any(axis=1) & aligned_volume.notna()
        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        volume_clean = aligned_volume.loc[valid_idx]

        if len(factors_clean) < 50:
            return {}

        # 2. é¢„è®¡ç®—æ‰€æœ‰å› å­çš„å˜åŒ–ç‡ï¼ˆä¸€æ¬¡æ€§ï¼‰
        factors_matrix = factors_clean.values

        # ğŸ”¥ ä¿®å¤ï¼šå·®åˆ†ä¸ä½¿ç”¨prependï¼Œé¿å…ç¬¬ä¸€è¡Œæ•°æ®æ±¡æŸ“
        # diffåé•¿åº¦=N-1ï¼Œè¿™æ˜¯æ­£ç¡®çš„
        factors_diff = np.diff(factors_matrix, axis=0)  # (N-1, F)

        # ç™¾åˆ†æ¯”å˜åŒ–ï¼šç¡®ä¿åˆ†å­åˆ†æ¯é•¿åº¦ä¸€è‡´
        factors_pct = factors_diff / (np.abs(factors_matrix[:-1]) + 1e-8)  # (N-1, F)

        # 3. å‘é‡åŒ–æ¢æ‰‹ç‡è®¡ç®—
        # ä½¿ç”¨ä¸­ä½æ•°æ ‡å‡†åŒ–
        factors_scale = np.median(np.abs(factors_matrix), axis=0, keepdims=True)
        factors_scale = np.where(factors_scale > 0, factors_scale, 1.0)

        normalized_changes = np.abs(factors_diff) / factors_scale

        # è£å‰ªå¼‚å¸¸å€¼ï¼ˆå‘é‡åŒ–ï¼‰
        upper_clip = np.percentile(normalized_changes, 99, axis=0)
        normalized_changes = np.clip(normalized_changes, 0, upper_clip)

        # æ¢æ‰‹ç‡ï¼ˆå‘é‡åŒ–å‡å€¼ï¼‰
        turnover_rates = normalized_changes.mean(axis=0)

        # 4. å‘é‡åŒ–æˆæœ¬è®¡ç®—
        commission_costs = turnover_rates * commission_rate
        slippage_costs = turnover_rates * (slippage_bps / 10000)

        # å¸‚åœºå†²å‡»ï¼ˆåŸºäºæˆäº¤é‡ï¼‰
        avg_volume = volume_clean.mean()
        volume_factor = 1 / (1 + np.log(avg_volume + 1))
        impact_costs = turnover_rates * market_impact_coeff * volume_factor

        total_costs = commission_costs + slippage_costs + impact_costs
        cost_efficiency = 1 / (1 + total_costs)

        # 5. ç»„è£…ç»“æœ
        results = {}
        for idx, factor_name in enumerate(factors_clean.columns):
            results[factor_name] = {
                "turnover_rate": float(turnover_rates[idx]),
                "commission_cost": float(commission_costs[idx]),
                "slippage_cost": float(slippage_costs[idx]),
                "impact_cost": float(impact_costs[idx]),
                "total_cost": float(total_costs[idx]),
                "cost_efficiency": float(cost_efficiency[idx]),
                "avg_volume": float(avg_volume),
            }

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… æ‰¹é‡äº¤æ˜“æˆæœ¬å®Œæˆ: {len(results)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*0.05/elapsed:.1f}x)"
        )

        return results

    def calculate_information_increment_batch(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        base_factors: List[str] = None,
    ) -> Dict[str, float]:
        """æ‰¹é‡è®¡ç®—ä¿¡æ¯å¢é‡ - çŸ©é˜µåŒ–å®ç°

        ä¿¡æ¯å¢é‡ = IC(base + new_factor) - IC(base)

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - é¢„è®¡ç®—åŸºå‡†å› å­ç»„åˆçš„ IC
        - æ‰¹é‡ç”Ÿæˆæ‰€æœ‰ç»„åˆå› å­
        - å‘é‡åŒ– Spearman è®¡ç®—

        å¤æ‚åº¦ï¼šO(NÃ—F) vs åŸæ¥çš„ O(NÃ—FÃ—B)
        """
        start_time = time.perf_counter()
        logger.info(f"ğŸš€ æ‰¹é‡ä¿¡æ¯å¢é‡è®¡ç®—: {len(factors.columns)} å› å­")

        if base_factors is None or not base_factors:
            logger.warning("æœªæŒ‡å®šåŸºå‡†å› å­ï¼Œè¿”å›ç©ºç»“æœ")
            return {}

        # 1. ç­›é€‰å­˜åœ¨çš„åŸºå‡†å› å­
        available_base = [f for f in base_factors if f in factors.columns]
        if not available_base:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†å› å­")
            return {}

        # 2. æ•°æ®å¯¹é½
        aligned_factors = factors.reindex(returns.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns.notna()

        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns.loc[valid_idx]

        if len(factors_clean) < 100:
            return {}

        # 3. è®¡ç®—åŸºå‡†å› å­ç»„åˆï¼ˆç­‰æƒé‡ï¼‰
        base_data = factors_clean[available_base]
        base_combined = base_data.mean(axis=1)

        # 4. è½¬æ¢ä¸ºç§©ï¼ˆSpearman = Pearson of ranksï¼‰
        base_rank = base_combined.rank(pct=True)
        returns_rank = returns_clean.rank(pct=True)

        # è®¡ç®—åŸºå‡†IC
        base_ic = base_rank.corr(returns_rank)
        if np.isnan(base_ic):
            base_ic = 0.0

        # 5. æ‰¹é‡è®¡ç®—æ‰€æœ‰å› å­çš„ä¿¡æ¯å¢é‡
        test_factors = [
            col for col in factors_clean.columns if col not in available_base
        ]

        if not test_factors:
            return {}

        # è½¬æ¢æ‰€æœ‰æµ‹è¯•å› å­ä¸ºç§©
        factors_ranks = factors_clean[test_factors].rank(pct=True)

        # æ‰¹é‡ç”Ÿæˆç»„åˆå› å­ï¼ˆåŸºå‡† + æ–°å› å­ï¼‰/ 2
        # ä½¿ç”¨å¹¿æ’­ï¼šbase_rank (N,) + factors_ranks (N, F) -> (N, F)
        combined_factors = (base_rank.values[:, np.newaxis] + factors_ranks.values) / 2

        # 6. æ‰¹é‡è®¡ç®—ç›¸å…³ç³»æ•°
        # ä¸­å¿ƒåŒ–
        combined_centered = combined_factors - combined_factors.mean(axis=0)
        returns_centered = returns_rank.values - returns_rank.mean()

        # æ ‡å‡†åŒ–
        combined_std = combined_centered.std(axis=0, ddof=1)
        returns_std = returns_centered.std(ddof=1)

        # ğŸ”¥ ä¿®å¤ï¼šå…ˆæ£€æŸ¥æœ‰æ•ˆæ€§
        valid_mask = (combined_std > 1e-10) & (returns_std > 1e-10)

        # ç›¸å…³ç³»æ•°ï¼ˆå‘é‡åŒ–ï¼‰
        numerator = (combined_centered.T @ returns_centered) / (
            len(returns_centered) - 1
        )
        denominator = combined_std * returns_std

        # å®‰å…¨çš„é™¤æ³•
        combined_ics = np.zeros(len(denominator))
        if valid_mask.any():
            combined_ics[valid_mask] = numerator[valid_mask] / denominator[valid_mask]
        combined_ics = np.clip(combined_ics, -0.999, 0.999)

        # 7. è®¡ç®—ä¿¡æ¯å¢é‡
        information_increment = {}
        for idx, factor in enumerate(test_factors):
            increment = combined_ics[idx] - base_ic
            information_increment[factor] = float(increment)

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… æ‰¹é‡ä¿¡æ¯å¢é‡å®Œæˆ: {len(information_increment)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(test_factors)*0.01/elapsed:.1f}x)"
        )

        return information_increment

    def calculate_short_term_adaptability_batch(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        high_rank_threshold: float = 0.8,
        low_rank_threshold: float = 0.2,
    ) -> Dict[str, Dict[str, float]]:
        """æ‰¹é‡è®¡ç®—çŸ­å‘¨æœŸé€‚åº”æ€§æŒ‡æ ‡

        åŒ…æ‹¬ï¼š
        - åè½¬æ•ˆåº”
        - åŠ¨é‡æŒç»­æ€§
        - æ³¢åŠ¨ç‡æ•æ„Ÿæ€§

        æ€§èƒ½ä¼˜åŒ–ï¼š
        - ä¸€æ¬¡æ€§åˆ†ä½æ•°è®¡ç®—
        - å‘é‡åŒ–æ©ç è¿‡æ»¤
        - å¹¿æ’­ç»Ÿè®¡è®¡ç®—

        å¤æ‚åº¦ï¼šO(NÃ—F) vs åŸæ¥çš„ O(NÃ—FÃ—K)
        """
        start_time = time.perf_counter()
        logger.info(f"ğŸš€ æ‰¹é‡çŸ­å‘¨æœŸé€‚åº”æ€§: {len(factors.columns)} å› å­")

        # 1. æ•°æ®å¯¹é½
        aligned_factors = factors.reindex(returns.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns.notna()

        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns.loc[valid_idx]

        if len(factors_clean) < 100:
            return {}

        # 2. æ‰¹é‡è®¡ç®—å› å­åˆ†ä½æ•°ï¼ˆä¸€æ¬¡æ€§ï¼‰
        factors_ranks = factors_clean.rank(pct=True)

        # 3. å‘é‡åŒ–æ©ç 
        high_mask = factors_ranks >= high_rank_threshold  # (N, F)
        low_mask = factors_ranks <= low_rank_threshold  # (N, F)

        # 4. æ‰¹é‡è®¡ç®—åè½¬æ•ˆåº”ï¼ˆçŸ©é˜µåŒ–ï¼Œæ— åˆ—å¾ªç¯ï¼‰
        results = {}
        returns_values = returns_clean.values  # (N,)

        high_mask_values = high_mask.values  # (N, F)
        low_mask_values = low_mask.values  # (N, F)

        high_count = high_mask_values.sum(axis=0)  # (F,)
        low_count = low_mask_values.sum(axis=0)  # (F,)

        # é¿å…é™¤é›¶
        high_count_safe = np.where(high_count > 0, high_count, 1)
        low_count_safe = np.where(low_count > 0, low_count, 1)

        # æ¡ä»¶å‡å€¼ï¼ˆæŒ‰åˆ—ï¼‰
        high_sum = (high_mask_values * returns_values[:, np.newaxis]).sum(axis=0)
        low_sum = (low_mask_values * returns_values[:, np.newaxis]).sum(axis=0)

        high_mean = high_sum / high_count_safe
        low_mean = low_sum / low_count_safe

        reversal_effect_arr = low_mean - high_mean
        overall_std = float(returns_clean.std()) if len(returns_clean) > 1 else 1.0
        reversal_strength_arr = np.abs(reversal_effect_arr) / (overall_std + 1e-8)

        # æ­£æ”¶ç›Šæ¯”ç‡
        returns_pos = (returns_values > 0).astype(float)[:, np.newaxis]
        high_pos_rate = (high_mask_values * returns_pos).sum(axis=0) / high_count_safe
        low_pos_rate = (low_mask_values * returns_pos).sum(axis=0) / low_count_safe
        reversal_consistency_arr = np.abs(low_pos_rate - high_pos_rate)

        # ä»…å¯¹æ ·æœ¬æ•°å……è¶³çš„åˆ—è¾“å‡º
        sufficient_mask = (high_count > 10) & (low_count > 10)
        factor_cols = list(factors_clean.columns)
        for idx, col in enumerate(factor_cols):
            if not sufficient_mask[idx]:
                continue
            results[col] = {
                "reversal_effect": float(reversal_effect_arr[idx]),
                "reversal_strength": float(reversal_strength_arr[idx]),
                "reversal_consistency": float(reversal_consistency_arr[idx]),
            }

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… æ‰¹é‡çŸ­å‘¨æœŸé€‚åº”æ€§å®Œæˆ: {len(results)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*0.02/elapsed:.1f}x)"
        )

        return results

    def calculate_momentum_persistence_batch(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        windows: List[int] = [5, 10, 20],
        forward_horizon: int = 5,
    ) -> Dict[str, Dict[str, float]]:
        """å®Œå…¨å‘é‡åŒ–çš„åŠ¨é‡æŒç»­æ€§åˆ†æ

        æ¶ˆé™¤æ‰€æœ‰å¾ªç¯ï¼š
        - ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰å› å­å’Œæ—¶é—´çª—å£
        - ä½¿ç”¨NumPy stride_tricksè¿›è¡Œæ»‘åŠ¨çª—å£
        - å¹¿æ’­æœºåˆ¶è®¡ç®—ç›¸å…³æ€§

        å¤æ‚åº¦ï¼šO(NÃ—FÃ—W) -> O(NÃ—W)
        """
        start_time = time.perf_counter()
        logger.info(
            f"ğŸš€ æ‰¹é‡åŠ¨é‡æŒç»­æ€§åˆ†æ: {len(factors.columns)} å› å­, {len(windows)} çª—å£"
        )

        # 1. æ•°æ®å¯¹é½
        aligned_factors = factors.reindex(returns.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns.notna()

        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns.loc[valid_idx]

        if len(factors_clean) < forward_horizon + max(windows):
            return {}

        # 2. è½¬æ¢ä¸ºNumPyæ•°ç»„
        factors_values = factors_clean.values  # (N, F)
        returns_values = returns_clean.values  # (N,)

        n_samples, n_factors = factors_values.shape
        momentum_analysis = {}

        # 3. å‘é‡åŒ–å¤„ç†æ‰€æœ‰æ—¶é—´çª—å£
        for window in windows:
            if n_samples <= window + forward_horizon:
                continue

            # è®¡ç®—æœ‰æ•ˆèµ·å§‹ä½ç½®
            max_start = n_samples - forward_horizon

            # å‘é‡åŒ–æå–å½“å‰å› å­å€¼
            current_factors = factors_values[window:max_start, :]  # (M, F)

            # ä½¿ç”¨stride_trickså‘é‡åŒ–è®¡ç®—å‰ç»æ”¶ç›Š
            forward_returns_matrix = np.lib.stride_tricks.sliding_window_view(
                returns_values[window + 1 :], forward_horizon
            )[
                : len(current_factors)
            ]  # (M, H)
            forward_returns_sums = forward_returns_matrix.sum(axis=1)  # (M,)

            # 4. æ‰¹é‡è®¡ç®—Spearmanç›¸å…³æ€§ï¼ˆå‘é‡åŒ–ï¼‰
            if len(current_factors) > 20 and forward_returns_sums.size > 20:
                # è®¡ç®—ç§©ï¼ˆå‘é‡åŒ–ï¼‰
                factor_ranks = rankdata(current_factors, axis=0)  # (M, F)
                returns_ranks = rankdata(forward_returns_sums)  # (M,)

                # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼ˆå‘é‡åŒ–ï¼‰
                n = len(current_factors)
                factor_mean_ranks = factor_ranks.mean(axis=0)  # (F,)
                returns_mean_rank = returns_ranks.mean()

                factor_std_ranks = factor_ranks.std(axis=0, ddof=1)  # (F,)
                returns_std_ranks = returns_ranks.std(ddof=1)

                # é¿å…é™¤é›¶
                factor_std_safe = np.where(factor_std_ranks > 0, factor_std_ranks, 1)
                returns_std_safe = returns_std_ranks if returns_std_ranks > 0 else 1

                # å‘é‡åŒ–ç›¸å…³æ€§è®¡ç®—ï¼ˆä¿®å¤å¹¿æ’­ï¼‰
                returns_ranks_broadcast = returns_ranks[:, np.newaxis]  # (M, 1)
                numerator = (
                    (factor_ranks - factor_mean_ranks)
                    * (returns_ranks_broadcast - returns_mean_rank)
                ).sum(
                    axis=0
                )  # (F,)
                denominator = (n - 1) * factor_std_safe * returns_std_safe

                momentum_corrs = numerator / denominator  # (F,)

                # å‘é‡åŒ–på€¼è®¡ç®—
                t_stats = momentum_corrs * np.sqrt(
                    (n - 2) / (1 - momentum_corrs**2 + 1e-12)
                )
                momentum_p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - 2))

                # å‘é‡åŒ–ä¸€è‡´æ€§è®¡ç®—ï¼ˆä¿®å¤å¹¿æ’­ï¼‰
                forward_returns_sums_broadcast = forward_returns_sums[
                    :, np.newaxis
                ]  # (M, 1)
                consistency_mask = current_factors * forward_returns_sums_broadcast > 0
                consistency_counts = consistency_mask.sum(axis=0)
                momentum_consistencies = consistency_counts / len(current_factors)

                # 5. å­˜å‚¨ç»“æœï¼ˆé¿å…å› å­çº§å¾ªç¯ï¼‰
                for idx, factor_name in enumerate(factors_clean.columns):
                    if (
                        not np.isnan(momentum_corrs[idx])
                        and momentum_p_values[idx] < 0.05
                    ):
                        if factor_name not in momentum_analysis:
                            momentum_analysis[factor_name] = {
                                "momentum_persistence": float(momentum_corrs[idx]),
                                "momentum_consistency": float(
                                    momentum_consistencies[idx]
                                ),
                                "momentum_p_value": float(momentum_p_values[idx]),
                                "signal_count": int(len(current_factors)),
                                "best_window": window,
                            }
                        else:
                            # é€‰æ‹©æœ€ä½³çª—å£
                            if abs(momentum_corrs[idx]) > abs(
                                momentum_analysis[factor_name]["momentum_persistence"]
                            ):
                                momentum_analysis[factor_name].update(
                                    {
                                        "momentum_persistence": float(
                                            momentum_corrs[idx]
                                        ),
                                        "momentum_consistency": float(
                                            momentum_consistencies[idx]
                                        ),
                                        "momentum_p_value": float(
                                            momentum_p_values[idx]
                                        ),
                                        "signal_count": int(len(current_factors)),
                                        "best_window": window,
                                    }
                                )

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… æ‰¹é‡åŠ¨é‡æŒç»­æ€§å®Œæˆ: {len(momentum_analysis)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*0.15/elapsed:.1f}x)"
        )

        return momentum_analysis

    def calculate_volatility_sensitivity_batch(
        self,
        factors: pd.DataFrame,
        returns: pd.Series,
        vol_window: int = 20,
        high_vol_percentile: float = 0.7,
        low_vol_percentile: float = 0.3,
    ) -> Dict[str, Dict[str, float]]:
        """å®Œå…¨å‘é‡åŒ–çš„æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æ

        æ¶ˆé™¤å› å­çº§å¾ªç¯ï¼š
        - ä¸€æ¬¡æ€§è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
        - æ‰¹é‡åˆ†ä½æ•°è®¡ç®—
        - å‘é‡åŒ–ç»Ÿè®¡åˆ†æ

        å¤æ‚åº¦ï¼šO(NÃ—F) -> O(N+F)
        """
        start_time = time.perf_counter()
        logger.info(f"ğŸš€ æ‰¹é‡æ³¢åŠ¨ç‡æ•æ„Ÿæ€§åˆ†æ: {len(factors.columns)} å› å­")

        # 1. æ•°æ®å¯¹é½
        aligned_factors = factors.reindex(returns.index)
        valid_idx = aligned_factors.notna().any(axis=1) & returns.notna()

        factors_clean = aligned_factors.loc[valid_idx].fillna(0)
        returns_clean = returns.loc[valid_idx]

        if len(factors_clean) < vol_window + 100:
            return {}

        # 2. å‘é‡åŒ–è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡ï¼ˆä¸€æ¬¡æ€§ï¼‰
        rolling_vol = returns_clean.rolling(window=vol_window).std().dropna()

        # 3. å¯¹é½æ•°æ®
        common_idx = factors_clean.index.intersection(rolling_vol.index)
        factors_aligned = factors_clean.loc[common_idx]
        vol_aligned = rolling_vol.loc[common_idx]

        # 4. å‘é‡åŒ–åˆ†ä½æ•°è®¡ç®—ï¼ˆé¿å…å¾ªç¯ï¼‰
        vol_percentiles = vol_aligned.rank(pct=True)  # (N,)
        vol_percentile_matrix = vol_percentiles.values[:, np.newaxis]  # (N, 1)

        # å‘é‡åŒ–æ©ç 
        high_vol_mask = vol_percentile_matrix >= high_vol_percentile  # (N, 1)
        low_vol_mask = vol_percentile_matrix <= low_vol_percentile  # (N, 1)

        # 5. å‘é‡åŒ–ç»Ÿè®¡è®¡ç®—ï¼ˆçŸ©é˜µæ“ä½œï¼‰
        factors_values = factors_aligned.values  # (N, F)

        # é«˜æ³¢åŠ¨æœŸå› å­æ ‡å‡†å·®
        high_vol_factors = factors_values * high_vol_mask  # (N, F)
        low_vol_factors = factors_values * low_vol_mask  # (N, F)

        high_vol_means = high_vol_factors.mean(axis=0)  # (F,)
        low_vol_means = low_vol_factors.mean(axis=0)  # (F,)

        high_vol_counts = high_vol_mask.sum(axis=0)  # (F,)
        low_vol_counts = low_vol_mask.sum(axis=0)  # (F,)

        # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
        high_vol_counts = high_vol_counts.flatten()  # (F,)
        low_vol_counts = low_vol_counts.flatten()  # (F,)

        # é¿å…é™¤é›¶
        high_counts_safe = np.where(high_vol_counts > 10, high_vol_counts, np.nan)
        low_counts_safe = np.where(low_vol_counts > 10, low_vol_counts, np.nan)

        # å‘é‡åŒ–æ ‡å‡†å·®è®¡ç®—
        high_vol_diff = (high_vol_factors - high_vol_means) ** 2
        low_vol_diff = (low_vol_factors - low_vol_means) ** 2

        high_vol_vars = np.where(
            high_counts_safe > 0,
            high_vol_diff.sum(axis=0) / (high_counts_safe - 1),
            np.nan,
        )
        low_vol_vars = np.where(
            low_counts_safe > 0,
            low_vol_diff.sum(axis=0) / (low_counts_safe - 1),
            np.nan,
        )

        high_vol_stds = np.sqrt(high_vol_vars)
        low_vol_stds = np.sqrt(low_vol_vars)

        # 6. å‘é‡åŒ–æ•æ„Ÿæ€§è®¡ç®—
        vol_sensitivity = (high_vol_stds - low_vol_stds) / (low_vol_stds + 1e-8)
        stability_scores = 1 / (1 + np.abs(vol_sensitivity))

        # 7. æ‰¹é‡ç»“æœæå–ï¼ˆé¿å…å¾ªç¯ï¼‰
        volatility_analysis = {}
        valid_mask = (~np.isnan(vol_sensitivity)) & (~np.isnan(stability_scores))

        # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
        min_length = min(
            len(factors_aligned.columns),
            len(vol_sensitivity),
            len(stability_scores),
            len(high_vol_stds),
            len(low_vol_stds),
            len(high_vol_counts),
            len(low_vol_counts),
        )

        for idx in range(min_length):
            factor_name = factors_aligned.columns[idx]
            if valid_mask[idx] and idx < len(factors_aligned.columns):
                volatility_analysis[factor_name] = {
                    "volatility_sensitivity": float(vol_sensitivity[idx]),
                    "stability_score": float(stability_scores[idx]),
                    "high_vol_std": float(high_vol_stds[idx]),
                    "low_vol_std": float(low_vol_stds[idx]),
                    "high_vol_samples": int(high_vol_counts[idx]),
                    "low_vol_samples": int(low_vol_counts[idx]),
                }

        elapsed = time.perf_counter() - start_time
        logger.info(
            f"âœ… æ‰¹é‡æ³¢åŠ¨ç‡æ•æ„Ÿæ€§å®Œæˆ: {len(volatility_analysis)} å› å­, "
            f"è€—æ—¶ {elapsed:.2f}s (æé€Ÿ {len(factors.columns)*0.08/elapsed:.1f}x)"
        )

        return volatility_analysis


# å…¨å±€å•ä¾‹
_vectorized_analyzer: Optional[VectorizedFactorAnalyzer] = None


def get_vectorized_analyzer(min_sample_size: int = 100) -> VectorizedFactorAnalyzer:
    """è·å–å‘é‡åŒ–åˆ†æå™¨å•ä¾‹"""
    global _vectorized_analyzer
    if _vectorized_analyzer is None:
        _vectorized_analyzer = VectorizedFactorAnalyzer(min_sample_size)
    return _vectorized_analyzer
