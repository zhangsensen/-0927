#!/usr/bin/env python3
"""
ç”Ÿäº§æµæ°´çº¿ä¸»è°ƒåº¦
æ•´åˆï¼šåˆ†æ± ç”Ÿäº§ â†’ å›æµ‹ â†’ å®¹é‡ â†’ CI â†’ æŒ‡æ ‡æ±‡æ€» â†’ é€šçŸ¥
"""
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path

from notification_handler import NotificationHandler, SnapshotManager
from path_utils import get_paths

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


class ProductionPipeline:
    """ç”Ÿäº§æµæ°´çº¿"""

    def __init__(self, base_dir: str = "factor_output/etf_rotation_production"):
        self.base_dir = Path(base_dir)
        self.notifier = NotificationHandler()
        self.snapshot_mgr = SnapshotManager(max_snapshots=10)
        self.pools = ["A_SHARE", "QDII", "OTHER"]
        self.failed_tasks = []

    def run_command(self, cmd: list, task_name: str) -> bool:
        """è¿è¡Œå‘½ä»¤å¹¶æ•è·é”™è¯¯"""
        logger.info(f"\n{'=' * 80}")
        logger.info(f"æ‰§è¡Œä»»åŠ¡: {task_name}")
        logger.info(f"å‘½ä»¤: {' '.join(cmd)}")
        logger.info(f"{'=' * 80}")

        try:
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                timeout=3600,  # 1å°æ—¶è¶…æ—¶
            )

            logger.info(result.stdout)
            logger.info(f"âœ… {task_name} å®Œæˆ")
            return True

        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ {task_name} å¤±è´¥")
            logger.error(f"è¿”å›ç : {e.returncode}")
            logger.error(f"é”™è¯¯è¾“å‡º:\n{e.stderr}")

            self.failed_tasks.append(
                {
                    "task": task_name,
                    "error": e.stderr[:500],  # æˆªæ–­
                    "returncode": e.returncode,
                }
            )

            # å‘é€å¤±è´¥é€šçŸ¥
            self.notifier.notify_failure(task_name, e.stderr[:200])

            return False

        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {task_name} è¶…æ—¶")
            self.failed_tasks.append({"task": task_name, "error": "Timeout"})
            self.notifier.notify_failure(task_name, "ä»»åŠ¡è¶…æ—¶ï¼ˆ>1å°æ—¶ï¼‰")
            return False

    def run_pool_management(self) -> bool:
        """è¿è¡Œåˆ†æ± ç®¡ç†ï¼ˆç”Ÿäº§é¢æ¿ï¼‰"""
        return self.run_command(
            ["python3", "etf_factor_engine_production/scripts/produce_full_etf_panel.py"],
            "åˆ†æ± é¢æ¿ç”Ÿäº§"
        )

    def run_aggregate_metrics(self) -> bool:
        """è¿è¡ŒæŒ‡æ ‡æ±‡æ€»"""
        # æ³¨æ„ï¼šaggregate_pool_metrics.py å·²åºŸå¼ƒï¼ŒåŠŸèƒ½å·²é›†æˆåˆ°æ–°æ¶æ„
        logger.info("âš ï¸  æŒ‡æ ‡æ±‡æ€»åŠŸèƒ½å·²é›†æˆåˆ°æ–°æ¶æ„ï¼Œæ­¤æ­¥éª¤å·²è·³è¿‡")
        return True

    def run_ci_checks(self) -> bool:
        """è¿è¡Œ CI æ£€æŸ¥ï¼ˆæ‰€æœ‰æ± ï¼‰"""
        all_passed = True

        for pool in self.pools:
            pool_dir = self.base_dir / f"panel_{pool}"
            if not pool_dir.exists():
                logger.warning(f"âš ï¸  {pool} ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡ CI")
                continue

            passed = self.run_command(
                ["python3", "scripts/ci_checks.py", "--output-dir", str(pool_dir)],
                f"CIæ£€æŸ¥ ({pool})",
            )

            if not passed:
                all_passed = False

        return all_passed

    def create_snapshot(self):
        """åˆ›å»ºå¿«ç…§"""
        if self.snapshot_mgr is None:
            logger.info("â­ï¸  è·³è¿‡å¿«ç…§åˆ›å»ºï¼ˆ--skip-snapshotï¼‰")
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.snapshot_mgr.create_snapshot(
            source_dir=str(self.base_dir), tag=f"production_{timestamp}"
        )

    def _dry_run_validation(self) -> bool:
        """Dry-run æ¨¡å¼ï¼šä»…æ ¡éªŒç°æœ‰æ–‡ä»¶å­˜åœ¨æ€§"""
        logger.info("\nğŸ” Dry-run æ¨¡å¼ï¼šæ ¡éªŒç°æœ‰æ–‡ä»¶...")

        all_valid = True

        # æ£€æŸ¥å„æ± ç›®å½•å’Œå…³é”®æ–‡ä»¶
        for pool in self.pools:
            pool_dir = self.base_dir / f"panel_{pool}"
            logger.info(f"\næ£€æŸ¥æ± : {pool}")

            # æ£€æŸ¥æ± ç›®å½•
            if not pool_dir.exists():
                logger.error(f"  âŒ æ± ç›®å½•ä¸å­˜åœ¨: {pool_dir}")
                all_valid = False
                continue
            else:
                logger.info(f"  âœ… æ± ç›®å½•å­˜åœ¨: {pool_dir}")

            # æ£€æŸ¥å›æµ‹æŒ‡æ ‡æ–‡ä»¶
            metrics_file = pool_dir / "backtest_metrics.json"
            if not metrics_file.exists():
                logger.warning(f"  âš ï¸  å›æµ‹æŒ‡æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {metrics_file}")
            else:
                logger.info(f"  âœ… å›æµ‹æŒ‡æ ‡æ–‡ä»¶å­˜åœ¨: {metrics_file}")

            # æ£€æŸ¥é¢æ¿æ–‡ä»¶ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªï¼‰
            panel_files = list(pool_dir.glob("panel_FULL_*.parquet"))
            if not panel_files:
                logger.warning(f"  âš ï¸  æœªæ‰¾åˆ°é¢æ¿æ–‡ä»¶: {pool_dir}/panel_FULL_*.parquet")
            else:
                logger.info(f"  âœ… æ‰¾åˆ° {len(panel_files)} ä¸ªé¢æ¿æ–‡ä»¶")

        # æ£€æŸ¥æ±‡æ€»æŠ¥å‘Š
        summary_file = self.base_dir / "pool_metrics_summary.csv"
        if not summary_file.exists():
            logger.warning(f"\nâš ï¸  æ±‡æ€»æŠ¥å‘Šä¸å­˜åœ¨: {summary_file}")
        else:
            logger.info(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå­˜åœ¨: {summary_file}")

        logger.info("\n" + "=" * 80)
        if all_valid:
            logger.info("âœ… Dry-run æ ¡éªŒé€šè¿‡")
        else:
            logger.error("âŒ Dry-run æ ¡éªŒå¤±è´¥ï¼ˆéƒ¨åˆ†æ± ç›®å½•ä¸å­˜åœ¨ï¼‰")
        logger.info("=" * 80)

        return all_valid

    def run_full_pipeline(self, dry_run: bool = False) -> bool:
        """è¿è¡Œå®Œæ•´æµæ°´çº¿

        Args:
            dry_run: ä»…æ ¡éªŒç°æœ‰æ–‡ä»¶ï¼Œä¸è§¦å‘é‡æ–°è®¡ç®—
        """
        logger.info("=" * 80)
        logger.info(f"ç”Ÿäº§æµæ°´çº¿å¯åŠ¨ {'(DRY-RUN æ¨¡å¼)' if dry_run else ''}")
        logger.info(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)

        start_time = datetime.now()

        # Dry-run æ¨¡å¼ï¼šä»…æ ¡éªŒæ–‡ä»¶å­˜åœ¨æ€§
        if dry_run:
            return self._dry_run_validation()

        # Step 1: åˆ†æ± ç”Ÿäº§
        if not self.run_pool_management():
            logger.error("âŒ åˆ†æ± ç”Ÿäº§å¤±è´¥ï¼Œç»ˆæ­¢æµæ°´çº¿")
            return False

        # Step 2: æŒ‡æ ‡æ±‡æ€»
        if not self.run_aggregate_metrics():
            logger.warning("âš ï¸  æŒ‡æ ‡æ±‡æ€»å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")

        # Step 3: CI æ£€æŸ¥
        ci_passed = self.run_ci_checks()

        # Step 4: åˆ›å»ºå¿«ç…§
        self.create_snapshot()

        # æ±‡æ€»ç»“æœ
        duration = (datetime.now() - start_time).total_seconds()

        logger.info("\n" + "=" * 80)
        logger.info("æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 80)
        logger.info(f"è€—æ—¶: {duration:.1f}ç§’")
        logger.info(f"å¤±è´¥ä»»åŠ¡æ•°: {len(self.failed_tasks)}")

        if self.failed_tasks:
            logger.error("\nå¤±è´¥ä»»åŠ¡åˆ—è¡¨:")
            for task in self.failed_tasks:
                logger.error(f"  - {task['task']}: {task['error'][:100]}")

        # å‘é€æˆåŠŸé€šçŸ¥
        if not self.failed_tasks and ci_passed:
            summary = f"**è€—æ—¶**: {duration:.1f}ç§’\n**çŠ¶æ€**: å…¨éƒ¨é€šè¿‡"
            self.notifier.notify_success("ç”Ÿäº§æµæ°´çº¿", summary)
            logger.info("\nâœ… æµæ°´çº¿å…¨éƒ¨é€šè¿‡")
            return True
        else:
            logger.error("\nâŒ æµæ°´çº¿å­˜åœ¨å¤±è´¥ä»»åŠ¡æˆ– CI æœªé€šè¿‡")
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    # ä»é…ç½®æ–‡ä»¶è¯»å–é»˜è®¤å€¼
    paths = get_paths()

    parser = argparse.ArgumentParser(description="ç”Ÿäº§æµæ°´çº¿ä¸»è°ƒåº¦")
    parser.add_argument(
        "--base-dir",
        default=str(paths["output_root"]),
        help="åŸºç¡€è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰",
    )
    parser.add_argument("--skip-snapshot", action="store_true", help="è·³è¿‡å¿«ç…§åˆ›å»º")
    parser.add_argument(
        "--dry-run", action="store_true", help="ä»…æ ¡éªŒç°æœ‰æ–‡ä»¶ï¼Œä¸è§¦å‘é‡æ–°è®¡ç®—"
    )
    args = parser.parse_args()

    pipeline = ProductionPipeline(base_dir=args.base_dir)

    if args.skip_snapshot:
        pipeline.snapshot_mgr = None

    success = pipeline.run_full_pipeline(dry_run=args.dry_run)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
