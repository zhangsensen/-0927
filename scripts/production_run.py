#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒå› å­è®¡ç®— - å®Œæ•´ç‰ˆ
1. è¯»å–é…ç½®æ–‡ä»¶
2. åŠ è½½ä»·æ ¼+èµ„é‡‘æµæ•°æ®
3. è®¡ç®—150+æŠ€æœ¯æŒ‡æ ‡ + 11ä¸ªèµ„é‡‘æµå› å­
4. å®Œæ•´æ—¥å¿—å’Œç»“æœå®¡æŸ¥
"""

import logging
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd
import yaml

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from factor_system.factor_engine.core.engine import FactorEngine
from factor_system.factor_engine.core.registry import get_global_registry
from factor_system.factor_engine.providers.parquet_provider import ParquetDataProvider
from factor_system.factor_engine.providers.combined_provider import CombinedMoneyFlowProvider


def setup_logging(log_dir: Path):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"production_{timestamp}.log"

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def load_config(config_path: Path) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def validate_data_paths(config: dict, project_root: Path, logger):
    """éªŒè¯æ•°æ®è·¯å¾„"""
    logger.info("=" * 70)
    logger.info("ğŸ“‚ æ•°æ®è·¯å¾„éªŒè¯")
    logger.info("=" * 70)

    # ä»·æ ¼æ•°æ®
    price_dir = project_root / config["data_paths"]["price_data_dir"]
    logger.info(f"ä»·æ ¼æ•°æ®ç›®å½•: {price_dir}")
    logger.info(f"  å­˜åœ¨: {price_dir.exists()}")
    if price_dir.exists():
        files = list(price_dir.glob("*.parquet"))
        logger.info(f"  æ–‡ä»¶æ•°: {len(files)}")
        logger.info(f"  æ ·æœ¬: {[f.name for f in files[:3]]}")

    # èµ„é‡‘æµæ•°æ®
    money_flow_dir = project_root / config["data_paths"]["money_flow_dir"]
    logger.info(f"èµ„é‡‘æµæ•°æ®ç›®å½•: {money_flow_dir}")
    logger.info(f"  å­˜åœ¨: {money_flow_dir.exists()}")
    if money_flow_dir.exists():
        files = list(money_flow_dir.glob("*.parquet"))
        logger.info(f"  æ–‡ä»¶æ•°: {len(files)}")
        logger.info(f"  æ ·æœ¬: {[f.name for f in files[:3]]}")


def get_available_symbols(price_dir: Path, money_flow_dir: Path, logger) -> list:
    """è·å–æœ‰ä»·æ ¼å’Œèµ„é‡‘æµæ•°æ®çš„è‚¡ç¥¨"""
    price_symbols = {f.stem for f in price_dir.glob("*.parquet")}
    mf_symbols = {
        f.stem.replace("_moneyflow", "").replace("_money_flow", "")
        for f in money_flow_dir.glob("*.parquet")
    }

    # äº¤é›†
    common_symbols = price_symbols & mf_symbols
    logger.info(f"ä»·æ ¼æ•°æ®: {len(price_symbols)} ä¸ªè‚¡ç¥¨")
    logger.info(f"èµ„é‡‘æµæ•°æ®: {len(mf_symbols)} ä¸ªè‚¡ç¥¨")
    logger.info(f"äº¤é›†: {len(common_symbols)} ä¸ªè‚¡ç¥¨")

    return sorted(list(common_symbols))[:5]  # å…ˆå–5ä¸ªæµ‹è¯•


def main():
    # 1. åŠ è½½é…ç½®
    config_path = project_root / "factor_system/config/money_flow_config.yaml"
    config = load_config(config_path)

    # 2. è®¾ç½®æ—¥å¿—
    log_dir = project_root / config["data_paths"]["log_dir"]
    logger = setup_logging(log_dir)

    logger.info("=" * 70)
    logger.info("ğŸš€ ç”Ÿäº§ç¯å¢ƒ - å› å­è®¡ç®—å¯åŠ¨")
    logger.info("=" * 70)
    logger.info(f"é…ç½®æ–‡ä»¶: {config_path}")

    # 3. éªŒè¯æ•°æ®è·¯å¾„
    validate_data_paths(config, project_root, logger)

    # 4. è·å–å¯ç”¨è‚¡ç¥¨
    price_dir = project_root / config["data_paths"]["price_data_dir"]
    money_flow_dir = project_root / config["data_paths"]["money_flow_dir"]
    symbols = get_available_symbols(price_dir, money_flow_dir, logger)

    if not symbols:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°åŒæ—¶æœ‰ä»·æ ¼å’Œèµ„é‡‘æµæ•°æ®çš„è‚¡ç¥¨")
        return None

    logger.info(f"âœ… é€‰å®šè‚¡ç¥¨: {symbols}")

    # 5. åˆå§‹åŒ–æ³¨å†Œè¡¨ï¼ˆåŒ…å«150+æŠ€æœ¯æŒ‡æ ‡ + 11ä¸ªèµ„é‡‘æµå› å­ï¼‰
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“‹ åˆå§‹åŒ–å› å­æ³¨å†Œè¡¨")
    logger.info("=" * 70)
    registry = get_global_registry(include_money_flow=True)
    logger.info(f"âœ… å·²æ³¨å†Œ {len(registry.metadata)} ä¸ªå› å­")

    # åˆ—å‡ºæ‰€æœ‰å› å­
    logger.info("\nå› å­åˆ†ç±»:")
    categories = {}
    for factor_id, meta in registry.metadata.items():
        cat = meta.get("category", "unknown")
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(factor_id)

    for cat, factors in sorted(categories.items()):
        logger.info(f"  {cat}: {len(factors)} ä¸ª")

    # 6. åˆå§‹åŒ–æ•°æ®æä¾›è€…ï¼ˆä½¿ç”¨CombinedMoneyFlowProviderï¼‰
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š åˆå§‹åŒ–æ•°æ®æä¾›è€…")
    logger.info("=" * 70)

    price_provider = ParquetDataProvider(raw_data_dir=project_root / "raw")
    logger.info(f"âœ… ä»·æ ¼æ•°æ®æä¾›è€…: {price_dir}")

    # ä½¿ç”¨ç»„åˆæä¾›è€…ï¼ˆè‡ªåŠ¨åˆå¹¶ä»·æ ¼+èµ„é‡‘æµï¼‰
    combined_provider = CombinedMoneyFlowProvider(
        price_provider=price_provider,
        money_flow_dir=money_flow_dir,
        enforce_t_plus_1=config["time_config"]["enforce_t_plus_1"],
    )
    logger.info(f"âœ… ç»„åˆæ•°æ®æä¾›è€…: ä»·æ ¼ + èµ„é‡‘æµ")
    logger.info(f"   èµ„é‡‘æµç›®å½•: {money_flow_dir}")
    logger.info(f"   T+1æ—¶åºå®‰å…¨: {config['time_config']['enforce_t_plus_1']}")

    # 7. åˆ›å»ºFactorEngine
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ”§ åˆ›å»ºFactorEngine")
    logger.info("=" * 70)

    engine = FactorEngine(
        data_provider=combined_provider,
        registry=registry,
    )
    logger.info("âœ… FactorEngineå·²å°±ç»ª")

    # 8. é€‰æ‹©å› å­é›†ï¼ˆä»YAMLé…ç½®åŠ è½½ï¼‰
    time_config = config["time_config"]
    start_date = datetime.strptime(time_config["start_date"], "%Y-%m-%d")
    end_date = datetime.strptime(time_config["end_date"], "%Y-%m-%d")
    # ç»Ÿä¸€timeframeï¼Œæ”¯æŒåˆ—è¡¨é…ç½®
    def normalize_timeframe(value: str) -> str:
        tf = str(value).strip().lower()
        return "daily" if tf in ("1day", "daily") else tf

    cfg_timeframes = time_config.get("timeframes")
    if cfg_timeframes:
        timeframes = [normalize_timeframe(tf) for tf in cfg_timeframes]
    else:
        timeframes = [normalize_timeframe(time_config.get("timeframe", "daily"))]

    # ä»é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–å› å­é›†åç§°
    import sys
    # é»˜è®¤è·‘å…¨é‡å› å­é›†
    factor_set_name = "all"  # é»˜è®¤é›†åˆï¼šall -> æ³¨å†Œè¡¨ä¸­çš„å…¨éƒ¨å› å­
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•° --set
    if "--set" in sys.argv:
        idx = sys.argv.index("--set")
        if idx + 1 < len(sys.argv):
            factor_set_name = sys.argv[idx + 1]
    
    logger.info(f"ä½¿ç”¨å› å­é›†: {factor_set_name}")
    
    # ä»æ³¨å†Œè¡¨è§£æå› å­é›†
    try:
        factor_ids = registry.get_factor_ids_by_set(factor_set_name)
        logger.info(f"âœ… å› å­é›†è§£ææˆåŠŸ: {len(factor_ids)} ä¸ªå› å­")
        logger.info(f"å‰20ä¸ªå› å­: {factor_ids[:20]}")
        if len(factor_ids) > 20:
            logger.info(f"... è¿˜æœ‰ {len(factor_ids) - 20} ä¸ªå› å­")
    except Exception as e:
        # å…œåº•ï¼šç›´æ¥ä½¿ç”¨æ³¨å†Œè¡¨ä¸­å…¨éƒ¨å¯ç”¨å› å­ï¼ˆfactors + metadataï¼‰
        logger.warning(f"âš ï¸ å› å­é›† '{factor_set_name}' åŠ è½½å¤±è´¥ï¼Œæ”¹ä¸ºå…¨é‡å› å­: {e}")
        all_from_registry = sorted(set(list(registry.factors.keys()) + list(registry.metadata.keys())))
        factor_ids = all_from_registry
        logger.info(f"âœ… å…¨é‡å› å­è§£ææˆåŠŸ: {len(factor_ids)} ä¸ªå› å­")
        logger.info(f"å‰20ä¸ªå› å­: {factor_ids[:20]}")

    logger.info("\n" + "=" * 70)
    logger.info("ğŸ¯ å…¨å±€é…ç½®")
    logger.info("=" * 70)
    logger.info(f"è‚¡ç¥¨: {symbols}")
    logger.info(f"æ—¶é—´: {start_date.date()} ~ {end_date.date()}")
    logger.info(f"æ—¶é—´æ¡†æ¶: {timeframes}")
    logger.info(f"å› å­é›†: {factor_set_name} ({len(factor_ids)} ä¸ª)")

    # 9. å¾ªç¯è®¡ç®—å¤šä¸ªæ—¶é—´æ¡†æ¶
    import time
    qc_cfg = config.get("quality_control", {})
    
    for timeframe in timeframes:
        logger.info("\n" + "=" * 70)
        logger.info(f"âš™ï¸ æ—¶é—´æ¡†æ¶: {timeframe}")
        logger.info("=" * 70)

        try:
            start_time = time.time()

            # éæ—¥çº¿å‘¨æœŸè‡ªåŠ¨è¿‡æ»¤èµ„é‡‘æµç±»å› å­
            current_factor_ids = list(factor_ids)
            if timeframe != "daily":
                categories = {fid: registry.metadata.get(fid, {}).get("category", "") for fid in factor_ids}
                current_factor_ids = [
                    fid for fid in factor_ids
                    if not categories.get(fid, "").startswith(("money_flow", "money_flow_enhanced"))
                    and not fid.lower().startswith(("moneyflow", "money_flow"))
                ]
                logger.info(f"â›³ è¿‡æ»¤èµ„é‡‘æµå› å­: {len(factor_ids)} â†’ {len(current_factor_ids)}")

            logger.info(f"å› å­æ•°: {len(current_factor_ids)}")

            # è®¡ç®—ï¼ˆ1minéœ€è¦æ›´å¤§å†…å­˜é™åˆ¶ï¼‰
            max_ram_mb = 8192 if timeframe == "1min" else 2048
            result = engine.calculate_factors(
                factor_ids=current_factor_ids,
                symbols=symbols,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date,
                use_cache=True,
                max_ram_mb=max_ram_mb,
            )

            elapsed_time = time.time() - start_time

            if result.empty:
                logger.warning(f"âš ï¸ {timeframe} ç»“æœä¸ºç©ºï¼Œè·³è¿‡")
                continue

            logger.info(f"âœ… æ•°æ®å½¢çŠ¶: {result.shape}")
            logger.info(f"â±ï¸ è€—æ—¶: {elapsed_time:.2f}ç§’")

            # æŒ‰è‚¡ç¥¨ç‹¬ç«‹ä¿å­˜
            output_dir = project_root / config["data_paths"]["output_root"] / "production" / timeframe
            output_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # æ‹†åˆ†æ¯ä¸ªè‚¡ç¥¨ç‹¬ç«‹å­˜å‚¨
            for symbol in result.index.get_level_values('symbol').unique():
                symbol_data = result.xs(symbol, level='symbol')
                output_file = output_dir / f"{symbol}_{timeframe}_{timestamp}.parquet"
                symbol_data.to_parquet(output_file, compression="snappy", index=True)
                file_size = output_file.stat().st_size / 1024 / 1024
                logger.info(f"ğŸ’¾ {symbol}: {output_file.name} ({file_size:.2f} MB)")

            # ç”ŸæˆæŠ¥å‘Šï¼ˆå«æ¯æ—¥Kçº¿æ•°æ ¡éªŒï¼‰
            report_file = output_dir / f"report_{timestamp}.md"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(f"# å› å­è®¡ç®—æŠ¥å‘Š - {timeframe}\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n\n")
                f.write(f"## é…ç½®\n\n")
                f.write(f"- è‚¡ç¥¨: {symbols}\n")
                f.write(f"- æ—¶é—´: {start_date.date()} ~ {end_date.date()}\n")
                f.write(f"- å‘¨æœŸ: {timeframe}\n")
                f.write(f"- å› å­æ•°: {len(current_factor_ids)}\n\n")
                f.write(f"## ç»“æœ\n\n")
                f.write(f"- æ•°æ®å½¢çŠ¶: {result.shape}\n")
                
                # Aè‚¡åˆ†é’Ÿçº§æ•°æ®ï¼šæ ¡éªŒæ¯æ—¥Kçº¿æ•°
                if timeframe != "daily" and any(s.endswith(('.SH', '.SZ')) for s in symbols):
                    expected_bars = {
                        '1min': 241, '5min': 48, '15min': 16, '30min': 8, '60min': 4,
                        '120min': 2, '240min': 1
                    }
                    expected = expected_bars.get(timeframe, None)
                    
                    if expected:
                        f.write(f"\n## æ•°æ®è´¨é‡æ ¡éªŒï¼ˆAè‚¡ä¼šè¯æ„ŸçŸ¥ï¼‰\n\n")
                        f.write(f"| è‚¡ç¥¨ | æ€»è¡Œæ•° | æ¯æ—¥Kçº¿æ•° | æœŸæœ›å€¼ | çŠ¶æ€ |\n")
                        f.write(f"|------|--------|----------|--------|------|\n")
                        
                        for symbol in symbols:
                            if not symbol.endswith(('.SH', '.SZ')):
                                continue
                            symbol_data = result.xs(symbol, level='symbol')
                            daily_counts = symbol_data.groupby(symbol_data.index.date).size()
                            avg_bars = daily_counts.mean()
                            status = "âœ…" if abs(avg_bars - expected) < 0.1 else "âŒ"
                            f.write(f"| {symbol} | {len(symbol_data):,} | {avg_bars:.1f} | {expected} | {status} |\n")
                        
                        f.write(f"\n**è¯´æ˜**: Aè‚¡äº¤æ˜“æ—¶é—´ 9:30-11:30, 13:00-15:00ï¼Œä¼šè¯æ„ŸçŸ¥é‡é‡‡æ ·ç¡®ä¿æ— è·¨åˆä¼‘Kçº¿ã€‚\n")
                f.write(f"- è®¡ç®—è€—æ—¶: {elapsed_time:.2f}ç§’\n")
                f.write(f"- æ•°æ®æ–‡ä»¶: {output_file.name}\n\n")
                f.write(f"## æ•°æ®è´¨é‡\n\n")
                f.write(result.describe().to_markdown())
            logger.info(f"ğŸ“„ æŠ¥å‘Š: {report_file.name}")

        except Exception as e:
            logger.error(f"âŒ {timeframe} è®¡ç®—å¤±è´¥: {e}", exc_info=True)
            continue

    logger.info("\n" + "=" * 70)
    logger.info("âœ… æ‰€æœ‰æ—¶é—´æ¡†æ¶è®¡ç®—å®Œæˆ")
    logger.info("=" * 70)
    return True


if __name__ == "__main__":
    result = main()
    sys.exit(0 if result is not None else 1)
