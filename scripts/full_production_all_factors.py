#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢ç”Ÿäº§ç¯å¢ƒ - æ‰€æœ‰ETF Ã— æ‰€æœ‰å› å­
ä½¿ç”¨5å¹´å®Œæ•´å†å²æ•°æ®
"""

import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import time

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from factor_system.factor_engine import api
from factor_system.factor_engine.factors.etf_cross_section import (
    create_etf_cross_section_manager,
    ETFCrossSectionConfig
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('full_production_all_factors.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_all_etf_symbols():
    """åŠ è½½æ‰€æœ‰ETFä»£ç """
    logger.info("="*80)
    logger.info("åŠ è½½ETFä»£ç åˆ—è¡¨")
    logger.info("="*80)
    
    data_dir = project_root / "raw" / "ETF" / "daily"
    etf_files = list(data_dir.glob("*.parquet"))
    
    symbols = []
    for f in etf_files:
        symbol = f.stem.split('_')[0]
        symbols.append(symbol)
    
    symbols = sorted(list(set(symbols)))
    logger.info(f"âœ… æ‰¾åˆ° {len(symbols)} åªETF")
    logger.info(f"ETFåˆ—è¡¨: {', '.join(symbols[:10])}...")
    
    return symbols


def get_data_date_range(symbols):
    """è·å–æ•°æ®çš„æ—¥æœŸèŒƒå›´"""
    logger.info("\n" + "="*80)
    logger.info("åˆ†ææ•°æ®æ—¥æœŸèŒƒå›´")
    logger.info("="*80)
    
    data_dir = project_root / "raw" / "ETF" / "daily"
    
    all_start_dates = []
    all_end_dates = []
    
    for symbol in symbols[:5]:  # é‡‡æ ·5åªETF
        files = list(data_dir.glob(f"{symbol}_*.parquet"))
        if files:
            df = pd.read_parquet(files[0])
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            all_start_dates.append(df['trade_date'].min())
            all_end_dates.append(df['trade_date'].max())
    
    if all_start_dates and all_end_dates:
        # ä½¿ç”¨æ‰€æœ‰ETFéƒ½æœ‰æ•°æ®çš„æ—¥æœŸèŒƒå›´
        common_start = max(all_start_dates)
        common_end = min(all_end_dates)
        
        logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´:")
        logger.info(f"  æœ€æ—©æ—¥æœŸ: {common_start.date()}")
        logger.info(f"  æœ€æ–°æ—¥æœŸ: {common_end.date()}")
        logger.info(f"  æ€»å¤©æ•°: {(common_end - common_start).days} å¤©")
        logger.info(f"  çº¦ {(common_end - common_start).days / 365:.1f} å¹´")
        
        return common_start, common_end
    
    # é»˜è®¤å€¼
    return datetime(2020, 1, 1), datetime(2025, 10, 14)


def get_all_available_factors(manager):
    """è·å–æ‰€æœ‰å¯ç”¨å› å­"""
    logger.info("\n" + "="*80)
    logger.info("è·å–æ‰€æœ‰å¯ç”¨å› å­")
    logger.info("="*80)

    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆå¼ºåˆ¶æ³¨å†Œæ‰€æœ‰åŠ¨æ€å› å­
    logger.info("ğŸ”§ æ³¨å†ŒåŠ¨æ€å› å­...")
    manager._register_all_dynamic_factors()

    available_factors = manager.get_available_factors()

    # åˆ†ç±»ç»Ÿè®¡
    legacy_factors = [f for f in available_factors if not f.startswith('VBT_') and not f.startswith('TALIB_') and not f.startswith('TA_')]
    vbt_factors = [f for f in available_factors if f.startswith('VBT_')]
    talib_factors = [f for f in available_factors if f.startswith('TALIB_') or f.startswith('TA_')]

    logger.info(f"å› å­æ€»æ•°: {len(available_factors)}")
    logger.info(f"  - ä¼ ç»Ÿå› å­: {len(legacy_factors)}")
    logger.info(f"  - VBTå› å­: {len(vbt_factors)}")
    logger.info(f"  - TA-Libå› å­: {len(talib_factors)}")

    return available_factors, legacy_factors, vbt_factors, talib_factors


def calculate_factors_in_batches(symbols, factor_ids, start_date, end_date, batch_size=20):
    """åˆ†æ‰¹è®¡ç®—å› å­"""
    logger.info("\n" + "="*80)
    logger.info(f"å¼€å§‹æ‰¹é‡è®¡ç®—å› å­")
    logger.info("="*80)
    logger.info(f"ETFæ•°é‡: {len(symbols)}")
    logger.info(f"å› å­æ•°é‡: {len(factor_ids)}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info(f"æ—¶é—´èŒƒå›´: {start_date.date()} ~ {end_date.date()}")
    
    total_batches = (len(factor_ids) + batch_size - 1) // batch_size
    all_results = []
    successful_factors = []
    failed_factors = []
    
    start_time = time.time()
    
    for batch_idx in range(total_batches):
        batch_start_idx = batch_idx * batch_size
        batch_end_idx = min(batch_start_idx + batch_size, len(factor_ids))
        batch_factors = factor_ids[batch_start_idx:batch_end_idx]
        
        logger.info(f"\n{'='*60}")
        logger.info(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: è®¡ç®— {len(batch_factors)} ä¸ªå› å­")
        logger.info(f"{'='*60}")
        
        batch_results = []
        
        for i, factor_id in enumerate(batch_factors, 1):
            try:
                factor_start = time.time()
                
                result = api.calculate_factors(
                    factor_ids=[factor_id],
                    symbols=symbols,
                    timeframe='daily',
                    start_date=start_date,
                    end_date=end_date
                )
                
                factor_time = time.time() - factor_start
                
                if result is not None and not result.empty:
                    batch_results.append(result)
                    successful_factors.append(factor_id)
                    logger.info(f"  [{batch_idx*batch_size + i}/{len(factor_ids)}] âœ… {factor_id}: "
                               f"{result.shape} ({factor_time:.2f}s)")
                else:
                    failed_factors.append((factor_id, "ç©ºç»“æœ"))
                    logger.warning(f"  [{batch_idx*batch_size + i}/{len(factor_ids)}] âš ï¸ {factor_id}: ç©ºç»“æœ")
                    
            except Exception as e:
                failed_factors.append((factor_id, str(e)))
                logger.error(f"  [{batch_idx*batch_size + i}/{len(factor_ids)}] âŒ {factor_id}: {str(e)[:100]}")
                continue
        
        # åˆå¹¶æ‰¹æ¬¡ç»“æœ
        if batch_results:
            batch_combined = pd.concat(batch_results, axis=1)
            all_results.append(batch_combined)
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ: {len(batch_results)}/{len(batch_factors)} ä¸ªå› å­æˆåŠŸ")
        else:
            logger.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} æ— æœ‰æ•ˆç»“æœ")
    
    total_time = time.time() - start_time
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    if all_results:
        combined_df = pd.concat(all_results, axis=1)
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… å› å­è®¡ç®—å®Œæˆ")
        logger.info(f"{'='*80}")
        logger.info(f"æˆåŠŸ: {len(successful_factors)}/{len(factor_ids)} ({len(successful_factors)/len(factor_ids)*100:.1f}%)")
        logger.info(f"å¤±è´¥: {len(failed_factors)}/{len(factor_ids)} ({len(failed_factors)/len(factor_ids)*100:.1f}%)")
        logger.info(f"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"å¹³å‡æ¯å› å­: {total_time/len(factor_ids):.2f}ç§’")
        logger.info(f"ç»“æœç»´åº¦: {combined_df.shape}")
        
        return combined_df, successful_factors, failed_factors
    
    logger.error("âŒ æ‰€æœ‰æ‰¹æ¬¡å‡å¤±è´¥")
    return None, successful_factors, failed_factors


def build_cross_sections_for_dates(combined_df, dates):
    """ä¸ºå¤šä¸ªæ—¥æœŸæ„å»ºæ¨ªæˆªé¢"""
    logger.info("\n" + "="*80)
    logger.info(f"æ„å»ºå¤šæ—¥æœŸæ¨ªæˆªé¢")
    logger.info("="*80)
    logger.info(f"æ—¥æœŸæ•°é‡: {len(dates)}")
    
    cross_sections = {}
    
    for date in dates:
        try:
            if isinstance(combined_df.index, pd.MultiIndex):
                try:
                    cross_section = combined_df.xs(date, level=0)
                except KeyError:
                    # æŸ¥æ‰¾æœ€è¿‘æ—¥æœŸ
                    available_dates = combined_df.index.get_level_values(0).unique()
                    closest_date = min(available_dates, key=lambda d: abs((d - date).total_seconds()))
                    cross_section = combined_df.xs(closest_date, level=0)
                    logger.info(f"  {date.date()}: ä½¿ç”¨æœ€è¿‘æ—¥æœŸ {closest_date.date()}")
            else:
                cross_section = combined_df
            
            cross_sections[date] = cross_section
            logger.info(f"  âœ… {date.date()}: {cross_section.shape}")
            
        except Exception as e:
            logger.error(f"  âŒ {date.date()}: {str(e)}")
            continue
    
    return cross_sections


def analyze_factor_effectiveness(cross_section, date):
    """åˆ†æå› å­æœ‰æ•ˆæ€§"""
    total_etfs = len(cross_section)
    total_factors = len(cross_section.columns)
    
    factor_stats = []
    for factor_id in cross_section.columns:
        values = cross_section[factor_id]
        valid_count = values.notna().sum()
        valid_rate = valid_count / len(values) * 100
        
        factor_stats.append({
            'factor_id': factor_id,
            'valid_count': valid_count,
            'valid_rate': valid_rate,
            'mean': values.mean() if valid_count > 0 else np.nan,
            'std': values.std() if valid_count > 0 else np.nan,
            'min': values.min() if valid_count > 0 else np.nan,
            'max': values.max() if valid_count > 0 else np.nan
        })
    
    stats_df = pd.DataFrame(factor_stats)
    
    # åˆ†ç±»
    effective = stats_df[stats_df['valid_rate'] >= 80]
    partial = stats_df[(stats_df['valid_rate'] >= 50) & (stats_df['valid_rate'] < 80)]
    weak = stats_df[(stats_df['valid_rate'] > 0) & (stats_df['valid_rate'] < 50)]
    invalid = stats_df[stats_df['valid_rate'] == 0]
    
    logger.info(f"\n{date.date()} å› å­æœ‰æ•ˆæ€§:")
    logger.info(f"  âœ… ä¼˜ç§€ (â‰¥80%): {len(effective)} ä¸ª ({len(effective)/total_factors*100:.1f}%)")
    logger.info(f"  ğŸŸ¡ è‰¯å¥½ (50-80%): {len(partial)} ä¸ª ({len(partial)/total_factors*100:.1f}%)")
    logger.info(f"  âš ï¸ è¾ƒå¼± (<50%): {len(weak)} ä¸ª ({len(weak)/total_factors*100:.1f}%)")
    logger.info(f"  âŒ æ— æ•ˆ (0%): {len(invalid)} ä¸ª ({len(invalid)/total_factors*100:.1f}%)")
    
    return stats_df


def save_results(cross_sections, stats_dfs, successful_factors, failed_factors):
    """ä¿å­˜æ‰€æœ‰ç»“æœ"""
    logger.info("\n" + "="*80)
    logger.info("ä¿å­˜ç»“æœ")
    logger.info("="*80)
    
    output_dir = project_root / "output" / "full_production"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ä¿å­˜æ¨ªæˆªé¢æ•°æ®
    for date, cross_section in cross_sections.items():
        filename = f"cross_section_{date.strftime('%Y%m%d')}.parquet"
        filepath = output_dir / filename
        cross_section.to_parquet(filepath)
        logger.info(f"  âœ… {filename}: {cross_section.shape}, {filepath.stat().st_size/1024:.1f} KB")
    
    # 2. ä¿å­˜å› å­ç»Ÿè®¡
    for date, stats_df in stats_dfs.items():
        filename = f"factor_stats_{date.strftime('%Y%m%d')}.csv"
        filepath = output_dir / filename
        stats_df.to_csv(filepath, index=False)
        logger.info(f"  âœ… {filename}: {len(stats_df)} ä¸ªå› å­")
    
    # 3. ä¿å­˜å› å­åˆ—è¡¨
    factor_list_file = output_dir / "factor_list.txt"
    with open(factor_list_file, 'w') as f:
        f.write("# æˆåŠŸè®¡ç®—çš„å› å­\n")
        for factor in successful_factors:
            f.write(f"{factor}\n")
        f.write(f"\n# å¤±è´¥çš„å› å­ ({len(failed_factors)}ä¸ª)\n")
        for factor, reason in failed_factors:
            f.write(f"{factor}: {reason}\n")
    
    logger.info(f"  âœ… factor_list.txt: {len(successful_factors)} æˆåŠŸ, {len(failed_factors)} å¤±è´¥")
    
    # 4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_file = output_dir / "summary_report.txt"
    with open(summary_file, 'w') as f:
        f.write("="*80 + "\n")
        f.write("å…¨é¢ç”Ÿäº§ç¯å¢ƒ - æ±‡æ€»æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"æ¨ªæˆªé¢æ•°é‡: {len(cross_sections)}\n")
        f.write(f"æˆåŠŸå› å­æ•°: {len(successful_factors)}\n")
        f.write(f"å¤±è´¥å› å­æ•°: {len(failed_factors)}\n")
        f.write(f"å› å­æˆåŠŸç‡: {len(successful_factors)/(len(successful_factors)+len(failed_factors))*100:.1f}%\n\n")
        
        for date, cross_section in cross_sections.items():
            f.write(f"\n{date.date()}:\n")
            f.write(f"  ETFæ•°é‡: {len(cross_section)}\n")
            f.write(f"  å› å­æ•°é‡: {len(cross_section.columns)}\n")
    
    logger.info(f"  âœ… summary_report.txt")
    logger.info(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80)
    logger.info("å…¨é¢ç”Ÿäº§ç¯å¢ƒ - æ‰€æœ‰ETF Ã— æ‰€æœ‰å› å­")
    logger.info("="*80)
    logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. åŠ è½½ETFåˆ—è¡¨
        symbols = load_all_etf_symbols()
        
        # 2. è·å–æ•°æ®æ—¥æœŸèŒƒå›´
        start_date, end_date = get_data_date_range(symbols)
        
        # ä½¿ç”¨æœ€è¿‘1å¹´çš„æ•°æ®ï¼ˆæ›´å¿«ï¼Œä¸”è¶³å¤Ÿæ”¯æŒå¤§éƒ¨åˆ†å› å­ï¼‰
        start_date = end_date - timedelta(days=365)
        logger.info(f"\nä½¿ç”¨æ•°æ®èŒƒå›´: {start_date.date()} ~ {end_date.date()} (çº¦1å¹´)")
        
        # 3. åˆå§‹åŒ–ç®¡ç†å™¨å¹¶è·å–æ‰€æœ‰å› å­
        logger.info("\nåˆå§‹åŒ–å› å­ç®¡ç†å™¨...")
        config = ETFCrossSectionConfig()
        config.enable_legacy_factors = True
        config.enable_dynamic_factors = True
        config.max_dynamic_factors = 500  # ä¸é™åˆ¶ï¼Œä½¿ç”¨æ‰€æœ‰å› å­
        
        manager = create_etf_cross_section_manager(config)
        available_factors, legacy_factors, vbt_factors, talib_factors = get_all_available_factors(manager)
        
        # 4. æ‰¹é‡è®¡ç®—æ‰€æœ‰å› å­
        combined_df, successful_factors, failed_factors = calculate_factors_in_batches(
            symbols=symbols,
            factor_ids=available_factors,
            start_date=start_date,
            end_date=end_date,
            batch_size=30  # æ¯æ‰¹30ä¸ªå› å­
        )
        
        if combined_df is None:
            logger.error("âŒ å› å­è®¡ç®—å¤±è´¥")
            return
        
        # 5. æ„å»ºå¤šä¸ªæ—¥æœŸçš„æ¨ªæˆªé¢ï¼ˆæœ€è¿‘5ä¸ªäº¤æ˜“æ—¥ï¼‰
        target_dates = [
            end_date,
            end_date - timedelta(days=1),
            end_date - timedelta(days=2),
            end_date - timedelta(days=7),
            end_date - timedelta(days=30)
        ]
        
        cross_sections = build_cross_sections_for_dates(combined_df, target_dates)
        
        # 6. åˆ†ææ¯ä¸ªæ¨ªæˆªé¢çš„å› å­æœ‰æ•ˆæ€§
        stats_dfs = {}
        for date, cross_section in cross_sections.items():
            stats_df = analyze_factor_effectiveness(cross_section, date)
            stats_dfs[date] = stats_df
        
        # 7. ä¿å­˜æ‰€æœ‰ç»“æœ
        save_results(cross_sections, stats_dfs, successful_factors, failed_factors)
        
        # 8. æœ€ç»ˆæ€»ç»“
        logger.info("\n" + "="*80)
        logger.info("âœ… å…¨é¢ç”Ÿäº§å®Œæˆï¼")
        logger.info("="*80)
        logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ETFæ•°é‡: {len(symbols)}")
        logger.info(f"å› å­æ€»æ•°: {len(available_factors)}")
        logger.info(f"æˆåŠŸå› å­: {len(successful_factors)} ({len(successful_factors)/len(available_factors)*100:.1f}%)")
        logger.info(f"æ¨ªæˆªé¢æ•°: {len(cross_sections)}")
        
    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()
