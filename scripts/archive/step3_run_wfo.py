#!/usr/bin/env python3
"""
Step 3: WFOä¼˜åŒ–æ‰§è¡Œ - ç‹¬ç«‹æ‰§è¡Œè„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å– Step 2 çš„æ ‡å‡†åŒ–å› å­æ•°æ®
2. æ‰§è¡ŒWalk-Forwardä¼˜åŒ–ï¼ˆ55ä¸ªçª—å£ï¼‰
3. ä¿å­˜WFOç»“æœåˆ° wfo/
4. è¯¦ç»†çš„çª—å£è¿›åº¦å’ŒICç»Ÿè®¡æ—¥å¿—

è¾“å…¥ï¼š
- factor_selection/{date}/{timestamp}/standardized/

è¾“å‡ºï¼š
- wfo/{timestamp}/wfo_results.pkl
- wfo/{timestamp}/wfo_report.txt
- wfo/{timestamp}/metadata.json
"""

import json
import logging
import pickle
import sys
from datetime import datetime
from pathlib import Path

import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent

from etf_strategy.core.constrained_walk_forward_optimizer import ConstrainedWalkForwardOptimizer


def setup_logging(output_dir: Path):
    """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
    log_file = output_dir / "step3_wfo.log"

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)s - %(message)s",
        datefmt="%H:%M:%S",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def find_latest_selection(results_dir: Path):
    """æŸ¥æ‰¾æœ€æ–°çš„å› å­ç­›é€‰æ•°æ®ç›®å½•"""
    selection_root = results_dir / "factor_selection"

    if not selection_root.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•
    all_runs = []
    for date_dir in selection_root.iterdir():
        if not date_dir.is_dir():
            continue
        for timestamp_dir in date_dir.iterdir():
            if not timestamp_dir.is_dir():
                continue
            # éªŒè¯æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
            if (timestamp_dir / "metadata.json").exists():
                all_runs.append(timestamp_dir)

    if not all_runs:
        return None

    # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°
    all_runs.sort(key=lambda p: p.name, reverse=True)
    return all_runs[0]


def load_standardized_factors(selection_dir: Path, logger):
    """åŠ è½½æ ‡å‡†åŒ–å› å­æ•°æ®"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 1/3: åŠ è½½æ ‡å‡†åŒ–å› å­")
    logger.info("-" * 80)
    logger.info(f"è¾“å…¥ç›®å½•: {selection_dir}")
    logger.info("")

    # åŠ è½½å…ƒæ•°æ®
    metadata_path = selection_dir / "metadata.json"
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    logger.info("ğŸ“‹ å› å­ç­›é€‰å…ƒæ•°æ®:")
    logger.info(f"  æ—¶é—´æˆ³: {metadata['timestamp']}")
    logger.info(f"  ETFæ•°é‡: {metadata['etf_count']}")
    logger.info(
        f"  æ—¥æœŸèŒƒå›´: {metadata['date_range'][0]} -> {metadata['date_range'][1]}"
    )
    logger.info(f"  æ ‡å‡†åŒ–å› å­æ•°: {metadata['standardized_factor_count']}")
    logger.info("")

    # åŠ è½½æ ‡å‡†åŒ–å› å­
    standardized_dir = selection_dir / "standardized"
    factors_dict = {}

    for factor_name in metadata["standardized_factor_names"]:
        parquet_path = standardized_dir / f"{factor_name}.parquet"
        factor_df = pd.read_parquet(parquet_path)
        # å› å­æ–‡ä»¶æ˜¯DataFrameï¼ˆæ—¥æœŸÃ—æ ‡çš„ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        factors_dict[factor_name] = factor_df

        nan_ratio = factor_df.isna().sum().sum() / factor_df.size
        logger.info(f"  âœ… {factor_name:25s} NaNç‡: {nan_ratio*100:.2f}%")

    logger.info("")

    return factors_dict, metadata


def run_wfo_optimization(factors_dict, metadata, ohlcv_data, output_dir, logger):
    """æ‰§è¡ŒWFOä¼˜åŒ–"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 2/3: WFOä¼˜åŒ–ï¼ˆWalk-Forward Optimizationï¼‰")
    logger.info("-" * 80)

    # WFOå‚æ•°
    in_sample_days = 252
    out_of_sample_days = 60
    step_days = 20
    target_factor_count = 5
    ic_threshold = 0.05

    logger.info("WFOå‚æ•°é…ç½®:")
    logger.info(f"  æ ·æœ¬å†…çª—å£: {in_sample_days} å¤©")
    logger.info(f"  æ ·æœ¬å¤–çª—å£: {out_of_sample_days} å¤©")
    logger.info(f"  æ»‘åŠ¨æ­¥é•¿: {step_days} å¤©")
    logger.info(f"  ç›®æ ‡å› å­æ•°: {target_factor_count}")
    logger.info(f"  ICé˜ˆå€¼: {ic_threshold}")
    logger.info("")

    # å‡†å¤‡æ•°æ®ï¼šè½¬æ¢ä¸º3D numpyæ•°ç»„
    factor_names = list(factors_dict.keys())
    close_df = ohlcv_data["close"]
    returns_df = close_df.pct_change()

    # ğŸ”§ ä¿®å¤ï¼špct_change()ç¬¬ä¸€è¡Œæ˜¯NaNï¼Œéœ€è¦å¯¹é½å› å­å’Œæ”¶ç›Šç‡çš„æ—¶é—´ç´¢å¼•
    # è·³è¿‡ç¬¬ä¸€è¡Œï¼Œç¡®ä¿å› å­å’Œæ”¶ç›Šç‡æ—¶é—´å¯¹é½
    returns_df = returns_df.iloc[1:]
    aligned_factors_dict = {k: v.iloc[1:] for k, v in factors_dict.items()}

    n_dates = len(returns_df)  # ä½¿ç”¨å¯¹é½åçš„é•¿åº¦
    n_symbols = len(close_df.columns)
    n_factors = len(factor_names)

    import numpy as np

    factors_3d = np.full((n_dates, n_symbols, n_factors), np.nan)
    for idx, fname in enumerate(factor_names):
        factors_3d[:, :, idx] = aligned_factors_dict[fname].values

    logger.info("æ•°æ®å½¢çŠ¶:")
    logger.info(f"  æ—¶é—´æ­¥: {n_dates}")
    logger.info(f"  æ ‡çš„æ•°: {n_symbols}")
    logger.info(f"  å› å­æ•°: {n_factors}")
    logger.info("")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = ConstrainedWalkForwardOptimizer()

    logger.info("ğŸ”„ å¼€å§‹WFOä¼˜åŒ–...")
    logger.info("")

    import time

    start_time = time.time()

    # æ‰§è¡Œä¼˜åŒ–
    wfo_df, constraint_reports = optimizer.run_constrained_wfo(
        factors_data=factors_3d,
        returns=returns_df.values,
        factor_names=factor_names,
        is_period=in_sample_days,
        oos_period=out_of_sample_days,
        step_size=step_days,
        target_factor_count=target_factor_count,
    )

    elapsed = time.time() - start_time
    logger.info("")
    logger.info(f"âœ… WFOä¼˜åŒ–å®Œæˆï¼ˆè€—æ—¶ {elapsed:.1f}ç§’ï¼‰")
    logger.info("")

    # è½¬æ¢ç»“æœæ ¼å¼
    results = {
        "results_df": wfo_df,
        "constraint_reports": constraint_reports,
        "total_windows": len(wfo_df),
        "valid_windows": len(wfo_df),
    }

    if len(wfo_df) > 0:
        results["avg_oos_ic"] = wfo_df["oos_ic_mean"].mean()
        results["std_oos_ic"] = wfo_df["oos_ic_mean"].std()
        results["avg_ic_decay"] = wfo_df["ic_drop"].mean()
        results["std_ic_decay"] = wfo_df["ic_drop"].std()

        # è®¡ç®—å› å­é€‰æ‹©é¢‘ç‡
        selected_lists = [
            [f.strip() for f in factors_str.split(",") if f.strip()]
            for factors_str in wfo_df["selected_factors"]
        ]
        selected_flat = [f for factors in selected_lists for f in factors]
        if selected_flat:
            factor_counts = (
                pd.Series(selected_flat).value_counts().sort_values(ascending=False)
            )
            selection_freq = [
                (fname, count / len(wfo_df)) for fname, count in factor_counts.items()
            ]
            results["factor_selection_freq"] = selection_freq
            results["top_oos_factors"] = [
                (fname, results["avg_oos_ic"]) for fname, _ in selection_freq[:5]
            ]
        else:
            results["factor_selection_freq"] = []
            results["top_oos_factors"] = []

        # çª—å£è¯¦ç»†ç»“æœ
        results["window_results"] = []
        for idx, row in wfo_df.iterrows():
            results["window_results"].append(
                {
                    "window_id": idx + 1,
                    "is_start": row.get("is_start", ""),
                    "is_end": row.get("is_end", ""),
                    "oos_start": row.get("oos_start", ""),
                    "oos_end": row.get("oos_end", ""),
                    "selected_factors": [
                        f.strip() for f in row["selected_factors"].split(",")
                    ],
                    "oos_ic": row["oos_ic_mean"],
                    "ic_decay": row["ic_drop"],
                }
            )
    else:
        results["avg_oos_ic"] = 0.0
        results["std_oos_ic"] = 0.0
        results["avg_ic_decay"] = 0.0
        results["std_ic_decay"] = 0.0
        results["factor_selection_freq"] = []
        results["top_oos_factors"] = []
        results["window_results"] = []

    # ç»Ÿè®¡æ±‡æ€»
    logger.info("ğŸ“Š WFOç»“æœç»Ÿè®¡:")
    logger.info(f"  çª—å£æ€»æ•°: {results['total_windows']}")
    logger.info(f"  æœ‰æ•ˆçª—å£: {results['valid_windows']}")
    logger.info("")

    logger.info("ICç»Ÿè®¡ï¼ˆæ ·æœ¬å¤–ï¼‰:")
    logger.info(f"  å¹³å‡OOS IC: {results['avg_oos_ic']:.4f}")
    logger.info(f"  OOS IC æ ‡å‡†å·®: {results['std_oos_ic']:.4f}")
    logger.info(f"  å¹³å‡ICè¡°å‡: {results['avg_ic_decay']:.4f}")
    logger.info(f"  ICè¡°å‡æ ‡å‡†å·®: {results['std_ic_decay']:.4f}")
    logger.info("")

    logger.info("TOP 5 æ ·æœ¬å¤–ICå› å­:")
    for idx, (fname, ic_val) in enumerate(results["top_oos_factors"][:5], start=1):
        logger.info(f"  {idx}. {fname:25s} IC={ic_val:.4f}")
    logger.info("")

    logger.info("å› å­é€‰æ‹©é¢‘ç‡ï¼ˆTOP 10ï¼‰:")
    for idx, (fname, freq) in enumerate(results["factor_selection_freq"][:10], start=1):
        logger.info(f"  {idx:02d}. {fname:25s} {freq*100:6.2f}%")
    logger.info("")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = output_dir / "wfo_report.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("WFOä¼˜åŒ–è¯¦ç»†æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"çª—å£æ€»æ•°: {results['total_windows']}\n")
        f.write(f"æœ‰æ•ˆçª—å£: {results['valid_windows']}\n\n")

        f.write("ICç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡OOS IC: {results['avg_oos_ic']:.4f}\n")
        f.write(f"  OOS IC æ ‡å‡†å·®: {results['std_oos_ic']:.4f}\n")
        f.write(f"  å¹³å‡ICè¡°å‡: {results['avg_ic_decay']:.4f}\n")
        f.write(f"  ICè¡°å‡æ ‡å‡†å·®: {results['std_ic_decay']:.4f}\n\n")

        f.write("å› å­é€‰æ‹©é¢‘ç‡:\n")
        for fname, freq in results["factor_selection_freq"]:
            f.write(f"  {fname:25s} {freq*100:6.2f}%\n")

        f.write("\n" + "=" * 80 + "\n")
        f.write("å„çª—å£è¯¦ç»†ç»“æœ\n")
        f.write("=" * 80 + "\n\n")

        for window_result in results["window_results"]:
            f.write(f"çª—å£ {window_result['window_id']}:\n")
            f.write(f"  IS: {window_result['is_start']} -> {window_result['is_end']}\n")
            f.write(
                f"  OOS: {window_result['oos_start']} -> {window_result['oos_end']}\n"
            )
            f.write(f"  é€‰ä¸­å› å­: {', '.join(window_result['selected_factors'])}\n")
            f.write(f"  OOS IC: {window_result['oos_ic']:.4f}\n")
            f.write(f"  ICè¡°å‡: {window_result['ic_decay']:.4f}\n\n")

    logger.info(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    logger.info("")

    # ä¿å­˜ç»“æœå¯¹è±¡
    results_path = output_dir / "wfo_results.pkl"
    with open(results_path, "wb") as f:
        pickle.dump(results, f)

    logger.info(f"ğŸ’¾ WFOç»“æœå¯¹è±¡å·²ä¿å­˜: {results_path}")
    logger.info("")

    return results


def save_wfo_metadata(results, metadata, output_dir, logger):
    """ä¿å­˜WFOå…ƒæ•°æ®"""
    logger.info("-" * 80)
    logger.info("é˜¶æ®µ 3/3: ä¿å­˜WFOå…ƒæ•°æ®")
    logger.info("-" * 80)

    wfo_metadata = {
        **metadata,
        "step": "wfo",
        "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "total_windows": results["total_windows"],
        "valid_windows": results["valid_windows"],
        "avg_oos_ic": results["avg_oos_ic"],
        "std_oos_ic": results["std_oos_ic"],
        "avg_ic_decay": results["avg_ic_decay"],
        "std_ic_decay": results["std_ic_decay"],
        "top_oos_factors": results["top_oos_factors"][:5],
        "factor_selection_freq": results["factor_selection_freq"],
        "output_dir": str(output_dir),
    }

    metadata_path = output_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(wfo_metadata, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    logger.info("")

    return wfo_metadata


def main(selection_dir: Path = None):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # è¾“å‡ºç›®å½•
    output_root = PROJECT_ROOT / "results"
    wfo_dir = output_root / "wfo" / timestamp
    wfo_dir.mkdir(parents=True, exist_ok=True)

    logger = setup_logging(wfo_dir)

    logger.info("=" * 80)
    logger.info("Step 3: WFOä¼˜åŒ–æ‰§è¡Œï¼ˆWalk-Forward Optimizationï¼‰")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {wfo_dir}")
    logger.info(f"æ—¶é—´æˆ³: {timestamp}")
    logger.info("")

    # æŸ¥æ‰¾è¾“å…¥æ•°æ®
    if selection_dir is None:
        logger.info("ğŸ” è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„å› å­ç­›é€‰æ•°æ®...")
        selection_dir = find_latest_selection(output_root)

        if selection_dir is None:
            logger.error("âŒ æœªæ‰¾åˆ°å› å­ç­›é€‰æ•°æ®ï¼è¯·å…ˆè¿è¡Œ step2_factor_selection.py")
            sys.exit(1)

        logger.info(f"âœ… æ‰¾åˆ°æœ€æ–°æ•°æ®: {selection_dir}")
        logger.info("")

    # 1. åŠ è½½æ•°æ®
    factors_dict, metadata = load_standardized_factors(selection_dir, logger)

    # åŠ è½½OHLCVæ•°æ®ï¼ˆéœ€è¦ç”¨äºè®¡ç®—returnsï¼‰
    # ä½¿ç”¨find_latest_cross_sectionå‡½æ•°æŸ¥æ‰¾æ¨ªæˆªé¢æ•°æ®
    cross_section_root = output_root / "cross_section"
    cross_section_dir = None

    if cross_section_root.exists():
        # æŸ¥æ‰¾æ‰€æœ‰æ—¶é—´æˆ³ç›®å½•
        all_runs = []
        for date_dir in cross_section_root.iterdir():
            if not date_dir.is_dir():
                continue
            for timestamp_dir in date_dir.iterdir():
                if not timestamp_dir.is_dir():
                    continue
                # éªŒè¯æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
                if (timestamp_dir / "metadata.json").exists():
                    all_runs.append(timestamp_dir)

        if all_runs:
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°
            all_runs.sort(key=lambda p: p.name, reverse=True)
            cross_section_dir = all_runs[0]

    if cross_section_dir is None:
        logger.error("âŒ æ— æ³•æ‰¾åˆ°æ¨ªæˆªé¢æ•°æ®ï¼è¯·å…ˆè¿è¡Œ step1_cross_section.py")
        sys.exit(1)

    ohlcv_dir = cross_section_dir / "ohlcv"
    ohlcv_data = {}
    for col_name in ["open", "high", "low", "close", "volume"]:
        parquet_path = ohlcv_dir / f"{col_name}.parquet"
        ohlcv_data[col_name] = pd.read_parquet(parquet_path)

    logger.info(f"âœ… åŠ è½½OHLCVæ•°æ®: {ohlcv_data['close'].shape}")
    logger.info("")

    # 2. è¿è¡ŒWFO
    results = run_wfo_optimization(factors_dict, metadata, ohlcv_data, wfo_dir, logger)

    # 3. ä¿å­˜å…ƒæ•°æ®
    wfo_metadata = save_wfo_metadata(results, metadata, wfo_dir, logger)

    # å®Œæˆ
    logger.info("=" * 80)
    logger.info("âœ… Step 3 å®Œæˆï¼WFOä¼˜åŒ–å·²æ‰§è¡Œ")
    logger.info("=" * 80)
    logger.info(f"è¾“å‡ºç›®å½•: {wfo_dir}")
    logger.info(f"  - wfo_results.pkl: WFOç»“æœå¯¹è±¡")
    logger.info(f"  - wfo_report.txt: è¯¦ç»†æŠ¥å‘Š")
    logger.info(f"  - metadata.json: å…ƒæ•°æ®")
    logger.info(f"  - step3_wfo.log: æ‰§è¡Œæ—¥å¿—")
    logger.info("")
    logger.info("ğŸ‰ å®Œæ•´çš„3æ­¥æµç¨‹æ‰§è¡Œå®Œæˆï¼")
    logger.info("")
    logger.info("ğŸ“Š å…³é”®ç»“æœ:")
    logger.info(f"  - çª—å£æ•°: {results['total_windows']}")
    logger.info(f"  - å¹³å‡OOS IC: {results['avg_oos_ic']:.4f}")
    logger.info(f"  - ICè¡°å‡: {results['avg_ic_decay']:.4f}")
    logger.info(
        f"  - TOPå› å­: {', '.join([f[0] for f in results['top_oos_factors'][:3]])}"
    )
    logger.info("")

    return wfo_dir


if __name__ == "__main__":
    main()
