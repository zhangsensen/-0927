#!/usr/bin/env python3
"""
åŸºäº QMT Bridge SDK çš„ ETF æ—¥çº¿æ•°æ®å¢é‡æ›´æ–°è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python update_daily_from_qmt_bridge.py --symbols 510300,510500
    python update_daily_from_qmt_bridge.py --config etf_list.json
    python update_daily_from_qmt_bridge.py --all  # æ›´æ–°æ‰€æœ‰é…ç½®çš„ETF
"""

import argparse
import asyncio
import json
import yaml
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import pandas as pd

try:
    from qmt_bridge import QMTClient, QMTClientConfig
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£… qmt-data-bridge:")
    print("   pip install qmt-data-bridge")
    exit(1)


class ETFDataUpdater:
    """ETFæ•°æ®å¢é‡æ›´æ–°å™¨"""
    
    def __init__(
        self,
        host: str = "192.168.122.132",
        port: int = 8001,
        data_dir: str = "./raw/ETF/daily"
    ):
        """
        Args:
            host: QMT Bridge æœåŠ¡å™¨åœ°å€
            port: QMT Bridge æœåŠ¡å™¨ç«¯å£
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.config = QMTClientConfig(host=host, port=port)
        self.client = QMTClient(self.config)
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    def _parse_timestamp(self, timestamp_ms: int) -> int:
        """
        å°†Unixæ¯«ç§’æ—¶é—´æˆ³è½¬æ¢ä¸ºYYYYMMDDæ•´æ•°
        
        Args:
            timestamp_ms: Unixæ¯«ç§’æ—¶é—´æˆ³
            
        Returns:
            YYYYMMDDæ ¼å¼çš„æ•´æ•°ï¼Œå¦‚ 20251213
        """
        dt = datetime.fromtimestamp(timestamp_ms / 1000)
        return int(dt.strftime("%Y%m%d"))
    
    def _get_last_trade_date(self, symbol: str) -> Optional[int]:
        """
        è·å–æœ¬åœ°æ•°æ®çš„æœ€åäº¤æ˜“æ—¥æœŸ
        
        Args:
            symbol: ETFä»£ç ï¼ˆä¸å«åç¼€ï¼‰
            
        Returns:
            æœ€åäº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›None
        """
        # æœç´¢åŒ¹é…çš„æ–‡ä»¶
        files = list(self.data_dir.glob(f"{symbol}.*_daily_*.parquet"))
        if not files:
            return None
            
        # å¦‚æœæœ‰å¤šä¸ªï¼Œå–æœ€æ–°çš„ä¸€ä¸ª
        parquet_file = files[0]
        
        try:
            df = pd.read_parquet(parquet_file)
            if len(df) > 0 and "trade_date" in df.columns:
                return int(df["trade_date"].max())
        except Exception as e:
            print(f"âš ï¸  è¯»å– {symbol} å†å²æ•°æ®å¤±è´¥: {e}")
            
        return None
    
    async def fetch_kline(
        self,
        code: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        count: int = 100
    ) -> List[Dict]:
        """
        è·å–Kçº¿æ•°æ®
        
        Args:
            code: å®Œæ•´è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ "510300.SH"ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD
            end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
            count: è·å–æ¡æ•°ï¼ˆå¦‚æœä¸æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰
            
        Returns:
            åŒ…å«Kçº¿æ•°æ®çš„å­—å…¸åˆ—è¡¨
        """
        try:
            result = await self.client.get_kline(
                code=code,
                period="1d",
                start_time=start_date,
                end_time=end_date,
                count=count,
                dividend_type="front"  # å‰å¤æƒ
            )
            
            bars = result.get("bars", [])
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            rows = []
            for bar in bars:
                timestamp_ms = bar.get("time")
                if timestamp_ms is None:
                    continue
                
                trade_date = self._parse_timestamp(timestamp_ms)
                open_p = bar.get("open")
                high_p = bar.get("high")
                low_p = bar.get("low")
                close_p = bar.get("close")
                vol = bar.get("volume")
                amount = bar.get("amount")
                
                row = {
                    "ts_code": code,
                    "trade_date": trade_date,
                    "pre_close": None,
                    "open": open_p,
                    "high": high_p,
                    "low": low_p,
                    "close": close_p,
                    "change": None,
                    "pct_chg": None,
                    "vol": vol,
                    "amount": amount,
                    "adj_factor": 1.0,
                    "adj_open": open_p,
                    "adj_high": high_p,
                    "adj_low": low_p,
                    "adj_close": close_p,
                }
                rows.append(row)
            
            return rows
            
        except Exception as e:
            print(f"âŒ {code} è·å–æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def update_symbol(
        self,
        symbol: str,
        exchange: str = "SH",
        force_days: Optional[int] = None
    ) -> bool:
        """
        å¢é‡æ›´æ–°å•ä¸ªETFæ•°æ®
        
        Args:
            symbol: ETFä»£ç ï¼ˆä¸å«åç¼€ï¼‰
            exchange: äº¤æ˜“æ‰€ä»£ç  (SH/SZ)
            force_days: å¼ºåˆ¶è·å–æœ€è¿‘Nå¤©ï¼ˆç”¨äºå…¨é‡æ›´æ–°ï¼‰
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        code = f"{symbol}.{exchange}"
        
        # æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶
        files = list(self.data_dir.glob(f"{code}_daily_*.parquet"))
        if files:
            parquet_file = files[0]
        else:
            # æ–°æ–‡ä»¶å‘½åè§„åˆ™
            parquet_file = self.data_dir / f"{code}_daily_20200101_{datetime.now().strftime('%Y%m%d')}.parquet"

        # ç¡®å®šè·å–æ•°æ®çš„æ—¶é—´èŒƒå›´
        if force_days:
            end_date = datetime.now().strftime("%Y%m%d")
            start_date = (datetime.now() - timedelta(days=force_days)).strftime("%Y%m%d")
            print(f"ğŸ“Š {code} - è·å–æœ€è¿‘ {force_days} å¤©æ•°æ®...")
            # å¿…é¡»æŒ‡å®šè¶³å¤Ÿå¤§çš„ countï¼Œå¦åˆ™é»˜è®¤ 100 æ¡
            rows = await self.fetch_kline(code, start_date, end_date, count=5000)
        else:
            last_date = self._get_last_trade_date(symbol)
            
            if last_date:
                start_dt = datetime.strptime(str(last_date), "%Y%m%d") + timedelta(days=1)
                start_date = start_dt.strftime("%Y%m%d")
                end_date = datetime.now().strftime("%Y%m%d")
                
                if int(start_date) > int(end_date):
                     print(f"âœ… {code} - å·²æ˜¯æœ€æ–° ({last_date})")
                     return False

                print(f"ğŸ“Š {code} - å¢é‡æ›´æ–° {start_date} ~ {end_date}...")
                rows = await self.fetch_kline(code, start_date, end_date, count=5000)
            else:
                print(f"ğŸ“Š {code} - é¦–æ¬¡è·å– (ä» 20200101)...")
                start_date = "20200101"
                end_date = datetime.now().strftime("%Y%m%d")
                rows = await self.fetch_kline(code, start_date, end_date, count=5000)
        
        if not rows:
            print(f"âš ï¸  {code} - æ— æ–°æ•°æ®")
            return False
        
        new_df = pd.DataFrame(rows)
        
        if parquet_file.exists():
            try:
                old_df = pd.read_parquet(parquet_file)
                for col in new_df.columns:
                    if col not in old_df.columns:
                        old_df[col] = None
                
                combined_df = pd.concat([old_df, new_df], ignore_index=True)
                combined_df = (
                    combined_df
                    .drop_duplicates(subset=["trade_date"], keep="last")
                    .sort_values("trade_date")
                    .reset_index(drop=True)
                )
                
                new_rows = len(combined_df) - len(old_df)
                print(f"âœ… {code} - æ–°å¢ {new_rows} æ¡ï¼Œæ€»è®¡ {len(combined_df)} æ¡")
                
            except Exception as e:
                print(f"âš ï¸  {code} - åˆå¹¶æ•°æ®å¤±è´¥ï¼Œä½¿ç”¨æ–°æ•°æ®: {e}")
                combined_df = new_df
        else:
            combined_df = new_df
            print(f"âœ… {code} - æ–°å»ºæ–‡ä»¶ï¼Œ{len(combined_df)} æ¡è®°å½•")
        
        if not combined_df.empty:
            min_date = combined_df["trade_date"].min()
            max_date = combined_df["trade_date"].max()
            new_filename = f"{code}_daily_{min_date}_{max_date}.parquet"
            new_path = self.data_dir / new_filename
            
            if parquet_file.exists() and parquet_file.name != new_filename:
                parquet_file.unlink()
                print(f"   é‡å‘½å: {parquet_file.name} -> {new_filename}")
            
            combined_df.to_parquet(new_path, index=False)
        
        return True
    
    async def update_batch(
        self,
        symbols: List[str],
        exchange: str = "SH",
        force_days: Optional[int] = None
    ):
        """æ‰¹é‡æ›´æ–°å¤šä¸ªETF"""
        total = len(symbols)
        success = 0
        
        print(f"\nå¼€å§‹æ›´æ–° {total} ä¸ªETF...")
        print(f"æ•°æ®ç›®å½•: {self.data_dir.absolute()}")
        print("=" * 60)
        
        for idx, symbol in enumerate(symbols, 1):
            print(f"\n[{idx}/{total}] ", end="")
            
            # å°è¯• SH å’Œ SZ
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®š exchangeï¼Œæˆ–è€…æˆ‘ä»¬ä¸çŸ¥é“ exchangeï¼Œå¯ä»¥å°è¯•ä¸¤ä¸ª
            # è¿™é‡Œç®€å•èµ·è§ï¼Œå…ˆè¯• SHï¼Œå¦‚æœå¤±è´¥æˆ–æ— æ•°æ®ï¼Œå†è¯• SZï¼Ÿ
            # æˆ–è€…æ ¹æ® symbol å‰ç¼€åˆ¤æ–­ï¼š51/58 -> SH, 15 -> SZ
            
            current_exchange = exchange
            if symbol.startswith("5"):
                current_exchange = "SH"
            elif symbol.startswith("1"):
                current_exchange = "SZ"
            
            if await self.update_symbol(symbol, current_exchange, force_days):
                success += 1
            
            if idx < total:
                await asyncio.sleep(0.5)
        
        print("\n" + "=" * 60)
        print(f"âœ… æ›´æ–°å®Œæˆ: {success}/{total} æˆåŠŸ")


def load_config(config_file: Path) -> List[str]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½ETFåˆ—è¡¨"""
    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return []
    
    if config_file.suffix == ".json":
        with open(config_file, "r") as f:
            data = json.load(f)
            return data.get("symbols", [])
    elif config_file.suffix in [".yaml", ".yml"]:
        with open(config_file, "r") as f:
            data = yaml.safe_load(f)
            symbols = set()
            
            # 1. å°è¯•ä» pools ä¸­æå– (etf_pools.yaml ç»“æ„)
            if isinstance(data, dict) and "pools" in data:
                pools = data["pools"]
                for pool_name, pool_data in pools.items():
                    if isinstance(pool_data, dict) and "symbols" in pool_data:
                        symbols.update(pool_data["symbols"])
            
            # 2. å°è¯•ç›´æ¥ä» symbols å­—æ®µæå– (combo_wfo_config.yaml ç»“æ„)
            if isinstance(data, dict) and "symbols" in data:
                if isinstance(data["symbols"], list):
                    symbols.update(data["symbols"])
            
            # 3. å°è¯•é¡¶å±‚åˆ—è¡¨
            if isinstance(data, list):
                symbols.update(data)
                
            return list(symbols)
    else:
        with open(config_file, "r") as f:
            return [line.strip() for line in f if line.strip()]


async def main():
    parser = argparse.ArgumentParser(description="åŸºäº QMT Bridge SDK çš„ ETF æ—¥çº¿æ•°æ®å¢é‡æ›´æ–°")
    parser.add_argument("--symbols", type=str, help="ETFä»£ç ï¼Œé€—å·åˆ†éš”")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--all", action="store_true", help="æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰ETF")
    parser.add_argument("--host", type=str, default="192.168.122.132")
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--data-dir", type=str, default="./raw/ETF/daily")
    parser.add_argument("--exchange", type=str, default="SH")
    parser.add_argument("--force-days", type=int, help="å¼ºåˆ¶è·å–æœ€è¿‘Nå¤©æ•°æ®")
    
    args = parser.parse_args()
    
    symbols = []
    if args.symbols:
        symbols = [s.strip() for s in args.symbols.split(",")]
    elif args.config:
        symbols = load_config(Path(args.config))
    elif args.all:
        # ä¼˜å…ˆå°è¯• etf_pools.yaml
        pool_config = Path("configs/etf_pools.yaml")
        if pool_config.exists():
            print(f"ğŸ“š åŠ è½½é…ç½®: {pool_config}")
            symbols = load_config(pool_config)
        else:
            default_config = Path("etf_list.json")
            if default_config.exists():
                print(f"ğŸ“š åŠ è½½é…ç½®: {default_config}")
                symbols = load_config(default_config)
            else:
                print("âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶")
                return
    else:
        parser.print_help()
        return
    
    if not symbols:
        print("âŒ æ²¡æœ‰è¦æ›´æ–°çš„ETF")
        return
    
    updater = ETFDataUpdater(host=args.host, port=args.port, data_dir=args.data_dir)
    await updater.update_batch(symbols=symbols, exchange=args.exchange, force_days=args.force_days)


if __name__ == "__main__":
    asyncio.run(main())
