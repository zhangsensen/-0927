#!/usr/bin/env python3
"""
Top1000ç­–ç•¥æ·±åº¦åˆ†æ (ä¿®å¤ç‰ˆ)
åˆ†ætop1000ç­–ç•¥çš„æƒé‡åˆ†å¸ƒã€å› å­é‡è¦æ€§ã€ç­–ç•¥èšç±»ç­‰
"""

import ast
import glob
import os
import warnings
from pathlib import Path

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


def load_and_combine_all_batches():
    """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ•°æ®"""
    print("ğŸ”„ åŠ è½½æ‰€æœ‰æ‰¹æ¬¡æ•°æ®...")

    results_dir = Path("strategies/results")
    batch_files = sorted(glob.glob(str(results_dir / "top35_batch*.csv")))

    all_data = []

    for i, file in enumerate(batch_files):
        try:
            df = pd.read_csv(file)
            all_data.append(df)
            print(f"  æ‰¹æ¬¡ {i}: åŠ è½½ {len(df)} è¡Œæ•°æ®")
        except Exception as e:
            print(f"  âŒ æ‰¹æ¬¡ {i} åŠ è½½å¤±è´¥: {e}")

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"âœ… åˆå¹¶å®Œæˆ: æ€»å…± {len(combined_df)} ä¸ªç­–ç•¥")

        # æŒ‰å¤æ™®æ¯”ç‡æ’åº
        combined_df = combined_df.sort_values("sharpe", ascending=False).reset_index(
            drop=True
        )
        print(
            f"   å¤æ™®æ¯”ç‡èŒƒå›´: {combined_df['sharpe'].min():.4f} - {combined_df['sharpe'].max():.4f}"
        )

        return combined_df
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
        return None


def parse_weights_data(df):
    """è§£ææƒé‡æ•°æ®"""
    print("\nğŸ”§ è§£ææƒé‡æ•°æ®...")

    # è§£æç¬¬ä¸€ä¸ªç­–ç•¥çš„å› å­åç§°
    factors_str = df.iloc[0]["factors"]
    factor_names = ast.literal_eval(factors_str)
    print(f"   å› å­æ•°é‡: {len(factor_names)}")

    # è§£ææ‰€æœ‰æƒé‡
    all_weights = []
    valid_indices = []

    for idx, row in df.iterrows():
        try:
            weights_str = row["weights"]
            weights = ast.literal_eval(weights_str)

            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            weights_float = [float(w) for w in weights]

            if len(weights_float) == len(factor_names):
                all_weights.append(weights_float)
                valid_indices.append(idx)
        except Exception as e:
            print(f"   è­¦å‘Š: ç­–ç•¥ {idx} æƒé‡è§£æå¤±è´¥: {e}")

    print(f"   æˆåŠŸè§£ææƒé‡: {len(all_weights)}/{len(df)} ä¸ªç­–ç•¥")

    # åˆ›å»ºæƒé‡DataFrame
    weight_cols = [f"weight_{i}" for i in range(len(factor_names))]
    weights_df = pd.DataFrame(all_weights, columns=weight_cols, index=valid_indices)

    # åªä¿ç•™æœ‰æ•ˆç­–ç•¥
    valid_df = df.loc[valid_indices].copy()
    valid_df = pd.concat(
        [valid_df.reset_index(drop=True), weights_df.reset_index(drop=True)], axis=1
    )

    return valid_df, factor_names


def analyze_performance_overview(df):
    """æ€§èƒ½æ¦‚è§ˆåˆ†æ"""
    print(f"\nğŸ“Š æ€§èƒ½æ¦‚è§ˆåˆ†æ (Top {len(df)} ç­–ç•¥)")
    print("=" * 60)

    # åŸºæœ¬ç»Ÿè®¡
    metrics = [
        "sharpe",
        "annual_return",
        "max_drawdown",
        "calmar",
        "win_rate",
        "turnover",
    ]

    print("æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡:")
    for metric in metrics:
        if metric in df.columns:
            mean_val = df[metric].mean()
            std_val = df[metric].std()
            min_val = df[metric].min()
            max_val = df[metric].max()

            if metric == "annual_return" or metric == "max_drawdown":
                print(
                    f"  {metric:12s}: {mean_val:.4f} ({mean_val*100:.2f}%) Â± {std_val:.4f}, èŒƒå›´: [{min_val:.4f}, {max_val:.4f}]"
                )
            elif metric == "turnover":
                print(
                    f"  {metric:12s}: {mean_val:.2f} Â± {std_val:.2f}, èŒƒå›´: [{min_val:.2f}, {max_val:.2f}]"
                )
            else:
                print(
                    f"  {metric:12s}: {mean_val:.4f} Â± {std_val:.4f}, èŒƒå›´: [{min_val:.4f}, {max_val:.4f}]"
                )

    # Top-Nåˆ†æ
    if "top_n" in df.columns:
        top_n_stats = (
            df.groupby("top_n")
            .agg({"sharpe": ["count", "mean", "std"], "annual_return": "mean"})
            .round(4)
        )

        print(f"\nTop-Næ€§èƒ½åˆ†æ:")
        for top_n in sorted(df["top_n"].unique()):
            subset = df[df["top_n"] == top_n]
            print(
                f"  Top-{int(top_n)}: {len(subset)} ä¸ªç­–ç•¥, "
                f"å¹³å‡å¤æ™® {subset['sharpe'].mean():.4f}, "
                f"å¹³å‡æ”¶ç›Š {subset['annual_return'].mean():.4f}"
            )

    # æ€§èƒ½åˆ†çº§
    print(f"\næ€§èƒ½åˆ†çº§:")
    sharpe_thresholds = [0.6, 0.7, 0.8]
    for threshold in sharpe_thresholds:
        count = len(df[df["sharpe"] >= threshold])
        percentage = count / len(df) * 100
        print(f"  å¤æ™® â‰¥ {threshold}: {count} ä¸ªç­–ç•¥ ({percentage:.1f}%)")

    return df


def analyze_factor_importance(df, factor_names):
    """å› å­é‡è¦æ€§åˆ†æ"""
    print(f"\nğŸ” å› å­é‡è¦æ€§åˆ†æ")
    print("=" * 60)

    # æå–æƒé‡åˆ—
    weight_cols = [f"weight_{i}" for i in range(len(factor_names))]
    weights_data = df[weight_cols]

    # è®¡ç®—æ¯ä¸ªå› å­çš„ç»Ÿè®¡ä¿¡æ¯
    factor_analysis = []

    for i, factor_name in enumerate(factor_names):
        weights = weights_data[f"weight_{i}"]

        # åªè€ƒè™‘éé›¶æƒé‡
        non_zero_weights = weights[weights > 0]

        stats = {
            "factor_name": factor_name,
            "mean_weight": weights.mean(),
            "std_weight": weights.std(),
            "max_weight": weights.max(),
            "non_zero_count": len(non_zero_weights),
            "non_zero_ratio": len(non_zero_weights) / len(weights),
            "avg_non_zero_weight": (
                non_zero_weights.mean() if len(non_zero_weights) > 0 else 0
            ),
            "usage_frequency": len(non_zero_weights)
            / len(weights)
            * 100,  # ä½¿ç”¨é¢‘ç‡ç™¾åˆ†æ¯”
        }
        factor_analysis.append(stats)

    factor_df = pd.DataFrame(factor_analysis)
    factor_df = factor_df.sort_values("mean_weight", ascending=False)

    print(f"Top 15 é‡è¦å› å­ (æŒ‰å¹³å‡æƒé‡æ’åº):")
    for i, (_, row) in enumerate(factor_df.head(15).iterrows()):
        print(
            f"  {i+1:2d}. {row['factor_name']:20s}: "
            f"å‡å€¼={row['mean_weight']:.4f}, "
            f"ä½¿ç”¨ç‡={row['usage_frequency']:.1f}%, "
            f"æœ€å¤§={row['max_weight']:.4f}"
        )

    # å› å­ç±»åˆ«åˆ†æ
    print(f"\nå› å­ç±»åˆ«åˆ†æ:")
    categories = {
        "ä»·æ ¼ä½ç½®": [f for f in factor_names if "PRICE_POSITION" in f],
        "æ³¢åŠ¨ç‡": [f for f in factor_names if "STDDEV" in f or "VAR" in f],
        "RSI": [f for f in factor_names if "RSI" in f],
        "éšæœºæŒ‡æ ‡": [f for f in factor_names if "STOCH" in f],
        "åŠ¨é‡": [f for f in factor_names if "MOMENTUM" in f],
        "å…¶ä»–": [],
    }

    # åˆ†ç±»å…¶ä»–å› å­
    assigned_factors = []
    for category_key, factor_list in categories.items():
        if category_key != "å…¶ä»–":
            assigned_factors.extend(factor_list)

    for f in factor_names:
        if f not in assigned_factors:
            categories["å…¶ä»–"].append(f)

    for category, factors in categories.items():
        if factors:
            category_weights = []
            for factor in factors:
                if factor in factor_names:
                    idx = factor_names.index(factor)
                    category_weights.extend(weights_data[f"weight_{idx}"].tolist())

            if category_weights:
                avg_weight = np.mean(category_weights)
                non_zero_ratio = sum(1 for w in category_weights if w > 0) / len(
                    category_weights
                )
                print(
                    f"  {category:8s}: å¹³å‡æƒé‡ {avg_weight:.4f}, ä½¿ç”¨ç‡ {non_zero_ratio:.2%}"
                )

    return factor_df, weights_data


def analyze_strategy_patterns(df, factor_names, weights_data):
    """ç­–ç•¥æ¨¡å¼åˆ†æ"""
    print(f"\nğŸ¯ ç­–ç•¥æ¨¡å¼åˆ†æ")
    print("=" * 60)

    # æƒé‡é›†ä¸­åº¦åˆ†æ
    concentration_scores = []
    factor_counts = []

    weight_cols = [f"weight_{i}" for i in range(len(factor_names))]

    for idx, row in df.iterrows():
        weights = row[weight_cols].values
        non_zero_weights = weights[weights > 0]

        # HHIé›†ä¸­åº¦æŒ‡æ•°
        if len(non_zero_weights) > 0:
            normalized_weights = non_zero_weights / non_zero_weights.sum()
            hhi = (normalized_weights**2).sum()
            concentration_scores.append(hhi)
            factor_counts.append(len(non_zero_weights))

    concentration_scores = np.array(concentration_scores)
    factor_counts = np.array(factor_counts)

    print(f"æƒé‡é›†ä¸­åº¦åˆ†æ:")
    print(f"  å¹³å‡HHIæŒ‡æ•°: {concentration_scores.mean():.4f}")
    print(f"  HHIæ ‡å‡†å·®: {concentration_scores.std():.4f}")
    print(f"  ä½é›†ä¸­åº¦(HHI<0.3): {np.sum(concentration_scores < 0.3)} ä¸ªç­–ç•¥")
    print(
        f"  ä¸­é›†ä¸­åº¦(0.3â‰¤HHI<0.5): {np.sum((concentration_scores >= 0.3) & (concentration_scores < 0.5))} ä¸ªç­–ç•¥"
    )
    print(f"  é«˜é›†ä¸­åº¦(HHIâ‰¥0.5): {np.sum(concentration_scores >= 0.5)} ä¸ªç­–ç•¥")

    print(f"\nå› å­æ•°é‡ä½¿ç”¨åˆ†æ:")
    print(f"  å¹³å‡ä½¿ç”¨å› å­æ•°: {factor_counts.mean():.1f}")
    print(f"  å› å­æ•°èŒƒå›´: {factor_counts.min()} - {factor_counts.max()}")
    print(f"  ä½¿ç”¨<10ä¸ªå› å­: {np.sum(factor_counts < 10)} ä¸ªç­–ç•¥")
    print(
        f"  ä½¿ç”¨10-20ä¸ªå› å­: {np.sum((factor_counts >= 10) & (factor_counts < 20))} ä¸ªç­–ç•¥"
    )
    print(f"  ä½¿ç”¨â‰¥20ä¸ªå› å­: {np.sum(factor_counts >= 20)} ä¸ªç­–ç•¥")

    # æ€§èƒ½ä¸ç­–ç•¥å¤æ‚åº¦çš„å…³ç³»
    df["factor_count"] = factor_counts
    df["concentration"] = concentration_scores

    complexity_performance = (
        df.groupby("factor_count")
        .agg({"sharpe": ["mean", "std", "count"], "annual_return": "mean"})
        .round(4)
    )

    print(f"\nç­–ç•¥å¤æ‚åº¦ä¸æ€§èƒ½å…³ç³»:")
    best_factor_count = None
    best_sharpe = 0

    for count in sorted(df["factor_count"].unique()):
        subset = df[df["factor_count"] == count]
        avg_sharpe = subset["sharpe"].mean()
        print(f"  {count} ä¸ªå› å­: {len(subset)} ä¸ªç­–ç•¥, å¹³å‡å¤æ™® {avg_sharpe:.4f}")

        if avg_sharpe > best_sharpe and len(subset) >= 5:  # è‡³å°‘5ä¸ªç­–ç•¥æ‰è€ƒè™‘
            best_sharpe = avg_sharpe
            best_factor_count = count

    if best_factor_count:
        print(f"\næœ€ä¼˜å› å­æ•°é‡: {best_factor_count} ä¸ª (å¹³å‡å¤æ™® {best_sharpe:.4f})")

    return concentration_scores, factor_counts


def identify_top_performers(df, factor_names, n=100):
    """è¯†åˆ«å¹¶åˆ†æé¡¶çº§è¡¨ç°è€…"""
    print(f"\nğŸ† Top {n} ç­–ç•¥æ·±åº¦åˆ†æ")
    print("=" * 60)

    top_strategies = df.head(n).copy()

    print(f"Top {n} ç­–ç•¥æ€§èƒ½:")
    print(
        f"  å¤æ™®æ¯”ç‡: {top_strategies['sharpe'].mean():.4f} Â± {top_strategies['sharpe'].std():.4f}"
    )
    print(
        f"  å¹´åŒ–æ”¶ç›Š: {top_strategies['annual_return'].mean():.4f} Â± {top_strategies['annual_return'].std():.4f}"
    )
    print(
        f"  æœ€å¤§å›æ’¤: {top_strategies['max_drawdown'].mean():.4f} Â± {top_strategies['max_drawdown'].std():.4f}"
    )
    print(
        f"  æ¢æ‰‹ç‡: {top_strategies['turnover'].mean():.2f} Â± {top_strategies['turnover'].std():.2f}"
    )

    # åˆ†æTopç­–ç•¥çš„å› å­åå¥½
    weight_cols = [f"weight_{i}" for i in range(len(factor_names))]
    top_weights = top_strategies[weight_cols]

    print(f"\nTop {n} ç­–ç•¥å› å­åå¥½ (å‰15ä¸ª):")
    top_factor_means = top_weights.mean().sort_values(ascending=False).head(15)

    for i, (col, mean_weight) in enumerate(top_factor_means.items()):
        factor_idx = int(col.split("_")[1])
        factor_name = factor_names[factor_idx]
        usage_rate = (top_weights[col] > 0).mean() * 100
        print(
            f"  {i+1:2d}. {factor_name:20s}: "
            f"å¹³å‡æƒé‡ {mean_weight:.4f}, "
            f"ä½¿ç”¨ç‡ {usage_rate:.1f}%"
        )

    # é£é™©è°ƒæ•´æ”¶ç›Šåˆ†æ
    top_strategies["risk_adjusted_return"] = (
        top_strategies["annual_return"] / top_strategies["max_drawdown"]
    )
    print(f"\né£é™©è°ƒæ•´æ”¶ç›Šåˆ†æ:")
    print(f"  å¹³å‡æ”¶ç›Š/å›æ’¤æ¯”: {top_strategies['risk_adjusted_return'].mean():.4f}")
    print(f"  æœ€é«˜æ”¶ç›Š/å›æ’¤æ¯”: {top_strategies['risk_adjusted_return'].max():.4f}")

    return top_strategies


def generate_practical_insights(df, factor_names, factor_df):
    """ç”Ÿæˆå®ç”¨ç­–ç•¥æ´å¯Ÿ"""
    print(f"\nğŸ’¡ å®ç”¨ç­–ç•¥æ´å¯Ÿ")
    print("=" * 60)

    print("1. æ ¸å¿ƒå› å­å»ºè®®:")
    top_factors = factor_df.head(8)["factor_name"].tolist()
    print(f"   å¿…é…å› å­: {', '.join(top_factors[:4])}")
    print(f"   å¢å¼ºå› å­: {', '.join(top_factors[4:8])}")

    print(f"\n2. æƒé‡åˆ†é…å»ºè®®:")
    # åˆ†æé¡¶çº§ç­–ç•¥çš„æƒé‡åˆ†é…
    weight_cols = [f"weight_{i}" for i in range(len(factor_names))]
    top_50 = df.head(50)
    top_weights = top_50[weight_cols]

    non_zero_weights = top_weights.values[top_weights.values > 0]
    if len(non_zero_weights) > 0:
        print(f"   æ ¸å¿ƒå› å­æƒé‡: 0.05 - 0.08")
        print(f"   è¾…åŠ©å› å­æƒé‡: 0.02 - 0.05")
        print(f"   å»ºè®®å› å­æ€»æ•°: 15-25 ä¸ª")

    print(f"\n3. é£é™©ç®¡ç†å»ºè®®:")
    avg_drawdown = df["max_drawdown"].mean()
    max_drawdown = df["max_drawdown"].max()
    print(f"   é¢„æœŸæœ€å¤§å›æ’¤: {avg_drawdown:.1%} - {max_drawdown:.1%}")
    print(f"   å»ºè®®æ­¢æŸçº¿: {max_drawdown * 1.2:.1%}")
    print(f"   å»ºè®®ä»“ä½æ§åˆ¶: å•ä¸ªå› å­æƒé‡ < 10%")

    print(f"\n4. ç­–ç•¥ç±»å‹å»ºè®®:")
    # åŸºäºå› å­åå¥½åˆ†ç±»
    price_position_factors = [f for f in top_factors if "PRICE_POSITION" in f]
    volatility_factors = [f for f in top_factors if "STDDEV" in f or "VAR" in f]
    momentum_factors = [f for f in top_factors if "STOCH" in f or "RSI" in f]

    print(f"   ä¸»è¦ç­–ç•¥ç±»å‹: å¤šå› å­å‡è¡¡ç­–ç•¥")
    print(f"   æ ¸å¿ƒé€»è¾‘: ä»·æ ¼ä½ç½® + æ³¢åŠ¨ç‡ + æŠ€æœ¯æŒ‡æ ‡")
    print(f"   é€‚ç”¨å¸‚åœº: éœ‡è¡å¸‚å’Œè¶‹åŠ¿å¸‚å‡è¡¡é…ç½®")

    print(f"\n5. å®æ–½å»ºè®®:")
    print(f"   è°ƒä»“é¢‘ç‡: åŸºäºæ¢æ‰‹ç‡ï¼Œå»ºè®®æœˆåº¦è°ƒä»“")
    print(f"   ç»„åˆè§„æ¨¡: Top 8 ä¸ªæ ‡çš„")
    print(f"   ä¸šç»©åŸºå‡†: å¤æ™®æ¯”ç‡ç›®æ ‡ > 0.7")
    print(f"   èµ„é‡‘åˆ†é…: å»ºè®®åˆ†æ‰¹å»ºä»“ï¼Œé™ä½å†²å‡»æˆæœ¬")


def save_comprehensive_results(df, factor_names, factor_df):
    """ä¿å­˜ç»¼åˆåˆ†æç»“æœ"""
    print(f"\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

    # ä¿å­˜å®Œæ•´åˆ†ææ•°æ®
    df.to_csv("strategies/results/top1000_complete_analysis.csv", index=False)
    factor_df.to_csv("strategies/results/factor_importance_detailed.csv", index=False)

    # ä¿å­˜ç­–ç•¥æ´å¯Ÿ
    insights = {
        "analysis_summary": {
            "total_strategies": len(df),
            "factor_count": len(factor_names),
            "avg_sharpe": float(df["sharpe"].mean()),
            "best_sharpe": float(df["sharpe"].max()),
            "avg_return": float(df["annual_return"].mean()),
            "avg_drawdown": float(df["max_drawdown"].mean()),
            "optimal_top_n": int(df.groupby("top_n")["sharpe"].mean().idxmax()),
        },
        "top_factors": factor_df.head(10)["factor_name"].tolist(),
        "strategy_recommendations": {
            "core_factors_count": 8,
            "recommended_weight_range": [0.03, 0.08],
            "optimal_factor_count": 20,
            "risk_management": {
                "stop_loss": 0.25,
                "max_single_factor_weight": 0.10,
                "rebalance_frequency": "monthly",
            },
        },
    }

    import json

    with open(
        "strategies/results/top1000_strategy_insights.json", "w", encoding="utf-8"
    ) as f:
        json.dump(insights, f, indent=2, ensure_ascii=False)

    print(f"   âœ… å®Œæ•´åˆ†ææ•°æ®: strategies/results/top1000_complete_analysis.csv")
    print(f"   âœ… å› å­é‡è¦æ€§åˆ†æ: strategies/results/factor_importance_detailed.csv")
    print(f"   âœ… ç­–ç•¥æ´å¯ŸæŠ¥å‘Š: strategies/results/top1000_strategy_insights.json")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Top1000ç­–ç•¥æ·±åº¦åˆ†æ (ä¿®å¤ç‰ˆ)")
    print("=" * 60)

    # åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
    df = load_and_combine_all_batches()

    if df is None or len(df) == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œåˆ†æ")
        return

    # è§£ææƒé‡æ•°æ®
    df, factor_names = parse_weights_data(df)

    if len(df) == 0:
        print("âŒ æƒé‡è§£æå¤±è´¥ï¼Œæ²¡æœ‰å¯ç”¨æ•°æ®")
        return

    # æ€§èƒ½æ¦‚è§ˆåˆ†æ
    df = analyze_performance_overview(df)

    # å› å­é‡è¦æ€§åˆ†æ
    factor_df, weights_data = analyze_factor_importance(df, factor_names)

    # ç­–ç•¥æ¨¡å¼åˆ†æ
    concentration_scores, factor_counts = analyze_strategy_patterns(
        df, factor_names, weights_data
    )

    # é¡¶çº§è¡¨ç°è€…åˆ†æ
    top_strategies = identify_top_performers(df, factor_names)

    # ç”Ÿæˆå®ç”¨æ´å¯Ÿ
    generate_practical_insights(df, factor_names, factor_df)

    # ä¿å­˜ç»“æœ
    save_comprehensive_results(df, factor_names, factor_df)

    print(f"\nâœ… Top1000ç­–ç•¥åˆ†æå®Œæˆ!")
    print(f"   åŸºäº {len(df)} ä¸ªæœ‰æ•ˆç­–ç•¥å’Œ {len(factor_names)} ä¸ªå› å­")
    print(f"   ä¸ºé‡åŒ–ç­–ç•¥æ„å»ºæä¾›äº†å…¨é¢çš„æ•°æ®æ”¯æ’‘")


if __name__ == "__main__":
    main()
