#!/usr/bin/env python3
"""
éªŒè¯"38åªAè‚¡ETFæˆªé¢alphaæœ¬è´¨ä¸€ç»´"å‡è®¾

ä¸¤é¡¹éªŒè¯:
  1) æˆªé¢æ”¶ç›Š PCA: PC1 è§£é‡Šåº¦æ˜¯å¦é•¿æœŸ >50%
  2) S1 å› å­ RankIC ç›¸å…³æ€§: 4ä¸ªå› å­æ˜¯å¦é«˜åº¦åŒå‘ (åˆ»ç”»åŒä¸€ latent trend)

è¾“å‡º: ç»ˆç«¯è¡¨æ ¼ + results/alpha_dimension_analysis/ ä¸‹çš„å›¾è¡¨
"""

import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(ROOT / "src"))

from etf_strategy.core.data_loader import DataLoader
from etf_strategy.core.precise_factor_library_v2 import PreciseFactorLibrary
from etf_strategy.core.cross_section_processor import CrossSectionProcessor


def load_data(config):
    """Load OHLCV and compute factors."""
    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=config["data"]["end_date"],
    )
    return ohlcv


def pca_analysis(ohlcv, qdii_tickers):
    """Cross-sectional return PCA analysis."""
    print("\n" + "=" * 80)
    print("éªŒè¯ 1: æˆªé¢æ”¶ç›Š PCA â€” PC1 è§£é‡Šåº¦")
    print("=" * 80)

    # Extract close prices
    close = ohlcv["close"].copy()

    # Filter to A-share only
    a_share_cols = [c for c in close.columns if c not in qdii_tickers]
    close = close[a_share_cols]

    # Daily returns
    returns = close.pct_change().dropna(how="all")
    # Drop columns with too many NaNs (late-IPO ETFs)
    min_obs = int(len(returns) * 0.5)
    returns = returns.dropna(axis=1, thresh=min_obs)
    returns = returns.fillna(0)

    print(f"  ETFæ•°é‡: {returns.shape[1]}")
    print(f"  æ—¥æœŸèŒƒå›´: {returns.index[0].date()} ~ {returns.index[-1].date()}")
    print(f"  äº¤æ˜“æ—¥æ•°: {len(returns)}")

    # Rolling PCA: 60-day windows
    window = 60
    pc1_ratios = []
    pc2_ratios = []
    dates = []

    from sklearn.decomposition import PCA

    for i in range(window, len(returns)):
        window_ret = returns.iloc[i - window : i].values
        # Skip if too many zeros
        if np.std(window_ret) < 1e-10:
            continue
        pca = PCA(n_components=min(5, window_ret.shape[1]))
        pca.fit(window_ret)
        pc1_ratios.append(pca.explained_variance_ratio_[0])
        pc2_ratios.append(pca.explained_variance_ratio_[1])
        dates.append(returns.index[i])

    pc1_series = pd.Series(pc1_ratios, index=dates)
    pc2_series = pd.Series(pc2_ratios, index=dates)

    print(f"\n  æ»šåŠ¨ {window}æ—¥ PCA ç»“æœ:")
    print(f"  PC1 è§£é‡Šåº¦ â€” å‡å€¼: {pc1_series.mean():.1%}, ä¸­ä½æ•°: {pc1_series.median():.1%}")
    print(f"  PC2 è§£é‡Šåº¦ â€” å‡å€¼: {pc2_series.mean():.1%}, ä¸­ä½æ•°: {pc2_series.median():.1%}")
    print(f"  PC1 > 50% çš„æ¯”ä¾‹: {(pc1_series > 0.5).mean():.1%}")
    print(f"  PC1 > 40% çš„æ¯”ä¾‹: {(pc1_series > 0.4).mean():.1%}")
    print(f"  PC1+PC2 > 60% çš„æ¯”ä¾‹: {((pc1_series + pc2_series) > 0.6).mean():.1%}")

    # Yearly breakdown
    yearly = pc1_series.groupby(pc1_series.index.year).agg(["mean", "median", "min", "max"])
    yearly.columns = ["å‡å€¼", "ä¸­ä½æ•°", "æœ€å°", "æœ€å¤§"]
    print(f"\n  å¹´åº¦ PC1 è§£é‡Šåº¦:")
    for year, row in yearly.iterrows():
        print(f"    {year}: å‡å€¼={row['å‡å€¼']:.1%}  ä¸­ä½={row['ä¸­ä½æ•°']:.1%}  "
              f"èŒƒå›´=[{row['æœ€å°']:.1%}, {row['æœ€å¤§']:.1%}]")

    return pc1_series, pc2_series


def factor_ic_correlation(ohlcv, config, qdii_tickers):
    """S1 factor RankIC correlation analysis."""
    print("\n" + "=" * 80)
    print("éªŒè¯ 2: S1 å› å­ RankIC ç›¸å…³æ€§ â€” æ˜¯å¦åˆ»ç”»åŒä¸€ latent trend")
    print("=" * 80)

    S1_FACTORS = ["ADX_14D", "OBV_SLOPE_10D", "SHARPE_RATIO_20D", "SLOPE_20D"]

    # Compute factors
    factor_lib = PreciseFactorLibrary()
    raw_factors_df = factor_lib.compute_all_factors(ohlcv)
    processor = CrossSectionProcessor(verbose=False)

    factor_names = sorted(raw_factors_df.columns.get_level_values(0).unique().tolist())
    raw_factors = {fname: raw_factors_df[fname] for fname in factor_names}
    std_factors = processor.process_all_factors(raw_factors)

    # Filter to A-share only
    a_share_cols = [c for c in ohlcv["close"].columns if c not in qdii_tickers]

    close = ohlcv["close"][a_share_cols]
    fwd_ret = close.pct_change().shift(-1)  # Forward 1-day return for IC calc

    # Compute daily RankIC for each S1 factor
    print(f"\n  S1 å› å­: {S1_FACTORS}")

    daily_ics = {}
    for fname in S1_FACTORS:
        if fname not in std_factors:
            print(f"  âš ï¸ {fname} not found in computed factors, skipping")
            continue
        factor_vals = std_factors[fname][a_share_cols]
        ics = []
        dates = []
        for date in factor_vals.index:
            if date not in fwd_ret.index:
                continue
            f_row = factor_vals.loc[date].dropna()
            r_row = fwd_ret.loc[date].reindex(f_row.index).dropna()
            common = f_row.index.intersection(r_row.index)
            if len(common) < 10:
                continue
            ic = f_row[common].rank().corr(r_row[common].rank())
            if not np.isnan(ic):
                ics.append(ic)
                dates.append(date)
        daily_ics[fname] = pd.Series(ics, index=dates)

    # RankIC summary
    print(f"\n  å› å­ RankIC æ±‡æ€» (æ—¥é¢‘):")
    print(f"  {'å› å­':<25} {'å‡å€¼IC':>8} {'IC>0æ¯”ä¾‹':>8} {'IC_IR':>8}")
    print(f"  {'-'*25} {'-'*8} {'-'*8} {'-'*8}")
    for fname in S1_FACTORS:
        if fname not in daily_ics:
            continue
        ic = daily_ics[fname]
        mean_ic = ic.mean()
        pos_rate = (ic > 0).mean()
        ic_ir = ic.mean() / ic.std() if ic.std() > 0 else 0
        print(f"  {fname:<25} {mean_ic:>+8.4f} {pos_rate:>8.1%} {ic_ir:>8.3f}")

    # Pairwise IC correlation (are the factors measuring the same thing?)
    print(f"\n  å› å­é—´ RankIC æ—¶é—´åºåˆ—ç›¸å…³æ€§ (Pearson):")
    ic_df = pd.DataFrame(daily_ics)
    ic_df = ic_df.dropna()
    corr_matrix = ic_df.corr()

    pairs = []
    for i, f1 in enumerate(S1_FACTORS):
        for f2 in S1_FACTORS[i + 1 :]:
            if f1 in corr_matrix.columns and f2 in corr_matrix.columns:
                r = corr_matrix.loc[f1, f2]
                pairs.append((f1, f2, r))
                print(f"  {f1:<25} Ã— {f2:<25} = {r:+.3f}")

    avg_corr = np.mean([p[2] for p in pairs]) if pairs else 0
    print(f"\n  å¹³å‡ä¸¤ä¸¤ç›¸å…³æ€§: {avg_corr:+.3f}")

    if avg_corr > 0.5:
        print(f"  â†’ é«˜ç›¸å…³ (>0.5): 4ä¸ªå› å­é«˜åº¦åŒå‘ï¼Œæ”¯æŒ'åŒä¸€ latent trend'å‡è®¾")
    elif avg_corr > 0.3:
        print(f"  â†’ ä¸­ç­‰ç›¸å…³ (0.3~0.5): éƒ¨åˆ†é‡å ä½†æœ‰ç‹¬ç«‹ä¿¡æ¯")
    else:
        print(f"  â†’ ä½ç›¸å…³ (<0.3): 4ä¸ªå› å­å„æœ‰ç‹¬ç«‹ä¿¡æ¯ï¼Œ'ä¸€ç»´'å‡è®¾ä¸æˆç«‹")

    # Also check: factor cross-sectional rank correlation (are ranks similar across assets?)
    print(f"\n  å› å­æˆªé¢ Rank ç›¸å…³æ€§ (æ—¶é—´å¹³å‡):")
    rank_corrs = {}
    for i, f1 in enumerate(S1_FACTORS):
        for f2 in S1_FACTORS[i + 1 :]:
            if f1 not in std_factors or f2 not in std_factors:
                continue
            v1 = std_factors[f1][a_share_cols]
            v2 = std_factors[f2][a_share_cols]
            daily_xcorr = []
            for date in v1.index:
                if date not in v2.index:
                    continue
                r1 = v1.loc[date].dropna().rank()
                r2 = v2.loc[date].reindex(r1.index).dropna().rank()
                common = r1.index.intersection(r2.index)
                if len(common) < 10:
                    continue
                c = r1[common].corr(r2[common])
                if not np.isnan(c):
                    daily_xcorr.append(c)
            avg_xc = np.mean(daily_xcorr) if daily_xcorr else 0
            rank_corrs[(f1, f2)] = avg_xc
            print(f"  {f1:<25} Ã— {f2:<25} = {avg_xc:+.3f}")

    avg_rank_corr = np.mean(list(rank_corrs.values())) if rank_corrs else 0
    print(f"\n  å¹³å‡æˆªé¢ Rank ç›¸å…³æ€§: {avg_rank_corr:+.3f}")
    if avg_rank_corr > 0.4:
        print(f"  â†’ é«˜æˆªé¢é‡å : å› å­é€‰å‡ºæ¥çš„ETFé«˜åº¦é‡åˆï¼Œä¿¡æ¯å†—ä½™ï¼Œæ”¯æŒ'ä¸€ç»´'å‡è®¾")
    elif avg_rank_corr > 0.2:
        print(f"  â†’ ä¸­ç­‰æˆªé¢é‡å : éƒ¨åˆ†é‡åˆï¼Œå› å­æœ‰ä¸€å®šäº’è¡¥")
    else:
        print(f"  â†’ ä½æˆªé¢é‡å : å› å­é€‰å‡ºä¸åŒETFï¼Œäº’è¡¥æ€§å¼ºï¼Œ'ä¸€ç»´'å‡è®¾ä¸æˆç«‹")

    return daily_ics, corr_matrix


def all_factor_ic_analysis(ohlcv, config, qdii_tickers):
    """Extended analysis: all 17 active factors' cross-sectional rank correlations."""
    print("\n" + "=" * 80)
    print("éªŒè¯ 2b: å…¨éƒ¨æ´»è·ƒå› å­æˆªé¢ Rank ç›¸å…³æ€§çŸ©é˜µ (éªŒè¯ä¿¡æ¯ç»´åº¦)")
    print("=" * 80)

    active_factors = config.get("active_factors", [])
    print(f"  æ´»è·ƒå› å­æ•°: {len(active_factors)}")

    factor_lib = PreciseFactorLibrary()
    raw_factors_df = factor_lib.compute_all_factors(ohlcv)
    processor = CrossSectionProcessor(verbose=False)

    factor_names = sorted(raw_factors_df.columns.get_level_values(0).unique().tolist())
    raw_factors = {fname: raw_factors_df[fname] for fname in factor_names}
    std_factors = processor.process_all_factors(raw_factors)

    a_share_cols = [c for c in ohlcv["close"].columns if c not in qdii_tickers]

    # Compute average cross-sectional rank correlation for all factor pairs
    available = [f for f in active_factors if f in std_factors]
    n = len(available)
    corr_mat = np.zeros((n, n))

    for i in range(n):
        corr_mat[i, i] = 1.0
        for j in range(i + 1, n):
            f1, f2 = available[i], available[j]
            v1 = std_factors[f1][a_share_cols]
            v2 = std_factors[f2][a_share_cols]
            daily_xcorr = []
            # Sample every 5th day for speed
            sample_dates = v1.index[::5]
            for date in sample_dates:
                if date not in v2.index:
                    continue
                r1 = v1.loc[date].dropna().rank()
                r2 = v2.loc[date].reindex(r1.index).dropna().rank()
                common = r1.index.intersection(r2.index)
                if len(common) < 10:
                    continue
                c = r1[common].corr(r2[common])
                if not np.isnan(c):
                    daily_xcorr.append(c)
            avg = np.mean(daily_xcorr) if daily_xcorr else 0
            corr_mat[i, j] = avg
            corr_mat[j, i] = avg

    corr_df = pd.DataFrame(corr_mat, index=available, columns=available)

    # PCA on the correlation matrix to find effective dimensionality
    from sklearn.decomposition import PCA
    eigenvalues = np.linalg.eigvalsh(corr_mat)[::-1]
    total = eigenvalues.sum()
    explained = eigenvalues / total

    print(f"\n  å› å­ç›¸å…³çŸ©é˜µ PCA (æœ‰æ•ˆç»´åº¦åˆ†æ):")
    cumulative = 0
    for i, (ev, ex) in enumerate(zip(eigenvalues[:8], explained[:8])):
        cumulative += ex
        marker = " â—€ 80%" if cumulative >= 0.8 and cumulative - ex < 0.8 else ""
        print(f"    PC{i+1}: ç‰¹å¾å€¼={ev:.2f}  è§£é‡Š={ex:.1%}  ç´¯ç§¯={cumulative:.1%}{marker}")

    effective_dim = np.sum(eigenvalues > 1.0)  # Kaiser criterion
    print(f"\n  Kaiserå‡†åˆ™æœ‰æ•ˆç»´åº¦ (ç‰¹å¾å€¼>1): {effective_dim}")
    print(f"  å‰3ä¸ªPCç´¯ç§¯è§£é‡Š: {explained[:3].sum():.1%}")

    # Print high-correlation pairs
    print(f"\n  é«˜ç›¸å…³å› å­å¯¹ (|r| > 0.4):")
    for i in range(n):
        for j in range(i + 1, n):
            if abs(corr_mat[i, j]) > 0.4:
                print(f"    {available[i]:<25} Ã— {available[j]:<25} = {corr_mat[i,j]:+.3f}")

    return corr_df, eigenvalues


def main():
    config_path = ROOT / "configs" / "combo_wfo_config.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    qdii_tickers = config.get("universe", {}).get("qdii_tickers", [])

    print("=" * 80)
    print("ğŸ“Š Alpha ç»´åº¦å‡è®¾éªŒè¯: '38åªAè‚¡ETFæˆªé¢alphaæœ¬è´¨ä¸€ç»´'")
    print("=" * 80)

    # Load data
    print("\nåŠ è½½æ•°æ®...")
    ohlcv = load_data(config)

    # Verification 1: PCA on cross-sectional returns
    pc1, pc2 = pca_analysis(ohlcv, qdii_tickers)

    # Verification 2: S1 factor IC correlation
    daily_ics, ic_corr = factor_ic_correlation(ohlcv, config, qdii_tickers)

    # Verification 2b: All-factor correlation dimensionality
    factor_corr, eigenvalues = all_factor_ic_analysis(ohlcv, config, qdii_tickers)

    # Save results
    out_dir = ROOT / "results" / "alpha_dimension_analysis"
    out_dir.mkdir(parents=True, exist_ok=True)

    factor_corr.to_csv(out_dir / "factor_cross_section_corr.csv")
    pd.DataFrame({"pc1": pc1, "pc2": pc2}).to_csv(out_dir / "pca_timeseries.csv")
    pd.DataFrame({"eigenvalue": eigenvalues}).to_csv(out_dir / "factor_pca_eigenvalues.csv")

    # Final verdict
    print("\n" + "=" * 80)
    print("ç»¼åˆåˆ¤å®š")
    print("=" * 80)

    pc1_median = pc1.median()
    kaiser_dim = np.sum(eigenvalues > 1.0)

    print(f"  æ”¶ç›Šæˆªé¢ PC1 ä¸­ä½è§£é‡Šåº¦: {pc1_median:.1%}", end="")
    if pc1_median > 0.5:
        print(" â†’ âœ… å¼ºä¸€ç»´ (>50%)")
    elif pc1_median > 0.35:
        print(" â†’ âš ï¸ ä¸­ç­‰ (35-50%), ä¸å®Œå…¨ä¸€ç»´ä½†ä¸»æˆåˆ†å ä¼˜")
    else:
        print(" â†’ âŒ å¤šç»´ (<35%), ä¸€ç»´å‡è®¾ä¸æˆç«‹")

    print(f"  å› å­ç©ºé—´æœ‰æ•ˆç»´åº¦ (Kaiser): {kaiser_dim}", end="")
    if kaiser_dim <= 3:
        print(f" â†’ âœ… ä½ç»´ (â‰¤3), å› å­é«˜åº¦å†—ä½™")
    elif kaiser_dim <= 5:
        print(f" â†’ âš ï¸ ä¸­ç­‰ (4-5), æœ‰éƒ¨åˆ†ç‹¬ç«‹ä¿¡æ¯")
    else:
        print(f" â†’ âŒ é«˜ç»´ (>5), å› å­ç©ºé—´ä¸°å¯Œ")

    print(f"\n  ç»“æœå·²ä¿å­˜è‡³: {out_dir}/")


if __name__ == "__main__":
    main()
