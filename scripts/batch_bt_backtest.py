#!/usr/bin/env python3
"""
æ‰¹é‡ BT å›æµ‹ï¼šéå† WFO è¾“å‡ºçš„å…¨éƒ¨ç»„åˆï¼Œé€ä¸ªç”¨ Backtrader GenericStrategy å›æµ‹å¹¶ä¿å­˜ç»“æœã€‚
"""
import gc
import sys
import argparse
from pathlib import Path

ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT / "etf_rotation_optimized"))
sys.path.insert(0, str(ROOT))

import yaml
import pandas as pd
import numpy as np
import backtrader as bt
from tqdm import tqdm
from datetime import datetime

from core.data_loader import DataLoader
from core.precise_factor_library_v2 import PreciseFactorLibrary
from core.cross_section_processor import CrossSectionProcessor
from core.market_timing import LightTimingModule
from core.utils.rebalance import shift_timing_signal, generate_rebalance_schedule
from strategy_auditor.core.engine import GenericStrategy, PandasData

FREQ = 8
POS_SIZE = 3
INITIAL_CAPITAL = 1_000_000.0
COMMISSION_RATE = 0.0002
LOOKBACK = 252


def run_bt_backtest(combined_score_df, timing_series, etf_codes, data_feeds, rebalance_schedule):
    """å•ç»„åˆ BT å›æµ‹å¼•æ“"""
    cerebro = bt.Cerebro()
    cerebro.broker.setcash(INITIAL_CAPITAL)
    cerebro.broker.setcommission(commission=COMMISSION_RATE, leverage=1.0)
    cerebro.broker.set_coc(True)
    cerebro.broker.set_checksubmit(False)

    for ticker, df in data_feeds.items():
        data = PandasData(dataname=df, name=ticker)
        cerebro.adddata(data)

    cerebro.addstrategy(
        GenericStrategy, 
        scores=combined_score_df, 
        timing=timing_series, 
        etf_codes=etf_codes, 
        freq=FREQ, 
        pos_size=POS_SIZE,
        rebalance_schedule=rebalance_schedule
    )

    start_val = cerebro.broker.getvalue()
    results = cerebro.run()
    end_val = cerebro.broker.getvalue()
    strat = results[0]

    bt_return = (end_val / start_val) - 1

    return bt_return, strat.margin_failures


import multiprocessing as mp
from functools import partial

# å…¨å±€å˜é‡ï¼Œç”¨äºå­è¿›ç¨‹å…±äº«æ•°æ® (Copy-on-Write)
_shared_data = {}

def init_worker(data_feeds, std_factors, timing_series, etf_codes):
    """å­è¿›ç¨‹åˆå§‹åŒ–ï¼šä¿å­˜å…±äº«æ•°æ®"""
    global _shared_data
    _shared_data['data_feeds'] = data_feeds
    _shared_data['std_factors'] = std_factors
    _shared_data['timing_series'] = timing_series
    _shared_data['etf_codes'] = etf_codes
    
    # âœ… é¢„è®¡ç®—è°ƒä»“æ—¥ç¨‹ (æ‰€æœ‰ç»„åˆå…±äº«)
    T = len(timing_series)
    _shared_data['rebalance_schedule'] = generate_rebalance_schedule(
        total_periods=T,
        lookback_window=LOOKBACK,
        freq=FREQ,
    )

import numpy as np

def process_combo(row_data):
    """å•ä¸ªç»„åˆçš„å¤„ç†å‡½æ•°"""
    # ç¦ç”¨ GC ä»¥æå‡æ€§èƒ½ï¼ˆå­è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸçŸ­ï¼Œæ— éœ€ GCï¼‰
    gc.disable()
    
    combo_str = row_data['combo']
    
    # ä»å…¨å±€å˜é‡è·å–æ•°æ®
    data_feeds = _shared_data['data_feeds']
    std_factors = _shared_data['std_factors']
    timing_series = _shared_data['timing_series']
    etf_codes = _shared_data['etf_codes']
    rebalance_schedule = _shared_data['rebalance_schedule']
    
    factors = [f.strip() for f in combo_str.split(" + ")]
    dates = timing_series.index

    # æ„é€ å¾—åˆ†çŸ©é˜µ (ä½¿ç”¨ DataFrame.add ä¿æŒ NaN å¤„ç†ä¸€è‡´æ€§)
    # âœ… ä¸ full_vec_bt_comparison.py ä¿æŒä¸€è‡´ï¼šfill_value=0 é¿å… NaN ä¼ æ’­
    combined_score_df = pd.DataFrame(0.0, index=dates, columns=etf_codes)
    for f in factors:
        combined_score_df = combined_score_df.add(std_factors[f], fill_value=0)

    # è¿è¡Œå›æµ‹
    bt_return, margin_failures = run_bt_backtest(
        combined_score_df, 
        timing_series, 
        etf_codes, 
        data_feeds,
        rebalance_schedule
    )
    
    return {
        "combo": combo_str,
        "bt_return": bt_return,
        "bt_margin_failures": margin_failures,
    }

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ BT å›æµ‹ (æ”¯æŒ Top-K ç­›é€‰)")
    parser.add_argument("--topk", type=int, default=None, help="ä»…å›æµ‹ VEC æ”¶ç›Šæœ€é«˜çš„ Top-K ä¸ªç»„åˆ")
    parser.add_argument("--sort-by", type=str, default="total_return", help="æ’åºå­—æ®µ (é»˜è®¤: total_return)")
    args = parser.parse_args()

    print("=" * 80)
    print("æ‰¹é‡ BT å›æµ‹ï¼šå¤šè¿›ç¨‹å¹¶è¡Œç‰ˆ (Ryzen 9950X Optimized)")
    if args.topk:
        print(f"ğŸ¯ ç­›é€‰æ¨¡å¼: Top {args.topk} (æŒ‰ {args.sort_by} æ’åº)")
    else:
        print("âš™ï¸ å…¨é‡æ¨¡å¼: å›æµ‹æ‰€æœ‰ç»„åˆ")
    print("=" * 80)

    # 1. åŠ è½½ WFO ç»“æœ
    wfo_dirs = sorted((ROOT / "results").glob("unified_wfo_*"))
    if not wfo_dirs:
        print("âŒ æœªæ‰¾åˆ° WFO ç»“æœç›®å½•")
        return
    latest_wfo = wfo_dirs[-1]
    combos_path = latest_wfo / "all_combos.parquet"
    if not combos_path.exists():
        print(f"âŒ æœªæ‰¾åˆ° {combos_path}")
        return

    df_combos = pd.read_parquet(combos_path)
    print(f"âœ… åŠ è½½ WFO ç»“æœï¼š{len(df_combos)} ä¸ªç»„åˆ")

    # ç­›é€‰ Top-K
    if args.topk:
        if args.sort_by not in df_combos.columns:
            print(f"âš ï¸ è­¦å‘Š: åˆ— {args.sort_by} ä¸å­˜åœ¨ï¼Œæ— æ³•æ’åºã€‚å°†ä½¿ç”¨åŸå§‹é¡ºåºã€‚")
        else:
            df_combos = df_combos.sort_values(args.sort_by, ascending=False).head(args.topk)
            print(f"âœ… å·²ç­›é€‰ Top {len(df_combos)} ç»„åˆ (Min {args.sort_by}: {df_combos[args.sort_by].min():.4f})")

    # 2. åŠ è½½æ•°æ®
    config_path = ROOT / "configs/combo_wfo_config.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    loader = DataLoader(
        data_dir=config["data"].get("data_dir"),
        cache_dir=config["data"].get("cache_dir"),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config["data"]["symbols"],
        start_date=config["data"]["start_date"],
        end_date=config["data"]["end_date"],
    )

    # 3. è®¡ç®—å› å­
    factor_lib = PreciseFactorLibrary()
    raw_factors_df = factor_lib.compute_all_factors(ohlcv)

    factor_names_list = raw_factors_df.columns.get_level_values(0).unique().tolist()
    raw_factors = {fname: raw_factors_df[fname] for fname in factor_names_list}

    processor = CrossSectionProcessor(verbose=False)
    std_factors = processor.process_all_factors(raw_factors)

    factor_names = sorted(std_factors.keys())
    first_factor = std_factors[factor_names[0]]
    dates = first_factor.index
    etf_codes = first_factor.columns.tolist()

    timing_module = LightTimingModule()
    timing_series_raw = timing_module.compute_position_ratios(ohlcv["close"])
    # âœ… ä½¿ç”¨ shift_timing_signal åš t-1 shiftï¼Œé¿å…æœªæ¥å‡½æ•°
    timing_arr_shifted = shift_timing_signal(timing_series_raw.reindex(dates).fillna(1.0).values)
    timing_series = pd.Series(timing_arr_shifted, index=dates)

    # å‡†å¤‡ data feeds
    data_feeds = {}
    for ticker in etf_codes:
        df = pd.DataFrame(
            {
                "open": ohlcv["open"][ticker],
                "high": ohlcv["high"][ticker],
                "low": ohlcv["low"][ticker],
                "close": ohlcv["close"][ticker],
                "volume": ohlcv["volume"][ticker],
            }
        )
        df = df.reindex(dates)
        df = df.ffill().fillna(0.01)
        data_feeds[ticker] = df

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼š{len(dates)} å¤© Ã— {len(etf_codes)} åª ETF")

    # 4. å¤šè¿›ç¨‹å›æµ‹
    # Ryzen 9950X æœ‰ 16 æ ¸ 32 çº¿ç¨‹ã€‚ä¿ç•™ä¸€ç‚¹ä½™é‡ï¼Œä½¿ç”¨ 28-30 ä¸ªè¿›ç¨‹ã€‚
    num_workers = 30
    print(f"ğŸš€ å¯åŠ¨å¤šè¿›ç¨‹å›æµ‹ (Workers: {num_workers})...")

    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ (è½¬æ¢ä¸º dict åˆ—è¡¨ä»¥ä¾¿ä¼ é€’)
    tasks = [row.to_dict() for _, row in df_combos.iterrows()]

    print(f"ğŸš€ å‡†å¤‡å›æµ‹ {len(tasks)} ä¸ªç»„åˆ...")

    results = []
    with mp.Pool(processes=num_workers, initializer=init_worker, initargs=(data_feeds, std_factors, timing_series, etf_codes)) as pool:
        # ä½¿ç”¨ imap_unordered è·å–å®æ—¶è¿›åº¦
        for res in tqdm(pool.imap(process_combo, tasks), total=len(tasks), desc="BT å¹¶è¡Œå›æµ‹"):
            results.append(res)

    # 5. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    suffix = f"_top{args.topk}" if args.topk else "_full"
    output_dir = ROOT / "results" / f"bt_backtest{suffix}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)

    df_results = pd.DataFrame(results)
    df_results.to_parquet(output_dir / "bt_results.parquet", index=False)
    df_results.to_csv(output_dir / "bt_results.csv", index=False)

    print(f"\nâœ… BT æ‰¹é‡å›æµ‹å®Œæˆ")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   ç»„åˆæ•°: {len(df_results)}")
    print(f"   Margin å¤±è´¥æ€»æ•°: {df_results['bt_margin_failures'].sum()}")



if __name__ == "__main__":
    main()
