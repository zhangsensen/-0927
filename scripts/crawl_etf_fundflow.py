#!/usr/bin/env python3
"""
ETFå¢é‡æ•°æ®çˆ¬è™« - èµ„é‡‘æµå‘(å¤§å•) + ä»½é¢å˜åŒ–

æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ
1. èµ„é‡‘æµå‘ï¼šè¶…å¤§å•/å¤§å•/ä¸­å•/å°å• å‡€æµå…¥ï¼ˆæ—¥çº§ï¼‰
2. ä»½é¢å˜åŒ–ï¼šETFä»½é¢å‡€å¢å‡ï¼ˆæ—¥çº§ï¼‰

ç”¨é€”ï¼šä¸ºå› å­åº“æä¾›éOHLCVç»´åº¦çš„å¢é‡æ•°æ®
"""

import json
import time
import logging
import sys
from pathlib import Path

import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# ETFå¸‚åœºä»£ç æ˜ å°„: 1=ä¸Šæµ·(51xxxx, 58xxxx), 0=æ·±åœ³(15xxxx)
def get_market_code(etf_code: str) -> str:
    if etf_code.startswith("15"):
        return "0"  # æ·±åœ³
    return "1"  # ä¸Šæµ·


class ETFFundFlowCrawler:
    """ä¸œè´¢ETFèµ„é‡‘æµå‘çˆ¬è™«"""

    FUND_FLOW_URL = "https://push2his.eastmoney.com/api/qt/stock/fflow/daykline/get"
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120.0.0.0",
        "Referer": "https://data.eastmoney.com/",
    }

    def __init__(self, output_dir: str = None):
        self.output_dir = Path(output_dir or ROOT / "raw" / "ETF" / "moneyflow")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.session = requests.Session()
        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503])
        self.session.mount("https://", HTTPAdapter(max_retries=retry))

    def crawl_fund_flow(self, etf_code: str, limit: int = 2000) -> pd.DataFrame:
        """
        çˆ¬å–å•ä¸ªETFçš„æ—¥çº§èµ„é‡‘æµå‘æ•°æ®

        è¿”å›åˆ—: date, main_net, xl_net, l_net, m_net, s_net
        - main_net: ä¸»åŠ›å‡€æµå…¥ (è¶…å¤§å•+å¤§å•)
        - xl_net: è¶…å¤§å•å‡€æµå…¥
        - l_net: å¤§å•å‡€æµå…¥
        - m_net: ä¸­å•å‡€æµå…¥
        - s_net: å°å•å‡€æµå…¥
        """
        market = get_market_code(etf_code)
        params = {
            "secid": f"{market}.{etf_code}",
            "fields1": "f1,f2,f3,f7",
            "fields2": "f51,f52,f53,f54,f55,f56,f57,f58,f59,f60,f61,f62,f63,f64,f65",
            "klt": "101",  # æ—¥çº¿
            "lmt": str(limit),
        }

        try:
            resp = self.session.get(
                self.FUND_FLOW_URL, params=params, headers=self.HEADERS, timeout=15
            )
            data = resp.json()

            if not data.get("data") or not data["data"].get("klines"):
                logger.warning(f"  {etf_code}: æ— èµ„é‡‘æµå‘æ•°æ®")
                return pd.DataFrame()

            rows = []
            for line in data["data"]["klines"]:
                parts = line.split(",")
                if len(parts) >= 13:
                    rows.append({
                        "date": parts[0],
                        "main_net": float(parts[1]),     # ä¸»åŠ›å‡€æµå…¥
                        "main_net_pct": float(parts[6]),  # ä¸»åŠ›å‡€å æ¯”%
                        "xl_net": float(parts[5]),        # è¶…å¤§å•å‡€æµå…¥
                        "xl_net_pct": float(parts[10]),   # è¶…å¤§å•å‡€å æ¯”%
                        "l_net": float(parts[3]),         # å¤§å•å‡€æµå…¥
                        "l_net_pct": float(parts[8]),     # å¤§å•å‡€å æ¯”%
                        "m_net": float(parts[11]),        # ä¸­å•å‡€æµå…¥
                        "m_net_pct": float(parts[12]),    # ä¸­å•å‡€å æ¯”%
                        "s_net": float(parts[9]),         # å°å•å‡€æµå…¥
                        "s_net_pct": float(parts[14]) if len(parts) > 14 else 0,
                    })

            df = pd.DataFrame(rows)
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date").reset_index(drop=True)
            return df

        except Exception as e:
            logger.error(f"  {etf_code}: è¯·æ±‚å¤±è´¥ - {e}")
            return pd.DataFrame()

    def crawl_share_change(self, etf_code: str) -> pd.DataFrame:
        """
        ä»ä¸œè´¢è¯¦æƒ…é¡µçˆ¬å–ETFä»½é¢å˜åŠ¨æ•°æ®
        """
        try:
            from etf_data.crawlers.sources.eastmoney_detail_crawler import EastmoneyDetailCrawler
            crawler = EastmoneyDetailCrawler()
            df = crawler.get_share_positions(etf_code)
            if df is not None and not df.empty:
                return df
        except Exception as e:
            logger.warning(f"  {etf_code}: ä»½é¢æ•°æ®çˆ¬å–å¤±è´¥ - {e}")
        return pd.DataFrame()

    def batch_crawl(self, etf_codes: list, sleep_sec: float = 0.5):
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰ETF"""
        total = len(etf_codes)
        success_flow = 0
        success_share = 0

        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– {total} ä¸ªETFçš„èµ„é‡‘æµå‘å’Œä»½é¢æ•°æ®")

        for i, code in enumerate(etf_codes, 1):
            logger.info(f"[{i}/{total}] {code}")

            # 1. èµ„é‡‘æµå‘
            df_flow = self.crawl_fund_flow(code, limit=2000)
            if not df_flow.empty:
                out_path = self.output_dir / f"fund_flow_{code}.parquet"
                df_flow.to_parquet(out_path, index=False)
                logger.info(f"  âœ… èµ„é‡‘æµå‘: {len(df_flow)}å¤© â†’ {out_path.name}")
                success_flow += 1
            else:
                logger.info(f"  âŒ èµ„é‡‘æµå‘: æ— æ•°æ®")

            # 2. ä»½é¢å˜åŒ–
            df_share = self.crawl_share_change(code)
            if not df_share.empty:
                share_dir = ROOT / "raw" / "ETF" / "shares"
                share_dir.mkdir(parents=True, exist_ok=True)
                out_path = share_dir / f"share_change_{code}.parquet"
                df_share.to_parquet(out_path, index=False)
                logger.info(f"  âœ… ä»½é¢æ•°æ®: {len(df_share)}æ¡ â†’ {out_path.name}")
                success_share += 1
            else:
                logger.info(f"  âŒ ä»½é¢æ•°æ®: æ— æ•°æ®")

            time.sleep(sleep_sec)

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“‹ çˆ¬å–å®Œæˆ:")
        logger.info(f"  èµ„é‡‘æµå‘: {success_flow}/{total} æˆåŠŸ")
        logger.info(f"  ä»½é¢å˜åŒ–: {success_share}/{total} æˆåŠŸ")


if __name__ == "__main__":
    import yaml

    # ä»é…ç½®è¯»å–ETFåˆ—è¡¨
    config_path = ROOT / "configs" / "combo_wfo_config.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    etf_codes = config["data"]["symbols"]
    logger.info(f"ETFåˆ—è¡¨: {len(etf_codes)} ä¸ª")

    crawler = ETFFundFlowCrawler()

    # å…ˆæµ‹è¯•ä¸€ä¸ª
    logger.info("=" * 60)
    logger.info("ğŸ§ª å…ˆæµ‹è¯• 510300 (Limit=2000)...")
    df = crawler.crawl_fund_flow("510300", limit=2000)
    if not df.empty:
        logger.info(f"âœ… æµ‹è¯•æˆåŠŸ! å…± {len(df)} å¤©æ•°æ®")
        logger.info(f"  åˆ—: {df.columns.tolist()}")
        logger.info(f"  æ—¥æœŸèŒƒå›´: {df['date'].min()} â†’ {df['date'].max()}")
        logger.info(f"  æœ€è¿‘5å¤©:")
        print(df.head(5).to_string(index=False)) # æ‰“å°å‰5å¤©çœ‹çœ‹å†å²å¤šé•¿
        print("...")
        print(df.tail(5).to_string(index=False))
        print()

        # å…¨é‡çˆ¬å–
        logger.info("=" * 60)
        crawler.batch_crawl(etf_codes, sleep_sec=0.3)
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œæ£€æŸ¥ç½‘ç»œ")
