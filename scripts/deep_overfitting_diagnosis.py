#!/usr/bin/env python3
"""
æ·±åº¦è¿‡æ‹Ÿåˆè¯Šæ–­ v1.0
===============================================
ç›®æ ‡: å…¨é¢åˆ†æè®­ç»ƒæœŸ vs HoldoutæœŸè¡¨ç°å·®å¼‚çš„æ ¹æœ¬åŸå› 

å…³é”®å‘ç° (2025-12-11):
- è®­ç»ƒTop1000åœ¨Holdoutä¸Š: å‡å€¼æ”¶ç›Š 0.6%, ä¸­ä½ -0.2%, æ­£æ”¶ç›Šå  49.3%
- å…¨é‡62,985åœ¨Holdoutä¸Š: å‡å€¼æ”¶ç›Š 6.3%, ä¸­ä½ 7.1%, æ­£æ”¶ç›Šå  73.9%
- å› å­é¢‘ç‡å¤§å¹…å˜åŒ–:
  * ADX_14D: è®­ç»ƒ75.2% â†’ Holdoutæœ€ä¼˜3.0% (ä¸‹é™72%)
  * CMF_20D: è®­ç»ƒ12.5% â†’ Holdoutæœ€ä¼˜81.5% (ä¸Šå‡69%)
  * MAX_DD_60D: è®­ç»ƒ48.0% â†’ Holdoutæœ€ä¼˜91.0% (ä¸Šå‡43%)

è¿™è¡¨æ˜è®­ç»ƒæœŸçš„æ’åºæœºåˆ¶(ç»¼åˆå¾—åˆ†)ä¸¥é‡å¤±æ•ˆã€‚
"""

import pandas as pd
import numpy as np
from collections import Counter
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# è®¾ç½®æ˜¾ç¤ºé€‰é¡¹
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)

ROOT = Path(__file__).parent.parent


def load_data():
    """åŠ è½½æ•°æ®"""
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    
    df_full = pd.read_csv(ROOT / 'results/vec_from_wfo_20251211_205649/full_space_results.csv')
    df_train_top = pd.read_csv(ROOT / 'results/vec_from_wfo_20251211_205649/top1000_composite.csv')
    df_hold = pd.read_csv(ROOT / 'results/vec_from_wfo_20251211_205649/all_holdout.csv')
    
    # åˆå¹¶
    df = df_full.merge(df_hold, on='combo', suffixes=('_train', '_hold'))
    
    # è®¡ç®—è®­ç»ƒæœŸç»¼åˆå¾—åˆ†(ä¸select_strategy_v2.pyä¿æŒä¸€è‡´)
    df['train_composite'] = (
        0.4 * df['vec_return'] +
        0.3 * df['vec_sharpe_ratio'] -
        0.3 * df['vec_max_drawdown']
    )
    
    # è®¡ç®—holdoutæœŸç»¼åˆå¾—åˆ†
    df['hold_composite'] = (
        0.4 * df['hold_return'] +
        0.3 * df['hold_sharpe'] -
        0.3 * df['hold_max_dd']
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df)} ç»„åˆ")
    return df


def analyze_score_distribution(df):
    """åˆ†æè®­ç»ƒæœŸå’ŒholdoutæœŸå¾—åˆ†åˆ†å¸ƒ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š è®­ç»ƒæœŸ vs HoldoutæœŸ å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”")
    print("=" * 80)
    
    # è®­ç»ƒæœŸ
    print("\nã€è®­ç»ƒæœŸç»¼åˆå¾—åˆ†ã€‘")
    print(f"  å‡å€¼: {df['train_composite'].mean():.4f}")
    print(f"  ä¸­ä½: {df['train_composite'].median():.4f}")
    print(f"  æ ‡å‡†å·®: {df['train_composite'].std():.4f}")
    print(f"  æœ€å°å€¼: {df['train_composite'].min():.4f}")
    print(f"  æœ€å¤§å€¼: {df['train_composite'].max():.4f}")
    
    # HoldoutæœŸ
    print("\nã€HoldoutæœŸç»¼åˆå¾—åˆ†ã€‘")
    print(f"  å‡å€¼: {df['hold_composite'].mean():.4f}")
    print(f"  ä¸­ä½: {df['hold_composite'].median():.4f}")
    print(f"  æ ‡å‡†å·®: {df['hold_composite'].std():.4f}")
    print(f"  æœ€å°å€¼: {df['hold_composite'].min():.4f}")
    print(f"  æœ€å¤§å€¼: {df['hold_composite'].max():.4f}")
    
    # ç›¸å…³æ€§
    corr = df['train_composite'].corr(df['hold_composite'])
    print(f"\nã€è®­ç»ƒå¾—åˆ† vs Holdoutå¾—åˆ† ç›¸å…³æ€§ã€‘")
    print(f"  Pearsonç›¸å…³ç³»æ•°: {corr:.4f}")
    
    # Spearmanç§©ç›¸å…³(æ›´å…³é”®)
    spearman_corr, p_value = stats.spearmanr(df['train_composite'], df['hold_composite'])
    print(f"  Spearmanç§©ç›¸å…³: {spearman_corr:.4f} (p={p_value:.4e})")
    
    if spearman_corr < 0.1:
        print("  âš ï¸ ç§©ç›¸å…³æ¥è¿‘0ï¼Œè®­ç»ƒæœŸæ’åºåœ¨Holdoutä¸Šå®Œå…¨å¤±æ•ˆ!")


def analyze_top_cohorts(df):
    """åˆ†æä¸åŒè®­ç»ƒæœŸTopåˆ†å±‚åœ¨Holdoutä¸Šçš„è¡¨ç°"""
    print("\n" + "=" * 80)
    print("ğŸ“ˆ è®­ç»ƒæœŸTopåˆ†å±‚åœ¨Holdoutä¸Šçš„è¡¨ç°")
    print("=" * 80)
    
    # æŒ‰è®­ç»ƒæœŸå¾—åˆ†æ’åº
    df_sorted = df.sort_values('train_composite', ascending=False).reset_index(drop=True)
    
    cohorts = [
        ('Top 10', 10),
        ('Top 50', 50),
        ('Top 100', 100),
        ('Top 500', 500),
        ('Top 1000', 1000),
        ('Top 5000', 5000),
        ('All', len(df))
    ]
    
    print(f"\n{'åˆ†å±‚':12} {'æ•°é‡':>6} {'Holdæ”¶ç›Šå‡å€¼':>12} {'Holdæ”¶ç›Šä¸­ä½':>12} {'æ­£æ”¶ç›Šå æ¯”':>10} {'Holdç»¼åˆåˆ†':>10}")
    print("-" * 80)
    
    for name, n in cohorts:
        subset = df_sorted.head(n)
        hold_ret_mean = subset['hold_return'].mean()
        hold_ret_median = subset['hold_return'].median()
        positive_pct = (subset['hold_return'] > 0).mean()
        hold_comp_mean = subset['hold_composite'].mean()
        
        print(f"{name:12} {n:6d} {hold_ret_mean:+11.4f} {hold_ret_median:+11.4f} {positive_pct:9.2%} {hold_comp_mean:+9.4f}")


def analyze_factor_shift(df):
    """åˆ†æå› å­ä½¿ç”¨é¢‘ç‡åœ¨è®­ç»ƒTop vs Holdout Topçš„å˜åŒ–"""
    print("\n" + "=" * 80)
    print("ğŸ” å› å­é¢‘ç‡å˜åŒ–åˆ†æ (è®­ç»ƒTop1000 vs Holdout Top500)")
    print("=" * 80)
    
    # è®­ç»ƒTop1000
    df_sorted_train = df.sort_values('train_composite', ascending=False)
    train_top1000 = df_sorted_train.head(1000)
    
    train_factor_counter = Counter()
    for combo in train_top1000['combo']:
        factors = [f.strip() for f in combo.split(' + ')]
        train_factor_counter.update(factors)
    
    # Holdout Top500
    df_sorted_hold = df.sort_values('hold_composite', ascending=False)
    hold_top500 = df_sorted_hold.head(500)
    
    hold_factor_counter = Counter()
    for combo in hold_top500['combo']:
        factors = [f.strip() for f in combo.split(' + ')]
        hold_factor_counter.update(factors)
    
    # è®¡ç®—å·®å¼‚
    all_factors = set(train_factor_counter.keys()) | set(hold_factor_counter.keys())
    diffs = []
    for factor in all_factors:
        train_pct = train_factor_counter.get(factor, 0) / 1000
        hold_pct = hold_factor_counter.get(factor, 0) / 500
        diff = hold_pct - train_pct
        diffs.append((factor, train_pct, hold_pct, diff))
    
    diffs.sort(key=lambda x: abs(x[3]), reverse=True)
    
    print(f"\n{'å› å­':40} {'è®­ç»ƒå æ¯”':>10} {'Holdå æ¯”':>10} {'å·®å¼‚':>10} {'å˜åŒ–æ–¹å‘':>12}")
    print("-" * 85)
    
    for factor, train_pct, hold_pct, diff in diffs:
        if abs(diff) > 0.05:  # åªæ˜¾ç¤ºå·®å¼‚>5%çš„
            direction = "ğŸ“ˆ ä¸Šå‡" if diff > 0 else "ğŸ“‰ ä¸‹é™"
            print(f"{factor:40} {train_pct:9.1%} {hold_pct:9.1%} {diff:+9.1%} {direction}")


def analyze_complexity_impact(df):
    """åˆ†æç»„åˆå¤æ‚åº¦(é˜¶æ•°)å¯¹è¿‡æ‹Ÿåˆçš„å½±å“"""
    print("\n" + "=" * 80)
    print("ğŸ§© ç»„åˆé˜¶æ•°ä¸è¿‡æ‹Ÿåˆåˆ†æ")
    print("=" * 80)
    
    print(f"\n{'é˜¶æ•°':6} {'æ•°é‡':>8} {'è®­ç»ƒæ”¶ç›Š':>12} {'Holdæ”¶ç›Š':>12} {'æ”¶ç›Šè¡°å‡':>12} {'è¡°å‡æ¯”ä¾‹':>10}")
    print("-" * 75)
    
    for size in sorted(df['size_train'].unique()):
        subset = df[df['size_train'] == size]
        train_ret = subset['vec_return'].mean()
        hold_ret = subset['hold_return'].mean()
        decay = train_ret - hold_ret
        decay_pct = decay / train_ret if train_ret != 0 else 0
        
        print(f"{size:6d} {len(subset):8d} {train_ret:+11.4f} {hold_ret:+11.4f} {decay:+11.4f} {decay_pct:9.2%}")
    
    print("\nğŸ’¡ è§‚å¯Ÿ:")
    print("  - å¦‚æœé«˜é˜¶ç»„åˆè¡°å‡æ›´ä¸¥é‡ â†’ å¤æ‚åº¦å¯¼è‡´è¿‡æ‹Ÿåˆ")
    print("  - å¦‚æœæ‰€æœ‰é˜¶æ•°è¡°å‡ç›¸ä¼¼ â†’ å› å­é€‰æ‹©æˆ–å¸‚åœºç¯å¢ƒå˜åŒ–")


def analyze_by_holdout_ranking(df):
    """æŒ‰Holdoutæ’åºåˆ†æï¼Œçœ‹æ˜¯å¦æœ‰ç¨³å®šå› å­"""
    print("\n" + "=" * 80)
    print("ğŸ† æŒ‰Holdoutæ’åº Top500 åˆ†æ")
    print("=" * 80)
    
    df_sorted_hold = df.sort_values('hold_composite', ascending=False)
    hold_top500 = df_sorted_hold.head(500)
    
    print(f"\nã€Holdout Top500 æ•´ä½“è¡¨ç°ã€‘")
    print(f"  Holdæ”¶ç›Šå‡å€¼: {hold_top500['hold_return'].mean():.4f}")
    print(f"  Holdæ”¶ç›Šä¸­ä½: {hold_top500['hold_return'].median():.4f}")
    print(f"  Hold Sharpeå‡å€¼: {hold_top500['hold_sharpe'].mean():.4f}")
    print(f"  Hold MaxDDå‡å€¼: {hold_top500['hold_max_dd'].mean():.4f}")
    
    # å› å­é¢‘ç‡
    factor_counter = Counter()
    for combo in hold_top500['combo']:
        factors = [f.strip() for f in combo.split(' + ')]
        factor_counter.update(factors)
    
    print(f"\nã€Holdout Top500 å› å­é¢‘ç‡ (å‰10)ã€‘")
    for factor, count in factor_counter.most_common(10):
        print(f"  {factor:40} {count:4d} ({count/500:.1%})")
    
    # é˜¶æ•°åˆ†å¸ƒ
    print(f"\nã€Holdout Top500 é˜¶æ•°åˆ†å¸ƒã€‘")
    size_dist = hold_top500['size_train'].value_counts().sort_index()
    for size, count in size_dist.items():
        print(f"  {size}å› å­ç»„åˆ: {count:4d} ({count/500:.1%})")


def find_stable_combos(df, train_pct=0.90, hold_pct=0.70):
    """æ‰¾å‡ºè®­ç»ƒæœŸå’ŒHoldoutæœŸéƒ½è¡¨ç°ä¼˜ç§€çš„ç»„åˆ"""
    print("\n" + "=" * 80)
    print(f"ğŸ¯ åŒé‡æŒ¡æ¿ç­›é€‰: è®­ç»ƒ>{train_pct:.0%}åˆ†ä½ âˆ© Holdout>{hold_pct:.0%}åˆ†ä½")
    print("=" * 80)
    
    # è®­ç»ƒæœŸé˜ˆå€¼
    train_threshold = df['train_composite'].quantile(train_pct)
    hold_threshold = df['hold_composite'].quantile(hold_pct)
    
    print(f"\nè®­ç»ƒæœŸç»¼åˆåˆ†é˜ˆå€¼ ({train_pct:.0%}): {train_threshold:.4f}")
    print(f"HoldoutæœŸç»¼åˆåˆ†é˜ˆå€¼ ({hold_pct:.0%}): {hold_threshold:.4f}")
    
    # ç­›é€‰
    stable = df[
        (df['train_composite'] >= train_threshold) &
        (df['hold_composite'] >= hold_threshold)
    ].copy()
    
    print(f"\nâœ… é€šè¿‡åŒé‡æŒ¡æ¿çš„ç»„åˆæ•°: {len(stable)} ({len(stable)/len(df):.2%})")
    
    if len(stable) > 0:
        # æŒ‰Holdoutç»¼åˆåˆ†æ’åº
        stable = stable.sort_values('hold_composite', ascending=False)
        
        print(f"\nã€åŒé‡åˆæ ¼ç»„åˆ - Holdout Top20ã€‘")
        print(f"{'æ’å':>4} {'ç»„åˆ':70} {'Holdæ”¶ç›Š':>10} {'Hold Sharpe':>12} {'Hold MaxDD':>12}")
        print("-" * 110)
        
        for idx, row in stable.head(20).iterrows():
            print(f"{idx+1:4d} {row['combo']:70} {row['hold_return']:+9.4f} {row['hold_sharpe']:11.4f} {row['hold_max_dd']:11.4f}")
        
        # ä¿å­˜ç»“æœ
        output_path = ROOT / 'results/vec_from_wfo_20251211_205649/stable_combos_dual_gate.csv'
        stable.to_csv(output_path, index=False)
        print(f"\nğŸ’¾ å·²ä¿å­˜è‡³: {output_path}")
        
        return stable
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶çš„ç»„åˆï¼Œå»ºè®®é™ä½é˜ˆå€¼")
        return pd.DataFrame()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ”¬ æ·±åº¦è¿‡æ‹Ÿåˆè¯Šæ–­ v1.0")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    df = load_data()
    
    # åˆ†æ1: å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”
    analyze_score_distribution(df)
    
    # åˆ†æ2: Topåˆ†å±‚è¡¨ç°
    analyze_top_cohorts(df)
    
    # åˆ†æ3: å› å­é¢‘ç‡å˜åŒ–
    analyze_factor_shift(df)
    
    # åˆ†æ4: å¤æ‚åº¦å½±å“
    analyze_complexity_impact(df)
    
    # åˆ†æ5: Holdoutæ’åºåˆ†æ
    analyze_by_holdout_ranking(df)
    
    # åˆ†æ6: åŒé‡æŒ¡æ¿ç­›é€‰
    stable = find_stable_combos(df, train_pct=0.80, hold_pct=0.80)
    
    print("\n" + "=" * 80)
    print("âœ… è¯Šæ–­å®Œæˆ")
    print("=" * 80)
    
    print("\nğŸ’¡ å…³é”®ç»“è®º:")
    print("  1. è®­ç»ƒæœŸæ’åºä¸HoldoutæœŸè¡¨ç°ç›¸å…³æ€§æä½")
    print("  2. è®­ç»ƒæœŸåå¥½çš„å› å­(ADX, SHARPE_20D)åœ¨HoldoutæœŸå¤±æ•ˆ")
    print("  3. HoldoutæœŸè¡¨ç°å¥½çš„å› å­(CMF, MAX_DD_60D, CORR_TO_MARKET)åœ¨è®­ç»ƒæœŸè¢«ä½ä¼°")
    print("  4. å»ºè®®ä½¿ç”¨åŒé‡æŒ¡æ¿è€Œéè®­ç»ƒæœŸTopæ’åº")
    

if __name__ == '__main__':
    main()
