#!/usr/bin/env python3
"""
é¢‘ç‡ Ã— æŒä»“æ•° è½»é‡ç½‘æ ¼æ‰«æ
=================================
ç›®æ ‡ï¼šå¯¹æœ€æ–°å›æµ‹ç»“æœä¸­ Sharpe æ’åå‰ TopK çš„ç»„åˆï¼Œæµ‹è¯•ä¸€ç»„å¤‡é€‰è°ƒä»“é¢‘ç‡ä¸æŒä»“æ•°çš„ç¬›å¡å°”ç§¯ï¼Œè¯„ä¼°é£é™©æ”¶ç›Šæ”¹å–„ç©ºé—´ã€‚

ç‰¹ç‚¹ï¼š
- ä»…ä½¿ç”¨å·²æœ‰çš„å› å­ä¸å›æµ‹é€»è¾‘ï¼ˆè°ƒç”¨ backtest_no_lookaheadï¼‰
- ä¸ä¿®æ”¹æ ¸å¿ƒæ¨¡å—ï¼›å¯å®‰å…¨åˆ é™¤ï¼Œä¸å½±å“ä¸»æµç¨‹
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ TopK, é¢‘ç‡é›†åˆ, æŒä»“æ•°é›†åˆï¼Œå¹¶è¡Œæ ¸æ•°
- è¾“å‡ºï¼šCSV + Markdown æ±‡æ€»æŠ¥å‘Š

ç”¨æ³•ç¤ºä¾‹ï¼š
    python scripts/run_freq_pos_grid.py \
        --topk 200 \
        --freqs 5,6,7,8,10 \
        --positions 4,5,6 \
        --jobs 8 \
        --output-dir results/grid_scan

å¯é€‰ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ RB_PROFILE_BACKTEST=1 ä»¥è®°å½•æ€§èƒ½æ‘˜è¦ã€‚
"""
from __future__ import annotations
import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import yaml
from joblib import Parallel, delayed

# å›æµ‹æ ¸å¿ƒå‡½æ•°
sys.path.append(str(Path(__file__).resolve().parent.parent / 'etf_rotation_v2_breadth' / 'real_backtest'))
from etf_rotation_v2_breadth.real_backtest.run_production_backtest import backtest_no_lookahead  # type: ignore

# å› å­ä¸æ•°æ®åŠ è½½ä¾èµ–
sys.path.append(str(Path(__file__).resolve().parent.parent / 'etf_rotation_v2_breadth'))
from etf_rotation_v2_breadth.core.cross_section_processor import CrossSectionProcessor  # type: ignore
from etf_rotation_v2_breadth.core.data_loader import DataLoader  # type: ignore
from etf_rotation_v2_breadth.core.precise_factor_library_v2 import PreciseFactorLibrary  # type: ignore


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description='é¢‘ç‡Ã—æŒä»“æ•°ç½‘æ ¼æ‰«æ')
    p.add_argument('--topk', type=int, default=200, help='é€‰å– Sharpe TopK ç»„åˆè¿›è¡Œæ‰«æ')
    p.add_argument('--freqs', type=str, default='5,6,7,8,10', help='è°ƒä»“é¢‘ç‡åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    p.add_argument('--positions', type=str, default='4,5,6', help='æŒä»“æ•°é‡åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    p.add_argument('--jobs', type=int, default=8, help='å¹¶è¡Œæ ¸æ•°')
    p.add_argument('--output-dir', type=str, default='results/grid_scan', help='è¾“å‡ºç›®å½•æ ¹è·¯å¾„')
    p.add_argument('--latest-backtest-dir', type=str, default='etf_rotation_v2_breadth/results_combo_wfo', help='æœç´¢æœ€æ–°å›æµ‹CSVçš„ç›®å½•')
    p.add_argument('--latest-backtest-pattern', type=str, default='top12597_backtest_by_ic_*_*.csv', help='åŒ¹é…æœ€æ–°ç»“æœçš„æ–‡ä»¶æ¨¡å¼')
    p.add_argument('--random-sample', type=int, default=0, help='å¦‚>0åˆ™å¯¹ TopK å†…éšæœºæŠ½æ ·è¯¥æ•°é‡ä»¥åŠ é€Ÿ')
    p.add_argument('--seed', type=int, default=42, help='éšæœºæŠ½æ ·ç§å­')
    return p.parse_args()


def find_latest_backtest_file(root_dir: str, pattern: str) -> Path:
    root = Path(root_dir)
    candidates = sorted(root.glob(f'**/{pattern}'))
    if not candidates:
        raise FileNotFoundError(f'æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {pattern} in {root_dir}')
    # ä»¥ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°ï¼‰
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return candidates[0]


def load_base_results(backtest_file: Path) -> pd.DataFrame:
    df = pd.read_csv(backtest_file)
    # å…¼å®¹åˆ—å annual_ret / annual_return å·®å¼‚
    if 'annual_ret' not in df.columns and 'annual_return' in df.columns:
        df = df.rename(columns={'annual_return': 'annual_ret'})
    return df


def select_topk(df: pd.DataFrame, topk: int, sample: int = 0, seed: int = 42) -> pd.DataFrame:
    df_sorted = df.sort_values('sharpe', ascending=False).head(topk).copy()
    if sample > 0 and sample < len(df_sorted):
        rng = np.random.default_rng(seed)
        idx = rng.choice(df_sorted.index, size=sample, replace=False)
        df_sorted = df_sorted.loc[idx].copy()
    return df_sorted.reset_index(drop=True)


def load_data_and_factors() -> tuple[dict, np.ndarray, np.ndarray, list[str], list[str]]:
    with open('etf_rotation_v2_breadth/configs/combo_wfo_config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    loader = DataLoader(
        data_dir=config['data'].get('data_dir'),
        cache_dir=config['data'].get('cache_dir'),
    )
    ohlcv = loader.load_ohlcv(
        etf_codes=config['data']['symbols'],
        start_date=config['data']['start_date'],
        end_date=config['data']['end_date'],
        use_cache=True,
    )
    factor_lib = PreciseFactorLibrary()
    factors_df = factor_lib.compute_all_factors(prices=ohlcv)
    factors_dict = {name: factors_df[name] for name in factor_lib.list_factors()}
    processor = CrossSectionProcessor(
        lower_percentile=config['cross_section']['winsorize_lower'] * 100,
        upper_percentile=config['cross_section']['winsorize_upper'] * 100,
        verbose=False,
    )
    standardized_factors = processor.process_all_factors(factors_dict)
    factor_names = sorted(standardized_factors.keys())
    factor_arrays = [standardized_factors[name].values for name in factor_names]
    factors_data = np.stack(factor_arrays, axis=-1)
    returns_df = ohlcv['close'].pct_change(fill_method=None)
    returns = returns_df.values
    etf_names = list(ohlcv['close'].columns)
    dates = returns_df.index.strftime('%Y-%m-%d').tolist()
    print(f'âœ… æ•°æ®åŠ è½½å®Œæˆ: {dates[0]} ~ {dates[-1]} å…±{len(dates)}æ—¥, ETF={len(etf_names)}, å› å­={len(factor_names)}')
    return config, factors_data, returns, etf_names, factor_names


def run_single(combo_row: pd.Series, factors_data_full, returns_full, etf_names, factor_names, freq: int, pos_size: int):
    combo = combo_row['combo']
    factor_list = [f.strip() for f in str(combo).split('+') if f.strip()]
    missing = [f for f in factor_list if f not in factor_names]
    if missing:
        return None
    idxs = [factor_names.index(f) for f in factor_list]
    factors_selected = factors_data_full[:, :, idxs]
    try:
        res = backtest_no_lookahead(
            factors_data=factors_selected,
            returns=returns_full,
            etf_names=etf_names,
            rebalance_freq=freq,
            lookback_window=252,
            position_size=pos_size,
            commission_rate=0.00005,
            initial_capital=1_000_000.0,
            factors_data_full=factors_data_full,
            factor_indices_for_cache=idxs,
        )
        return {
            'combo': combo,
            'combo_size': combo_row.get('combo_size', len(factor_list)),
            'test_freq': freq,
            'test_position_size': pos_size,
            'annual_ret': res['annual_ret'],
            'sharpe': res['sharpe'],
            'max_dd': res['max_dd'],
            'win_rate': res['win_rate'],
            'avg_turnover': res['avg_turnover'],
            'avg_n_holdings': res['avg_n_holdings'],
            'calmar_ratio': res.get('calmar_ratio'),
            'sortino_ratio': res.get('sortino_ratio'),
        }
    except Exception as e:
        print(f'âŒ å›æµ‹å¤±è´¥ combo={combo[:60]} freq={freq} pos={pos_size}: {e}')
        return None


def build_tasks(top_df: pd.DataFrame, freqs: list[int], pos_sizes: list[int]):
    tasks = []
    for _, row in top_df.iterrows():
        for f in freqs:
            for p in pos_sizes:
                tasks.append((row, f, p))
    return tasks


def main():
    args = parse_args()
    freqs = [int(x) for x in args.freqs.split(',') if x.strip()]
    pos_sizes = [int(x) for x in args.positions.split(',') if x.strip()]
    print(f'ğŸ”§ å‚æ•°: TopK={args.topk} freqs={freqs} pos_sizes={pos_sizes} jobs={args.jobs}')

    # æ‰¾æœ€æ–°å›æµ‹æ–‡ä»¶
    latest_file = find_latest_backtest_file(args.latest_backtest_dir, args.latest_backtest_pattern)
    print(f'ğŸ“„ ä½¿ç”¨æœ€æ–°å›æµ‹æ–‡ä»¶: {latest_file}')
    base_df = load_base_results(latest_file)

    if 'combo' not in base_df.columns or 'sharpe' not in base_df.columns:
        raise RuntimeError('å›æµ‹ç»“æœç¼ºå°‘å¿…è¦åˆ—: combo æˆ– sharpe')

    top_df = select_topk(base_df, args.topk, sample=args.random_sample, seed=args.seed)
    print(f'âœ… é€‰å– {len(top_df)} ä¸ªç»„åˆç”¨äºæ‰«æ (éšæœºæŠ½æ ·={args.random_sample})')

    # åŠ è½½æ•°æ®ä¸å› å­
    config, factors_data_full, returns_full, etf_names, factor_names = load_data_and_factors()

    tasks = build_tasks(top_df, freqs, pos_sizes)
    print(f'ğŸ“‹ ä»»åŠ¡æ€»æ•°: {len(tasks)} (ç»„åˆ{len(top_df)} Ã— é¢‘ç‡{len(freqs)} Ã— æŒä»“{len(pos_sizes)})')

    # å¹¶è¡Œæ‰§è¡Œ
    def _runner(row, f, p):
        return run_single(row, factors_data_full, returns_full, etf_names, factor_names, f, p)

    results = Parallel(n_jobs=args.jobs, verbose=10)(delayed(_runner)(row, f, p) for row, f, p in tasks)
    valid = [r for r in results if r is not None]
    print(f'âœ… å®Œæˆ {len(valid)}/{len(results)} ä¸ªä»»åŠ¡')

    if not valid:
        print('âŒ æ— æœ‰æ•ˆç»“æœï¼Œé€€å‡º')
        return

    df_res = pd.DataFrame(valid)

    # è¾“å‡ºç›®å½•
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_root = Path(args.output_dir)
    out_root.mkdir(parents=True, exist_ok=True)
    csv_path = out_root / f'freq_pos_grid_{ts}.csv'
    df_res.to_csv(csv_path, index=False)
    print(f'ğŸ’¾ å·²ä¿å­˜ç»“æœ: {csv_path}')

    # æ±‡æ€»ç»Ÿè®¡
    summary_freq = df_res.groupby('test_freq').agg({'sharpe':'mean','annual_ret':'mean','max_dd':'mean'}).round(4)
    summary_pos = df_res.groupby('test_position_size').agg({'sharpe':'mean','annual_ret':'mean','max_dd':'mean'}).round(4)
    summary_pair = df_res.groupby(['test_freq','test_position_size']).agg({'sharpe':'mean','annual_ret':'mean','max_dd':'mean'}).round(4).reset_index()

    best_row = summary_pair.sort_values('sharpe', ascending=False).iloc[0]
    best_freq = int(best_row['test_freq'])
    best_pos = int(best_row['test_position_size'])
    best_sharpe = best_row['sharpe']
    best_annual = best_row['annual_ret']

    # ç”ŸæˆæŠ¥å‘Š
    md_path = out_root / f'GRID_FREQ_POS_REPORT_{ts}.md'
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write('# é¢‘ç‡ Ã— æŒä»“æ•° ç½‘æ ¼æ‰«ææŠ¥å‘Š\n\n')
        f.write(f'- æ—¶é—´æˆ³: {ts}\n')
        f.write(f'- TopK: {args.topk}\n')
        f.write(f'- é¢‘ç‡é›†åˆ: {freqs}\n')
        f.write(f'- æŒä»“æ•°é›†åˆ: {pos_sizes}\n')
        f.write(f'- æ€»ä»»åŠ¡æ•°: {len(tasks)}\n')
        f.write(f'- æœ‰æ•ˆç»“æœæ•°: {len(valid)}\n')
        f.write(f'- æœ€ä¼˜å‚æ•°: freq={best_freq}, position_size={best_pos}, Sharpe={best_sharpe:.4f}, annual_ret={best_annual:.2%}\n\n')
        f.write('## æŒ‰é¢‘ç‡æ±‡æ€»\n\n')
        f.write(summary_freq.to_markdown() + '\n\n')
        f.write('## æŒ‰æŒä»“æ•°æ±‡æ€»\n\n')
        f.write(summary_pos.to_markdown() + '\n\n')
        f.write('## é¢‘ç‡Ã—æŒä»“æ•°ç»„åˆæ±‡æ€» (Sharpe/Annual/MaxDD å‡å€¼)\n\n')
        f.write(summary_pair.to_markdown(index=False) + '\n')
    print(f'ğŸ“ å·²ç”ŸæˆæŠ¥å‘Š: {md_path}')

    print('\nğŸ¯ æœ€ä¼˜ç»“æœæ‘˜è¦:')
    print(f'   freq={best_freq} pos={best_pos} Sharpe={best_sharpe:.4f} annual={best_annual:.2%}')

if __name__ == '__main__':
    main()
