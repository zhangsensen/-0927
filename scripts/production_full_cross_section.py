#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç¯å¢ƒå®Œæ•´æ¨ªæˆªé¢æ„å»º
ä½¿ç”¨çœŸå®ETFæ•°æ®ï¼Œè®¡ç®—æ‰€æœ‰å› å­
"""

import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from factor_system.factor_engine.factors.etf_cross_section import (
    create_etf_cross_section_manager,
    ETFCrossSectionConfig
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('production_full_cross_section.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_etf_symbols():
    """åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„ETFä»£ç """
    logger.info("åŠ è½½ETFä»£ç ...")
    
    data_dir = project_root / "raw" / "ETF" / "daily"
    etf_files = list(data_dir.glob("*.parquet"))
    
    symbols = []
    for f in etf_files:
        # æå–ETFä»£ç : 515030.SH_daily_20200102_20251014.parquet -> 515030.SH
        symbol = f.stem.split('_')[0]
        symbols.append(symbol)
    
    logger.info(f"âœ… æ‰¾åˆ° {len(symbols)} åªETF")
    return sorted(list(set(symbols)))  # å»é‡å¹¶æ’åº


def get_latest_common_date(symbols):
    """è·å–æ‰€æœ‰ETFéƒ½æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ"""
    logger.info("æŸ¥æ‰¾æœ€æ–°å…±åŒæ—¥æœŸ...")
    
    data_dir = project_root / "raw" / "ETF" / "daily"
    latest_dates = []
    
    for symbol in symbols[:10]:  # åªæ£€æŸ¥å‰10åªï¼ŒåŠ å¿«é€Ÿåº¦
        files = list(data_dir.glob(f"{symbol}_*.parquet"))
        if files:
            df = pd.read_parquet(files[0])
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            latest_dates.append(df['trade_date'].max())
    
    if latest_dates:
        common_date = min(latest_dates)
        logger.info(f"âœ… æœ€æ–°å…±åŒæ—¥æœŸ: {common_date.date()}")
        return common_date
    
    return datetime(2025, 10, 14)


def calculate_factors_batch(manager, symbols, factor_ids, start_date, end_date):
    """æ‰¹é‡è®¡ç®—å› å­ - ä½¿ç”¨managerç»Ÿä¸€æ¥å£"""
    logger.info(f"è®¡ç®—å› å­: {len(symbols)}åªETF Ã— {len(factor_ids)}ä¸ªå› å­")
    
    try:
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä½¿ç”¨manager.calculate_factorsç»Ÿä¸€æ¥å£
        result = manager.calculate_factors(
            symbols=symbols,
            timeframe='daily',
            start_date=start_date,
            end_date=end_date,
            factor_ids=factor_ids  # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å› å­
        )
        
        if result.factors_df is not None and not result.factors_df.empty:
            logger.info(f"âœ… å› å­è®¡ç®—å®Œæˆ: {result.factors_df.shape}")
            logger.info(f"   æˆåŠŸ: {len(result.successful_factors)} ä¸ª")
            logger.info(f"   å¤±è´¥: {len(result.failed_factors)} ä¸ª")
            return result.factors_df
        else:
            logger.error("âŒ å› å­è®¡ç®—è¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ å› å­è®¡ç®—å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def build_cross_section(combined_df, target_date):
    """ä»æ—¶é—´åºåˆ—æ•°æ®æ„å»ºæ¨ªæˆªé¢"""
    logger.info(f"æ„å»ºæ¨ªæˆªé¢: {target_date.date()}")
    
    if combined_df is None or combined_df.empty:
        logger.error("âŒ è¾“å…¥æ•°æ®ä¸ºç©º")
        return None
    
    # æå–æŒ‡å®šæ—¥æœŸçš„æ•°æ®
    if isinstance(combined_df.index, pd.MultiIndex):
        try:
            # å°è¯•ç²¾ç¡®åŒ¹é…
            cross_section = combined_df.xs(target_date, level=0)
        except KeyError:
            # æŸ¥æ‰¾æœ€è¿‘æ—¥æœŸ
            dates = combined_df.index.get_level_values(0).unique()
            closest_date = min(dates, key=lambda d: abs((d - target_date).total_seconds()))
            cross_section = combined_df.xs(closest_date, level=0)
            logger.warning(f"ä½¿ç”¨æœ€è¿‘æ—¥æœŸ: {closest_date.date()}")
    else:
        cross_section = combined_df
    
    logger.info(f"âœ… æ¨ªæˆªé¢æ„å»ºå®Œæˆ: {cross_section.shape}")
    return cross_section


def analyze_factors(cross_section):
    """åˆ†æå› å­æœ‰æ•ˆæ€§"""
    logger.info("\n" + "="*80)
    logger.info("å› å­æœ‰æ•ˆæ€§åˆ†æ")
    logger.info("="*80)
    
    total_etfs = len(cross_section)
    total_factors = len(cross_section.columns)
    
    logger.info(f"æ¨ªæˆªé¢ç»´åº¦: {total_etfs} åªETF Ã— {total_factors} ä¸ªå› å­")
    
    # ç»Ÿè®¡æ¯ä¸ªå› å­
    factor_stats = []
    for factor_id in cross_section.columns:
        values = cross_section[factor_id]
        valid_count = values.notna().sum()
        valid_rate = valid_count / len(values) * 100
        
        factor_stats.append({
            'factor_id': factor_id,
            'valid_count': valid_count,
            'valid_rate': valid_rate,
            'mean': values.mean() if valid_count > 0 else np.nan,
            'std': values.std() if valid_count > 0 else np.nan
        })
    
    stats_df = pd.DataFrame(factor_stats)
    
    # åˆ†ç±»
    effective = stats_df[stats_df['valid_rate'] >= 50]
    partial = stats_df[(stats_df['valid_rate'] > 0) & (stats_df['valid_rate'] < 50)]
    invalid = stats_df[stats_df['valid_rate'] == 0]
    
    logger.info("\nå› å­ç”Ÿæ•ˆæƒ…å†µ:")
    logger.info(f"  âœ… å®Œå…¨ç”Ÿæ•ˆ (â‰¥50%): {len(effective)} ä¸ª ({len(effective)/total_factors*100:.1f}%)")
    logger.info(f"  âš ï¸ éƒ¨åˆ†ç”Ÿæ•ˆ (<50%): {len(partial)} ä¸ª ({len(partial)/total_factors*100:.1f}%)")
    logger.info(f"  âŒ æœªç”Ÿæ•ˆ (0%):    {len(invalid)} ä¸ª ({len(invalid)/total_factors*100:.1f}%)")
    
    # æ˜¾ç¤ºç”Ÿæ•ˆå› å­
    if len(effective) > 0:
        logger.info("\nå®Œå…¨ç”Ÿæ•ˆçš„å› å­ (å‰30ä¸ª):")
        for _, row in effective.head(30).iterrows():
            logger.info(f"  {row['factor_id']}: {row['valid_rate']:.1f}% ({row['valid_count']}/{total_etfs})")
    
    # ä¿å­˜ç»Ÿè®¡
    output_dir = project_root / "output" / "cross_sections"
    output_dir.mkdir(parents=True, exist_ok=True)
    stats_file = output_dir / "factor_effectiveness_stats.csv"
    stats_df.to_csv(stats_file, index=False)
    logger.info(f"\nâœ… è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
    
    return stats_df


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80)
    logger.info("ç”Ÿäº§ç¯å¢ƒå®Œæ•´æ¨ªæˆªé¢æ„å»º")
    logger.info("="*80)
    
    try:
        # 1. åŠ è½½ETFåˆ—è¡¨
        symbols = load_etf_symbols()
        logger.info(f"ETFåˆ—è¡¨: {symbols[:10]}...")
        
        # 2. ç¡®å®šæ—¥æœŸ - ä½¿ç”¨å®Œæ•´æ•°æ®çª—å£
        target_date = get_latest_common_date(symbols)
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨250ä¸ªäº¤æ˜“æ—¥ï¼ˆçº¦1å¹´ï¼‰çª—å£ï¼Œè¶³å¤Ÿè®¡ç®—MA120ç­‰é•¿å‘¨æœŸæŒ‡æ ‡
        start_date = target_date - timedelta(days=365)  # 1å¹´æ•°æ®
        
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date.date()} ~ {target_date.date()}")
        logger.info(f"æ•°æ®çª—å£: ~250ä¸ªäº¤æ˜“æ—¥ï¼Œæ”¯æŒMA120ç­‰é•¿å‘¨æœŸæŒ‡æ ‡")
        
        # 3. è·å–å¯ç”¨å› å­åˆ—è¡¨
        logger.info("\nåˆå§‹åŒ–å› å­ç®¡ç†å™¨...")
        config = ETFCrossSectionConfig()
        config.enable_dynamic_factors = True
        config.max_dynamic_factors = 1000  # ç§»é™¤å› å­æ•°é‡é™åˆ¶

        manager = create_etf_cross_section_manager(config)

        # å¼ºåˆ¶æ³¨å†Œæ‰€æœ‰åŠ¨æ€å› å­
        manager._register_all_dynamic_factors()

        available_factors = manager.get_available_factors()

        logger.info(f"âœ… å¯ç”¨å› å­: {len(available_factors)} ä¸ª")
        logger.info(f"   åŠ¨æ€å› å­: {len(manager.factor_registry.list_factors(is_dynamic=True))} ä¸ª")
        logger.info(f"   ä¼ ç»Ÿå› å­: {len(available_factors) - len(manager.factor_registry.list_factors(is_dynamic=True))} ä¸ª")

        # 4. æ‰¹é‡è®¡ç®—å› å­
        logger.info("\n" + "="*80)
        logger.info("å¼€å§‹è®¡ç®—å› å­")
        logger.info("="*80)

        combined_df = calculate_factors_batch(
            manager=manager,  # ğŸ”¥ ä¼ å…¥manager
            symbols=symbols,
            factor_ids=available_factors,  # è®¡ç®—æ‰€æœ‰å› å­ï¼
            start_date=start_date,
            end_date=target_date
        )
        
        if combined_df is None:
            logger.error("âŒ å› å­è®¡ç®—å¤±è´¥")
            return
        
        # 5. æ„å»ºæ¨ªæˆªé¢
        cross_section = build_cross_section(combined_df, target_date)
        
        if cross_section is None:
            logger.error("âŒ æ¨ªæˆªé¢æ„å»ºå¤±è´¥")
            return
        
        # 6. åˆ†æå› å­æœ‰æ•ˆæ€§
        analyze_factors(cross_section)
        
        # 7. ä¿å­˜æ¨ªæˆªé¢æ•°æ®
        output_dir = project_root / "output" / "cross_sections"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"cross_section_{target_date.strftime('%Y%m%d')}.parquet"
        cross_section.to_parquet(output_file)
        
        logger.info(f"\nâœ… æ¨ªæˆªé¢æ•°æ®å·²ä¿å­˜: {output_file}")
        logger.info(f"   å¤§å°: {output_file.stat().st_size / 1024:.2f} KB")
        
        logger.info("\n" + "="*80)
        logger.info("âœ… å…¨éƒ¨å®Œæˆï¼")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == "__main__":
    main()
