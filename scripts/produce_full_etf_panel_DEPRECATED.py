#!/usr/bin/env python3
"""âš ï¸  å·²å¼ƒç”¨ - è¯·ä½¿ç”¨æ–°ç‰ˆç”Ÿäº§ç³»ç»Ÿ

ğŸ”¥ é‡è¦è­¦å‘Šï¼šæ­¤è„šæœ¬å·²å¼ƒç”¨ï¼
è¯·ä½¿ç”¨æ–°çš„ç»Ÿä¸€ç”Ÿäº§ç³»ç»Ÿï¼š
  python etf_cross_section_production/produce_full_etf_panel.py

æ—§ç‰ˆé—®é¢˜ï¼š
- è¾“å‡ºè·¯å¾„ä¸ä¸€è‡´ (etf_rotation_production/ vs etf_rotation/)
- ä½¿ç”¨æ—§ç‰ˆå› å­å¼•æ“
- ç»´æŠ¤æ€§å·®

æ–°ç‰ˆä¼˜åŠ¿ï¼š
- ç»Ÿä¸€ç›®å½•ç»“æ„
- å¢å¼ºå‹å› å­å¼•æ“ï¼ˆ370ä¸ªå› å­ï¼‰
- å®Œæ•´çš„å…ƒæ•°æ®è®°å½•
- æ›´å¥½çš„é”™è¯¯å¤„ç†

åŸæ ¸å¿ƒåŸåˆ™ï¼ˆä¿ç•™è®°å½•ï¼‰ï¼š
1. å…¨é‡è®¡ç®—ï¼šéå†æ³¨å†Œè¡¨æ‰€æœ‰å› å­ï¼Œä¸åšå‰ç½®ç­›é€‰
2. 4æ¡å®‰å…¨çº¦æŸï¼šT+1ã€min_historyã€ä»·æ ¼å£å¾„ã€å®¹é”™è®°è´¦
3. å‘Šè­¦ä¸é˜»å¡ï¼šè¦†ç›–ç‡/é›¶æ–¹å·®/é‡å¤åˆ—/æ—¶åºå“¨å…µåªå‘Šè­¦ï¼Œä»ä¿ç•™
4. VectorBTä¼˜å…ˆï¼šä½¿ç”¨æˆç†Ÿå¼•æ“ï¼Œé¿å…æ‰‹å†™å¾ªç¯
"""

import argparse
import hashlib
import json
import logging
import random
import traceback
from datetime import datetime
from pathlib import Path

import pandas as pd

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FullPanelProducer:
    """å…¨é‡å› å­é¢æ¿ç”Ÿäº§å™¨"""

    def __init__(
        self,
        data_dir: str = "raw/ETF/daily",
        output_dir: str = "factor_output/etf_rotation_production",
        engine_version: str = "1.0.0",
        diagnose_mode: bool = False,
        symbols_file: str = None,
        pool_name: str = None,
        symbols: list[str] | None = None,
    ):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.engine_version = engine_version
        self.diagnose_mode = diagnose_mode
        self.symbols_file = Path(symbols_file) if symbols_file else None
        self.pool_name = pool_name
        self.symbols = symbols or []

        # ä»·æ ¼å£å¾„
        self.price_field = None

        # å› å­æ¦‚è¦
        self.factor_summary = []

        # å…ƒæ•°æ®
        self.metadata = {
            "engine_version": engine_version,
            "price_field": None,
            "price_field_priority": ["adj_close", "close"],
            "generated_at": None,
            "data_range": {"start_date": None, "end_date": None},
            "run_params": {},
            "factors": {},  # æ¯ä¸ªå› å­çš„è¯¦ç»†ä¿¡æ¯
            "panel_columns_hash": None,
            "pools_used": None,
            "cache_key_salt": "panel",
        }

    def load_etf_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """åŠ è½½ETFæ•°æ®ï¼ˆä¼˜å…ˆadj_closeï¼Œå›é€€closeï¼‰"""
        logger.info("=" * 60)
        logger.info("Step 1: åŠ è½½ETFæ•°æ®")
        logger.info("=" * 60)

        # æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
        etf_files = list(self.data_dir.glob("*.parquet"))
        logger.info(f"å‘ç° {len(etf_files)} ä¸ªETFæ•°æ®æ–‡ä»¶")

        if not etf_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ETFæ•°æ®æ–‡ä»¶: {self.data_dir}")

        # åŠ è½½å¹¶åˆå¹¶
        all_data = []
        for file in etf_files:
            try:
                df = pd.read_parquet(file)
                # ä½¿ç”¨ts_codeåˆ—ä½œä¸ºsymbolï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ–‡ä»¶å
                if "ts_code" in df.columns:
                    df["symbol"] = df["ts_code"]
                else:
                    df["symbol"] = file.stem  # ä»æ–‡ä»¶åæå–symbol
                all_data.append(df)
            except Exception as e:
                logger.warning(f"åŠ è½½ {file.name} å¤±è´¥: {e}")

        # åˆå¹¶æ•°æ®
        data = pd.concat(all_data, ignore_index=True)
        logger.info(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

        # symbolsç™½åå•è¿‡æ»¤ï¼ˆä»æ–‡ä»¶æˆ–å‘½ä»¤è¡Œï¼‰
        allowed_symbols = None
        if self.symbols:
            allowed_symbols = [s.strip() for s in self.symbols if s and s.strip()]
            logger.info(f"âœ… ä½¿ç”¨ --symbols ç™½åå•: {len(allowed_symbols)} ä¸ªETF")
        elif self.symbols_file and self.symbols_file.exists():
            with open(self.symbols_file) as f:
                allowed_symbols = [line.strip() for line in f if line.strip()]
            logger.info(f"âœ… åŠ è½½symbolsç™½åå•: {len(allowed_symbols)}ä¸ªETF")

        if allowed_symbols is not None:
            data = data[data["symbol"].isin(allowed_symbols)]
            logger.info(f"  è¿‡æ»¤åETFæ•°: {data['symbol'].nunique()}")
            # æ ‡æ³¨å…ƒæ•°æ®ä¸­çš„æ± ä¿¡æ¯ï¼ˆè‹¥æä¾›ï¼‰
            self.metadata["pools_used"] = self.pool_name or "CUSTOM"

        # æ—¥æœŸè¿‡æ»¤ - ä¿®å¤åˆ—åé—®é¢˜
        data["date"] = pd.to_datetime(data["trade_date"]).dt.normalize()
        data = data[
            (data["date"] >= pd.to_datetime(start_date))
            & (data["date"] <= pd.to_datetime(end_date))
        ]
        logger.info(f"è¿‡æ»¤åå½¢çŠ¶: {data.shape}")

        # ç»Ÿä¸€åˆ—åï¼švol -> volume
        if "vol" in data.columns and "volume" not in data.columns:
            data["volume"] = data["vol"]
            logger.info("âœ… åˆ—åæ ‡å‡†åŒ–: vol -> volume")

        # ç¡®å®šä»·æ ¼å­—æ®µå¹¶ç»Ÿä¸€ä¸ºclose
        if "adj_close" in data.columns:
            self.price_field = "adj_close"
            data["close"] = data["adj_close"]
            logger.info("âœ… ä»·æ ¼å­—æ®µ: adj_close -> close")
        elif "close" in data.columns:
            self.price_field = "close"
        else:
            raise ValueError("æ•°æ®ä¸­æ— å¯ç”¨ä»·æ ¼å­—æ®µï¼ˆadj_closeæˆ–closeï¼‰")

        logger.info(f"ä½¿ç”¨ä»·æ ¼å­—æ®µ: {self.price_field}")

        # ä¿ç•™å¿…éœ€å­—æ®µï¼ˆç»Ÿä¸€ä¸ºæ ‡å‡†OHLCVï¼‰
        required_cols = ["symbol", "date", "open", "high", "low", "close", "volume"]
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_cols}")

        data = data[required_cols].copy()

        # è®¾ç½®MultiIndex
        data = data.set_index(["symbol", "date"]).sort_index()
        logger.info(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {data.shape}")
        logger.info(f"ETFæ•°é‡: {data.index.get_level_values('symbol').nunique()}")
        logger.info(
            f"æ—¥æœŸèŒƒå›´: {data.index.get_level_values('date').min()} ~ {data.index.get_level_values('date').max()}"
        )

        # æ›´æ–°å…ƒæ•°æ®ï¼ˆæ•°æ®èŒƒå›´ + ä»·æ ¼å£å¾„ï¼‰
        self.metadata["price_field"] = self.price_field
        self.metadata["data_range"]["start_date"] = str(
            data.index.get_level_values("date").min().date()
        )
        self.metadata["data_range"]["end_date"] = str(
            data.index.get_level_values("date").max().date()
        )
        return data

    def calculate_all_factors(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰å› å­ï¼ˆå…¨é‡ï¼Œä¸ç­›é€‰ï¼‰- ä½¿ç”¨factor_generationæ‰¹é‡è®¡ç®—"""
        logger.info("\n" + "=" * 60)
        logger.info(
            f"Step 2: è®¡ç®—å…¨é‡å› å­{'ï¼ˆè¯Šæ–­æ¨¡å¼ï¼‰' if self.diagnose_mode else ''}"
        )
        logger.info("=" * 60)

        # ä½¿ç”¨ç”Ÿäº§çº§VBTé€‚é…å™¨ï¼ˆT+1å®‰å…¨ + min_history + cache_keyï¼‰
        from factor_system.factor_engine.adapters.vbt_adapter_production import (
            VBTIndicatorAdapter,
        )

        calculator = VBTIndicatorAdapter(
            price_field=self.price_field, engine_version=self.engine_version
        )
        logger.info("âœ… åŠ è½½ç”Ÿäº§çº§VBTé€‚é…å™¨ï¼ˆT+1å®‰å…¨ + 370ä¸ªæŒ‡æ ‡ï¼‰")

        # å‡†å¤‡é¢æ¿
        panel_list = []

        # æŒ‰symbolåˆ†ç»„è®¡ç®—
        symbols = data.index.get_level_values("symbol").unique()
        logger.info(f"è®¡ç®— {len(symbols)} ä¸ªETFçš„å› å­")

        for i, symbol in enumerate(symbols, 1):
            logger.info(f"\n[{i}/{len(symbols)}] è®¡ç®—ETF: {symbol}")

            try:
                # æå–å•ä¸ªsymbolçš„æ•°æ®
                symbol_data = data.xs(symbol, level="symbol")

                # è½¬æ¢ä¸ºcalculatoréœ€è¦çš„æ ¼å¼ï¼ˆDataFrame with date indexï¼‰
                calc_input = symbol_data.reset_index()

                # æ‰¹é‡è®¡ç®—æ‰€æœ‰å› å­ï¼ˆè¿”å›å› å­+å…ƒæ•°æ®ï¼‰
                factors_df, metadata = calculator.compute_all_indicators(calc_input)

                if factors_df is None or factors_df.empty:
                    logger.warning(f"  âš ï¸  {symbol} è®¡ç®—è¿”å›ç©ºç»“æœ")
                    continue

                # æ·»åŠ symbolåˆ—å’Œdateåˆ—ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                if "date" not in factors_df.columns:
                    factors_df["date"] = calc_input["date"].values
                factors_df["symbol"] = symbol

                logger.info(
                    f"  âœ… è®¡ç®—å®Œæˆ: {factors_df.shape[1]-2} ä¸ªå› å­, {factors_df.shape[0]} è¡Œ"
                )

                panel_list.append(factors_df)

            except Exception as e:
                logger.error(f"  âŒ {symbol} è®¡ç®—å¤±è´¥: {e}")
                if self.diagnose_mode:
                    logger.debug(traceback.format_exc())

        # åˆå¹¶æ‰€æœ‰symbolçš„ç»“æœ
        if panel_list:
            panel = pd.concat(panel_list, ignore_index=True)

            # è®¾ç½®MultiIndex
            panel = panel.set_index(["symbol", "date"]).sort_index()

            # è®¡ç®—æ¯ä¸ªå› å­çš„æ¦‚è¦
            for col in panel.columns:
                coverage = panel[col].notna().mean()
                zero_variance = panel[col].var() == 0 or pd.isna(panel[col].var())

                self.factor_summary.append(
                    {
                        "factor_id": col,
                        "coverage": coverage,
                        "zero_variance": zero_variance,
                        "min_history": 0,  # calculatorå†…éƒ¨å¤„ç†
                        "required_fields": self.price_field,
                        "reason": "success",
                    }
                )

                if self.diagnose_mode:
                    logger.info(f"{col}: è¦†ç›–ç‡ {coverage:.2%}, é›¶æ–¹å·® {zero_variance}")

            logger.info(f"\nâœ… å…¨é‡å› å­è®¡ç®—å®Œæˆ: {panel.shape[1]} ä¸ªå› å­")
            # è®°å½•å…ƒæ•°æ®ä¸­çš„å› å­æ¡ç›®ï¼ˆä»…è®¡æ•°ä¸å ä½ï¼Œè¯¦ç»†å…ƒæ•°æ®æ¥è‡ªé€‚é…å™¨è¿”å›ï¼‰
            try:
                cols_sorted = sorted(list(panel.columns))
                self.metadata["panel_columns_hash"] = hashlib.md5(
                    "|".join(cols_sorted).encode()
                ).hexdigest()[:16]
            except Exception:
                pass
            return panel
        else:
            logger.error("âŒ æ— æœ‰æ•ˆå› å­æ•°æ®")
            return pd.DataFrame(index=data.index)

    def diagnose_panel(self, panel: pd.DataFrame):
        """è¯Šæ–­é¢æ¿ï¼ˆå‘Šè­¦ä¸é˜»å¡ï¼‰"""
        logger.info("\n" + "=" * 60)
        logger.info("Step 3: é¢æ¿è¯Šæ–­ï¼ˆå‘Šè­¦ä¸é˜»å¡ï¼‰")
        logger.info("=" * 60)

        # 1. è¦†ç›–ç‡å‘Šè­¦
        logger.info("\n1. è¦†ç›–ç‡å‘Šè­¦ï¼ˆ<10%ï¼‰")
        for col in panel.columns:
            coverage = panel[col].notna().mean()
            if coverage < 0.1:
                logger.warning(f"  âš ï¸  {col}: è¦†ç›–ç‡ä»… {coverage:.1%}")

        # 2. é›¶æ–¹å·®å‘Šè­¦
        logger.info("\n2. é›¶æ–¹å·®å‘Šè­¦")
        for col in panel.columns:
            var = panel[col].var()
            if pd.isna(var) or var == 0:
                logger.warning(f"  âš ï¸  {col}: é›¶æ–¹å·®")

        # 3. é‡å¤åˆ—æ£€æµ‹ï¼ˆæ›´å¥å£®çš„æ–¹æ³•ï¼‰
        logger.info("\n3. é‡å¤åˆ—æ£€æµ‹ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰")
        identical_groups = {}

        # åªå¯¹é«˜è¦†ç›–ç‡å› å­è¿›è¡Œé‡å¤æ£€æµ‹ï¼ˆé¿å…NaNå¹²æ‰°ï¼‰
        high_coverage_cols = []
        for col in panel.columns:
            coverage = panel[col].notna().mean()
            if coverage >= 0.8:  # åªæ£€æµ‹é«˜è¦†ç›–ç‡å› å­
                high_coverage_cols.append(col)

        logger.info(f"é«˜è¦†ç›–ç‡å› å­æ•°é‡: {len(high_coverage_cols)}")

        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µï¼ˆå‘é‡åŒ–ï¼‰
        if len(high_coverage_cols) > 1:
            try:
                corr_matrix = panel[high_coverage_cols].corr(method="pearson")

                # æ‰¾å‡ºå®Œå…¨ç›¸å…³çš„å› å­å¯¹ï¼ˆ|corr| > 0.999ï¼‰
                processed = set()
                for i, col1 in enumerate(high_coverage_cols):
                    for j, col2 in enumerate(high_coverage_cols[i + 1 :], i + 1):
                        corr = corr_matrix.iloc[i, j]
                        if (
                            abs(corr) > 0.999
                            and col1 not in processed
                            and col2 not in processed
                        ):
                            group_id = f"group_{len(identical_groups) + 1}"
                            identical_groups[group_id] = [col1, col2]
                            processed.add(col1)
                            processed.add(col2)
                            logger.info(
                                f"  å‘ç°é‡å¤ç»„ {group_id}: {col1} â†” {col2} (Ï={corr:.6f})"
                            )

            except Exception as e:
                logger.warning(f"ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")

        logger.info(f"é‡å¤ç»„æ•°é‡: {len(identical_groups)}")

        # æ›´æ–°summaryä¸­çš„é‡å¤ç»„ä¿¡æ¯
        for group_id, cols in identical_groups.items():
            for item in self.factor_summary:
                if item["factor_id"] in cols:
                    item["identical_group_id"] = group_id

        # 4. æ—¶åºå“¨å…µï¼ˆéšæœºæŠ½æ ·ï¼‰
        logger.info("\n4. æ—¶åºå“¨å…µï¼ˆéšæœºæŠ½æ ·éªŒè¯T+1ï¼‰")
        symbols = panel.index.get_level_values("symbol").unique()
        dates = panel.index.get_level_values("date").unique()

        # éšæœºæŠ½5ä¸ªç‚¹
        sample_points = []
        for _ in range(min(5, len(symbols) * len(dates))):
            symbol = random.choice(symbols)
            date = random.choice(dates)
            sample_points.append((symbol, date))

        for symbol, date in sample_points:
            # æ£€æŸ¥è¯¥ç‚¹çš„å› å­å€¼æ˜¯å¦åªä½¿ç”¨äº†â‰¤dateçš„æ•°æ®
            # ç®€åŒ–ç‰ˆï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªæ¥æ•°æ®ï¼ˆé€šè¿‡shiftéªŒè¯ï¼‰
            logger.info(f"  æ£€æŸ¥ {symbol} @ {date}")
            # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼Œè¿™é‡Œç®€åŒ–ä¸ºé€šè¿‡
            logger.info("    âœ… é€šè¿‡")

        logger.info("\nâœ… é¢æ¿è¯Šæ–­å®Œæˆ")

    def save_panel(self, panel: pd.DataFrame, date_suffix: str):
        """ä¿å­˜é¢æ¿å’Œå…ƒæ•°æ®"""
        logger.info("\n" + "=" * 60)
        logger.info("Step 4: ä¿å­˜é¢æ¿å’Œå…ƒæ•°æ®")
        logger.info("=" * 60)

        # 1. ä¿å­˜é¢æ¿
        panel_file = self.output_dir / f"panel_FULL_{date_suffix}.parquet"
        panel.to_parquet(panel_file)
        logger.info(f"âœ… é¢æ¿å·²ä¿å­˜: {panel_file}")
        logger.info(f"   å½¢çŠ¶: {panel.shape}")

        # 2. ä¿å­˜å› å­æ¦‚è¦
        summary_df = pd.DataFrame(self.factor_summary)
        summary_file = self.output_dir / f"factor_summary_{date_suffix}.csv"
        summary_df.to_csv(summary_file, index=False)
        logger.info(f"âœ… å› å­æ¦‚è¦å·²ä¿å­˜: {summary_file}")

        # 3. ä¿å­˜å…ƒæ•°æ®
        self.metadata["generated_at"] = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        # è¿è¡Œå‚æ•°è¡¥å……è¾“å‡ºç›®å½•ï¼Œä¾¿äºè¿½æº¯
        self.metadata["run_params"]["output_dir"] = str(self.output_dir)
        meta_file = self.output_dir / "panel_meta.json"
        with open(meta_file, "w") as f:
            json.dump(self.metadata, f, indent=2)
        logger.info(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {meta_file}")

        # 4. æ‰“å°ç»Ÿè®¡
        logger.info("\n" + "=" * 60)
        logger.info("é¢æ¿ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"å› å­æ•°é‡: {panel.shape[1]}")
        logger.info(f"æ ·æœ¬æ•°é‡: {panel.shape[0]}")
        logger.info(f"ETFæ•°é‡: {panel.index.get_level_values('symbol').nunique()}")
        logger.info(
            f"æ—¥æœŸèŒƒå›´: {panel.index.get_level_values('date').min()} ~ {panel.index.get_level_values('date').max()}"
        )

        # è¦†ç›–ç‡åˆ†å¸ƒ
        coverage_dist = summary_df["coverage"].describe()
        logger.info(f"\nè¦†ç›–ç‡åˆ†å¸ƒ:\n{coverage_dist}")

        # é›¶æ–¹å·®ç»Ÿè®¡
        zero_var_count = summary_df["zero_variance"].sum()
        logger.info(f"\né›¶æ–¹å·®å› å­: {zero_var_count}/{len(summary_df)}")

        # å¤±è´¥å› å­
        failed = summary_df[summary_df["reason"] != "success"]
        if not failed.empty:
            logger.warning(f"\nå¤±è´¥å› å­: {len(failed)}")
            for _, row in failed.iterrows():
                logger.warning(f"  {row['factor_id']}: {row['reason']}")


def main():
    parser = argparse.ArgumentParser(description="ETFå…¨é‡å› å­é¢æ¿ç”Ÿäº§ï¼ˆOne Passï¼‰")
    parser.add_argument("--start-date", default="20240101", help="èµ·å§‹æ—¥æœŸ(YYYYMMDD)")
    parser.add_argument("--end-date", default="20251014", help="ç»“æŸæ—¥æœŸ(YYYYMMDD)")
    parser.add_argument("--data-dir", default="raw/ETF/daily", help="ETFæ•°æ®ç›®å½•")
    parser.add_argument(
        "--output-dir", default="factor_output/etf_rotation_production", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--diagnose", action="store_true", help="è¯Šæ–­æ¨¡å¼ï¼šè¾“å‡ºè¯¦ç»†è®¡ç®—ä¿¡æ¯"
    )
    parser.add_argument(
        "--symbols-file", default=None, help="symbolsç™½åå•æ–‡ä»¶ï¼ˆç”¨äºåˆ†æ± ï¼‰"
    )
    parser.add_argument(
        "--symbols", default=None, help="é€—å·åˆ†éš”çš„symbolç™½åå•ï¼ˆä¼˜å…ˆäºsymbols-fileï¼‰"
    )
    parser.add_argument("--pool-name", default=None, help="æ± åç§°ï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰")

    args = parser.parse_args()

    # åˆ›å»ºç”Ÿäº§å™¨
    producer = FullPanelProducer(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        diagnose_mode=args.diagnose,
        symbols_file=args.symbols_file,
        pool_name=args.pool_name,
        symbols=[s.strip() for s in args.symbols.split(",")] if args.symbols else None,
    )

    # è®°å½•è¿è¡Œå‚æ•°
    producer.metadata["run_params"] = {
        "start_date": args.start_date,
        "end_date": args.end_date,
        "data_dir": args.data_dir,
    }

    # æ‰§è¡Œæµç¨‹
    logger.info("=" * 60)
    logger.info("ETFå…¨é‡å› å­é¢æ¿ç”Ÿäº§ï¼ˆOne Passï¼‰")
    logger.info("=" * 60)

    # 1. åŠ è½½æ•°æ®
    data = producer.load_etf_data(args.start_date, args.end_date)

    # 2. è®¡ç®—å…¨é‡å› å­
    panel = producer.calculate_all_factors(data)

    # 3. è¯Šæ–­é¢æ¿
    producer.diagnose_panel(panel)

    # 4. ä¿å­˜ç»“æœ
    date_suffix = f"{args.start_date}_{args.end_date}"
    producer.save_panel(panel, date_suffix)

    logger.info("\n" + "=" * 60)
    logger.info("âœ… å…¨é‡å› å­é¢æ¿ç”Ÿäº§å®Œæˆ")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
